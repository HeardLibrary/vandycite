{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code for finding what properties and values are most common\n",
    "\n",
    "This code allows you to define a set of items based on a screening graph pattern, or by a list of item Q IDs in a file. You can then determine the frequency of use of properties in those items, or the frequency of use of values for a particular property used in those items.\n",
    "\n",
    "(c) 2021 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "\n",
    "Author: Steve Baskauf\n",
    "2021-01-18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code in this block from the notebook\n",
    "# https://github.com/HeardLibrary/digital-scholarship/blob/2cabda778b585e367527f4dd024b6a7e82613e18/code/wikidata/template.ipynb\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import sys # Read CLI arguments\n",
    "import datetime\n",
    "#import os\n",
    "#from time import sleep\n",
    "#from pathlib import Path\n",
    "\n",
    "# Use the following code for a stand-alone script if you want to pass in a value (e.g. file path) when running\n",
    "# the script from the command line. If no arguments are passed, the \"else\" value will be used.\n",
    "\n",
    "if len(sys.argv) == 2: # if exactly one argument passed (i.e. the configuration file path)\n",
    "    file_path = sys.argv[1] # sys.argv[0] is the script name\n",
    "else:\n",
    "    file_path = 'file.csv'\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# File IO\n",
    "# ----------------\n",
    "\n",
    "# Many functions operate on a list of dictionaries, where each item in the list represents a spreadsheet row\n",
    "# and each column is identified by a dictionary item whose key is the column header in the spreadsheet.\n",
    "# The first two functions read and write from files into this data structure.\n",
    "\n",
    "# Read from a CSV file into a list of dictionaries\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# Write a list of dictionaries to a CSV file\n",
    "# The fieldnames object is a list of strings whose items are the keys in the row dictionaries that are chosen\n",
    "# to be the columns in the output spreadsheet. The order in the list determines the order of the columns.\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "# If configuration or other data are stored in a file as JSON, this function loads them into a Python data structure\n",
    "\n",
    "# Load JSON file data into a Python data structure\n",
    "def load_json_into_data_struct(path):\n",
    "    with open(path, 'rt', encoding='utf-8') as file_object:\n",
    "        file_text = file_object.read()\n",
    "    structure = json.loads(file_text)\n",
    "    # uncomment the following line to view the data\n",
    "    # print(json.loads(structure, indent = 2))\n",
    "    return(structure)\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# Code for interacting with a Wikibase query interface (SPARQL endpoint). Typically, it's the Wikidata Query Service\n",
    "# ----------------\n",
    "\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "# Replace this value with your own user agent header string\n",
    "user_agent_header = 'VanderBot/1.6.1 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "\n",
    "# The following code generates a request header dictionary suitable for sending to a SPARQL endpoint.\n",
    "# If the query is SELECT, use the JSON media type above. For CONSTRUCT queryies use text/turtle to get RDF/Turtle\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type,user_agent_header):\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "# The following function requires the request header generated above\n",
    "request_header = generate_header_dictionary(accept_media_type,user_agent_header)\n",
    "# The query is a valid SPARQL query string\n",
    "\n",
    "# Sends a query to the query service endpoint. \n",
    "def send_sparql_query(query_string, request_header):\n",
    "    response = requests.post(endpoint, data=query_string.encode('utf-8'), headers=request_header)\n",
    "    #print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the values from the response JSON\n",
    "    results = data['results']['bindings']\n",
    "    \n",
    "    # You can delete the print statement if the queries are short. However, for large/long queries,\n",
    "    # it's good to let the user know what's going on.\n",
    "    print('done retrieving data')\n",
    "    #print(json.dumps(results, indent=2))\n",
    "    return(results)\n",
    "\n",
    "# ----------------\n",
    "# Utility code\n",
    "# ----------------\n",
    "\n",
    "# Generate the current UTC xsd:date\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "# Extracts the local name part of an IRI, e.g. a qNumber from a Wikidata IRI\n",
    "def extract_local_name(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    last_piece = len(pieces)\n",
    "    return pieces[last_piece - 1]\n",
    "\n",
    "# Extracts the UUID and qId from a statement IRI and returns them as a tuple\n",
    "def extract_statement_uuid(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/statement/Q7552806-8B88E0CA-BCC8-49D5-9AC2-F1755464F1A2\n",
    "    pieces = iri.split('/')\n",
    "    statement_id = pieces[5]\n",
    "    pieces = statement_id.split('-')\n",
    "    # UUID is the first item of the tuple, Q ID is the second item\n",
    "    return pieces[1] + '-' + pieces[2] + '-' + pieces[3] + '-' + pieces[4] + '-' + pieces[5], pieces[0]\n",
    "\n",
    "# To sort a list of dictionaries by a particular dictionary key's values, define the following function\n",
    "# then invoke the sort using the code that follows\n",
    "\n",
    "# function to use in sort\n",
    "def sort_funct(row):\n",
    "    return int(row['count']) # sort by the count key\n",
    "\n",
    "'''\n",
    "output_list.sort(key = sort_funct) # sort by the filename field\n",
    "'''\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This block contains functions that are ideosyncratic to this script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# Functions specific to this script\n",
    "# ----------------\n",
    "\n",
    "# Function to load a column of item Q IDs from a spreadsheet (header row label: qid) and combine their CURIE forms\n",
    "# into a single string with newlines between each one \n",
    "def create_values_list_from_file(path):\n",
    "    data = read_dicts_from_csv(path)\n",
    "    qid_values = ''  # VALUES list for query\n",
    "    for record in data:\n",
    "        qid = record['qid']\n",
    "        qid_values += 'wd:' + qid + '\\n'\n",
    "\n",
    "    # remove trailing newline\n",
    "    qid_values = qid_values[:len(qid_values)-1]\n",
    "    return qid_values\n",
    "\n",
    "# Function to create a list of unabbreviated IRIs into a single string. Each IRI is surrounded by angle brackets\n",
    "# as required for RDF/Turtle syntax i.e. SPARQL syntax, and separated by newlines.\n",
    "def create_id_values_list(list):\n",
    "    qid_values = ''  # VALUES list for query\n",
    "    for record in list:\n",
    "        qid = record['value']\n",
    "        qid_values += '<' + qid + '>\\n'\n",
    "\n",
    "    # remove trailing newline\n",
    "    qid_values = qid_values[:len(qid_values)-1]\n",
    "    return qid_values\n",
    "\n",
    "# This function builds a SPARQL query string to search for entities (values or properties)\n",
    "# for items that are specified by either a query pattern or a list of IRIs passed in as the parameter: screen\n",
    "# Whether the entity is a property or value depends on whether a property is passed in. If a property is \n",
    "# passed in, values of that property are retrieved. If not, the search retrieves all statement properties used by the items.\n",
    "# The query returns entity IDs (not exclusively Q IDs; can also be Q ID or string values) and counts of entities.\n",
    "def build_id_query(property, screen):\n",
    "    query = '''select distinct ?entity (count(distinct ?qid) as ?count) where\n",
    "    {'''\n",
    "    \n",
    "    # Screening of Q IDs done by a graph pattern\n",
    "    if screen[0:4] == '?qid':\n",
    "        query += '\\n    ' + screen\n",
    "        \n",
    "    # Screening of Q IDs done by a list\n",
    "    else:\n",
    "        query += '''\n",
    "    VALUES ?qid\n",
    "        {\n",
    "''' + screen + '''\n",
    "        }'''\n",
    "    \n",
    "    # If a property is passed in, search for the values of that property\n",
    "    if property != '':\n",
    "        query += '''\n",
    "    ?qid wdt:''' + property + ''' ?entity.'''\n",
    "        \n",
    "    # If no property passed in, then see what properties were used\n",
    "    # The wikibase:directClaim triple pattern is necessary to eliminate other kinds of non-statement properties\n",
    "    else:\n",
    "        query += '''\n",
    "    ?qid ?truthy ?value.\n",
    "    ?entity wikibase:directClaim ?truthy.'''\n",
    "        \n",
    "    query += '''\n",
    "    }\n",
    "    group by ?entity'''\n",
    "    \n",
    "    return query\n",
    "\n",
    "# This function builds a SPARQL query to acquire the English labels for a list (separated by newlines) of \n",
    "# unabbreviated item or property IRIs. \n",
    "def build_label_query(screen):\n",
    "    query = '''select distinct ?entity ?label where {\n",
    "    VALUES ?entity\n",
    "        {\n",
    "''' + screen + '''\n",
    "        }'''\n",
    "\n",
    "        # If a property is passed in, the label is linked directly to the item that is the value\n",
    "    if property != '':\n",
    "        query += '''\n",
    "    ?entity rdfs:label ?label.\n",
    "    filter(lang(?label) = 'en')\n",
    "    }'''\n",
    "    \n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining code is the main routine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# Main routine\n",
    "# ----------------\n",
    "\n",
    "# The following file path points to a CSV file with a column having header: qid \n",
    "# and Q ID values in that column.\n",
    "# Set the filepath to empty string to use the screening query to determine the Q IDs instead.\n",
    "#file_path = 'edit_a_thon.csv'\n",
    "file_path = ''\n",
    "\n",
    "# Set the test property to empty string to retrieve all of the properties used by the set of items and the \n",
    "# frequency of their use among items. Set to a particular propert ID to find all of the values for that \n",
    "# property and their frequency of use in those items.\n",
    "#test_property = 'P5008'\n",
    "#test_property = 'P281'\n",
    "test_property = ''\n",
    "\n",
    "screening_query = '?qid wdt:P195 wd:Q18563658.' # items in the Fine Arts gallery\n",
    "\n",
    "if file_path != '':\n",
    "    screen = create_values_list_from_file(file_path)\n",
    "else:\n",
    "    screen = screening_query\n",
    "\n",
    "# Create the query string to retrieve the entity IDs (or value strings) that meet the screening criteria\n",
    "query_string = build_id_query(test_property, screen)\n",
    "# print(query_string)\n",
    "\n",
    "# You can delete the print statements if the queries are short. However, for large/long queries,\n",
    "# it's good to let the user know what's going on.\n",
    "print('querying SPARQL endpoint to acquire entity counts')\n",
    "\n",
    "# Retrieve the list of entities (properties or values) meeting the screening criteria\n",
    "results = send_sparql_query(query_string, request_header)\n",
    "#print(json.dumps(results, indent=2))\n",
    "\n",
    "# Extract IRIs or string values and their counts from the results\n",
    "# If the entity values are IRIs, do a second step to get their labels. Otherwise the values are strings.\n",
    "interim_results = []\n",
    "all_iris = True\n",
    "for result in results:\n",
    "    value = result['entity']['value']\n",
    "    if value[0:4] != 'http':  # detect non-IRI strings\n",
    "        all_iris = False\n",
    "    count = result['count']['value']\n",
    "    interim_results.append({'value': value, 'count': count})\n",
    "\n",
    "if all_iris:\n",
    "    # Create a query string to get the labels for IRIs of properties or item values.\n",
    "    values = create_id_values_list(interim_results)\n",
    "    query_string = build_label_query(values)\n",
    "    # print(query_string)\n",
    "\n",
    "    print('querying SPARQL endpoint to acquire labels')\n",
    "    results = send_sparql_query(query_string, request_header)\n",
    "    #print(json.dumps(results, indent=2))\n",
    "\n",
    "    # Extract labels from the results and match them to their IDs and counts.\n",
    "    output_list = []\n",
    "    for interim_result in interim_results:\n",
    "        for result in results:\n",
    "            final_result = {}\n",
    "            if result['entity']['value'] == interim_result['value']:\n",
    "                final_result['value'] = extract_local_name(interim_result['value'])\n",
    "                final_result['label'] = result['label']['value']\n",
    "                final_result['count'] = extract_local_name(interim_result['count'])\n",
    "                break\n",
    "        output_list.append(final_result)\n",
    "else:\n",
    "    output_list = list(interim_results)\n",
    "\n",
    "# Order the results be descending counts\n",
    "output_list.sort(key = sort_funct, reverse=True)\n",
    "#print(json.dumps(output_list, indent=2))\n",
    "\n",
    "# Output the results to a spreadsheet\n",
    "if test_property != '':\n",
    "    filename = test_property + '_summary.csv'\n",
    "else:\n",
    "    filename = 'properties_summary.csv'\n",
    "    \n",
    "if all_iris:\n",
    "    fieldnames = ['value', 'label', 'count']\n",
    "else:\n",
    "    fieldnames = ['value', 'count']\n",
    "write_dicts_to_csv(output_list, filename, fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop\n",
    "\n",
    "The following block constructs a complex query to get the IDs, labels, and counts in a single query. But for whatever reason, it takes ridiculously long to run and usually times out. So I switched to doing the queries separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_full_query(property, screen):\n",
    "    query = '''select distinct ?entity ?label ?count where {'''\n",
    "    \n",
    "    # If a property is passed in, the label is linked directly to the item that is the value\n",
    "    if property != '':\n",
    "        query += '''\n",
    "    ?entity rdfs:label ?label.'''\n",
    "        \n",
    "    # If no property is passed in, the label is linked to the generic property associated with the resulting property\n",
    "    else:\n",
    "        query += '''\n",
    "    ?genProp wikibase:directClaim ?entity.\n",
    "    ?genProp rdfs:label ?label.'''\n",
    "\n",
    "    query += '''\n",
    "    filter(lang(?label) = 'en')\n",
    "    {\n",
    "    select distinct ?entity (count(distinct ?qid) as ?count) where\n",
    "        {'''\n",
    "    \n",
    "    # Screening of Q IDs done by a graph pattern\n",
    "    if screen[0:4] == '?qid':\n",
    "        query += '\\n        ' + screen\n",
    "        \n",
    "    # Screening of Q IDs done by a list\n",
    "    else:\n",
    "        query += '''\n",
    "        VALUES ?qid\n",
    "        {\n",
    "''' + screen + '''\n",
    "        }'''\n",
    "    \n",
    "    # If a property is passed in, search for the values of that property\n",
    "    if property != '':\n",
    "        query += '''\n",
    "        ?qid wdt:''' + property + ''' ?entity.'''\n",
    "        \n",
    "    # If no property passed in, then see what properties were used\n",
    "    else:\n",
    "        query += '''\n",
    "        ?qid ?entity ?value.'''\n",
    "        \n",
    "    query += '''\n",
    "        }\n",
    "        group by ?entity\n",
    "    }\n",
    "}\n",
    "order by desc(?count)'''\n",
    "    \n",
    "    return query\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
