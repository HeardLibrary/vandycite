{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code in this block from the template.ipynb notebook\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import sys # Read CLI arguments\n",
    "import datetime\n",
    "#import os\n",
    "#from time import sleep\n",
    "#from pathlib import Path\n",
    "\n",
    "# Use the following code for a stand-alone script if you want to pass in a value (e.g. file path) when running\n",
    "# the script from the command line. If no arguments are passed, the \"else\" value will be used.\n",
    "\n",
    "if len(sys.argv) == 2: # if exactly one argument passed (i.e. the configuration file path)\n",
    "    file_path = sys.argv[1] # sys.argv[0] is the script name\n",
    "else:\n",
    "    file_path = 'file.csv'\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# File IO\n",
    "# ----------------\n",
    "\n",
    "# Many functions operate on a list of dictionaries, where each item in the list represents a spreadsheet row\n",
    "# and each column is identified by a dictionary item whose key is the column header in the spreadsheet.\n",
    "# The first two functions read and write from files into this data structure.\n",
    "\n",
    "# Read from a CSV file into a list of dictionaries\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# Write a list of dictionaries to a CSV file\n",
    "# The fieldnames object is a list of strings whose items are the keys in the row dictionaries that are chosen\n",
    "# to be the columns in the output spreadsheet. The order in the list determines the order of the columns.\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "# If configuration or other data are stored in a file as JSON, this function loads them into a Python data structure\n",
    "\n",
    "# Load JSON file data into a Python data structure\n",
    "def load_json_into_data_struct(path):\n",
    "    with open(path, 'rt', encoding='utf-8') as file_object:\n",
    "        file_text = file_object.read()\n",
    "    structure = json.loads(file_text)\n",
    "    # uncomment the following line to view the data\n",
    "    # print(json.loads(structure, indent = 2))\n",
    "    return(structure)\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# Code for interacting with a Wikibase query interface (SPARQL endpoint). Typically, it's the Wikidata Query Service\n",
    "# ----------------\n",
    "\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "# Replace this value with your own user agent header string\n",
    "user_agent_header = 'VanderBot/1.6.1 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "\n",
    "# The following code generates a request header dictionary suitable for sending to a SPARQL endpoint.\n",
    "# If the query is SELECT, use the JSON media type above. For CONSTRUCT queryies use text/turtle to get RDF/Turtle\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type,user_agent_header):\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "# The following function requires the request header generated above\n",
    "request_header = generate_header_dictionary(accept_media_type,user_agent_header)\n",
    "# The query is a valid SPARQL query string\n",
    "\n",
    "# Sends a query to the query service endpoint. \n",
    "def send_sparql_query(query_string, request_header):\n",
    "    # You can delete the two print statements if the queries are short. However, for large/long queries,\n",
    "    # it's good to let the user know what's going on.\n",
    "    print('querying SPARQL endpoint to acquire item metadata')\n",
    "    response = requests.post(endpoint, data=query_string.encode('utf-8'), headers=request_header)\n",
    "    #print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the values from the response JSON\n",
    "    results = data['results']['bindings']\n",
    "    \n",
    "    print('done retrieving data')\n",
    "    #print(json.dumps(results, indent=2))\n",
    "    return(results)\n",
    "\n",
    "# ----------------\n",
    "# Utility code\n",
    "# ----------------\n",
    "\n",
    "# Generate the current UTC xsd:date\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "# Extracts the local name part of an IRI, e.g. a qNumber from a Wikidata IRI\n",
    "def extract_local_name(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    last_piece = len(pieces)\n",
    "    return pieces[last_piece - 1]\n",
    "\n",
    "# Extracts the UUID and qId from a statement IRI and returns them as a tuple\n",
    "def extract_statement_uuid(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/statement/Q7552806-8B88E0CA-BCC8-49D5-9AC2-F1755464F1A2\n",
    "    pieces = iri.split('/')\n",
    "    statement_id = pieces[5]\n",
    "    pieces = statement_id.split('-')\n",
    "    # UUID is the first item of the tuple, Q ID is the second item\n",
    "    return pieces[1] + '-' + pieces[2] + '-' + pieces[3] + '-' + pieces[4] + '-' + pieces[5], pieces[0]\n",
    "\n",
    "# To sort a list of dictionaries by a particular dictionary key's values, define the following function\n",
    "# then invoke the sort using the code that follows\n",
    "\n",
    "# function to use in sort\n",
    "def sort_funct(row):\n",
    "    return int(row['count']) # sort by the count key\n",
    "\n",
    "'''\n",
    "output_list.sort(key = sort_funct) # sort by the filename field\n",
    "'''\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_values_list_from_file(path):\n",
    "    data = read_dicts_from_csv(path)\n",
    "    qid_values = ''  # VALUES list for query\n",
    "    for record in data:\n",
    "        qid = record['qid']\n",
    "        qid_values += 'wd:' + qid + '\\n'\n",
    "\n",
    "    # remove trailing newline\n",
    "    qid_values = qid_values[:len(qid_values)-1]\n",
    "    return qid_values\n",
    "\n",
    "def create_id_values_list(list):\n",
    "    qid_values = ''  # VALUES list for query\n",
    "    for record in list:\n",
    "        qid = record['iri']\n",
    "        qid_values += '<' + qid + '>\\n'\n",
    "\n",
    "    # remove trailing newline\n",
    "    qid_values = qid_values[:len(qid_values)-1]\n",
    "    return qid_values\n",
    "\n",
    "def build_full_query(property, screen):\n",
    "    query = '''select distinct ?entity ?label ?count where {'''\n",
    "    \n",
    "    # If a property is passed in, the label is linked directly to the item that is the value\n",
    "    if property != '':\n",
    "        query += '''\n",
    "    ?entity rdfs:label ?label.'''\n",
    "        \n",
    "    # If no property is passed in, the label is linked to the generic property associated with the resulting property\n",
    "    else:\n",
    "        query += '''\n",
    "    ?genProp wikibase:directClaim ?entity.\n",
    "    ?genProp rdfs:label ?label.'''\n",
    "\n",
    "    query += '''\n",
    "    filter(lang(?label) = 'en')\n",
    "    {\n",
    "    select distinct ?entity (count(distinct ?qid) as ?count) where\n",
    "        {'''\n",
    "    \n",
    "    # Screening of Q IDs done by a graph pattern\n",
    "    if screen[0:4] == '?qid':\n",
    "        query += '\\n        ' + screen\n",
    "        \n",
    "    # Screening of Q IDs done by a list\n",
    "    else:\n",
    "        query += '''\n",
    "        VALUES ?qid\n",
    "        {\n",
    "''' + screen + '''\n",
    "        }'''\n",
    "    \n",
    "    # If a property is passed in, search for the values of that property\n",
    "    if property != '':\n",
    "        query += '''\n",
    "        ?qid wdt:''' + property + ''' ?entity.'''\n",
    "        \n",
    "    # If no property passed in, then see what properties were used\n",
    "    else:\n",
    "        query += '''\n",
    "        ?qid ?entity ?value.'''\n",
    "        \n",
    "    query += '''\n",
    "        }\n",
    "        group by ?entity\n",
    "    }\n",
    "}\n",
    "order by desc(?count)'''\n",
    "    \n",
    "    return query\n",
    "\n",
    "def build_id_query(property, screen):\n",
    "    query = '''select distinct ?entity (count(distinct ?qid) as ?count) where\n",
    "    {'''\n",
    "    \n",
    "    # Screening of Q IDs done by a graph pattern\n",
    "    if screen[0:4] == '?qid':\n",
    "        query += '\\n    ' + screen\n",
    "        \n",
    "    # Screening of Q IDs done by a list\n",
    "    else:\n",
    "        query += '''\n",
    "    VALUES ?qid\n",
    "        {\n",
    "''' + screen + '''\n",
    "        }'''\n",
    "    \n",
    "    # If a property is passed in, search for the values of that property\n",
    "    if property != '':\n",
    "        query += '''\n",
    "    ?qid wdt:''' + property + ''' ?entity.'''\n",
    "        \n",
    "    # If no property passed in, then see what properties were used\n",
    "    else:\n",
    "        query += '''\n",
    "    ?qid ?truthy ?value.\n",
    "    ?entity wikibase:directClaim ?truthy.'''\n",
    "        \n",
    "    query += '''\n",
    "    }\n",
    "    group by ?entity'''\n",
    "    \n",
    "    return query\n",
    "\n",
    "def build_label_query(screen):\n",
    "    query = '''select distinct ?entity ?label where {\n",
    "    VALUES ?entity\n",
    "        {\n",
    "''' + screen + '''\n",
    "        }'''\n",
    "\n",
    "        # If a property is passed in, the label is linked directly to the item that is the value\n",
    "    if property != '':\n",
    "        query += '''\n",
    "    ?entity rdfs:label ?label.\n",
    "    filter(lang(?label) = 'en')\n",
    "    }'''\n",
    "    \n",
    "    return query\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#query_string = build_id_query('', '?qid wdt:P195 wd:Q18563658.') # items in the Fine Arts gallery\n",
    "\n",
    "filename = 'edit_a_thon.csv'\n",
    "values = create_values_list_from_file(filename)\n",
    "query_string = build_id_query('P5008', values)\n",
    "print(query_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = send_sparql_query(query_string, request_header)\n",
    "#print(json.dumps(results, indent=2))\n",
    "\n",
    "interim_results = []\n",
    "for result in results:\n",
    "    iri = result['entity']['value']\n",
    "    count = result['count']['value']\n",
    "    interim_results.append({'iri': iri, 'count': count})\n",
    "    \n",
    "values = create_id_values_list(interim_results)\n",
    "query_string = build_label_query(values)\n",
    "print(query_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = send_sparql_query(query_string, request_header)\n",
    "#print(json.dumps(results, indent=2))\n",
    "\n",
    "output_list = []\n",
    "for interim_result in interim_results:\n",
    "    for result in results:\n",
    "        final_result = {}\n",
    "        if result['entity']['value'] == interim_result['iri']:\n",
    "            final_result['id'] = extract_local_name(interim_result['iri'])\n",
    "            final_result['label'] = result['label']['value']\n",
    "            final_result['count'] = extract_local_name(interim_result['count'])\n",
    "            break\n",
    "    output_list.append(final_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list.sort(key = sort_funct, reverse=True)\n",
    "print(json.dumps(output_list, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'summary.csv'\n",
    "fieldnames = ['id', 'label', 'count']\n",
    "write_dicts_to_csv(output_list, filename, fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
