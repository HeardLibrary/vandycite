{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only once if you need to install the python-Levenshtein package\n",
    "\n",
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve data using DOI\n",
    "\n",
    "Currently the script only uses the CrossRef API, but potentially could use others like DataCite\n",
    "\n",
    "## Configuration\n",
    "\n",
    "This section imports libraries, sets default values, and defines functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2020 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "import requests   # best library to manage HTTP transactions\n",
    "#from bs4 import BeautifulSoup # web-scraping library\n",
    "import json\n",
    "import re\n",
    "from time import sleep\n",
    "import csv\n",
    "import sys\n",
    "#import math\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "#from fuzzywuzzy import process\n",
    "#import xml.etree.ElementTree as et # library to traverse XML tree\n",
    "import urllib\n",
    "import datetime\n",
    "#import string\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "accept_media_type = 'application/json'\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "user_agent_header = 'VanderBot/1.7.1 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "sparql_sleep = 0.1\n",
    "default_language = 'en'\n",
    "\n",
    "# Option to log to a file instead of the console\n",
    "log_path = '' # path to log file, default to none\n",
    "log_object = sys.stdout # log output defaults to the console screen\n",
    "\n",
    "opts = [opt for opt in sys.argv[1:] if opt.startswith('-')]\n",
    "args = [arg for arg in sys.argv[1:] if not arg.startswith('-')]\n",
    "\n",
    "if '--log' in opts: # set output to specified log file or path including file name\n",
    "    log_path = args[opts.index('--log')]\n",
    "    log_object = open(log_path, 'wt', encoding='utf-8') # direct output sent to log_object to log file instead of sys.stdout\n",
    "if '-L' in opts: # set output to specified log file or path including file name\n",
    "    log_path = args[opts.index('-L')]\n",
    "    log_object = open(log_path, 'wt', encoding='utf-8') # direct output sent to log_object to log file instead of sys.stdout\n",
    "\n",
    "# --------------------------\n",
    "# *** For now, hard-code logging to errors.log\n",
    "log_path = 'errors.log'\n",
    "log_object = open(log_path, 'wt', encoding='utf-8')\n",
    "# --------------------------\n",
    "\n",
    "# List of work types used by CrossRef\n",
    "work_types = [\n",
    "    {\n",
    "    'crossref_type_string': 'journal-article',\n",
    "    'qid': 'Q18918145', # academic journal article, alternatively Q13442814 scholarly article\n",
    "    'description': 'journal article'\n",
    "    },\n",
    "    {\n",
    "    'crossref_type_string': 'book-chapter',\n",
    "    'qid': 'Q21481766', # academic journal article, alternatively Q13442814 scholarly article\n",
    "    'description': 'academic book chapter'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Output CSV label/description fields to be populated without references\n",
    "out_fields_labdes = ['label_' + default_language, 'description_' + default_language]\n",
    "\n",
    "# Output CSV property fields to be populated without references\n",
    "out_fields_noref = ['instance_of']\n",
    "\n",
    "# Output CSV fields that include reference fields\n",
    "out_fields_ref = ['doi', 'published', 'title_' + default_language, 'journal', 'volume', 'page', 'issue']\n",
    "    \n",
    "# generates a dictionary to be passed in a requests GET method to generate the request header\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "# Determine the current CrossRef rate limit by an initial ping\n",
    "response = requests.get('https://api.crossref.org/works/10.3233/SW-150203', headers=generate_header_dictionary(accept_media_type))\n",
    "crossref_headers = response.headers\n",
    "limit_count = int(crossref_headers['x-rate-limit-limit'])\n",
    "interval_string = crossref_headers['x-rate-limit-interval']\n",
    "interval_sec = int(interval_string[:len(interval_string)-1]) # remove the \"s\" from the end\n",
    "api_sleep = interval_sec / limit_count + 0.005\n",
    "\n",
    "# Due to problems with direct POST of UTF-8, changed to POST with URL-encoded parameters\n",
    "# See https://www.w3.org/TR/sparql11-protocol/#update-via-post-urlencoded\n",
    "# and https://stackoverflow.com/questions/34618149/post-unicode-string-to-web-service-using-python-requests-library\n",
    "def generate_sparql_header_dictionary(accept_media_type,user_agent_header):\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "#        'Content-Type': 'application/sparql-query',\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "# The following function requires the request header generated above\n",
    "sparql_request_header = generate_sparql_header_dictionary(accept_media_type, user_agent_header)\n",
    "\n",
    "# Generate the current UTC xsd:date\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "# Extracts the local name part of an IRI, e.g. a qNumber from a Wikidata IRI\n",
    "def extract_local_name(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    last_piece = len(pieces)\n",
    "    return pieces[last_piece - 1]\n",
    "\n",
    "# Read from a CSV file into a list of dictionaries\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# Write list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Load JSON file data from local drive into a Python data structure\n",
    "def load_json_into_data_struct(path):\n",
    "    with open(path, 'rt', encoding='utf-8') as file_object:\n",
    "        file_text = file_object.read()\n",
    "    structure = json.loads(file_text)\n",
    "    # uncomment the following line to view the data\n",
    "    # print(json.loads(structure, indent = 2))\n",
    "    return(structure)\n",
    "\n",
    "# Sends a query to the query service endpoint. \n",
    "# NOTE: request_header and endpoint are global variables defined earlier in the script\n",
    "def send_sparql_query(query_string):\n",
    "    # You can delete the two print statements if the queries are short. However, for large/long queries,\n",
    "    # it's good to let the user know what's going on.\n",
    "    #print('querying SPARQL endpoint to acquire item metadata')\n",
    "    #response = requests.post(endpoint, data=query_string.encode('utf-8'), headers=sparql_request_header)\n",
    "    response = requests.post(endpoint, data=dict(query=query_string), headers=sparql_request_header)\n",
    "    #print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the values from the response JSON\n",
    "    results = data['results']['bindings']\n",
    "    \n",
    "    #print('done retrieving data')\n",
    "    # print(json.dumps(results, indent=2))\n",
    "    \n",
    "    sleep(sparql_sleep) # delay to avoid hitting the Query Service too fast\n",
    "    return results\n",
    "\n",
    "# Use the CrossRef registration agency test to find out who issued the non-CrosRef DOI\n",
    "def discover_issuing_agency(doi):\n",
    "    encoded_doi = urllib.parse.quote(doi)\n",
    "    search_url = 'https://api.crossref.org/works/' + encoded_doi + '/agency'\n",
    "    response = requests.get(search_url, headers=generate_header_dictionary(accept_media_type))\n",
    "    sleep(api_sleep) # delay to avoid hitting the API faster than acceptable rate\n",
    "    if response.status_code == 404: # return \"not found\" if URL doesn't dereference\n",
    "        return 'not found'\n",
    "    try: # Try to parse as JSON\n",
    "        data = response.json()\n",
    "        return data['message']['agency']['id']\n",
    "    except: # if the response isn't JSON, then just return the response text\n",
    "        return response.text\n",
    "\n",
    "# See https://github.com/CrossRef/rest-api-doc for API details\n",
    "# Note: no authentication required, but must be \"nice\": observe rate limit, provide mailto:\n",
    "def retrieve_crossref_data(doi):\n",
    "    crossref_endpoint_url = 'https://api.crossref.org/works/'\n",
    "    # urllib.parse.quote performs URL encoding of a string\n",
    "    encoded_doi = urllib.parse.quote(doi)\n",
    "    search_url = crossref_endpoint_url + encoded_doi\n",
    "    response = requests.get(search_url, headers=generate_header_dictionary(accept_media_type))\n",
    "    article_dict = {}\n",
    "    if response.status_code == 404: # return empty dict if not found\n",
    "        # *** NOTE: at some future time, look up data at alternative issuing agencies\n",
    "        # For example, see https://support.datacite.org/docs/api-get-doi\n",
    "        \n",
    "        # For now, just log it\n",
    "        print('article:', article['qid'], 'DOI issuing agency:', discover_issuing_agency(article['doi']),'\\n', file=log_object)\n",
    "        sleep(api_sleep)\n",
    "        return article_dict\n",
    "    else:\n",
    "        author_list = []\n",
    "        try:\n",
    "            response_structure = response.json()\n",
    "            data = response_structure['message']\n",
    "        except:\n",
    "            # if not JSON, just return the response text\n",
    "            article_dict['message'] = response.text\n",
    "            sleep(api_sleep)\n",
    "            return article_dict\n",
    "        #print(json.dumps(data, indent = 2))\n",
    "        article_dict['doi'] = doi\n",
    "        if 'author' in data:\n",
    "            authors = data['author']\n",
    "            for author in authors:\n",
    "                authorDict = {}\n",
    "                if 'ORCID' in author:\n",
    "                    authorDict['orcid'] = author['ORCID']\n",
    "                else:\n",
    "                    authorDict['orcid'] = ''\n",
    "                if 'sequence' in author:\n",
    "                    authorDict['sequence'] = author['sequence']\n",
    "                else:\n",
    "                    authorDict['sequence'] = ''\n",
    "                if 'given' in author:\n",
    "                    authorDict['givenName'] = author['given']\n",
    "                else:\n",
    "                    authorDict['givenName'] = ''\n",
    "                if 'family' in author:\n",
    "                    authorDict['familyName'] = author['family']\n",
    "                else:\n",
    "                    authorDict['familyName'] = ''\n",
    "                affiliationList = []\n",
    "                if 'affiliation' in author:\n",
    "                    for affiliation in author['affiliation']:\n",
    "                        affiliationList.append(affiliation['name'])\n",
    "                # if there aren't any affiliations, the list will remain empty\n",
    "                authorDict['affiliation'] = affiliationList\n",
    "                author_list.append(authorDict)\n",
    "            article_dict['authors'] = author_list\n",
    "        if 'issued' in data:\n",
    "            issued = data['issued']['date-parts'][0]\n",
    "            issued_date = str(issued[0])\n",
    "            if len(issued) > 1:\n",
    "                if len(str(issued[1])) == 1:\n",
    "                    issued_date += '-0'+ str(issued[1])\n",
    "                else:\n",
    "                    issued_date += '-'+ str(issued[1])\n",
    "                if len(issued) > 2:                \n",
    "                    if len(str(issued[2])) == 1:\n",
    "                        issued_date += '-0'+ str(issued[2])\n",
    "                    else:\n",
    "                        issued_date += '-'+ str(issued[2])\n",
    "            article_dict['published'] = issued_date\n",
    "        else:\n",
    "            article_dict['published'] = ''\n",
    "        if 'volume' in data:\n",
    "            article_dict['volume'] = data['volume']\n",
    "        else:\n",
    "            article_dict['volume'] = ''\n",
    "        if 'issue' in data:\n",
    "            article_dict['issue'] = data['issue']\n",
    "        else:\n",
    "            article_dict['issue'] = ''\n",
    "        if 'page' in data:\n",
    "            article_dict['page'] = data['page']\n",
    "        else:\n",
    "            article_dict['page'] = ''\n",
    "        if 'ISSN' in data:\n",
    "            article_dict['journal_issn'] = data['ISSN']\n",
    "        else:\n",
    "            article_dict['journal_issn'] = []\n",
    "        if 'title' in data:\n",
    "            if len(data['title']) > 0:\n",
    "                article_dict['title_' + default_language] = data['title'][0]\n",
    "                article_dict['label_' + default_language] = data['title'][0]\n",
    "        else:\n",
    "            article_dict['title_' + default_language] = ''\n",
    "            article_dict['label_' + default_language] = ''\n",
    "        if 'container-title' in data:\n",
    "            if len(data['container-title']) > 0:\n",
    "                article_dict['journal_title'] = data['container-title'][0]\n",
    "        else:\n",
    "            article_dict['journal_title'] = ''\n",
    "         \n",
    "        if 'type' in data:\n",
    "            found = False\n",
    "            for work_type in work_types:\n",
    "                if data['type'] == work_type['crossref_type_string']:\n",
    "                    found = True\n",
    "                    article_dict['instance_of'] = work_type['qid']\n",
    "                    article_dict['description_' + default_language] = work_type['description']\n",
    "            if not found:\n",
    "                article_dict['instance_of'] = ''\n",
    "                article_dict['description_' + default_language] = ''\n",
    "        else:\n",
    "            article_dict['instance_of'] = ''\n",
    "            article_dict['description_' + default_language] = ''\n",
    "\n",
    "    sleep(api_sleep)\n",
    "    return article_dict\n",
    "\n",
    "# Look up the ISSN from CrossRef in Wikidata\n",
    "def extract_journal_qid(crossref_results, article):\n",
    "    if len(crossref_results['journal_issn']) == 0:\n",
    "        crossref_results['journal'] = ''\n",
    "        print('article:', article['qid'], 'has no ISSN.\\n', file=log_object)\n",
    "        return crossref_results\n",
    "\n",
    "    # Create VALUES list for items\n",
    "    issns_string = ''\n",
    "    for issn in crossref_results['journal_issn']:\n",
    "        issns_string += '\"' + issn + '\"\\n'\n",
    "    # Remove trailing newline\n",
    "    issns_string = issns_string[:len(issns_string)-1]\n",
    "\n",
    "    # Build query string\n",
    "    query_string = '''select distinct ?journal ?journalLabel where {\n",
    "      VALUES ?issn\n",
    "        {\n",
    "    ''' + issns_string + '''\n",
    "        }\n",
    "      ?journal wdt:P236 ?issn.\n",
    "      ?journal rdfs:label ?journalLabel.\n",
    "      filter(lang(?journalLabel)=\"en\")\n",
    "    }'''\n",
    "    #print(query_string)\n",
    "\n",
    "    # Send query to endpoint\n",
    "    query_results = send_sparql_query(query_string)\n",
    "    #pp.pprint(query_results)\n",
    "\n",
    "    if len(query_results) > 1:\n",
    "        print('Warning! More than one publication in Wikidata matched the ISSN for article', article['qid'], file=log_object)\n",
    "        print(query_results, '\\n', file=log_object)\n",
    "\n",
    "    # Extract Q ID from SPARQL query results. If there is more than one result, the last one will be used for the Q ID\n",
    "    for result in query_results:\n",
    "        journal_qid = extract_local_name(result['journal']['value'])\n",
    "        journal_name = result['journalLabel']['value']\n",
    "        if journal_name != crossref_results['journal_title']:\n",
    "            # NOTE: did empirical testing to see which kind of fuzzy matching worked best\n",
    "            #ratio = fuzz.ratio(journal_name, crossref_results['journal_title'])\n",
    "            #partial_ratio = fuzz.partial_ratio(journal_name, crossref_results['journal_title'])\n",
    "            #sort_ratio = fuzz.token_sort_ratio(journal_name, crossref_results['journal_title'])\n",
    "            #set_ratio = fuzz.token_set_ratio(journal_name, crossref_results['journal_title'])\n",
    "            w_ratio = fuzz.WRatio(journal_name, crossref_results['journal_title'])\n",
    "            #print('name similarity ratio', ratio)\n",
    "            #print('partial ratio', partial_ratio)\n",
    "            #print('sort_ratio', sort_ratio)\n",
    "            #print('set_ratio', set_ratio)\n",
    "            if w_ratio < 99:\n",
    "                print('article:', article['qid'], 'w_ratio:', w_ratio, 'Warning: Wikidata journal: \"' + journal_name + '\"', journal_qid, 'does not match CrossRef journal title: \"' + crossref_results['journal_title'] + '\"\\n', file=log_object)\n",
    "        #print('article:', article['qid'], 'journal:', journal_qid, journal_name)\n",
    "    crossref_results['journal'] = journal_qid\n",
    "    return crossref_results\n",
    "\n",
    "def generate_name_alternatives(name):\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods, sometimes periods are close up with no spaces\n",
    "    name = name.replace('.', ' ')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "\n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        # make sure first character is alphabetic\n",
    "        # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "        # typical cases are like (Kit) or \"Kit\"\n",
    "        if not piece[0:1].isalpha():\n",
    "            piece = piece[1:len(piece)] # remove the first non-alphabetic character\n",
    "        if len(piece) > 0:\n",
    "            initials.append(piece[0:1])\n",
    "        \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += pieces[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # full name with suffix\n",
    "    if suffix != '':\n",
    "        name_version = ''\n",
    "        for piece_number in range(0, len(pieces)-1):\n",
    "            name_version += pieces[piece_number] + ' '\n",
    "        name_version += pieces[len(pieces)-1] + suffix\n",
    "        alternatives.append(name_version)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    name_version = pieces[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    name_version = pieces[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first and last name only\n",
    "    name_version = pieces[0] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial and last name only\n",
    "    name_version = initials[0] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    name_version = initials[0] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials with last name\n",
    "    name_version = initials[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += initials[piece_number]\n",
    "    name_version += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def search_name_at_wikidata(name):\n",
    "    # carry out search for most languages that use Latin characters, plus some other commonly used languages\n",
    "    # See https://doi.org/10.1145/3233391.3233965\n",
    "    language_codes = [\n",
    "        'en',\n",
    "        'es',\n",
    "        'pt',\n",
    "        'fr',\n",
    "        'it',\n",
    "        'nl',\n",
    "        'de',\n",
    "        'da',\n",
    "        'et',\n",
    "        'hu',\n",
    "        'ga',\n",
    "        'ro',\n",
    "        'sk',\n",
    "        'sl',\n",
    "        'zu',\n",
    "        'tr',\n",
    "        'sv',\n",
    "        'zh',\n",
    "        'ru',\n",
    "        'ja',\n",
    "        'ar',\n",
    "        'pl',\n",
    "        'uk',\n",
    "        'ca',\n",
    "        'cs',\n",
    "        'la',\n",
    "        'nb',\n",
    "        'hu',\n",
    "        'he',\n",
    "        'eo',\n",
    "        'fi',\n",
    "        'ko'\n",
    "      ]\n",
    "    name_list = generate_name_alternatives(name)\n",
    "    alternatives = ''\n",
    "    for language_code in language_codes:\n",
    "        for alternative in name_list:\n",
    "            # get rid of quotes, which will break the query\n",
    "            alternative = alternative.replace('\"', '')\n",
    "            alternative = alternative.replace(\"'\", '')\n",
    "            alternatives += '\"' + alternative + '\"@' + language_code + '\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(dict(data=query))\n",
    "    #print('searching for ', name)\n",
    "    results = []\n",
    "#    r = requests.post(endpoint, data=query.encode('utf-8'), headers=sparql_request_header)\n",
    "    r = requests.post(endpoint, data=dict(query=query), headers=sparql_request_header)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidata_iri = statement['item']['value']\n",
    "            if 'label' in statement:\n",
    "                name = statement['label']['value']\n",
    "            else:\n",
    "                name = ''\n",
    "            qnumber = extract_local_name(wikidata_iri)\n",
    "            results.append({'qid': qnumber, 'name': name})\n",
    "    except:\n",
    "        results = [{'error': r.text}]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return results\n",
    "\n",
    "def find_surname_givens(name):\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', ' ')\n",
    "    name = name.replace(',', ' ')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Get rid of empty pieces formed from extra spaces\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "    \n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "def screen_qids(qids, screens):\n",
    "    qid_values =''\n",
    "    for qid in qids:\n",
    "        qid_values += 'wd:' + qid + '\\n'\n",
    "\n",
    "    graph_pattern = ''\n",
    "    first_screen = True\n",
    "    for screen in screens:\n",
    "        # Each requirement in a screen has an AND relationship (all must be satisfied)\n",
    "        subgraph_pattern = ''\n",
    "        for requirement in screen:\n",
    "\n",
    "            # Set the value if required or use a dummy variable if any value is allowed\n",
    "            if requirement['entity'] == '':\n",
    "                value = '?var' + requirement['property'] # add the property string to the variable to guarantee uniqueness\n",
    "            elif re.match(r'Q\\d+', requirement['entity']): # regex to match Q IDs\n",
    "                value = 'wd:' + requirement['entity']\n",
    "            else: # if not nothing or a Q ID, assume it's a string literal\n",
    "                if requirement['lang'] == '':\n",
    "                    value = '\"' + requirement['entity'] + '\"'\n",
    "                else:\n",
    "                    value = '\"' + requirement['entity'] + '\"@' + requirement['lang']\n",
    "\n",
    "            # Set the property (label, description, or P value)\n",
    "            if requirement['property'] == 'label':\n",
    "                property = 'rdfs:label'\n",
    "            elif requirement['property'] == 'description':\n",
    "                property = 'schema:description'\n",
    "            else:\n",
    "                property = 'wdt:' + requirement['property']\n",
    "\n",
    "            # Place the value in either the subject or object position in the triple\n",
    "            if requirement['position'] == 'object':\n",
    "                triple_pattern = '?qid ' + property + ' ' + value + '.'\n",
    "            else:\n",
    "                triple_pattern = value + ' ' + property + ' ?qid.'\n",
    "\n",
    "            # Add filters if needed\n",
    "            if requirement['filter_type'] == '<' or requirement['filter_type'] == '>': \n",
    "                # note: string comparison only e.g. for datetimes, needs modification for actual numbers\n",
    "                triple_pattern += '\\nFILTER (STR(?var' + requirement['property'] + ') ' + requirement['filter_type'] + ' \"' + requirement['filter_string'] + '\")'\n",
    "\n",
    "            if requirement['filter_type'] == 'in': \n",
    "                # note: string comparison only\n",
    "                triple_pattern += '\\nFILTER (CONTAINS(?var' + requirement['property'] + ', \"' + requirement['filter_string'] + '\"))'\n",
    "\n",
    "            # Use MINUS if you want to exclude items that fit the pattern.\n",
    "            if requirement['require'] == 'exclude':\n",
    "                triple_pattern = 'minus {' + triple_pattern + '}'\n",
    "\n",
    "            triple_pattern += '\\n'\n",
    "            #print(triple_pattern)\n",
    "            subgraph_pattern += triple_pattern\n",
    "\n",
    "        # Now attach the subgraph pattern to any previous subgraph patterns using UNION to great an OR relationship\n",
    "        subgraph_pattern = '{\\n' + subgraph_pattern + '}\\n' # create a subgraph pattern so that several can be UNIONed\n",
    "        if first_screen: # The first subgraph pattern doesn't need the UNION inserted\n",
    "            first_screen = False\n",
    "        else:\n",
    "            graph_pattern = graph_pattern + 'UNION\\n'\n",
    "        graph_pattern += subgraph_pattern \n",
    "\n",
    "    query_string = '''\n",
    "    select distinct ?qid ?label ?description where {\n",
    "      VALUES ?qid\n",
    "      {\n",
    "      ''' + qid_values + '''}\n",
    "    ''' + graph_pattern + '''\n",
    "    \n",
    "    ?qid rdfs:label ?label.\n",
    "    FILTER(lang(?label)=\"'''+ default_language + '''\")\n",
    "    \n",
    "    OPTIONAL {\n",
    "    ?qid schema:description ?description.\n",
    "    FILTER(lang(?description)=\"'''+ default_language + '''\")\n",
    "    }\n",
    "      }\n",
    "    '''\n",
    "    #print(query_string)\n",
    "\n",
    "    results = send_sparql_query(query_string)\n",
    "    #print(json.dumps(results, indent=2))\n",
    "\n",
    "    return_list = []\n",
    "    for result in results:\n",
    "        out_dict = {\n",
    "            'qid': extract_local_name(result['qid']['value']),\n",
    "            'label': result['label']['value']\n",
    "            }\n",
    "        if 'description' in result:\n",
    "            out_dict['description'] = result['description']['value']\n",
    "        else:\n",
    "            out_dict['description'] = ''           \n",
    "        return_list.append(out_dict)\n",
    "    return return_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve DOI data for existing work records\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "filename = 'articles.csv'\n",
    "articles = read_dicts_from_csv(filename)\n",
    "for article in articles[50:100]:\n",
    "    if article['doi'] != '':\n",
    "        crossref_results = retrieve_crossref_data(article['doi'])\n",
    "        if crossref_results != {}:        \n",
    "            crossref_results = extract_journal_qid(crossref_results, article)\n",
    "\n",
    "if log_path != '':\n",
    "    log_object.close()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve DOI data for new record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_en': 'Leftover opioids following adult surgical procedures: a systematic review and meta-analysis', 'description_en': 'journal article', 'instance_of_uuid': '', 'instance_of': 'Q18918145', 'doi_uuid': '', 'doi': '10.1186/S13643-020-01393-8', 'doi_ref1_hash': '', 'doi_ref1_referenceUrl': 'http://doi.org/10.1186/S13643-020-01393-8', 'doi_ref1_retrieved_nodeId': '', 'doi_ref1_retrieved_val': '2021-03-26', 'doi_ref1_retrieved_prec': '', 'published_uuid': '', 'published_nodeId': '', 'published_val': '2020-06-11', 'published_prec': '', 'published_ref1_hash': '', 'published_ref1_referenceUrl': 'http://doi.org/10.1186/S13643-020-01393-8', 'published_ref1_retrieved_nodeId': '', 'published_ref1_retrieved_val': '2021-03-26', 'published_ref1_retrieved_prec': '', 'title_en_uuid': '', 'title_en': 'Leftover opioids following adult surgical procedures: a systematic review and meta-analysis', 'title_en_ref1_hash': '', 'title_en_ref1_referenceUrl': 'http://doi.org/10.1186/S13643-020-01393-8', 'title_en_ref1_retrieved_nodeId': '', 'title_en_ref1_retrieved_val': '2021-03-26', 'title_en_ref1_retrieved_prec': '', 'journal_uuid': '', 'journal': 'Q18216009', 'journal_ref1_hash': '', 'journal_ref1_referenceUrl': 'http://doi.org/10.1186/S13643-020-01393-8', 'journal_ref1_retrieved_nodeId': '', 'journal_ref1_retrieved_val': '2021-03-26', 'journal_ref1_retrieved_prec': '', 'volume_uuid': '', 'volume': '9', 'volume_ref1_hash': '', 'volume_ref1_referenceUrl': 'http://doi.org/10.1186/S13643-020-01393-8', 'volume_ref1_retrieved_nodeId': '', 'volume_ref1_retrieved_val': '2021-03-26', 'volume_ref1_retrieved_prec': '', 'page_uuid': '', 'page': '', 'page_ref1_hash': '', 'page_ref1_referenceUrl': '', 'page_ref1_retrieved_nodeId': '', 'page_ref1_retrieved_val': '', 'page_ref1_retrieved_prec': '', 'issue_uuid': '', 'issue': '1', 'issue_ref1_hash': '', 'issue_ref1_referenceUrl': 'http://doi.org/10.1186/S13643-020-01393-8', 'issue_ref1_retrieved_nodeId': '', 'issue_ref1_retrieved_val': '2021-03-26', 'issue_ref1_retrieved_prec': ''}\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "doi = '10.1186/S13643-020-01393-8'\n",
    "today = generate_utc_date()\n",
    "\n",
    "filename = 'articles.csv'\n",
    "articles = read_dicts_from_csv(filename)\n",
    "fieldnames = articles[0].keys() # get the field names from the existing file\n",
    "\n",
    "crossref_results = retrieve_crossref_data(doi)\n",
    "if crossref_results != {}:        \n",
    "    crossref_results = extract_journal_qid(crossref_results, article)\n",
    "    #print(crossref_results)\n",
    "    \n",
    "    out_dict = {}\n",
    "    for field in out_fields_labdes:   \n",
    "        #print(field, crossref_results[field])\n",
    "        out_dict[field] = crossref_results[field]\n",
    "    #print()\n",
    "    for field in out_fields_noref:   \n",
    "        #print(field, crossref_results[field])\n",
    "        out_dict[field + '_uuid'] = ''\n",
    "        out_dict[field] = crossref_results[field]\n",
    "    #print()\n",
    "    for field in out_fields_ref:\n",
    "        #print(field, crossref_results[field])\n",
    "        out_dict[field + '_uuid'] = ''\n",
    "        if field == 'published':\n",
    "            out_dict[field + '_nodeId'] = ''\n",
    "            out_dict[field + '_val'] = crossref_results[field]\n",
    "            out_dict[field + '_prec'] = ''\n",
    "        else:\n",
    "            out_dict[field] = crossref_results[field]\n",
    "        # Only add a reference if there is a value for that field\n",
    "        if crossref_results[field] == '':\n",
    "            out_dict[field + '_ref1_hash'] = ''\n",
    "            out_dict[field + '_ref1_referenceUrl'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_nodeId'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_val'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_prec'] = ''\n",
    "        else:\n",
    "            out_dict[field + '_ref1_hash'] = ''\n",
    "            out_dict[field + '_ref1_referenceUrl'] = 'http://doi.org/' + doi\n",
    "            out_dict[field + '_ref1_retrieved_nodeId'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_val'] = today\n",
    "            out_dict[field + '_ref1_retrieved_prec'] = ''\n",
    "\n",
    "    #print()\n",
    "print(out_dict)\n",
    "articles.append(out_dict)\n",
    "\n",
    "#write_dicts_to_csv(articles, filename, fieldnames)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests of  name searches at Wikidata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'qId': 'Q1399', 'name': 'Niccolò Machiavelli'}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#name = '尼可罗·马基亚维利'\n",
    "#name = 'Nicolás Maquiavelo'\n",
    "name = 'Никколо Макиавелли'\n",
    "#generate_name_alternatives(name)\n",
    "search_name_at_wikidata(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lori Schirle\n",
      "[]\n",
      "\n",
      "Amanda L. Stone\n",
      "[{'qId': 'Q96192760', 'name': 'Amanda Stone'}, {'qId': 'Q67224428', 'name': 'A Stone'}, {'qId': 'Q4757730', 'name': 'Andrew Leete Stone'}, {'qId': 'Q20646521', 'name': 'Amanda Addams'}, {'qId': 'Q52156179', 'name': 'Alice Balch Stone'}]\n",
      "\n",
      "Matthew C. Morris\n",
      "[{'qId': 'Q6789094', 'name': 'Matthew Morris'}, {'qId': 'Q1188592', 'name': 'MattyBRaps'}, {'qId': 'Q57488067', 'name': 'Mary Morris'}, {'qId': 'Q64597505', 'name': 'Matthew C Morris'}, {'qId': 'Q6790991', 'name': 'Matthew Morris'}, {'qId': 'Q56935999', 'name': 'Matthew C Morris'}, {'qId': 'Q71304625', 'name': 'Matthew Morris'}, {'qId': 'Q75568259', 'name': 'Matthew Morris'}, {'qId': 'Q92420603', 'name': 'Mackenzie Morris'}]\n",
      "\n",
      "Sarah S. Osmundson\n",
      "[{'qId': 'Q57059281', 'name': 'Sarah S. Osmundson'}]\n",
      "\n",
      "Philip D. Walker\n",
      "[{'qId': 'Q96400978', 'name': 'Philip D. Walker'}, {'qId': 'Q60025039', 'name': 'Philip Walker'}, {'qId': 'Q76759075', 'name': 'Philip Walker'}, {'qId': 'Q59292007', 'name': 'Peter Walker'}]\n",
      "\n",
      "Mary S. Dietrich\n",
      "[{'qId': 'Q95976272', 'name': 'Manuela Dietrich'}, {'qId': 'Q89749783', 'name': 'Mary S. Dietrich'}]\n",
      "\n",
      "Stephen Bruehl\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doi = '10.1186/S13643-020-01393-8'\n",
    "crossref_results = retrieve_crossref_data(doi)\n",
    "for author in crossref_results['authors']:\n",
    "    name = author['givenName'] + ' ' + author['familyName']\n",
    "    hit = search_name_at_wikidata(name)\n",
    "    print(name)\n",
    "    print(hit)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests of searches for names at Vanderbilt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "OrderedDict([('qid', 'Q100300828'), ('label_en', 'William C. Binkley'), ('affiliation_uuid', ''), ('affiliation', ''), ('affiliation_start_time_nodeId', ''), ('affiliation_start_time_val', ''), ('affiliation_start_time_prec', ''), ('affiliation_end_time_nodeId', ''), ('affiliation_end_time_val', ''), ('affiliation_end_time_prec', ''), ('affiliation_ref1_hash', ''), ('affiliation_ref1_referenceUrl', ''), ('affiliation_ref1_retrieved_nodeId', ''), ('affiliation_ref1_retrieved_val', ''), ('affiliation_ref1_retrieved_prec', '')])\n",
      "\n",
      "OrderedDict([('qid', 'Q14710143'), ('label_en', 'Owen Graduate School of Management'), ('parent_org', 'Q29052')])\n"
     ]
    }
   ],
   "source": [
    "filename = 'researchers.csv'\n",
    "researchers = read_dicts_from_csv(filename)\n",
    "\n",
    "filename = 'vanderbilt_wikidata_altlabels.csv'\n",
    "altnames = read_dicts_from_csv(filename)\n",
    "\n",
    "filename = 'departments.csv'\n",
    "departments = read_dicts_from_csv(filename)\n",
    "\n",
    "filename = 'department_labels.csv'\n",
    "department_labels = read_dicts_from_csv(filename)\n",
    "\n",
    "#print(researchers[0])\n",
    "#print()\n",
    "#print(altnames[0])\n",
    "print()\n",
    "print(departments[0])\n",
    "print()\n",
    "print(department_labels[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"orcid\": \"http://orcid.org/0000-0002-8436-595X\",\n",
      "    \"sequence\": \"first\",\n",
      "    \"givenName\": \"Jacob L.\",\n",
      "    \"familyName\": \"Steenwyk\",\n",
      "    \"affiliation\": []\n",
      "  },\n",
      "  {\n",
      "    \"orcid\": \"http://orcid.org/0000-0001-5765-1419\",\n",
      "    \"sequence\": \"additional\",\n",
      "    \"givenName\": \"Xing-Xing\",\n",
      "    \"familyName\": \"Shen\",\n",
      "    \"affiliation\": []\n",
      "  },\n",
      "  {\n",
      "    \"orcid\": \"http://orcid.org/0000-0002-9579-4178\",\n",
      "    \"sequence\": \"additional\",\n",
      "    \"givenName\": \"Abigail L.\",\n",
      "    \"familyName\": \"Lind\",\n",
      "    \"affiliation\": []\n",
      "  },\n",
      "  {\n",
      "    \"orcid\": \"http://orcid.org/0000-0002-2986-350X\",\n",
      "    \"sequence\": \"additional\",\n",
      "    \"givenName\": \"Gustavo H.\",\n",
      "    \"familyName\": \"Goldman\",\n",
      "    \"affiliation\": []\n",
      "  },\n",
      "  {\n",
      "    \"orcid\": \"http://orcid.org/0000-0002-7248-6551\",\n",
      "    \"sequence\": \"additional\",\n",
      "    \"givenName\": \"Antonis\",\n",
      "    \"familyName\": \"Rokas\",\n",
      "    \"affiliation\": []\n",
      "  }\n",
      "]\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#doi = '10.1093/GBE/EVY012'\n",
    "doi = '10.1101/370429'\n",
    "#doi = '10.1371/JOURNAL.PGEN.1005922'\n",
    "#doi = '10.1371/JOURNAL.PONE.0007509'\n",
    "crossref_results = retrieve_crossref_data(doi)\n",
    "authors = crossref_results['authors']\n",
    "print(json.dumps(authors, indent=2))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "researcher exact label match: Q77515995 Yao Xu\n",
      "Vanderbilt Department of Biological Sciences\n",
      "\n",
      "researcher exact label match: Q59538970 Tetsuya Mori\n",
      "Vanderbilt Department of Biological Sciences\n",
      "\n",
      "100 Ximing Qin  /  https://www.wikidata.org/wiki/Q91168236 Ximing Qin\n",
      "researcher (ORCID 0000-0003-2496-2434)\n",
      "\n",
      "\n",
      "researcher exact label match: Q28036851 Martin Egli\n",
      "Vanderbilt Department of Biochemistry\n",
      "\n",
      "researcher altname match: Q28530058 Carl Hirschie Johnson\n",
      "Vanderbilt Department of Biological Sciences\n",
      "\n",
      "\n",
      "Looking for potential coauthor matches\n",
      "Ximing Qin\n",
      "Heping Yan\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# screens.json is a configuration file that defines the kinds of screens to be performed on potential Q ID matches from Wikidata\n",
    "screens = load_json_into_data_struct('screens.json')\n",
    "\n",
    "found_qid_values = ''\n",
    "not_found_author_list = []\n",
    "for author in authors:\n",
    "    found = False\n",
    "    # screen for exact match to Wikidata labels\n",
    "    for researcher in researchers:\n",
    "        if researcher['label_en'] == author['givenName'] + ' ' + author['familyName']:\n",
    "            found = True\n",
    "            result_string = 'researcher exact label match: ' + researcher['qid'] + ' ' + researcher['label_en']\n",
    "            name = researcher['label_en']\n",
    "            qid = researcher['qid']\n",
    "            break\n",
    "    if not found:\n",
    "        # screen for exact match to alternate names\n",
    "        for altname in altnames:\n",
    "            if altname['altLabel'] == author['givenName'] + ' ' + author['familyName']:\n",
    "                found = True\n",
    "                result_string = 'researcher altname match: ' + altname['qid'] + ' ' + altname['altLabel']\n",
    "                name = altname['altLabel']\n",
    "                qid = altname['qid']\n",
    "                break\n",
    "        if not found:\n",
    "            # If the researcher has an ORCID, see if it's at Wikidata\n",
    "            \n",
    "            \n",
    "            # screen for fuzzy match to Wikidata-derived labels\n",
    "            for researcher in researchers:\n",
    "                # Require the surname to match the label surname exactly\n",
    "                split_names = find_surname_givens(researcher['label_en'])\n",
    "                if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                    w_ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], researcher['label_en'])\n",
    "                    #w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], researcher['label_en'])\n",
    "                    if w_ratio > 90:\n",
    "                        found = True\n",
    "                        result_string = 'fuzzy label match: ' + str(w_ratio) + ' ' + researcher['qid'] + ' ' + researcher['label_en'] + ' / ' + author['givenName'] + ' ' + author['familyName']\n",
    "                        name = researcher['label_en']\n",
    "                        qid = researcher['qid']\n",
    "                        break\n",
    "            if not found:\n",
    "                # screen for fuzzy match to alternate names\n",
    "                for altname in altnames:\n",
    "                    split_names = find_surname_givens(altname['altLabel'])\n",
    "                    if split_names: # skip names that don't have 2 parts\n",
    "                        if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                            w_ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], altname['altLabel'])\n",
    "                            #w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], altname['altLabel'])\n",
    "                            if w_ratio > 90:\n",
    "                                found = True\n",
    "                                result_string = 'researcher altname fuzzy match: ' + str(w_ratio) + ' ' + altname['qid'] + ' ' + altname['altLabel'] + ' / ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                name = altname['altLabel']\n",
    "                                qid = altname['qid']\n",
    "                                break\n",
    "                if not found:\n",
    "                    name = author['givenName'] + ' ' + author['familyName']\n",
    "                    hits = search_name_at_wikidata(name)\n",
    "                    #print(hits)\n",
    "\n",
    "                    qids = []\n",
    "                    for hit in hits:\n",
    "                        qids.append(hit['qid'])\n",
    "                    return_list = screen_qids(qids, screens)\n",
    "\n",
    "                    for hit in return_list:\n",
    "                        #print(return_list)\n",
    "                        split_names = find_surname_givens(hit['label'])\n",
    "\n",
    "                        # Require the surname to match the Wikidata label surname exactly\n",
    "                        # This prevents a high fraction of fuzzy matches where the last names are similar but not the same\n",
    "                        if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                            w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "\n",
    "                            # This screen requires a high degree of similarity between the overall ORCID names and Wikidata labels\n",
    "                            if w_ratio > 90:\n",
    "                                print(w_ratio, author['givenName'] + ' ' + author['familyName'], ' / ', 'https://www.wikidata.org/wiki/'+ hit['qid'], hit['label'])\n",
    "                                print(hit['description'])\n",
    "                                #print()\n",
    "\n",
    "    if not found:\n",
    "        not_found_author_list.append(author)\n",
    "        #print('not found:', author['givenName'] + ' ' + author['familyName'])\n",
    "    else:\n",
    "        found_qid_values += 'wd:' + qid + '\\n'\n",
    "        print(result_string)\n",
    "        for department in departments:\n",
    "            if qid == department['qid']:\n",
    "                for department_label in department_labels:\n",
    "                    if department_label['qid'] == department['affiliation']:\n",
    "                        print(department_label['label_en'])\n",
    "                        break\n",
    "    print()\n",
    "\n",
    "print()\n",
    "\n",
    "# Query to look for situations where one of the unlinked resarchers are coauthors of identified ones from Vanderbilt\n",
    "print('Looking for potential coauthor matches')\n",
    "query_string = '''\n",
    "select distinct ?coauthor ?label where {\n",
    "  VALUES ?researcher\n",
    "  {\n",
    "  ''' + found_qid_values + '''}\n",
    "?publication wdt:P50 ?researcher.\n",
    "?publication wdt:P50 ?coauthor.\n",
    "?coauthor rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "FILTER(?researcher != ?coauthor)\n",
    "  }\n",
    "'''\n",
    "#print(query_string)\n",
    "results = send_sparql_query(query_string)\n",
    "for author in not_found_author_list:\n",
    "    print(author['givenName'] + ' ' + author['familyName'])\n",
    "    for result in results:\n",
    "        w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], result['label'])\n",
    "        if w_ratio > 90:\n",
    "            print('fuzzy match: ' + str(w_ratio) + ' ' + result['coauthor'] + ' ' + result['label'] + ' / ' + author['givenName'] + ' ' + author['familyName'])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"orcid\": \"\",\n",
      "    \"sequence\": \"first\",\n",
      "    \"givenName\": \"Amelia R I\",\n",
      "    \"familyName\": \"Lindsey\",\n",
      "    \"affiliation\": [\n",
      "      \"Department of Entomology, University of California Riverside\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"orcid\": \"\",\n",
      "    \"sequence\": \"additional\",\n",
      "    \"givenName\": \"Danny W\",\n",
      "    \"familyName\": \"Rice\",\n",
      "    \"affiliation\": [\n",
      "      \"Department of Biology, Indiana University, Bloomington\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"orcid\": \"\",\n",
      "    \"sequence\": \"additional\",\n",
      "    \"givenName\": \"Sarah R\",\n",
      "    \"familyName\": \"Bordenstein\",\n",
      "    \"affiliation\": [\n",
      "      \"Department of Biological Sciences, Vanderbilt University\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"orcid\": \"\",\n",
      "    \"sequence\": \"additional\",\n",
      "    \"givenName\": \"Andrew W\",\n",
      "    \"familyName\": \"Brooks\",\n",
      "    \"affiliation\": [\n",
      "      \"Department of Biological Sciences, Vanderbilt University\",\n",
      "      \"Vanderbilt Genetics Institute, Vanderbilt University\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"orcid\": \"\",\n",
      "    \"sequence\": \"additional\",\n",
      "    \"givenName\": \"Seth R\",\n",
      "    \"familyName\": \"Bordenstein\",\n",
      "    \"affiliation\": [\n",
      "      \"Department of Biological Sciences, Vanderbilt University\",\n",
      "      \"Vanderbilt Genetics Institute, Vanderbilt University\",\n",
      "      \"Vanderbilt Institute for Infection, Immunology and Inflammation, Vanderbilt University\",\n",
      "      \"Department of Pathology, Microbiology and Immunology, Vanderbilt University\"\n",
      "    ]\n",
      "  },\n",
      "  {\n",
      "    \"orcid\": \"\",\n",
      "    \"sequence\": \"additional\",\n",
      "    \"givenName\": \"Irene L G\",\n",
      "    \"familyName\": \"Newton\",\n",
      "    \"affiliation\": [\n",
      "      \"Department of Biology, Indiana University, Bloomington\"\n",
      "    ]\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(authors, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data used in development of Q ID-screening function\n",
    "\n",
    "The hard-coded screens below can be used with the test Q IDs to screen out \"George Washington\"s of different sorts. They are an alternative to loading the screens via:\n",
    "\n",
    "```\n",
    "screens = load_json_into_data_struct('screens.json')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"qid\": \"Q10288976\",\n",
      "    \"label\": \"George Washington\",\n",
      "    \"description\": \"sculpture by Horacio Greenough\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q20539851\",\n",
      "    \"label\": \"George Washington\",\n",
      "    \"description\": \"painting by Henry Brintnell Bounetheau\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "qids = ['Q586680', 'Q1406257', 'Q1508517', 'Q10288976', 'Q20539851', 'Q23', 'Q79483233', 'Q103915646', 'Q5545912']\n",
    "\n",
    "screens1 = [\n",
    "    [\n",
    "        {\n",
    "            'property': 'P31',\n",
    "            'entity': 'Q5', # use empty string if any value is allowed, or if filtering value strings\n",
    "            'lang': '',\n",
    "            'position': 'object',\n",
    "            'require': 'exclude', # options: include, exclude\n",
    "            'filter_type': '', # options: in, <, >\n",
    "            'filter_string': ''\n",
    "        },\n",
    "        {\n",
    "            'property': 'P170',\n",
    "            'entity': '',\n",
    "            'lang': '',\n",
    "            'position': 'subject',\n",
    "            'require': 'exclude', # options: include, exclude\n",
    "            'filter_type': '', # options: in, <, >\n",
    "            'filter_string': ''\n",
    "        }\n",
    "    ],[\n",
    "        {\n",
    "            'property': 'description',\n",
    "            'entity': 'American jazz trombonist',\n",
    "            'lang': 'en',\n",
    "            'position': 'object',\n",
    "            'require': 'include', # options: include, exclude\n",
    "            'filter_type': '', # options: in, <, >\n",
    "            'filter_string': ''\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "screens2 = [\n",
    "    [\n",
    "        {\n",
    "            'property': 'P31',\n",
    "            'entity': 'Q3305213', # paintings\n",
    "            'lang': '',\n",
    "            'position': 'object',\n",
    "            'require': 'include', # options: include, exclude\n",
    "            'filter_type': '', # options: in, <, >\n",
    "            'filter_string': ''\n",
    "        }\n",
    "    ],[\n",
    "        {\n",
    "            'property': 'P31',\n",
    "            'entity': 'Q179700', # sculptures\n",
    "            'lang': '',\n",
    "            'position': 'object',\n",
    "            'require': 'include', # options: include, exclude\n",
    "            'filter_type': '', # options: in, <, >\n",
    "            'filter_string': ''\n",
    "        }\n",
    "    ]\n",
    "]\n",
    "\n",
    "screens = load_json_into_data_struct('screens.json')\n",
    "return_list = screen_qids(qids, screens)\n",
    "print(json.dumps(return_list, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
