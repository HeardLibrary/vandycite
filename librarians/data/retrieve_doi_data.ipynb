{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only once if you need to install the python-Levenshtein package\n",
    "\n",
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve data using DOI\n",
    "\n",
    "Currently the script only uses the CrossRef API, but potentially could use others like DataCite\n",
    "\n",
    "## Configuration\n",
    "\n",
    "This section imports libraries, sets default values, and defines functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2020 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "import requests   # best library to manage HTTP transactions\n",
    "#from bs4 import BeautifulSoup # web-scraping library\n",
    "import json\n",
    "from time import sleep\n",
    "import csv\n",
    "import sys\n",
    "#import math\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "#from fuzzywuzzy import process\n",
    "#import xml.etree.ElementTree as et # library to traverse XML tree\n",
    "import urllib\n",
    "import datetime\n",
    "#import string\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "accept_media_type = 'application/json'\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "user_agent_header = 'VanderBot/1.7 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "sparql_sleep = 0.1\n",
    "default_language = 'en'\n",
    "\n",
    "# Option to log to a file instead of the console\n",
    "log_path = '' # path to log file, default to none\n",
    "log_object = sys.stdout # log output defaults to the console screen\n",
    "\n",
    "opts = [opt for opt in sys.argv[1:] if opt.startswith('-')]\n",
    "args = [arg for arg in sys.argv[1:] if not arg.startswith('-')]\n",
    "\n",
    "if '--log' in opts: # set output to specified log file or path including file name\n",
    "    log_path = args[opts.index('--log')]\n",
    "    log_object = open(log_path, 'wt', encoding='utf-8') # direct output sent to log_object to log file instead of sys.stdout\n",
    "if '-L' in opts: # set output to specified log file or path including file name\n",
    "    log_path = args[opts.index('-L')]\n",
    "    log_object = open(log_path, 'wt', encoding='utf-8') # direct output sent to log_object to log file instead of sys.stdout\n",
    "\n",
    "# --------------------------\n",
    "# *** For now, hard-code logging to errors.log\n",
    "log_path = 'errors.log'\n",
    "log_object = open(log_path, 'wt', encoding='utf-8')\n",
    "# --------------------------\n",
    "\n",
    "# List of work types used by CrossRef\n",
    "work_types = [\n",
    "    {\n",
    "    'crossref_type_string': 'journal-article',\n",
    "    'qid': 'Q18918145', # academic journal article, alternatively Q13442814 scholarly article\n",
    "    'description': 'journal article'\n",
    "    },\n",
    "    {\n",
    "    'crossref_type_string': 'book-chapter',\n",
    "    'qid': 'Q21481766', # academic journal article, alternatively Q13442814 scholarly article\n",
    "    'description': 'academic book chapter'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Output CSV label/description fields to be populated without references\n",
    "out_fields_labdes = ['label_' + default_language, 'description_' + default_language]\n",
    "\n",
    "# Output CSV property fields to be populated without references\n",
    "out_fields_noref = ['instance_of']\n",
    "\n",
    "# Output CSV fields that include reference fields\n",
    "out_fields_ref = ['doi', 'published', 'title_' + default_language, 'journal', 'volume', 'page', 'issue']\n",
    "    \n",
    "# generates a dictionary to be passed in a requests GET method to generate the request header\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "# Determine the current CrossRef rate limit by an initial ping\n",
    "response = requests.get('https://api.crossref.org/works/10.3233/SW-150203', headers=generate_header_dictionary(accept_media_type))\n",
    "crossref_headers = response.headers\n",
    "limit_count = int(crossref_headers['x-rate-limit-limit'])\n",
    "interval_string = crossref_headers['x-rate-limit-interval']\n",
    "interval_sec = int(interval_string[:len(interval_string)-1]) # remove the \"s\" from the end\n",
    "api_sleep = interval_sec / limit_count + 0.005\n",
    "\n",
    "# Due to problems with direct POST of UTF-8, changed to POST with URL-encoded parameters\n",
    "# See https://www.w3.org/TR/sparql11-protocol/#update-via-post-urlencoded\n",
    "# and https://stackoverflow.com/questions/34618149/post-unicode-string-to-web-service-using-python-requests-library\n",
    "def generate_sparql_header_dictionary(accept_media_type,user_agent_header):\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "#        'Content-Type': 'application/sparql-query',\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "# The following function requires the request header generated above\n",
    "sparql_request_header = generate_sparql_header_dictionary(accept_media_type, user_agent_header)\n",
    "\n",
    "# Generate the current UTC xsd:date\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "# Extracts the local name part of an IRI, e.g. a qNumber from a Wikidata IRI\n",
    "def extract_local_name(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    last_piece = len(pieces)\n",
    "    return pieces[last_piece - 1]\n",
    "\n",
    "# Read from a CSV file into a list of dictionaries\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# Write list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Sends a query to the query service endpoint. \n",
    "# NOTE: request_header and endpoint are global variables defined earlier in the script\n",
    "def send_sparql_query(query_string):\n",
    "    # You can delete the two print statements if the queries are short. However, for large/long queries,\n",
    "    # it's good to let the user know what's going on.\n",
    "    #print('querying SPARQL endpoint to acquire item metadata')\n",
    "    #response = requests.post(endpoint, data=query_string.encode('utf-8'), headers=sparql_request_header)\n",
    "    response = requests.post(endpoint, data=dict(query=query_string), headers=sparql_request_header)\n",
    "    #print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the values from the response JSON\n",
    "    results = data['results']['bindings']\n",
    "    \n",
    "    #print('done retrieving data')\n",
    "    # print(json.dumps(results, indent=2))\n",
    "    \n",
    "    sleep(sparql_sleep) # delay to avoid hitting the Query Service too fast\n",
    "    return results\n",
    "\n",
    "# Use the CrossRef registration agency test to find out who issued the non-CrosRef DOI\n",
    "def discover_issuing_agency(doi):\n",
    "    encoded_doi = urllib.parse.quote(doi)\n",
    "    search_url = 'https://api.crossref.org/works/' + encoded_doi + '/agency'\n",
    "    response = requests.get(search_url, headers=generate_header_dictionary(accept_media_type))\n",
    "    sleep(api_sleep) # delay to avoid hitting the API faster than acceptable rate\n",
    "    if response.status_code == 404: # return \"not found\" if URL doesn't dereference\n",
    "        return 'not found'\n",
    "    try: # Try to parse as JSON\n",
    "        data = response.json()\n",
    "        return data['message']['agency']['id']\n",
    "    except: # if the response isn't JSON, then just return the response text\n",
    "        return response.text\n",
    "\n",
    "# See https://github.com/CrossRef/rest-api-doc for API details\n",
    "# Note: no authentication required, but must be \"nice\": observe rate limit, provide mailto:\n",
    "def retrieve_crossref_data(doi):\n",
    "    crossref_endpoint_url = 'https://api.crossref.org/works/'\n",
    "    # urllib.parse.quote performs URL encoding of a string\n",
    "    encoded_doi = urllib.parse.quote(doi)\n",
    "    search_url = crossref_endpoint_url + encoded_doi\n",
    "    response = requests.get(search_url, headers=generate_header_dictionary(accept_media_type))\n",
    "    article_dict = {}\n",
    "    if response.status_code == 404: # return empty dict if not found\n",
    "        # *** NOTE: at some future time, look up data at alternative issuing agencies\n",
    "        # For example, see https://support.datacite.org/docs/api-get-doi\n",
    "        \n",
    "        # For now, just log it\n",
    "        print('article:', article['qid'], 'DOI issuing agency:', discover_issuing_agency(article['doi']),'\\n', file=log_object)\n",
    "        sleep(api_sleep)\n",
    "        return article_dict\n",
    "    else:\n",
    "        author_list = []\n",
    "        try:\n",
    "            response_structure = response.json()\n",
    "            data = response_structure['message']\n",
    "        except:\n",
    "            # if not JSON, just return the response text\n",
    "            article_dict['message'] = response.text\n",
    "            sleep(api_sleep)\n",
    "            return article_dict\n",
    "        #print(json.dumps(data, indent = 2))\n",
    "        article_dict['doi'] = doi\n",
    "        if 'author' in data:\n",
    "            authors = data['author']\n",
    "            for author in authors:\n",
    "                authorDict = {}\n",
    "                if 'ORCID' in author:\n",
    "                    authorDict['orcid'] = author['ORCID']\n",
    "                else:\n",
    "                    authorDict['orcid'] = ''\n",
    "                if 'sequence' in author:\n",
    "                    authorDict['sequence'] = author['sequence']\n",
    "                else:\n",
    "                    authorDict['sequence'] = ''\n",
    "                if 'given' in author:\n",
    "                    authorDict['givenName'] = author['given']\n",
    "                else:\n",
    "                    authorDict['givenName'] = ''\n",
    "                if 'family' in author:\n",
    "                    authorDict['familyName'] = author['family']\n",
    "                else:\n",
    "                    authorDict['familyName'] = ''\n",
    "                affiliationList = []\n",
    "                if 'affiliation' in author:\n",
    "                    for affiliation in author['affiliation']:\n",
    "                        affiliationList.append(affiliation['name'])\n",
    "                # if there aren't any affiliations, the list will remain empty\n",
    "                authorDict['affiliation'] = affiliationList\n",
    "                author_list.append(authorDict)\n",
    "            article_dict['authors'] = author_list\n",
    "        if 'issued' in data:\n",
    "            issued = data['issued']['date-parts'][0]\n",
    "            issued_date = str(issued[0])\n",
    "            if len(issued) > 1:\n",
    "                if len(str(issued[1])) == 1:\n",
    "                    issued_date += '-0'+ str(issued[1])\n",
    "                else:\n",
    "                    issued_date += '-'+ str(issued[1])\n",
    "                if len(issued) > 2:                \n",
    "                    if len(str(issued[2])) == 1:\n",
    "                        issued_date += '-0'+ str(issued[2])\n",
    "                    else:\n",
    "                        issued_date += '-'+ str(issued[2])\n",
    "            article_dict['published'] = issued_date\n",
    "        else:\n",
    "            article_dict['published'] = ''\n",
    "        if 'volume' in data:\n",
    "            article_dict['volume'] = data['volume']\n",
    "        else:\n",
    "            article_dict['volume'] = ''\n",
    "        if 'issue' in data:\n",
    "            article_dict['issue'] = data['issue']\n",
    "        else:\n",
    "            article_dict['issue'] = ''\n",
    "        if 'page' in data:\n",
    "            article_dict['page'] = data['page']\n",
    "        else:\n",
    "            article_dict['page'] = ''\n",
    "        if 'ISSN' in data:\n",
    "            article_dict['journal_issn'] = data['ISSN']\n",
    "        else:\n",
    "            article_dict['journal_issn'] = []\n",
    "        if 'title' in data:\n",
    "            if len(data['title']) > 0:\n",
    "                article_dict['title_' + default_language] = data['title'][0]\n",
    "                article_dict['label_' + default_language] = data['title'][0]\n",
    "        else:\n",
    "            article_dict['title_' + default_language] = ''\n",
    "            article_dict['label_' + default_language] = ''\n",
    "        if 'container-title' in data:\n",
    "            if len(data['container-title']) > 0:\n",
    "                article_dict['journal_title'] = data['container-title'][0]\n",
    "        else:\n",
    "            article_dict['journal_title'] = ''\n",
    "         \n",
    "        if 'type' in data:\n",
    "            found = False\n",
    "            for work_type in work_types:\n",
    "                if data['type'] == work_type['crossref_type_string']:\n",
    "                    found = True\n",
    "                    article_dict['instance_of'] = work_type['qid']\n",
    "                    article_dict['description_' + default_language] = work_type['description']\n",
    "            if not found:\n",
    "                article_dict['instance_of'] = ''\n",
    "                article_dict['description_' + default_language] = ''\n",
    "        else:\n",
    "            article_dict['instance_of'] = ''\n",
    "            article_dict['description_' + default_language] = ''\n",
    "\n",
    "    sleep(api_sleep)\n",
    "    return article_dict\n",
    "\n",
    "# Look up the ISSN from CrossRef in Wikidata\n",
    "def extract_journal_qid(crossref_results, article):\n",
    "    if len(crossref_results['journal_issn']) == 0:\n",
    "        crossref_results['journal'] = ''\n",
    "        print('article:', article['qid'], 'has no ISSN.\\n', file=log_object)\n",
    "        return crossref_results\n",
    "\n",
    "    # Create VALUES list for items\n",
    "    issns_string = ''\n",
    "    for issn in crossref_results['journal_issn']:\n",
    "        issns_string += '\"' + issn + '\"\\n'\n",
    "    # Remove trailing newline\n",
    "    issns_string = issns_string[:len(issns_string)-1]\n",
    "\n",
    "    # Build query string\n",
    "    query_string = '''select distinct ?journal ?journalLabel where {\n",
    "      VALUES ?issn\n",
    "        {\n",
    "    ''' + issns_string + '''\n",
    "        }\n",
    "      ?journal wdt:P236 ?issn.\n",
    "      ?journal rdfs:label ?journalLabel.\n",
    "      filter(lang(?journalLabel)=\"en\")\n",
    "    }'''\n",
    "    #print(query_string)\n",
    "\n",
    "    # Send query to endpoint\n",
    "    query_results = send_sparql_query(query_string)\n",
    "    #pp.pprint(query_results)\n",
    "\n",
    "    if len(query_results) > 1:\n",
    "        print('Warning! More than one publication in Wikidata matched the ISSN for article', article['qid'], file=log_object)\n",
    "        print(query_results, '\\n', file=log_object)\n",
    "\n",
    "    # Extract Q ID from SPARQL query results. If there is more than one result, the last one will be used for the Q ID\n",
    "    for result in query_results:\n",
    "        journal_qid = extract_local_name(result['journal']['value'])\n",
    "        journal_name = result['journalLabel']['value']\n",
    "        if journal_name != crossref_results['journal_title']:\n",
    "            # NOTE: did empirical testing to see which kind of fuzzy matching worked best\n",
    "            #ratio = fuzz.ratio(journal_name, crossref_results['journal_title'])\n",
    "            #partial_ratio = fuzz.partial_ratio(journal_name, crossref_results['journal_title'])\n",
    "            #sort_ratio = fuzz.token_sort_ratio(journal_name, crossref_results['journal_title'])\n",
    "            #set_ratio = fuzz.token_set_ratio(journal_name, crossref_results['journal_title'])\n",
    "            w_ratio = fuzz.WRatio(journal_name, crossref_results['journal_title'])\n",
    "            #print('name similarity ratio', ratio)\n",
    "            #print('partial ratio', partial_ratio)\n",
    "            #print('sort_ratio', sort_ratio)\n",
    "            #print('set_ratio', set_ratio)\n",
    "            if w_ratio < 99:\n",
    "                print('article:', article['qid'], 'w_ratio:', w_ratio, 'Warning: Wikidata journal: \"' + journal_name + '\"', journal_qid, 'does not match CrossRef journal title: \"' + crossref_results['journal_title'] + '\"\\n', file=log_object)\n",
    "        #print('article:', article['qid'], 'journal:', journal_qid, journal_name)\n",
    "    crossref_results['journal'] = journal_qid\n",
    "    return crossref_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve DOI data for existing work records\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "filename = 'articles.csv'\n",
    "articles = read_dicts_from_csv(filename)\n",
    "for article in articles[50:100]:\n",
    "    if article['doi'] != '':\n",
    "        crossref_results = retrieve_crossref_data(article['doi'])\n",
    "        if crossref_results != {}:        \n",
    "            crossref_results = extract_journal_qid(crossref_results, article)\n",
    "\n",
    "if log_path != '':\n",
    "    log_object.close()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve DOI data for new record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_en': 'Leftover opioids following adult surgical procedures: a systematic review and meta-analysis', 'description_en': 'journal article', 'instance_of_uuid': '', 'instance_of': 'Q18918145', 'doi_uuid': '', 'doi': '10.1186/S13643-020-01393-8', 'doi_ref1_hash': '', 'doi_ref1_referenceUrl': 'http://doi.org/10.1186/S13643-020-01393-8', 'doi_ref1_retrieved_nodeId': '', 'doi_ref1_retrieved_val': '2021-03-26', 'doi_ref1_retrieved_prec': '', 'published_uuid': '', 'published_nodeId': '', 'published_val': '2020-06-11', 'published_prec': '', 'published_ref1_hash': '', 'published_ref1_referenceUrl': 'http://doi.org/10.1186/S13643-020-01393-8', 'published_ref1_retrieved_nodeId': '', 'published_ref1_retrieved_val': '2021-03-26', 'published_ref1_retrieved_prec': '', 'title_en_uuid': '', 'title_en': 'Leftover opioids following adult surgical procedures: a systematic review and meta-analysis', 'title_en_ref1_hash': '', 'title_en_ref1_referenceUrl': 'http://doi.org/10.1186/S13643-020-01393-8', 'title_en_ref1_retrieved_nodeId': '', 'title_en_ref1_retrieved_val': '2021-03-26', 'title_en_ref1_retrieved_prec': '', 'journal_uuid': '', 'journal': 'Q18216009', 'journal_ref1_hash': '', 'journal_ref1_referenceUrl': 'http://doi.org/10.1186/S13643-020-01393-8', 'journal_ref1_retrieved_nodeId': '', 'journal_ref1_retrieved_val': '2021-03-26', 'journal_ref1_retrieved_prec': '', 'volume_uuid': '', 'volume': '9', 'volume_ref1_hash': '', 'volume_ref1_referenceUrl': 'http://doi.org/10.1186/S13643-020-01393-8', 'volume_ref1_retrieved_nodeId': '', 'volume_ref1_retrieved_val': '2021-03-26', 'volume_ref1_retrieved_prec': '', 'page_uuid': '', 'page': '', 'page_ref1_hash': '', 'page_ref1_referenceUrl': '', 'page_ref1_retrieved_nodeId': '', 'page_ref1_retrieved_val': '', 'page_ref1_retrieved_prec': '', 'issue_uuid': '', 'issue': '1', 'issue_ref1_hash': '', 'issue_ref1_referenceUrl': 'http://doi.org/10.1186/S13643-020-01393-8', 'issue_ref1_retrieved_nodeId': '', 'issue_ref1_retrieved_val': '2021-03-26', 'issue_ref1_retrieved_prec': ''}\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "doi = '10.1186/S13643-020-01393-8'\n",
    "today = generate_utc_date()\n",
    "\n",
    "filename = 'articles.csv'\n",
    "articles = read_dicts_from_csv(filename)\n",
    "fieldnames = articles[0].keys() # get the field names from the existing file\n",
    "\n",
    "crossref_results = retrieve_crossref_data(doi)\n",
    "if crossref_results != {}:        \n",
    "    crossref_results = extract_journal_qid(crossref_results, article)\n",
    "    #print(crossref_results)\n",
    "    \n",
    "    out_dict = {}\n",
    "    for field in out_fields_labdes:   \n",
    "        #print(field, crossref_results[field])\n",
    "        out_dict[field] = crossref_results[field]\n",
    "    #print()\n",
    "    for field in out_fields_noref:   \n",
    "        #print(field, crossref_results[field])\n",
    "        out_dict[field + '_uuid'] = ''\n",
    "        out_dict[field] = crossref_results[field]\n",
    "    #print()\n",
    "    for field in out_fields_ref:\n",
    "        #print(field, crossref_results[field])\n",
    "        out_dict[field + '_uuid'] = ''\n",
    "        if field == 'published':\n",
    "            out_dict[field + '_nodeId'] = ''\n",
    "            out_dict[field + '_val'] = crossref_results[field]\n",
    "            out_dict[field + '_prec'] = ''\n",
    "        else:\n",
    "            out_dict[field] = crossref_results[field]\n",
    "        # Only add a reference if there is a value for that field\n",
    "        if crossref_results[field] == '':\n",
    "            out_dict[field + '_ref1_hash'] = ''\n",
    "            out_dict[field + '_ref1_referenceUrl'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_nodeId'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_val'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_prec'] = ''\n",
    "        else:\n",
    "            out_dict[field + '_ref1_hash'] = ''\n",
    "            out_dict[field + '_ref1_referenceUrl'] = 'http://doi.org/' + doi\n",
    "            out_dict[field + '_ref1_retrieved_nodeId'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_val'] = today\n",
    "            out_dict[field + '_ref1_retrieved_prec'] = ''\n",
    "\n",
    "    #print()\n",
    "print(out_dict)\n",
    "articles.append(out_dict)\n",
    "\n",
    "#write_dicts_to_csv(articles, filename, fieldnames)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"doi\": \"10.1186/S13643-020-01393-8\",\n",
      "  \"authors\": [\n",
      "    {\n",
      "      \"orcid\": \"http://orcid.org/0000-0003-2551-019X\",\n",
      "      \"sequence\": \"first\",\n",
      "      \"givenName\": \"Lori\",\n",
      "      \"familyName\": \"Schirle\",\n",
      "      \"affiliation\": []\n",
      "    },\n",
      "    {\n",
      "      \"orcid\": \"\",\n",
      "      \"sequence\": \"additional\",\n",
      "      \"givenName\": \"Amanda L.\",\n",
      "      \"familyName\": \"Stone\",\n",
      "      \"affiliation\": []\n",
      "    },\n",
      "    {\n",
      "      \"orcid\": \"\",\n",
      "      \"sequence\": \"additional\",\n",
      "      \"givenName\": \"Matthew C.\",\n",
      "      \"familyName\": \"Morris\",\n",
      "      \"affiliation\": []\n",
      "    },\n",
      "    {\n",
      "      \"orcid\": \"\",\n",
      "      \"sequence\": \"additional\",\n",
      "      \"givenName\": \"Sarah S.\",\n",
      "      \"familyName\": \"Osmundson\",\n",
      "      \"affiliation\": []\n",
      "    },\n",
      "    {\n",
      "      \"orcid\": \"\",\n",
      "      \"sequence\": \"additional\",\n",
      "      \"givenName\": \"Philip D.\",\n",
      "      \"familyName\": \"Walker\",\n",
      "      \"affiliation\": []\n",
      "    },\n",
      "    {\n",
      "      \"orcid\": \"\",\n",
      "      \"sequence\": \"additional\",\n",
      "      \"givenName\": \"Mary S.\",\n",
      "      \"familyName\": \"Dietrich\",\n",
      "      \"affiliation\": []\n",
      "    },\n",
      "    {\n",
      "      \"orcid\": \"\",\n",
      "      \"sequence\": \"additional\",\n",
      "      \"givenName\": \"Stephen\",\n",
      "      \"familyName\": \"Bruehl\",\n",
      "      \"affiliation\": []\n",
      "    }\n",
      "  ],\n",
      "  \"published\": \"2020-06-11\",\n",
      "  \"volume\": \"9\",\n",
      "  \"issue\": \"1\",\n",
      "  \"page\": \"\",\n",
      "  \"journal_issn\": [\n",
      "    \"2046-4053\"\n",
      "  ],\n",
      "  \"title_en\": \"Leftover opioids following adult surgical procedures: a systematic review and meta-analysis\",\n",
      "  \"label_en\": \"Leftover opioids following adult surgical procedures: a systematic review and meta-analysis\",\n",
      "  \"journal_title\": \"Systematic Reviews\",\n",
      "  \"instance_of\": \"Q18918145\",\n",
      "  \"description_en\": \"journal article\",\n",
      "  \"journal\": \"Q18216009\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(crossref_results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_name_alternatives(name):\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods, sometimes periods are close up with no spaces\n",
    "    name = name.replace('.', ' ')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "\n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        # make sure first character is alphabetic\n",
    "        # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "        # typical cases are like (Kit) or \"Kit\"\n",
    "        if not piece[0:1].isalpha():\n",
    "            piece = piece[1:len(piece)] # remove the first non-alphabetic character\n",
    "        if len(piece) > 0:\n",
    "            initials.append(piece[0:1])\n",
    "        \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += pieces[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # full name with suffix\n",
    "    if suffix != '':\n",
    "        name_version = ''\n",
    "        for piece_number in range(0, len(pieces)-1):\n",
    "            name_version += pieces[piece_number] + ' '\n",
    "        name_version += pieces[len(pieces)-1] + suffix\n",
    "        alternatives.append(name_version)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    name_version = pieces[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    name_version = pieces[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first and last name only\n",
    "    name_version = pieces[0] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial and last name only\n",
    "    name_version = initials[0] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    name_version = initials[0] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials with last name\n",
    "    name_version = initials[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += initials[piece_number]\n",
    "    name_version += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def search_name_at_wikidata(name):\n",
    "    # carry out search for most languages that use Latin characters, plus some other commonly used languages\n",
    "    # See https://doi.org/10.1145/3233391.3233965\n",
    "    language_codes = [\n",
    "        'en',\n",
    "        'es',\n",
    "        'pt',\n",
    "        'fr',\n",
    "        'it',\n",
    "        'nl',\n",
    "        'de',\n",
    "        'da',\n",
    "        'et',\n",
    "        'hu',\n",
    "        'ga',\n",
    "        'ro',\n",
    "        'sk',\n",
    "        'sl',\n",
    "        'zu',\n",
    "        'tr',\n",
    "        'sv',\n",
    "        'zh',\n",
    "        'ru',\n",
    "        'ja',\n",
    "        'ar',\n",
    "        'pl',\n",
    "        'uk',\n",
    "        'ca',\n",
    "        'cs',\n",
    "        'la',\n",
    "        'nb',\n",
    "        'hu',\n",
    "        'he',\n",
    "        'eo',\n",
    "        'fi',\n",
    "        'ko'\n",
    "      ]\n",
    "    name_list = generate_name_alternatives(name)\n",
    "    alternatives = ''\n",
    "    for language_code in language_codes:\n",
    "        for alternative in name_list:\n",
    "            # get rid of quotes, which will break the query\n",
    "            alternative = alternative.replace('\"', '')\n",
    "            alternative = alternative.replace(\"'\", '')\n",
    "            alternatives += '\"' + alternative + '\"@' + language_code + '\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(dict(data=query))\n",
    "    #print('searching for ', name)\n",
    "    results = []\n",
    "#    r = requests.post(endpoint, data=query.encode('utf-8'), headers=sparql_request_header)\n",
    "    r = requests.post(endpoint, data=dict(query=query), headers=sparql_request_header)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidata_iri = statement['item']['value']\n",
    "            if 'label' in statement:\n",
    "                name = statement['label']['value']\n",
    "            else:\n",
    "                name = ''\n",
    "            qnumber = extract_local_name(wikidata_iri)\n",
    "            results.append({'qId': qnumber, 'name': name})\n",
    "    except:\n",
    "        results = [{'error': r.text}]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'qId': 'Q1399', 'name': 'Niccolò Machiavelli'}]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#name = '尼可罗·马基亚维利'\n",
    "#name = 'Nicolás Maquiavelo'\n",
    "name = 'Никколо Макиавелли'\n",
    "#generate_name_alternatives(name)\n",
    "search_name_at_wikidata(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lori Schirle\n",
      "[]\n",
      "\n",
      "Amanda L. Stone\n",
      "[{'qId': 'Q96192760', 'name': 'Amanda Stone'}, {'qId': 'Q67224428', 'name': 'A Stone'}, {'qId': 'Q4757730', 'name': 'Andrew Leete Stone'}, {'qId': 'Q20646521', 'name': 'Amanda Addams'}, {'qId': 'Q52156179', 'name': 'Alice Balch Stone'}]\n",
      "\n",
      "Matthew C. Morris\n",
      "[{'qId': 'Q6789094', 'name': 'Matthew Morris'}, {'qId': 'Q6790991', 'name': 'Matthew Morris'}, {'qId': 'Q56935999', 'name': 'Matthew C Morris'}, {'qId': 'Q64597505', 'name': 'Matthew C Morris'}, {'qId': 'Q71304625', 'name': 'Matthew Morris'}, {'qId': 'Q57488067', 'name': 'Mary Morris'}, {'qId': 'Q75568259', 'name': 'Matthew Morris'}, {'qId': 'Q92420603', 'name': 'Mackenzie Morris'}, {'qId': 'Q1188592', 'name': 'MattyBRaps'}]\n",
      "\n",
      "Sarah S. Osmundson\n",
      "[{'qId': 'Q57059281', 'name': 'Sarah S. Osmundson'}]\n",
      "\n",
      "Philip D. Walker\n",
      "[{'qId': 'Q96400978', 'name': 'Philip D. Walker'}, {'qId': 'Q60025039', 'name': 'Philip Walker'}, {'qId': 'Q76759075', 'name': 'Philip Walker'}, {'qId': 'Q59292007', 'name': 'Peter Walker'}]\n",
      "\n",
      "Mary S. Dietrich\n",
      "[{'qId': 'Q89749783', 'name': 'Mary S. Dietrich'}, {'qId': 'Q95976272', 'name': 'Manuela Dietrich'}]\n",
      "\n",
      "Stephen Bruehl\n",
      "[]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for author in crossref_results['authors']:\n",
    "    name = author['givenName'] + ' ' + author['familyName']\n",
    "    hit = search_name_at_wikidata(name)\n",
    "    print(name)\n",
    "    print(hit)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C Rukasin',\n",
       " 'Christine R. F. Rukasin',\n",
       " 'C. R. F. Rukasin',\n",
       " 'C. Rukasin',\n",
       " 'CRF Rukasin',\n",
       " 'Christine R F Rukasin',\n",
       " 'C R F Rukasin',\n",
       " 'Christine Rukasin']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name = 'Christine R.F. Rukasin'\n",
    "generate_name_alternatives(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
