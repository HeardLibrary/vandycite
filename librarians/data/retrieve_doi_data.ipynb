{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "literary-montana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run only once if you need to install the python-Levenshtein package\n",
    "\n",
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install python-Levenshtein"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-listing",
   "metadata": {},
   "source": [
    "# Retrieve data using DOI\n",
    "\n",
    "Currently the script only uses the CrossRef API, but potentially could use others like DataCite\n",
    "\n",
    "## Configuration\n",
    "\n",
    "This section imports libraries, sets default values, and defines functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "wired-thumb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "#from bs4 import BeautifulSoup # web-scraping library\n",
    "import json\n",
    "from time import sleep\n",
    "import csv\n",
    "import sys\n",
    "#import math\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "#from fuzzywuzzy import process\n",
    "#import xml.etree.ElementTree as et # library to traverse XML tree\n",
    "import urllib\n",
    "import datetime\n",
    "#import string\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "accept_media_type = 'application/json'\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "user_agent_header = 'VanderBot/1.7 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "sparql_sleep = 0.1\n",
    "default_language = 'en'\n",
    "\n",
    "# Option to log to a file instead of the console\n",
    "log_path = '' # path to log file, default to none\n",
    "log_object = sys.stdout # log output defaults to the console screen\n",
    "\n",
    "opts = [opt for opt in sys.argv[1:] if opt.startswith('-')]\n",
    "args = [arg for arg in sys.argv[1:] if not arg.startswith('-')]\n",
    "\n",
    "if '--log' in opts: # set output to specified log file or path including file name\n",
    "    log_path = args[opts.index('--log')]\n",
    "    log_object = open(log_path, 'wt', encoding='utf-8') # direct output sent to log_object to log file instead of sys.stdout\n",
    "if '-L' in opts: # set output to specified log file or path including file name\n",
    "    log_path = args[opts.index('-L')]\n",
    "    log_object = open(log_path, 'wt', encoding='utf-8') # direct output sent to log_object to log file instead of sys.stdout\n",
    "\n",
    "# --------------------------\n",
    "# *** For now, hard-code logging to errors.log\n",
    "log_path = 'errors.log'\n",
    "log_object = open(log_path, 'wt', encoding='utf-8')\n",
    "# --------------------------\n",
    "\n",
    "# List of work types used by CrossRef\n",
    "work_types = [\n",
    "    {\n",
    "    'crossref_type_string': 'journal-article',\n",
    "    'qid': 'Q18918145', # academic journal article, alternatively Q13442814 scholarly article\n",
    "    'description': 'journal article'\n",
    "    },\n",
    "    {\n",
    "    'crossref_type_string': 'book-chapter',\n",
    "    'qid': 'Q21481766', # academic journal article, alternatively Q13442814 scholarly article\n",
    "    'description': 'academic book chapter'\n",
    "    }\n",
    "]\n",
    "\n",
    "# Output CSV label/description fields to be populated without references\n",
    "out_fields_labdes = ['label_' + default_language, 'description_' + default_language]\n",
    "\n",
    "# Output CSV property fields to be populated without references\n",
    "out_fields_noref = ['instance_of']\n",
    "\n",
    "# Output CSV fields that include reference fields\n",
    "out_fields_ref = ['doi', 'published', 'title_' + default_language, 'journal', 'volume', 'page', 'issue']\n",
    "    \n",
    "# generates a dictionary to be passed in a requests GET method to generate the request header\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "# Determine the current CrossRef rate limit by an initial ping\n",
    "response = requests.get('https://api.crossref.org/works/10.3233/SW-150203', headers=generate_header_dictionary(accept_media_type))\n",
    "crossref_headers = response.headers\n",
    "limit_count = int(crossref_headers['x-rate-limit-limit'])\n",
    "interval_string = crossref_headers['x-rate-limit-interval']\n",
    "interval_sec = int(interval_string[:len(interval_string)-1]) # remove the \"s\" from the end\n",
    "api_sleep = interval_sec / limit_count + 0.005\n",
    "\n",
    "def generate_sparql_header_dictionary(accept_media_type,user_agent_header):\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "# The following function requires the request header generated above\n",
    "sparql_request_header = generate_sparql_header_dictionary(accept_media_type, user_agent_header)\n",
    "\n",
    "# Generate the current UTC xsd:date\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "# Extracts the local name part of an IRI, e.g. a qNumber from a Wikidata IRI\n",
    "def extract_local_name(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    last_piece = len(pieces)\n",
    "    return pieces[last_piece - 1]\n",
    "\n",
    "# Read from a CSV file into a list of dictionaries\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# Write list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Sends a query to the query service endpoint. \n",
    "# NOTE: request_header and endpoint are global variables defined earlier in the script\n",
    "def send_sparql_query(query_string):\n",
    "    # You can delete the two print statements if the queries are short. However, for large/long queries,\n",
    "    # it's good to let the user know what's going on.\n",
    "    #print('querying SPARQL endpoint to acquire item metadata')\n",
    "    response = requests.post(endpoint, data=query_string.encode('utf-8'), headers=sparql_request_header)\n",
    "    #print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the values from the response JSON\n",
    "    results = data['results']['bindings']\n",
    "    \n",
    "    #print('done retrieving data')\n",
    "    # print(json.dumps(results, indent=2))\n",
    "    \n",
    "    sleep(sparql_sleep) # delay to avoid hitting the Query Service too fast\n",
    "    return results\n",
    "\n",
    "# Use the CrossRef registration agency test to find out who issued the non-CrosRef DOI\n",
    "def discover_issuing_agency(doi):\n",
    "    encoded_doi = urllib.parse.quote(doi)\n",
    "    search_url = 'https://api.crossref.org/works/' + encoded_doi + '/agency'\n",
    "    response = requests.get(search_url, headers=generate_header_dictionary(accept_media_type))\n",
    "    sleep(api_sleep) # delay to avoid hitting the API faster than acceptable rate\n",
    "    if response.status_code == 404: # return \"not found\" if URL doesn't dereference\n",
    "        return 'not found'\n",
    "    try: # Try to parse as JSON\n",
    "        data = response.json()\n",
    "        return data['message']['agency']['id']\n",
    "    except: # if the response isn't JSON, then just return the response text\n",
    "        return response.text\n",
    "\n",
    "# See https://github.com/CrossRef/rest-api-doc for API details\n",
    "# Note: no authentication required, but must be \"nice\": observe rate limit, provide mailto:\n",
    "def retrieve_crossref_data(doi):\n",
    "    crossref_endpoint_url = 'https://api.crossref.org/works/'\n",
    "    # urllib.parse.quote performs URL encoding of a string\n",
    "    encoded_doi = urllib.parse.quote(doi)\n",
    "    search_url = crossref_endpoint_url + encoded_doi\n",
    "    response = requests.get(search_url, headers=generate_header_dictionary(accept_media_type))\n",
    "    article_dict = {}\n",
    "    if response.status_code == 404: # return empty dict if not found\n",
    "        # *** NOTE: at some future time, look up data at alternative issuing agencies\n",
    "        # For example, see https://support.datacite.org/docs/api-get-doi\n",
    "        \n",
    "        # For now, just log it\n",
    "        print('article:', article['qid'], 'DOI issuing agency:', discover_issuing_agency(article['doi']),'\\n', file=log_object)\n",
    "        sleep(api_sleep)\n",
    "        return article_dict\n",
    "    else:\n",
    "        author_list = []\n",
    "        try:\n",
    "            response_structure = response.json()\n",
    "            data = response_structure['message']\n",
    "        except:\n",
    "            # if not JSON, just return the response text\n",
    "            article_dict['message'] = response.text\n",
    "            sleep(api_sleep)\n",
    "            return article_dict\n",
    "        #print(json.dumps(data, indent = 2))\n",
    "        article_dict['doi'] = doi\n",
    "        if 'author' in data:\n",
    "            authors = data['author']\n",
    "            for author in authors:\n",
    "                authorDict = {}\n",
    "                if 'ORCID' in author:\n",
    "                    authorDict['orcid'] = author['ORCID']\n",
    "                else:\n",
    "                    authorDict['orcid'] = ''\n",
    "                if 'sequence' in author:\n",
    "                    authorDict['sequence'] = author['sequence']\n",
    "                else:\n",
    "                    authorDict['sequence'] = ''\n",
    "                if 'given' in author:\n",
    "                    authorDict['givenName'] = author['given']\n",
    "                else:\n",
    "                    authorDict['givenName'] = ''\n",
    "                if 'family' in author:\n",
    "                    authorDict['familyName'] = author['family']\n",
    "                else:\n",
    "                    authorDict['familyName'] = ''\n",
    "                affiliationList = []\n",
    "                if 'affiliation' in author:\n",
    "                    for affiliation in author['affiliation']:\n",
    "                        affiliationList.append(affiliation['name'])\n",
    "                # if there aren't any affiliations, the list will remain empty\n",
    "                authorDict['affiliation'] = affiliationList\n",
    "                author_list.append(authorDict)\n",
    "            article_dict['authors'] = author_list\n",
    "        if 'issued' in data:\n",
    "            issued = data['issued']['date-parts'][0]\n",
    "            issued_date = str(issued[0])\n",
    "            if len(issued) > 1:\n",
    "                if len(str(issued[1])) == 1:\n",
    "                    issued_date += '-0'+ str(issued[1])\n",
    "                else:\n",
    "                    issued_date += '-'+ str(issued[1])\n",
    "                if len(issued) > 2:                \n",
    "                    if len(str(issued[2])) == 1:\n",
    "                        issued_date += '-0'+ str(issued[2])\n",
    "                    else:\n",
    "                        issued_date += '-'+ str(issued[2])\n",
    "            article_dict['published'] = issued_date\n",
    "        else:\n",
    "            article_dict['published'] = ''\n",
    "        if 'volume' in data:\n",
    "            article_dict['volume'] = data['volume']\n",
    "        else:\n",
    "            article_dict['volume'] = ''\n",
    "        if 'issue' in data:\n",
    "            article_dict['issue'] = data['issue']\n",
    "        else:\n",
    "            article_dict['issue'] = ''\n",
    "        if 'page' in data:\n",
    "            article_dict['page'] = data['page']\n",
    "        else:\n",
    "            article_dict['page'] = ''\n",
    "        if 'ISSN' in data:\n",
    "            article_dict['journal_issn'] = data['ISSN']\n",
    "        else:\n",
    "            article_dict['journal_issn'] = []\n",
    "        if 'title' in data:\n",
    "            if len(data['title']) > 0:\n",
    "                article_dict['title_' + default_language] = data['title'][0]\n",
    "                article_dict['label_' + default_language] = data['title'][0]\n",
    "        else:\n",
    "            article_dict['title_' + default_language] = ''\n",
    "            article_dict['label_' + default_language] = ''\n",
    "        if 'container-title' in data:\n",
    "            if len(data['container-title']) > 0:\n",
    "                article_dict['journal_title'] = data['container-title'][0]\n",
    "        else:\n",
    "            article_dict['journal_title'] = ''\n",
    "         \n",
    "        if 'type' in data:\n",
    "            found = False\n",
    "            for work_type in work_types:\n",
    "                if data['type'] == work_type['crossref_type_string']:\n",
    "                    found = True\n",
    "                    article_dict['instance_of'] = work_type['qid']\n",
    "                    article_dict['description_' + default_language] = work_type['description']\n",
    "            if not found:\n",
    "                article_dict['instance_of'] = ''\n",
    "                article_dict['description_' + default_language] = ''\n",
    "        else:\n",
    "            article_dict['instance_of'] = ''\n",
    "            article_dict['description_' + default_language] = ''\n",
    "\n",
    "    sleep(api_sleep)\n",
    "    return article_dict\n",
    "\n",
    "# Look up the ISSN from CrossRef in Wikidata\n",
    "def extract_journal_qid(crossref_results, article):\n",
    "    if len(crossref_results['journal_issn']) == 0:\n",
    "        crossref_results['journal'] = ''\n",
    "        print('article:', article['qid'], 'has no ISSN.\\n', file=log_object)\n",
    "        return crossref_results\n",
    "\n",
    "    # Create VALUES list for items\n",
    "    issns_string = ''\n",
    "    for issn in crossref_results['journal_issn']:\n",
    "        issns_string += '\"' + issn + '\"\\n'\n",
    "    # Remove trailing newline\n",
    "    issns_string = issns_string[:len(issns_string)-1]\n",
    "\n",
    "    # Build query string\n",
    "    query_string = '''select distinct ?journal ?journalLabel where {\n",
    "      VALUES ?issn\n",
    "        {\n",
    "    ''' + issns_string + '''\n",
    "        }\n",
    "      ?journal wdt:P236 ?issn.\n",
    "      ?journal rdfs:label ?journalLabel.\n",
    "      filter(lang(?journalLabel)=\"en\")\n",
    "    }'''\n",
    "    #print(query_string)\n",
    "\n",
    "    # Send query to endpoint\n",
    "    query_results = send_sparql_query(query_string)\n",
    "    #pp.pprint(query_results)\n",
    "\n",
    "    if len(query_results) > 1:\n",
    "        print('Warning! More than one publication in Wikidata matched the ISSN for article', article['qid'], file=log_object)\n",
    "        print(query_results, '\\n', file=log_object)\n",
    "\n",
    "    # Extract Q ID from SPARQL query results. If there is more than one result, the last one will be used for the Q ID\n",
    "    for result in query_results:\n",
    "        journal_qid = extract_local_name(result['journal']['value'])\n",
    "        journal_name = result['journalLabel']['value']\n",
    "        if journal_name != crossref_results['journal_title']:\n",
    "            # NOTE: did empirical testing to see which kind of fuzzy matching worked best\n",
    "            #ratio = fuzz.ratio(journal_name, crossref_results['journal_title'])\n",
    "            #partial_ratio = fuzz.partial_ratio(journal_name, crossref_results['journal_title'])\n",
    "            #sort_ratio = fuzz.token_sort_ratio(journal_name, crossref_results['journal_title'])\n",
    "            #set_ratio = fuzz.token_set_ratio(journal_name, crossref_results['journal_title'])\n",
    "            w_ratio = fuzz.WRatio(journal_name, crossref_results['journal_title'])\n",
    "            #print('name similarity ratio', ratio)\n",
    "            #print('partial ratio', partial_ratio)\n",
    "            #print('sort_ratio', sort_ratio)\n",
    "            #print('set_ratio', set_ratio)\n",
    "            if w_ratio < 99:\n",
    "                print('article:', article['qid'], 'w_ratio:', w_ratio, 'Warning: Wikidata journal: \"' + journal_name + '\"', journal_qid, 'does not match CrossRef journal title: \"' + crossref_results['journal_title'] + '\"\\n', file=log_object)\n",
    "        #print('article:', article['qid'], 'journal:', journal_qid, journal_name)\n",
    "    crossref_results['journal'] = journal_qid\n",
    "    return crossref_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "secondary-strength",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content-length': '107', 'cache-control': 'no-cache', 'content-type': 'text/html', 'connection': 'close'}\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "patient-customs",
   "metadata": {},
   "source": [
    "## Retrieve DOI data for existing work records\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "smart-tract",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "filename = 'articles.csv'\n",
    "articles = read_dicts_from_csv(filename)\n",
    "for article in articles[50:100]:\n",
    "    if article['doi'] != '':\n",
    "        crossref_results = retrieve_crossref_data(article['doi'])\n",
    "        if crossref_results != {}:        \n",
    "            crossref_results = extract_journal_qid(crossref_results, article)\n",
    "\n",
    "if log_path != '':\n",
    "    log_object.close()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opened-compilation",
   "metadata": {},
   "source": [
    "## Retrieve DOI data for new record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "antique-listing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label_en': 'Stealing the Limelight? Examining the Relationship Between New Librarians and Their Supervisors', 'description_en': 'journal article', 'instance_of_uuid': '', 'instance_of': 'Q18918145', 'doi_uuid': '', 'doi': '10.1016/j.acalib.2014.10.004', 'doi_ref1_hash': '', 'doi_ref1_referenceUrl': 'http://doi.org/10.1016/j.acalib.2014.10.004', 'doi_ref1_retrieved_nodeId': '', 'doi_ref1_retrieved_val': '2021-03-19', 'doi_ref1_retrieved_prec': '', 'published_uuid': '', 'published_nodeId': '', 'published_val': '2014-11', 'published_prec': '', 'published_ref1_hash': '', 'published_ref1_referenceUrl': 'http://doi.org/10.1016/j.acalib.2014.10.004', 'published_ref1_retrieved_nodeId': '', 'published_ref1_retrieved_val': '2021-03-19', 'published_ref1_retrieved_prec': '', 'title_en_uuid': '', 'title_en': 'Stealing the Limelight? Examining the Relationship Between New Librarians and Their Supervisors', 'title_en_ref1_hash': '', 'title_en_ref1_referenceUrl': 'http://doi.org/10.1016/j.acalib.2014.10.004', 'title_en_ref1_retrieved_nodeId': '', 'title_en_ref1_retrieved_val': '2021-03-19', 'title_en_ref1_retrieved_prec': '', 'journal_uuid': '', 'journal': 'Q7743547', 'journal_ref1_hash': '', 'journal_ref1_referenceUrl': 'http://doi.org/10.1016/j.acalib.2014.10.004', 'journal_ref1_retrieved_nodeId': '', 'journal_ref1_retrieved_val': '2021-03-19', 'journal_ref1_retrieved_prec': '', 'volume_uuid': '', 'volume': '40', 'volume_ref1_hash': '', 'volume_ref1_referenceUrl': 'http://doi.org/10.1016/j.acalib.2014.10.004', 'volume_ref1_retrieved_nodeId': '', 'volume_ref1_retrieved_val': '2021-03-19', 'volume_ref1_retrieved_prec': '', 'page_uuid': '', 'page': '597-603', 'page_ref1_hash': '', 'page_ref1_referenceUrl': 'http://doi.org/10.1016/j.acalib.2014.10.004', 'page_ref1_retrieved_nodeId': '', 'page_ref1_retrieved_val': '2021-03-19', 'page_ref1_retrieved_prec': '', 'issue_uuid': '', 'issue': '6', 'issue_ref1_hash': '', 'issue_ref1_referenceUrl': 'http://doi.org/10.1016/j.acalib.2014.10.004', 'issue_ref1_retrieved_nodeId': '', 'issue_ref1_retrieved_val': '2021-03-19', 'issue_ref1_retrieved_prec': ''}\n"
     ]
    }
   ],
   "source": [
    "doi = '10.1016/j.acalib.2014.10.004'\n",
    "today = generate_utc_date()\n",
    "\n",
    "filename = 'articles.csv'\n",
    "articles = read_dicts_from_csv(filename)\n",
    "fieldnames = articles[0].keys() # get the field names from the existing file\n",
    "\n",
    "crossref_results = retrieve_crossref_data(doi)\n",
    "if crossref_results != {}:        \n",
    "    crossref_results = extract_journal_qid(crossref_results, article)\n",
    "    #print(crossref_results)\n",
    "    \n",
    "    out_dict = {}\n",
    "    for field in out_fields_labdes:   \n",
    "        #print(field, crossref_results[field])\n",
    "        out_dict[field] = crossref_results[field]\n",
    "    #print()\n",
    "    for field in out_fields_noref:   \n",
    "        #print(field, crossref_results[field])\n",
    "        out_dict[field + '_uuid'] = ''\n",
    "        out_dict[field] = crossref_results[field]\n",
    "    #print()\n",
    "    for field in out_fields_ref:\n",
    "        #print(field, crossref_results[field])\n",
    "        out_dict[field + '_uuid'] = ''\n",
    "        if field == 'published':\n",
    "            out_dict[field + '_nodeId'] = ''\n",
    "            out_dict[field + '_val'] = crossref_results[field]\n",
    "            out_dict[field + '_prec'] = ''\n",
    "        else:\n",
    "            out_dict[field] = crossref_results[field]\n",
    "        # Only add a reference if there is a value for that field\n",
    "        if crossref_results[field] == '':\n",
    "            out_dict[field + '_ref1_hash'] = ''\n",
    "            out_dict[field + '_ref1_referenceUrl'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_nodeId'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_val'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_prec'] = ''\n",
    "        else:\n",
    "            out_dict[field + '_ref1_hash'] = ''\n",
    "            out_dict[field + '_ref1_referenceUrl'] = 'http://doi.org/' + doi\n",
    "            out_dict[field + '_ref1_retrieved_nodeId'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_val'] = today\n",
    "            out_dict[field + '_ref1_retrieved_prec'] = ''\n",
    "\n",
    "    #print()\n",
    "print(out_dict)\n",
    "articles.append(out_dict)\n",
    "\n",
    "write_dicts_to_csv(articles, filename, fieldnames)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "hundred-electricity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"authors\": [\n",
      "    {\n",
      "      \"orcid\": \"http://orcid.org/0000-0003-2551-019X\",\n",
      "      \"sequence\": \"first\",\n",
      "      \"givenName\": \"Lori\",\n",
      "      \"familyName\": \"Schirle\",\n",
      "      \"affiliation\": []\n",
      "    },\n",
      "    {\n",
      "      \"orcid\": \"\",\n",
      "      \"sequence\": \"additional\",\n",
      "      \"givenName\": \"Amanda L.\",\n",
      "      \"familyName\": \"Stone\",\n",
      "      \"affiliation\": []\n",
      "    },\n",
      "    {\n",
      "      \"orcid\": \"\",\n",
      "      \"sequence\": \"additional\",\n",
      "      \"givenName\": \"Matthew C.\",\n",
      "      \"familyName\": \"Morris\",\n",
      "      \"affiliation\": []\n",
      "    },\n",
      "    {\n",
      "      \"orcid\": \"\",\n",
      "      \"sequence\": \"additional\",\n",
      "      \"givenName\": \"Sarah S.\",\n",
      "      \"familyName\": \"Osmundson\",\n",
      "      \"affiliation\": []\n",
      "    },\n",
      "    {\n",
      "      \"orcid\": \"\",\n",
      "      \"sequence\": \"additional\",\n",
      "      \"givenName\": \"Philip D.\",\n",
      "      \"familyName\": \"Walker\",\n",
      "      \"affiliation\": []\n",
      "    },\n",
      "    {\n",
      "      \"orcid\": \"\",\n",
      "      \"sequence\": \"additional\",\n",
      "      \"givenName\": \"Mary S.\",\n",
      "      \"familyName\": \"Dietrich\",\n",
      "      \"affiliation\": []\n",
      "    },\n",
      "    {\n",
      "      \"orcid\": \"\",\n",
      "      \"sequence\": \"additional\",\n",
      "      \"givenName\": \"Stephen\",\n",
      "      \"familyName\": \"Bruehl\",\n",
      "      \"affiliation\": []\n",
      "    }\n",
      "  ],\n",
      "  \"issued\": \"2020-06-11\",\n",
      "  \"volume\": \"9\",\n",
      "  \"issue\": \"1\",\n",
      "  \"pages\": \"\",\n",
      "  \"type\": \"journal-article\",\n",
      "  \"journal_issn\": [\n",
      "    \"2046-4053\"\n",
      "  ],\n",
      "  \"journal_title\": \"Systematic Reviews\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "touched-portsmouth",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
