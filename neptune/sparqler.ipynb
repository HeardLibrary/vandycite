{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Template code for interacting with Wikidata\n",
    "\n",
    "This notebook is a collection of functions and code blocks to use when interacting with Wikidata or other instances of Wikibase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2022 Steven J. Baskauf\n",
    "# This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "# Date: 2022-06-07\n",
    "\n",
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "import datetime\n",
    "\n",
    "\n",
    "class Sparqler:\n",
    "    \"\"\"Build SPARQL queries of various sorts\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    useragent : str\n",
    "        Required if using the Wikidata Query Service, otherwise optional.\n",
    "        Use the form: appname/v.v (URL; mailto:email@domain.com)\n",
    "        See https://meta.wikimedia.org/wiki/User-Agent_policy\n",
    "    endpoint: URL\n",
    "        Defaults to Wikidata Query Service if not provided.\n",
    "    method: str\n",
    "        Possible values are \"post\" (default) or \"get\". Use \"get\" if read-only query endpoint.\n",
    "        Must be \"post\" for update endpoint.\n",
    "    sleep: float\n",
    "        Number of seconds to wait between queries. Defaults to 0.1\n",
    "        \n",
    "    Required modules:\n",
    "    -------------\n",
    "    import requests\n",
    "    \n",
    "    from time import sleep\n",
    "    \"\"\"\n",
    "    def __init__(self, method='post', endpoint='https://query.wikidata.org/sparql', useragent=None, sleep=0.1):\n",
    "        # attributes for all methods\n",
    "        self.http_method = method\n",
    "        self.endpoint = endpoint\n",
    "        if useragent is None:\n",
    "            if self.endpoint == 'https://query.wikidata.org/sparql':\n",
    "                print('You must provide a value for the useragent argument when using the Wikidata Query Service.')\n",
    "                print()\n",
    "                raise KeyboardInterrupt # Use keyboard interrupt instead of sys.exit() because it works in Jupyter notebooks\n",
    "        self.sleep = sleep\n",
    "\n",
    "        self.requestheader = {}\n",
    "        if useragent:\n",
    "            self.requestheader['User-Agent'] = useragent\n",
    "        \n",
    "        if self.http_method == 'post':\n",
    "            self.requestheader['Content-Type'] = 'application/x-www-form-urlencoded'\n",
    "\n",
    "    def query(self, query_string, form='select', verbose=False, **kwargs):\n",
    "        \"\"\"Sends a SPARQL query to the endpoint.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        form : str\n",
    "            The SPARQL query form.\n",
    "            Possible values are: \"select\" (default), \"ask\", \"construct\", and \"describe\".\n",
    "        mediatype: str\n",
    "            The response media type (MIME type) of the query results.\n",
    "            Some possible values for \"select\" and \"ask\" are: \"application/sparql-results+json\" (default) and \"application/sparql-results+xml\".\n",
    "            Some possible values for \"construct\" and \"describe\" are: \"text/turtle\" (default) and \"application/rdf+xml\".\n",
    "            See https://docs.aws.amazon.com/neptune/latest/userguide/sparql-media-type-support.html#sparql-serialization-formats-neptune-output\n",
    "            for response serializations supported by Neptune.\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        default: list of str\n",
    "            The graphs to be merged to form the default graph. List items must be URIs in string form.\n",
    "            If omitted, no graphs will be specified and default graph composition will be controlled by FROM clauses\n",
    "            in the query itself. \n",
    "            See https://www.w3.org/TR/sparql11-query/#namedGraphs and https://www.w3.org/TR/sparql11-protocol/#dataset\n",
    "            for details.\n",
    "        named: list of str\n",
    "            Graphs that may be specified by IRI in a query. List items must be URIs in string form.\n",
    "            If omitted, named graphs will be specified by FROM NAMED clauses in the query itself.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        If the form is \"select\" and mediatype is \"application/json\", a list of dictionaries containing the data.\n",
    "        If the form is \"ask\" and mediatype is \"application/json\", a boolean is returned.\n",
    "        If the mediatype is \"application/json\" and an error occurs, None is returned.\n",
    "        For other forms and mediatypes, the raw output is returned.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        To get UTF-8 text in the SPARQL queries to work properly, send URL-encoded text rather than raw text.\n",
    "        That is done automatically by the requests module for GET. I guess it also does it for POST when the\n",
    "        data are sent as a dict with the urlencoded header. \n",
    "        See SPARQL 1.1 protocol notes at https://www.w3.org/TR/sparql11-protocol/#query-operation        \n",
    "        \"\"\"\n",
    "        query_form = form\n",
    "        if 'mediatype' in kwargs:\n",
    "            media_type = kwargs['mediatype']\n",
    "        else:\n",
    "            if query_form == 'construct' or query_form == 'describe':\n",
    "            #if query_form == 'construct':\n",
    "                media_type = 'text/turtle'\n",
    "            else:\n",
    "                media_type = 'application/sparql-results+json' # default for SELECT and ASK query forms\n",
    "        self.requestheader['Accept'] = media_type\n",
    "            \n",
    "        # Build the payload dictionary (query and graph data) to be sent to the endpoint\n",
    "        payload = {'query' : query_string}\n",
    "        if 'default' in kwargs:\n",
    "            payload['default-graph-uri'] = kwargs['default']\n",
    "        \n",
    "        if 'named' in kwargs:\n",
    "            payload['named-graph-uri'] = kwargs['named']\n",
    "\n",
    "        if verbose:\n",
    "            print('querying SPARQL endpoint')\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "        if self.http_method == 'post':\n",
    "            response = requests.post(self.endpoint, data=payload, headers=self.requestheader)\n",
    "        else:\n",
    "            response = requests.get(self.endpoint, params=payload, headers=self.requestheader)\n",
    "        elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        self.response = response.text\n",
    "        sleep(self.sleep) # Throttle as a courtesy to avoid hitting the endpoint too fast.\n",
    "\n",
    "        if verbose:\n",
    "            print('done retrieving data in', int(elapsed_time), 's')\n",
    "\n",
    "        if query_form == 'construct' or query_form == 'describe':\n",
    "            return response.text\n",
    "        else:\n",
    "            if media_type != 'application/sparql-results+json':\n",
    "                return response.text\n",
    "            else:\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                except:\n",
    "                    return None # Returns no value if an error. \n",
    "\n",
    "                if query_form == 'select':\n",
    "                    # Extract the values from the response JSON\n",
    "                    results = data['results']['bindings']\n",
    "                else:\n",
    "                    results = data['boolean'] # True or False result from ASK query \n",
    "                return results           \n",
    "\n",
    "    def update(self, request_string, mediatype='application/json', verbose=False, **kwargs):\n",
    "        \"\"\"Sends a SPARQL update to the endpoint.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        mediatype : str\n",
    "            The response media type (MIME type) from the endpoint after the update.\n",
    "            Default is \"application/json\"; probably no need to use anything different.\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        default: list of str\n",
    "            The graphs to be merged to form the default graph. List items must be URIs in string form.\n",
    "            If omitted, no graphs will be specified and default graph composition will be controlled by USING\n",
    "            clauses in the query itself. \n",
    "            See https://www.w3.org/TR/sparql11-update/#deleteInsert\n",
    "            and https://www.w3.org/TR/sparql11-protocol/#update-operation for details.\n",
    "        named: list of str\n",
    "            Graphs that may be specified by IRI in the graph pattern. List items must be URIs in string form.\n",
    "            If omitted, named graphs will be specified by USING NAMED clauses in the query itself.\n",
    "        \"\"\"\n",
    "        media_type = mediatype\n",
    "        self.requestheader['Accept'] = media_type\n",
    "        \n",
    "        # Build the payload dictionary (update request and graph data) to be sent to the endpoint\n",
    "        payload = {'update' : request_string}\n",
    "        if 'default' in kwargs:\n",
    "            payload['using-graph-uri'] = kwargs['default']\n",
    "        \n",
    "        if 'named' in kwargs:\n",
    "            payload['using-named-graph-uri'] = kwargs['named']\n",
    "\n",
    "        if verbose:\n",
    "            print('beginning update')\n",
    "            \n",
    "        start_time = datetime.datetime.now()\n",
    "        response = requests.post(self.endpoint, data=payload, headers=self.requestheader)\n",
    "        elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        self.response = response.text\n",
    "        sleep(self.sleep) # Throttle as a courtesy to avoid hitting the endpoint too fast.\n",
    "\n",
    "        if verbose:\n",
    "            print('done updating data in', int(elapsed_time), 's')\n",
    "\n",
    "        if media_type != 'application/json':\n",
    "            return response.text\n",
    "        else:\n",
    "            try:\n",
    "                data = response.json()\n",
    "            except:\n",
    "                return None # Returns no value if an error converting to JSON (e.g. plain text) \n",
    "            return data           \n",
    "\n",
    "    def load(self, file_location, graph_uri, s3='', verbose=False, **kwargs):\n",
    "        \"\"\"Loads an RDF document into a specified graph.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        s3 : str\n",
    "            Name of an AWS S3 bucket containing the file. Omit load a generic URL.\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        The triplestore may or may not rely on receiving a correct Content-Type header with the file to\n",
    "        determine the type of serialization. Blazegraph requires it, AWS Neptune does not and apparently\n",
    "        interprets serialization based on the file extension.\n",
    "        \"\"\"\n",
    "        if s3:\n",
    "            request_string = 'LOAD <https://' + s3 + '.s3.amazonaws.com/' + file_location + '> INTO GRAPH <' + graph_uri + '>'\n",
    "        else:\n",
    "            request_string = 'LOAD <' + file_location + '> INTO GRAPH <' + graph_uri + '>'\n",
    "        \n",
    "        if verbose:\n",
    "            print('Loading file:', file_location, ' into graph: ', graph_uri)\n",
    "        data = self.update(request_string, verbose=verbose)\n",
    "        return data\n",
    "\n",
    "    def drop(self, graph_uri, verbose=False, **kwargs):\n",
    "        \"\"\"Drop a specified graph.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        \"\"\"\n",
    "        request_string = 'DROP GRAPH <' + graph_uri + '>'\n",
    "\n",
    "        if verbose:\n",
    "            print('Deleting graph:', graph_uri)\n",
    "        data = self.update(request_string, verbose=verbose)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test querying\n",
    "\n",
    "Query WDQS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\n",
    "    {'string': '尼可罗·马基亚维利', 'language_code': 'zh'},\n",
    "    {'string': '\"I Hate You For Hitting My Mother,\" Minneapolis', 'language_code': 'en'},\n",
    "    {'string': \"A Picture from an Outline of Women's Manners - The Wedding Ceremony\", 'language_code': 'en'}    \n",
    "]\n",
    "\n",
    "values = ''\n",
    "for label in labels:\n",
    "    values += \"'''\" + label['string'] + \"'''@\" + label['language_code'] + '\\n'\n",
    "\n",
    "query_string = '''select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + values + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "#print(query)\n",
    "\n",
    "user_agent = 'VanderBot/1.9 (https://github.com/HeardLibrary/linked-data/tree/master/publications; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "wdqs = Sparqler(useragent=user_agent)\n",
    "data = wdqs.query(query_string)\n",
    "if wdqs.response[0] == '{':\n",
    "    print('no error')\n",
    "else:\n",
    "    print('error')\n",
    "print()\n",
    "print(json.dumps(data, indent=2))\n",
    "# print(wdqs.response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query public endpoint of Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://nomenclature_2022-02-02\n",
    "# http://AATOut_2Terms\n",
    "\n",
    "query_string1 = '''select distinct ?graph where {\n",
    "graph ?graph {?s ?o ?p.}\n",
    "}'''\n",
    "\n",
    "query_string = '''select distinct ?s ?o ?p where {\n",
    "?s ?o ?p.\n",
    "}\n",
    "limit 5'''\n",
    "\n",
    "from_graphs = ['http://nomenclature_2022-02-02', 'http://AATOut_2Terms']\n",
    "endpoint_url = 'https://5j6diw4i0h.execute-api.us-east-1.amazonaws.com/sparql'\n",
    "sve = Sparqler(endpoint=endpoint_url, method='get')\n",
    "#data = sve.query(query_string)\n",
    "data = sve.query(query_string, default=from_graphs)\n",
    "if sve.response[0] == '{':\n",
    "    print('no error')\n",
    "else:\n",
    "    print('error')\n",
    "print()\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test SPARQL Update using SSH tunnel/Neptune write endpoint\n",
    "\n",
    "Insert data (one triple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_string = 'INSERT DATA { <https://test.com/s> <https://test.com/p> <https://test.com/o> . }'\n",
    "endpoint_url = 'https://triplestore1.cluster-cml0hq81gymg.us-east-1.neptune.amazonaws.com:8182/sparql'\n",
    "neptune = Sparqler(endpoint=endpoint_url, sleep=0)\n",
    "data = neptune.update(request_string, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query to see if the triple is there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = '''select distinct ?o ?p where {\n",
    "<https://test.com/s> ?o ?p.\n",
    "}'''\n",
    "\n",
    "endpoint_url = 'https://5j6diw4i0h.execute-api.us-east-1.amazonaws.com/sparql'\n",
    "sve = Sparqler(endpoint=endpoint_url, method='get')\n",
    "data = sve.query(query_string)\n",
    "#data = sve.query(query_string, default=from_graphs)\n",
    "if sve.response[0] == '{':\n",
    "    print('no error')\n",
    "else:\n",
    "    print('error')\n",
    "print()\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the triple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_string = 'DELETE DATA { <https://test.com/s> <https://test.com/p> <https://test.com/o> . }'\n",
    "data = neptune.update(request_string, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test file load from S3 bucket, then drop it\n",
    "\n",
    "Load the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_bucket_name = 'triplestore-upload'\n",
    "filename = 'bluffton.ttl'\n",
    "file_url = 'https://triplestore-upload.s3.amazonaws.com/bluffton.ttl'\n",
    "graph_name = 'http://bluffton'\n",
    "endpoint_url = 'https://triplestore1.cluster-cml0hq81gymg.us-east-1.neptune.amazonaws.com:8182/sparql'\n",
    "neptune = Sparqler(endpoint=endpoint_url, sleep=0)\n",
    "data = neptune.load(filename, graph_name, s3=s3_bucket_name, verbose=True)\n",
    "#data = neptune.load(file_url, graph_name, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if the data are there"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = '''select distinct ?s ?o ?p where {\n",
    "?s ?o ?p.\n",
    "}\n",
    "limit 5'''\n",
    "\n",
    "from_graphs = ['http://bluffton']\n",
    "endpoint_url = 'https://5j6diw4i0h.execute-api.us-east-1.amazonaws.com/sparql'\n",
    "sve = Sparqler(endpoint=endpoint_url, method='get')\n",
    "data = sve.query(query_string, default=from_graphs)\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_name = 'http://bluffton'\n",
    "data = neptune.drop(graph_name, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
