{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loader script for Neptune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2022 Vanderbilt University, except Sparqler class: (c) Steven J. Baskauf (same license)\n",
    "# This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "# Date: 2022-06-06\n",
    "\n",
    "# ----------------\n",
    "# Configuration\n",
    "# ----------------\n",
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "import datetime\n",
    "import os\n",
    "from os.path import exists\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# AWS Python SDK\n",
    "import boto3\n",
    "# import botocore  # not used\n",
    "\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import re # regex\n",
    "import urllib.parse\n",
    "\n",
    "# Global variables\n",
    "loader_endpoint_url = 'https://triplestore1.cluster-cml0hq81gymg.us-east-1.neptune.amazonaws.com:8182/sparql'\n",
    "#reader_endpoint_url = 'https://5j6diw4i0h.execute-api.us-east-1.amazonaws.com/sparql'\n",
    "reader_endpoint_url = 'https://sparql.vanderbilt.edu/sparql'\n",
    "local_upload_directory = '/Users/baskausj/triplestore_upload/'\n",
    "s3_bucket_name = 'triplestore-upload'\n",
    "utc_offset = '-06:00'\n",
    "\n",
    "# See https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3.html#uploads\n",
    "# local_file_path = local_directory + local_filename\n",
    "# s3_file_key = local_filename\n",
    "# s3_file_key = s3_iiif_project_directory + '/' + subdirectory + '/' + local_filename\n",
    "\n",
    "def update_upload_status(row_index, dataframe, field, filename, message):\n",
    "    \"\"\"Adds status message to DataFrame and saves out to CSV in case of crash.\"\"\"\n",
    "    dataframe.at[row_index, field] = message\n",
    "    dataframe.to_csv(filename, index = False)\n",
    "\n",
    "def parse_filename(filename):\n",
    "    \"\"\"Separates and returns the root of a filename and the extension as a tuple.\"\"\"\n",
    "    pieces = filename.split('.')\n",
    "    file_name_root = '.'.join(pieces[:-1])\n",
    "    extension = pieces[len(pieces)-1]\n",
    "    return file_name_root, extension\n",
    "\n",
    "def csv_read(path, **kwargs):\n",
    "    \"\"\"Loads a CSV table into a Pandas DataFrame with all cells as strings and blank cells as empty strings\n",
    "    \n",
    "    Keyword argument:\n",
    "    rows -- the number of rows of the table to return when used for testing. When omitted, all rows are returned.\n",
    "    \"\"\"\n",
    "    dataframe = pd.read_csv(path, na_filter=False, dtype = str)\n",
    "    if 'rows' in kwargs:\n",
    "        return dataframe.head(kwargs['rows']).copy(deep=True)\n",
    "    else:\n",
    "        return dataframe\n",
    "\n",
    "class Sparqler:\n",
    "    \"\"\"Build SPARQL queries of various sorts\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    useragent : str\n",
    "        Required if using the Wikidata Query Service, otherwise optional.\n",
    "        Use the form: appname/v.v (URL; mailto:email@domain.com)\n",
    "        See https://meta.wikimedia.org/wiki/User-Agent_policy\n",
    "    endpoint: URL\n",
    "        Defaults to Wikidata Query Service if not provided.\n",
    "    method: str\n",
    "        Possible values are \"post\" (default) or \"get\". Use \"get\" if read-only query endpoint.\n",
    "        Must be \"post\" for update endpoint.\n",
    "    sleep: float\n",
    "        Number of seconds to wait between queries. Defaults to 0.1\n",
    "        \n",
    "    Required modules:\n",
    "    -------------\n",
    "    import requests\n",
    "    from time import sleep\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # attributes for all methods\n",
    "        try:\n",
    "            self.http_method = kwargs['method']\n",
    "        except:\n",
    "            self.http_method = 'post' # default to POST\n",
    "        try:\n",
    "            self.endpoint = kwargs['endpoint']\n",
    "        except:\n",
    "            self.endpoint = 'https://query.wikidata.org/sparql' # default to Wikidata endpoint\n",
    "        try:\n",
    "            self.useragent = kwargs['useragent']\n",
    "        except:\n",
    "            if self.endpoint == 'https://query.wikidata.org/sparql':\n",
    "                print('You must provide a value for the useragent argument when using the Wikidata Query Service.')\n",
    "                print()\n",
    "                raise KeyboardInterrupt # Use keyboard interrupt instead of sys.exit() because it works in Jupyter notebooks\n",
    "            else:\n",
    "                self.useragent = ''\n",
    "        try:\n",
    "            self.sleep = kwargs['sleep']\n",
    "        except:\n",
    "            self.sleep = 0.1 # default throtting of 0.1 seconds\n",
    "\n",
    "        self.requestheader = {}\n",
    "        if self.useragent:\n",
    "            self.requestheader['User-Agent'] = self.useragent\n",
    "        \n",
    "        if self.http_method == 'post':\n",
    "            self.requestheader['Content-Type'] = 'application/x-www-form-urlencoded'\n",
    "\n",
    "    def query(self, query_string, **kwargs):\n",
    "        \"\"\"Sends a SPARQL query to the endpoint.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        form : str\n",
    "            The SPARQL query form.\n",
    "            Possible values are: \"select\" (default), \"ask\", \"construct\", and \"describe\".\n",
    "        mediatype: str\n",
    "            The response media type (MIME type) of the query results.\n",
    "            Some possible values for \"select\" and \"ask\" are: \"application/sparql-results+json\" (default) and \"application/sparql-results+xml\".\n",
    "            Some possible values for \"construct\" and \"describe\" are: \"text/turtle\" (default) and \"application/rdf+xml\".\n",
    "            See https://docs.aws.amazon.com/neptune/latest/userguide/sparql-media-type-support.html#sparql-serialization-formats-neptune-output\n",
    "            for response serializations supported by Neptune.\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        default: list of str\n",
    "            The graphs to be merged to form the default graph. List items must be URIs in string form.\n",
    "            If omitted, no graphs will be specified and default graph composition will be controlled by FROM clauses\n",
    "            in the query itself. \n",
    "            See https://www.w3.org/TR/sparql11-query/#namedGraphs and https://www.w3.org/TR/sparql11-protocol/#dataset\n",
    "            for details.\n",
    "        named: list of str\n",
    "            Graphs that may be specified by IRI in a query. List items must be URIs in string form.\n",
    "            If omitted, named graphs will be specified by FROM NAMED clauses in the query itself.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        If the form is \"select\" and mediatype is \"application/json\", a list of dictionaries containing the data.\n",
    "        If the form is \"ask\" and mediatype is \"application/json\", a boolean is returned.\n",
    "        If the mediatype is \"application/json\" and an error occurs, None is returned.\n",
    "        For other forms and mediatypes, the raw output is returned.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        To get UTF-8 text in the SPARQL queries to work properly, send URL-encoded text rather than raw text.\n",
    "        That is done automatically by the requests module for GET. I guess it also does it for POST when the\n",
    "        data are sent as a dict with the urlencoded header. \n",
    "        See SPARQL 1.1 protocol notes at https://www.w3.org/TR/sparql11-protocol/#query-operation        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            query_form = kwargs['form']\n",
    "        except:\n",
    "            query_form = 'select' # default to SELECT query form\n",
    "        try:\n",
    "            media_type = kwargs['mediatype']\n",
    "        except:\n",
    "            #if query_form == 'construct' or query_form == 'describe':\n",
    "            if query_form == 'construct':\n",
    "                media_type = 'text/turtle'\n",
    "            else:\n",
    "                media_type = 'application/sparql-results+json' # default for SELECT and ASK query forms\n",
    "        self.requestheader['Accept'] = media_type\n",
    "        try:\n",
    "            verbose = kwargs['verbose']\n",
    "        except:\n",
    "            verbose = False # default to no printouts\n",
    "            \n",
    "        # Build the payload dictionary (query and graph data) to be sent to the endpoint\n",
    "        payload = {'query' : query_string}\n",
    "        try:\n",
    "            payload['default-graph-uri'] = kwargs['default']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            payload['named-graph-uri'] = kwargs['named']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if verbose:\n",
    "            print('querying SPARQL endpoint')\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "        if self.http_method == 'post':\n",
    "            response = requests.post(self.endpoint, data=payload, headers=self.requestheader)\n",
    "        else:\n",
    "            response = requests.get(self.endpoint, params=payload, headers=self.requestheader)\n",
    "        elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        self.response = response.text\n",
    "        sleep(self.sleep) # Throttle as a courtesy to avoid hitting the endpoint too fast.\n",
    "\n",
    "        if verbose:\n",
    "            print('done retrieving data in', int(elapsed_time), 's')\n",
    "\n",
    "        if query_form == 'construct' or query_form == 'describe':\n",
    "            return response.text\n",
    "        else:\n",
    "            if media_type != 'application/sparql-results+json':\n",
    "                return response.text\n",
    "            else:\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                except:\n",
    "                    return None # Returns no value if an error. \n",
    "\n",
    "                if query_form == 'select':\n",
    "                    # Extract the values from the response JSON\n",
    "                    results = data['results']['bindings']\n",
    "                else:\n",
    "                    results = data['boolean'] # True or False result from ASK query \n",
    "                return results           \n",
    "\n",
    "    def update(self, request_string, **kwargs):\n",
    "        \"\"\"Sends a SPARQL update to the endpoint.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        mediatype : str\n",
    "            The response media type (MIME type) from the endpoint after the update.\n",
    "            Default is \"application/json\"; probably no need to use anything different.\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        default: list of str\n",
    "            The graphs to be merged to form the default graph. List items must be URIs in string form.\n",
    "            If omitted, no graphs will be specified and default graph composition will be controlled by USING\n",
    "            clauses in the query itself. \n",
    "            See https://www.w3.org/TR/sparql11-update/#deleteInsert\n",
    "            and https://www.w3.org/TR/sparql11-protocol/#update-operation for details.\n",
    "        named: list of str\n",
    "            Graphs that may be specified by IRI in the graph pattern. List items must be URIs in string form.\n",
    "            If omitted, named graphs will be specified by USING NAMED clauses in the query itself.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            media_type = kwargs['mediatype']\n",
    "        except:\n",
    "            media_type = 'application/json' # default response type after update\n",
    "        self.requestheader['Accept'] = media_type\n",
    "        try:\n",
    "            verbose = kwargs['verbose']\n",
    "        except:\n",
    "            verbose = False # default to no printouts\n",
    "        \n",
    "        # Build the payload dictionary (update request and graph data) to be sent to the endpoint\n",
    "        payload = {'update' : request_string}\n",
    "        try:\n",
    "            payload['using-graph-uri'] = kwargs['default']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            payload['using-named-graph-uri'] = kwargs['named']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if verbose:\n",
    "            print('  beginning update')\n",
    "            \n",
    "        start_time = datetime.datetime.now()\n",
    "        response = requests.post(self.endpoint, data=payload, headers=self.requestheader)\n",
    "        elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        self.response = response.text\n",
    "        sleep(self.sleep) # Throttle as a courtesy to avoid hitting the endpoint too fast.\n",
    "\n",
    "        if verbose:\n",
    "            print('  done updating data in', int(elapsed_time), 's')\n",
    "\n",
    "        if media_type != 'application/json':\n",
    "            return response.text\n",
    "        else:\n",
    "            try:\n",
    "                data = response.json()\n",
    "            except:\n",
    "                return None # Returns no value if an error converting to JSON (e.g. plain text) \n",
    "            return data           \n",
    "\n",
    "    def load(self, file_location, graph_uri, **kwargs):\n",
    "        \"\"\"Loads an RDF document into a specified graph.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        s3 : str\n",
    "            Name of an AWS S3 bucket containing the file. Omit load a generic URL.\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        The triplestore may or may not rely on receiving a correct Content-Type header with the file to\n",
    "        determine the type of serialization. Blazegraph requires it, AWS Neptune does not and apparently\n",
    "        interprets serialization based on the file extension.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            s3 = kwargs['s3']\n",
    "        except:\n",
    "            s3 = ''\n",
    "        try:\n",
    "            verbose = kwargs['verbose']\n",
    "        except:\n",
    "            verbose = False # default to no printouts\n",
    "\n",
    "        if s3:\n",
    "            request_string = 'LOAD <https://' + s3 + '.s3.amazonaws.com/' + file_location + '> INTO GRAPH <' + graph_uri + '>'\n",
    "        else:\n",
    "            request_string = 'LOAD <' + file_location + '> INTO GRAPH <' + graph_uri + '>'\n",
    "        \n",
    "        if verbose:\n",
    "            print('Loading file:', file_location, ' into graph: ', graph_uri)\n",
    "        data = self.update(request_string, verbose=verbose)\n",
    "        return data\n",
    "\n",
    "    def drop(self, graph_uri, **kwargs):\n",
    "        \"\"\"Drop a specified graph.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            verbose = kwargs['verbose']\n",
    "        except:\n",
    "            verbose = False # default to no printouts\n",
    "\n",
    "        request_string = 'DROP GRAPH <' + graph_uri + '>'\n",
    "\n",
    "        if verbose:\n",
    "            print('Deleting graph:', graph_uri)\n",
    "        data = self.update(request_string, verbose=verbose)\n",
    "        return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect and test the SSH tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run this cell until we figure out how to make this command complete. The cell never stops executing and\n",
    "# that locks up the rest of the notebook. For now, run it in a separate terminal window (left open).\n",
    "\n",
    "# The EC2 server acting as the SSH tunnel has the IP address: 35.173.230.91\n",
    "\n",
    "# The following lines need to be added to the ~/.ssh/config file:\n",
    "\n",
    "\"\"\"\n",
    "host neptune\n",
    " ForwardAgent yes\n",
    " User ec2-user\n",
    " HostName 172.31.90.204\n",
    " IdentitiesOnly yes\n",
    " IdentityFile ~/NeptuneSSHtunnel.pem\n",
    " LocalForward 8182 triplestore1.cluster-cml0hq81gymg.us-east-1.neptune.amazonaws.com:8182\n",
    "\"\"\"\n",
    "\n",
    "# and the NeptuneSSHtunnel.pem file needs to be in the home directory\n",
    "# NOTE: The HostName is the public IP address of the EC2 server. It will change if the server is restarted.\n",
    "# You can get it from the AWS console.\n",
    "\n",
    "# !!!!! As of 2022-09-12, this does not work. Probably need to use an actual SSH Python\n",
    "# library.\n",
    "\n",
    "# Start up SSH tunnel\n",
    "#os.system('ssh neptune -N &') # start as a background process\n",
    "#os.system('echo $! > process_id.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Does not  yet work\n",
    "\n",
    "The following line is here for testing, but in production will be added at the end of the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('process_id.txt', 'rt') as file_object:\n",
    "    process_id_string = file_object.read()\n",
    "print(process_id_string)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the SSH tunnel \n",
    "os.system('kill -9 ' + process_id_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acceptedQueryCount': 627, 'runningQueryCount': 0, 'queries': []}\n"
     ]
    }
   ],
   "source": [
    "# Run this to make sure SSH tunnel is working\n",
    "try:\n",
    "    response = requests.get(loader_endpoint_url + '/status')\n",
    "    print(response.json())\n",
    "except Exception as e:\n",
    "    print('error', e.args[0])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combined uploader script\n",
    "\n",
    "Combines code blocks previously developed to run in a single loop\n",
    "\n",
    "## Setup and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_dump = False\n",
    "\n",
    "# Instantiate objects for data transfer\n",
    "s3 = boto3.client('s3')\n",
    "sve = Sparqler(endpoint=reader_endpoint_url, method='get', sleep=0)\n",
    "neptune = Sparqler(endpoint=loader_endpoint_url, sleep=0)\n",
    "\n",
    "# ---------\n",
    "# Load data\n",
    "# ---------\n",
    "\n",
    "# Load the query prefix section (for named graph metadata in Service Description) from a text file\n",
    "with open('prefixes.txt', 'rt', encoding='utf-8') as file_object:\n",
    "    prefixes_text = file_object.read()\n",
    "    \n",
    "# Load the named graph data CSV configuration\n",
    "if exists('named_graphs_config.yml'):\n",
    "    with open('named_graphs_config.yml') as file_object:\n",
    "        config_data = yaml.safe_load(file_object)\n",
    "else:\n",
    "    print('Must have a named_graphs_config.yml file for this script to operate.')\n",
    "    print()\n",
    "    raise KeyboardInterrupt\n",
    "\n",
    "# Load the data about named graphs to be updated\n",
    "named_graphs_df = csv_read(local_upload_directory + 'named_graphs.csv')\n",
    "\n",
    "# Load the data relating the graph names to the datafiles that contain the serializations\n",
    "graph_file_associations_df = csv_read(local_upload_directory + 'graph_file_associations.csv')\n",
    "\n",
    "# Check that all of the files exist before moving on\n",
    "for index, file in graph_file_associations_df.iterrows():\n",
    "    if not exists(local_upload_directory + file['filename']):\n",
    "        print(file['filename'], 'does not exist in the directory:', local_upload_directory)\n",
    "        print('Put it there, or adjust the graph_file_associations.csv file, and try again.')\n",
    "        print()\n",
    "        raise KeyboardInterrupt\n",
    "\n",
    "#    else:\n",
    "#        print(file['filename'], 'exists')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !! Run the following cell ONLY ONCE when the triplestore is set up !!\n",
    "\n",
    "To prevent you from running it accidentally, you need to uncomment the next to last line to actually load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-time generation of service description metadata\n",
    "\n",
    "dcterms_modified = datetime.datetime.now().isoformat() + utc_offset\n",
    "\n",
    "request_string = '''prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "prefix xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "prefix dc: <http://purl.org/dc/elements/1.1/>\n",
    "prefix dcterms: <http://purl.org/dc/terms/>\n",
    "prefix sd: <http://www.w3.org/ns/sparql-service-description#>\n",
    "prefix tdwgutility: <http://rs.tdwg.org/dwc/terms/attributes/>\n",
    "\n",
    "insert data {\n",
    "graph <https://sparql.vanderbilt.edu/graphs> {\n",
    "<https://sparql.vanderbilt.edu/graphs> dcterms:modified \"''' + dcterms_modified + '''\"^^xsd:dateTime;\n",
    "    sd:name <https://sparql.vanderbilt.edu/graphs>;\n",
    "    dc:publisher \"Vanderbilt Heard Libraries\";\n",
    "    a sd:NamedGraph;\n",
    "    tdwgutility:status \"production\".\n",
    "<https://sparql.vanderbilt.edu/sparql#service> a sd:Service;\n",
    "    sd:endpoint <https://sparql.vanderbilt.edu/sparql>;\n",
    "    sd:availableGraphs <https://sparql.vanderbilt.edu/graphs#collection>.\n",
    "<https://sparql.vanderbilt.edu/graphs#collection> a sd:GraphCollection;\n",
    "    sd:namedGraph <https://sparql.vanderbilt.edu/graphs>.\n",
    "}}\n",
    "'''\n",
    "\n",
    "#print(request_string)\n",
    "\n",
    "#data = neptune.update(request_string, verbose=True) # Uncomment this line to make functional\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data and update metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing named graph: https://spear-prosop.org\n",
      "Deleting graph: https://spear-prosop.org\n",
      "  beginning update\n",
      "  done updating data in 0 s\n",
      "Deleting previous graph metadata\n",
      "  beginning update\n",
      "  done updating data in 0 s\n",
      "Uploading to s3: allSPEAR.ttl\n",
      "  upload complete in 7.029591 s.\n",
      "Loading file: allSPEAR.ttl  into graph:  https://spear-prosop.org\n",
      "  beginning update\n",
      "  done updating data in 0 s\n",
      "Inserting link from named graph to graph description\n",
      "  beginning update\n",
      "  done updating data in 0 s\n",
      "Updating current graph metadata\n",
      "  beginning update\n",
      "  done updating data in 0 s\n",
      "Updating modified time for graph of graphs\n",
      "  beginning update\n",
      "  done updating data in 0 s\n",
      "Inserting link to named graph\n",
      "  beginning update\n",
      "  done updating data in 0 s\n",
      "Update complete for graph\n",
      "\n",
      "All graphs loaded\n"
     ]
    }
   ],
   "source": [
    "for index, graph in named_graphs_df.iterrows():\n",
    "    named_graph_iri = graph['sd:name']\n",
    "    print('Processing named graph:', named_graph_iri)\n",
    "    \n",
    "    # Slice the rows whose graph name matches the current named graph\n",
    "    matching_files_df = graph_file_associations_df[graph_file_associations_df['sd:name']==named_graph_iri]\n",
    "\n",
    "    # Drop the existing version of the graph\n",
    "    update_upload_status(index, named_graphs_df, 'load_status', local_upload_directory + 'named_graphs.csv', 'dropping previous version')\n",
    "    start_time = datetime.datetime.now()\n",
    "    data = neptune.drop(named_graph_iri, verbose=True)\n",
    "    elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "    update_upload_status(index, named_graphs_df, 'load_status', local_upload_directory + 'named_graphs.csv', 'dropped previous version in ' + str(elapsed_time) + 's')\n",
    "    if print_dump:\n",
    "        print(json.dumps(data, indent=2))\n",
    "        print()        \n",
    "\n",
    "    # Delete old metadata about that graph\n",
    "    request_string = '''delete where {\n",
    "graph <https://sparql.vanderbilt.edu/graphs> {\\n'''\n",
    "    request_string += '<' + named_graph_iri + '> ?o ?p.\\n'\n",
    "    request_string += '}}\\n'    \n",
    "    #print(request_string)\n",
    "    \n",
    "    print('Deleting previous graph metadata')\n",
    "    update_upload_status(index, named_graphs_df, 'load_status', local_upload_directory + 'named_graphs.csv', 'deleting previous metadata')\n",
    "    data = neptune.update(request_string, verbose=True)\n",
    "    update_upload_status(index, named_graphs_df, 'load_status', local_upload_directory + 'named_graphs.csv', 'deleted previous metadata')\n",
    "    if print_dump:\n",
    "        print(json.dumps(data, indent=2))\n",
    "        print()\n",
    "        \n",
    "    # Step through each file to be loaded into that graph\n",
    "    for i, matching_file in matching_files_df.iterrows():\n",
    "    \n",
    "        # Upload local file data to S3 bucket\n",
    "        print('Uploading to s3:', matching_file['filename'])\n",
    "        update_upload_status(i, graph_file_associations_df, 's3_upload_status', local_upload_directory + 'graph_file_associations.csv', 'upload initiated')\n",
    "        start_time = datetime.datetime.now()\n",
    "        s3.upload_file(local_upload_directory + matching_file['filename'], s3_bucket_name, matching_file['filename'])\n",
    "        elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        update_upload_status(i, graph_file_associations_df, 's3_upload_status', local_upload_directory + 'graph_file_associations.csv', 'upload complete in ' + str(elapsed_time) + 's')\n",
    "        print('  upload complete in', elapsed_time, 's.')\n",
    "        \n",
    "        # Load file from S3 bucket to triplestore\n",
    "        update_upload_status(i, graph_file_associations_df, 'graph_load_status', local_upload_directory + 'graph_file_associations.csv', 'load initiated')\n",
    "        start_time = datetime.datetime.now()\n",
    "        data = neptune.load(matching_file['filename'], named_graph_iri, s3=s3_bucket_name, verbose=True)\n",
    "        elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        update_upload_status(i, graph_file_associations_df, 'graph_load_status', local_upload_directory + 'graph_file_associations.csv', 'load complete in ' + str(elapsed_time) + 's')\n",
    "        if print_dump:\n",
    "            print(json.dumps(data, indent=2))\n",
    "            print()\n",
    "\n",
    "        # Insert the linking triple from the named graph to the uploaded triples using the sd:graph property\n",
    "        # These triples may be considered a dcat:Dataset https://www.w3.org/TR/void/#datasethttps://www.w3.org/TR/vocab-dcat-2/#Class:Dataset\n",
    "        # or a void:Dataset https://www.w3.org/TR/void/#dataset or both. \n",
    "        # The range of sd:graph also implies that it's a \"graph description\", a sd:Graph .\n",
    "        request_string = '''prefix sd: <http://www.w3.org/ns/sparql-service-description#>\n",
    "insert data {\n",
    "graph <https://sparql.vanderbilt.edu/graphs> {\n",
    "<''' + named_graph_iri + '> sd:graph <' + matching_file['sd:graph'] + '''>.\n",
    "}}'''\n",
    "        #print(request_string)\n",
    "\n",
    "        print('Inserting link from named graph to graph description')\n",
    "        update_upload_status(index, named_graphs_df, 'load_status', local_upload_directory + 'named_graphs.csv', 'inserting graph description link')\n",
    "        data = neptune.update(request_string, verbose=True)\n",
    "        update_upload_status(index, named_graphs_df, 'load_status', local_upload_directory + 'named_graphs.csv', 'inserted graph description link')\n",
    "        if print_dump:\n",
    "            print(json.dumps(data, indent=2))\n",
    "            print()\n",
    "\n",
    "    # Add updated metadata about that graph\n",
    "    dcterms_modified = datetime.datetime.now().isoformat() + utc_offset\n",
    "\n",
    "    request_string = prefixes_text + '''insert data {\n",
    "graph <https://sparql.vanderbilt.edu/graphs> {\\n'''\n",
    "    request_string += '<' + named_graph_iri + '> dcterms:modified \"' + dcterms_modified + '\"^^xsd:dateTime.\\n'\n",
    "    for column in config_data:\n",
    "        if graph[column['column_header']]: # skip if the column has an empty string value\n",
    "            triple_pattern = '<' + named_graph_iri + '> ' + column['column_header'] + ' '\n",
    "            if column['object_type'] == 'iri':\n",
    "                triple_pattern += '<' + graph[column['column_header']] + '>'\n",
    "            elif column['object_type'] == 'curie':\n",
    "                triple_pattern += graph[column['column_header']]\n",
    "            elif column['object_type'] == 'literal':\n",
    "                triple_pattern += '\"' + graph[column['column_header']] + '\"'\n",
    "                if 'datatype' in column:\n",
    "                    triple_pattern += '^^' + column['datatype']\n",
    "                if 'lang' in column:\n",
    "                    triple_pattern += '@' + column['lang']\n",
    "            triple_pattern += '.\\n'\n",
    "            request_string += triple_pattern\n",
    "    request_string += '}}\\n'    \n",
    "    #print(request_string)\n",
    "\n",
    "    print('Updating current graph metadata')\n",
    "    update_upload_status(index, named_graphs_df, 'load_status', local_upload_directory + 'named_graphs.csv', 'uploading current metadata')\n",
    "    data = neptune.update(request_string, verbose=True)\n",
    "    update_upload_status(index, named_graphs_df, 'load_status', local_upload_directory + 'named_graphs.csv', 'uploaded current metadata')\n",
    "    if print_dump:\n",
    "        print(json.dumps(data, indent=2))\n",
    "        print()\n",
    "\n",
    "    # Update the modified time for the graph of graphs data \n",
    "    dcterms_modified = datetime.datetime.now().isoformat() + utc_offset\n",
    "    request_string = '''prefix dcterms: <http://purl.org/dc/terms/>\n",
    "with <https://sparql.vanderbilt.edu/graphs> \n",
    "delete {\n",
    "<https://sparql.vanderbilt.edu/graphs> dcterms:modified ?dateTime.\n",
    "}\n",
    "insert {\n",
    "<https://sparql.vanderbilt.edu/graphs> dcterms:modified \"''' + dcterms_modified + '''\"^^xsd:dateTime.\n",
    "}\n",
    "where {\n",
    "<https://sparql.vanderbilt.edu/graphs> dcterms:modified ?dateTime.\n",
    "}'''\n",
    "    #print(request_string)\n",
    "\n",
    "    print('Updating modified time for graph of graphs')\n",
    "    update_upload_status(index, named_graphs_df, 'load_status', local_upload_directory + 'named_graphs.csv', 'updating modified time')\n",
    "    data = neptune.update(request_string, verbose=True)\n",
    "    update_upload_status(index, named_graphs_df, 'load_status', local_upload_directory + 'named_graphs.csv', 'updated modified time')\n",
    "    if print_dump:\n",
    "        print(json.dumps(data, indent=2))\n",
    "        print()\n",
    "\n",
    "    # Insert the linking triple from the GraphCollection to the named graph\n",
    "    request_string = '''prefix sd: <http://www.w3.org/ns/sparql-service-description#>\n",
    "insert data {\n",
    "graph <https://sparql.vanderbilt.edu/graphs> {\\n'''\n",
    "    request_string += '<https://sparql.vanderbilt.edu/graphs#collection> sd:namedGraph <' + named_graph_iri + '>.\\n'\n",
    "    request_string += '}}\\n'    \n",
    "    #print(request_string)\n",
    "\n",
    "    print('Inserting link to named graph')\n",
    "    update_upload_status(index, named_graphs_df, 'load_status', local_upload_directory + 'named_graphs.csv', 'inserting link to named graph')\n",
    "    data = neptune.update(request_string, verbose=True)\n",
    "    update_upload_status(index, named_graphs_df, 'load_status', local_upload_directory + 'named_graphs.csv', 'update complete')\n",
    "    if print_dump:\n",
    "        print(json.dumps(data, indent=2))\n",
    "        print()\n",
    "    \n",
    "    print('Update complete for graph')\n",
    "    print()\n",
    "\n",
    "# Close the SSH tunnel\n",
    "\n",
    "print('All graphs loaded')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities\n",
    "\n",
    "Not necessarily part of the workflow, but can be used to accomplish various tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List graphs loaded in Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://AATOut_1Subjects\n",
      "http://AATOut_2Terms\n",
      "http://AATOut_AssociativeRels\n",
      "http://AATOut_ContribRels\n",
      "http://AATOut_Contribs\n",
      "http://AATOut_HierarchicalRels\n",
      "http://AATOut_LCSHAlignment\n",
      "http://AATOut_Lang_sameAs\n",
      "http://AATOut_Notations\n",
      "http://AATOut_ObsoleteSubjects\n",
      "http://AATOut_OrderedCollections\n",
      "http://AATOut_ScopeNotes\n",
      "http://AATOut_SemanticLinks\n",
      "http://AATOut_SourceRels\n",
      "http://AATOut_Sources\n",
      "http://AATOut_WikidataCoref\n",
      "http://architecturasinica.org/place/building\n",
      "http://architecturasinica.org/place/image\n",
      "http://architecturasinica.org/place/site\n",
      "http://bioimages.vanderbilt.edu/images\n",
      "http://bioimages.vanderbilt.edu/organisms\n",
      "http://bioimages.vanderbilt.edu/people\n",
      "http://bioimages.vanderbilt.edu/places\n",
      "http://bioimages.vanderbilt.edu/rdf/stdview\n",
      "http://bioimages.vanderbilt.edu/specimens\n",
      "http://bioimages.vanderbilt.edu/vocabs\n",
      "http://lod.vanderbilt.edu/apulian/scene\n",
      "http://lod.vanderbilt.edu/vase\n",
      "http://nomenclature_2022-02-02\n",
      "http://rs.tdwg.org/\n",
      "http://rs.tdwg.org/cv/status\n",
      "http://syriaca.org/bibl#graph\n",
      "http://syriaca.org/geo#graph\n",
      "http://syriaca.org/persons#graph\n",
      "http://syriaca.org/taxonomy#graph\n",
      "http://syriaca.org/works#graph\n",
      "http://tcadrt.org/timeline\n",
      "https://sparql.vanderbilt.edu/graphs\n"
     ]
    }
   ],
   "source": [
    "sve = Sparqler(endpoint=reader_endpoint_url, method='get', sleep=0)\n",
    "query_string = '''select distinct ?graph where {\n",
    "graph ?graph {?s ?o ?p.}\n",
    "}\n",
    "order by ?graph'''\n",
    "\n",
    "data = sve.query(query_string)\n",
    "#print(json.dumps(data, indent=2))\n",
    "\n",
    "for graph in data:\n",
    "    print(graph['graph']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine some triples in a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "query_string = '''select distinct ?s ?o ?p where {\n",
    "?s ?o ?p.\n",
    "}\n",
    "limit 5'''\n",
    "\n",
    "from_graphs = ['https://spear-prosop.org']\n",
    "#from_graphs = ['put graph IRI here']\n",
    "sve = Sparqler(endpoint=reader_endpoint_url, method='get', sleep=0)\n",
    "data = sve.query(query_string, default=from_graphs)\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dump triples from a graph to a CSV\n",
    "\n",
    "Note: SPARQL Query results serializations supported are listed at https://docs.aws.amazon.com/neptune/latest/userguide/sparql-media-type-support.html#sparql-serialization-formats-neptune-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = '''select distinct ?s ?o ?p where {\n",
    "?s ?o ?p.\n",
    "}'''\n",
    "\n",
    "from_graphs = ['https://sparql.vanderbilt.edu/graphs']\n",
    "sve = Sparqler(endpoint=reader_endpoint_url, method='get', sleep=0)\n",
    "data = sve.query(query_string, default=from_graphs, mediatype='text/csv')\n",
    "\n",
    "with open('graph_dump.csv', 'wt', encoding='utf-8') as file_object:\n",
    "    file_object.write(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop a graph\n",
    "\n",
    "Be careful! Both dropping and reloading large graphs can take a long time!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_name = 'put graph IRI here'\n",
    "neptune = Sparqler(endpoint=loader_endpoint_url, sleep=0)\n",
    "data = neptune.drop(graph_name, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obsolete development code\n",
    "\n",
    "Left for historical reference\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test of loading a triple twice\n",
    "\n",
    "Needed to know if in Neptune adding a triple that is already in a graph creates a second triple or if a single triple represents the old and added triple.\n",
    "\n",
    "Answer: It does not create a second identical triple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neptune = Sparqler(endpoint=loader_endpoint_url, sleep=0)\n",
    "sve = Sparqler(endpoint=reader_endpoint_url, method='get', sleep=0)\n",
    "\n",
    "request_string = '''insert data { \n",
    "graph <https://test> {\n",
    "<https://test.com/s> <https://test.com/p> <https://test.com/o> . \n",
    "<https://test.com/s> <https://test.com/p> <https://test.com/o> . \n",
    "    }\n",
    "}'''\n",
    "data = neptune.update(request_string, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n",
    "print()\n",
    "\n",
    "query_string = '''select * \n",
    "from <https://test>\n",
    "where {?s ?o ?p}\n",
    "'''\n",
    "data = sve.query(query_string)\n",
    "print(json.dumps(data, indent=2))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use AWS SDK to upload files to the public S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# Upload RDF triples to S3 bucket\n",
    "# ----------------\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# The upload directory should contain only RDF serializations that are to be uploaded.\n",
    "file_list = os.listdir(local_upload_directory)\n",
    "try:\n",
    "    file_list.remove('.DS_Store') # Macs sometimes generate this hidden file - try to remove from upload list\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for file_name in file_list:\n",
    "    #file_name_root = parse_filename(file_name)[0]\n",
    "    local_file_path = local_upload_directory + file_name\n",
    "    s3_file_key = file_name\n",
    "\n",
    "    print('Uploading to s3:', file_name)\n",
    "    start_time = datetime.datetime.now()\n",
    "    s3.upload_file(local_file_path, s3_bucket_name, s3_file_key)\n",
    "    elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "    print('Upload complete in', elapsed_time, 's.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files in S3 into triplestore\n",
    "\n",
    "For reference, Neptune's conformance with SPARQL is described at https://docs.aws.amazon.com/neptune/latest/userguide/feature-sparql-compliance.html . Most specifically, triples are always associated with a graph, even if one is not explicitly specified. See https://docs.aws.amazon.com/neptune/latest/userguide/feature-sparql-compliance.html#sparql-default-graph . The fallback URI `http://aws.amazon.com/neptune/vocab/v01/DefaultNamedGraph` is used.\n",
    "\n",
    "Setup and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Neptune SPARQL object\n",
    "neptune = Sparqler(endpoint=loader_endpoint_url, sleep=0)\n",
    "\n",
    "# Load the query prefix section from a text file\n",
    "with open('prefixes.txt', 'rt', encoding='utf-8') as file_object:\n",
    "    prefixes_text = file_object.read()\n",
    "    \n",
    "# Load the named graph data CSV configuration\n",
    "if exists('named_graphs_config.yml'):\n",
    "    with open('named_graphs_config.yml') as file_object:\n",
    "        config_data = yaml.safe_load(file_object)\n",
    "else:\n",
    "    print('Must have a named_graphs_config.yml file for this script to operate.')\n",
    "    print()\n",
    "    raise KeyboardInterrupt\n",
    "\n",
    "# Load the data about named graphs to be updated\n",
    "named_graphs_df = csv_read('named_graphs.csv')\n",
    "\n",
    "# Load the data relating the graph names to the datafiles that contain the serializations\n",
    "graph_file_associations_df = csv_read('graph_file_associations.csv')\n",
    "\n",
    "# Check that all of the files exist before moving on\n",
    "for index, file in graph_file_associations_df.iterrows():\n",
    "    if not exists(local_upload_directory + file['filename']):\n",
    "        print(file['filename'], 'does not exist in the directory:', local_upload_directory)\n",
    "        print('Put it there, or adjust the graph_file_associations.csv file, and try again.')\n",
    "        print()\n",
    "        raise KeyboardInterrupt\n",
    "\n",
    "#    else:\n",
    "#        print(file['filename'], 'exists')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the triples from the S3 bucket into the triplestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, file in graph_file_associations_df.iterrows():\n",
    "    graph_name = file['sd:name']\n",
    "    data = neptune.load(file['filename'], graph_name, s3=s3_bucket_name, verbose=True)\n",
    "    print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the metadata about graphs to reflect the changes\n",
    "\n",
    "If there are old triples in the graphs metadata associated with the named graphs being updated, they are deleted from the <https://sparql.vanderbilt.edu/graphs> graph. See https://www.w3.org/TR/sparql11-update/#deleteWhere for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that attempting to delete triples that don't exist or from a graph that doesn't exist has no effect\n",
    "# and generates no errors. \n",
    "\n",
    "\"\"\"Generates a request string of the form:\n",
    "delete where {\n",
    "graph <https://sparql.vanderbilt.edu/graphs> {\n",
    "<http://syriaca.org/bibl#graph> ?o ?p.\n",
    "\n",
    "...\n",
    "\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# NOTE: prefixes not needed since the subject IRI is unabbreviated.\n",
    "request_string = '''delete where {\n",
    "graph <https://sparql.vanderbilt.edu/graphs> {\\n'''\n",
    "for index, graph in named_graphs_df.iterrows():\n",
    "    request_string += '<' + graph['sd:name'] + '> ?o ?p.\\n'\n",
    "request_string += '}}\\n'    \n",
    "print(request_string)\n",
    "        \n",
    "data = neptune.update(request_string, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the new metadata for the named graphs in the service description and load it into the <https://sparql.vanderbilt.edu/graphs> graph. See https://www.w3.org/TR/sparql11-update/#insertData for reference. Note: unlike the examples at https://www.w3.org/TR/sparql11-service-description/#example-turtle and https://www.w3.org/TR/void/#sparql-sd , we don't use a blank node for the sd:NamedGraph instance. This makes it easier to manage linking it to the graph collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Generates a request string of the form:\n",
    "\n",
    "prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "prefix xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "prefix sd: <http://www.w3.org/ns/sparql-service-description#>\n",
    "prefix dc: <http://purl.org/dc/elements/1.1/>\n",
    "prefix dcterms: <http://purl.org/dc/terms/>\n",
    "prefix tdwgutility: <http://rs.tdwg.org/dwc/terms/attributes/>\n",
    "insert data {\n",
    "graph <https://sparql.vanderbilt.edu/graphs> {\n",
    "<http://syriaca.org/bibl#graph> dcterms:modified \"2022-06-06T14:40:05.430286-06:00\"^^xsd:dateTime.\n",
    "<http://syriaca.org/bibl#graph> sd:name <http://syriaca.org/bibl#graph>.\n",
    "<http://syriaca.org/bibl#graph> sd:graph <http://syriaca.org/bibl>.\n",
    "<http://syriaca.org/bibl#graph> dcterms:issued \"2018-03-06\"^^xsd:date.\n",
    "<http://syriaca.org/bibl#graph> dc:publisher \"syriaca.org\".\n",
    "<http://syriaca.org/bibl#graph> rdf:type sd:NamedGraph.\n",
    "<http://syriaca.org/bibl#graph> dcterms:isPartOf <http://syriaca.org/>.\n",
    "<http://syriaca.org/bibl#graph> tdwgutility:status \"production\".\n",
    "\n",
    "...\n",
    "\n",
    "}}\n",
    "\"\"\"\n",
    "dcterms_modified = datetime.datetime.now().isoformat() + utc_offset\n",
    "\n",
    "request_string = prefixes_text + '''insert data {\n",
    "graph <https://sparql.vanderbilt.edu/graphs> {\\n'''\n",
    "for index, graph in named_graphs_df.iterrows():\n",
    "    named_graph_iri = graph['sd:name']\n",
    "    request_string += '<' + named_graph_iri + '> dcterms:modified \"' + dcterms_modified + '\"^^xsd:dateTime.\\n'\n",
    "    for column in config_data:\n",
    "        triple_pattern = '<' + named_graph_iri + '> ' + column['column_header'] + ' '\n",
    "        if column['object_type'] == 'iri':\n",
    "            triple_pattern += '<' + graph[column['column_header']] + '>'\n",
    "        elif column['object_type'] == 'curie':\n",
    "            triple_pattern += graph[column['column_header']]\n",
    "        elif column['object_type'] == 'literal':\n",
    "            triple_pattern += '\"' + graph[column['column_header']] + '\"'\n",
    "            if 'datatype' in column:\n",
    "                triple_pattern += '^^' + column['datatype']\n",
    "            if 'lang' in column:\n",
    "                triple_pattern += '@' + column['lang']\n",
    "        triple_pattern += '.\\n'\n",
    "        request_string += triple_pattern\n",
    "request_string += '}}\\n'    \n",
    "print(request_string)\n",
    "        \n",
    "data = neptune.update(request_string, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to add the named graph to the GraphCollection if it isn't there already and update the modified dateTime for the https://sparql.vanderbilt.edu/graphs graph. This uses the WITH clause to carry out both the insert and delete sequentially. See https://www.w3.org/TR/sparql11-update/#deleteInsert for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the modified time for the graph of graphs data \n",
    "dcterms_modified = datetime.datetime.now().isoformat() + utc_offset\n",
    "request_string = '''prefix dcterms: <http://purl.org/dc/terms/>\n",
    "\n",
    "with <https://sparql.vanderbilt.edu/graphs> \n",
    "delete {\n",
    "<https://sparql.vanderbilt.edu/graphs> dcterms:modified ?dateTime.\n",
    "}\n",
    "insert {\n",
    "<https://sparql.vanderbilt.edu/graphs> dcterms:modified \"''' + dcterms_modified + '''\"^^xsd:dateTime.\n",
    "}\n",
    "where {\n",
    "<https://sparql.vanderbilt.edu/graphs> dcterms:modified ?dateTime.\n",
    "}'''\n",
    "\n",
    "print(request_string)\n",
    "\n",
    "data = neptune.update(request_string, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n",
    "print()\n",
    "\n",
    "# Delete the links from the GraphCollection to the named graphs so that the triples won't get duplicated when added\n",
    "# NOTE: this may not be necessary since it doesn't appear that duplicates get generated if we don't delete. \n",
    "# But it take negligible time and doesn't hurt anything.\n",
    "\n",
    "# NOTE 2022-06-07: Did additional testing (see code cell above) and it doesn't add the triple again. So this code\n",
    "# isn't needed and will be removed from the main script.\n",
    "request_string = '''prefix sd: <http://www.w3.org/ns/sparql-service-description#>\n",
    "delete where {\n",
    "graph <https://sparql.vanderbilt.edu/graphs> {\\n'''\n",
    "for index, graph in named_graphs_df.iterrows():\n",
    "    request_string += '<https://sparql.vanderbilt.edu/graphs#collection> sd:namedGraph <' + graph['sd:name'] + '>.\\n'\n",
    "request_string += '}}\\n'    \n",
    "print(request_string)\n",
    "        \n",
    "data = neptune.update(request_string, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n",
    "print()\n",
    "\n",
    "# Now insert the linking triples\n",
    "request_string = '''prefix sd: <http://www.w3.org/ns/sparql-service-description#>\n",
    "insert data {\n",
    "graph <https://sparql.vanderbilt.edu/graphs> {\\n'''\n",
    "for index, graph in named_graphs_df.iterrows():\n",
    "    request_string += '<https://sparql.vanderbilt.edu/graphs#collection> sd:namedGraph <' + graph['sd:name'] + '>.\\n'\n",
    "request_string += '}}\\n'    \n",
    "print(request_string)\n",
    "        \n",
    "data = neptune.update(request_string, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO NOT RUN this cell! It will delete everything in the triplestore\n",
    "\n",
    "Actually, I've left it with an error so that it will fail if it's accidentally run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST SPARQL Update DROP ALL command\n",
    "# Warning! This deletes all triples for all graphs !!!!\n",
    "\n",
    "query_string = 'DROP ALL'\n",
    "start_time = datetime.datetime.now()\n",
    "response = requests.post(loader_endpoint_url + '/sparql', data=query_string.encode('utf-8'), headers=update_request_header_dictionary)\n",
    "#print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "data = response.json()\n",
    "print(json.dumps(data, indent = 2))\n",
    "\n",
    "elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "print('time to load:', int(elapsed_time), 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code to test the Neptune-specific loader\n",
    "\n",
    "There did not seem to be any real benefit to using this loader with respect to speed. It also does not have any mechanism for specifying the graph into which triples should be loaded, so the only way to load them into a specific graph is to use the n-quads format. See following cell to convert from n-triples to n-quads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST loader command\n",
    "\n",
    "loader_request_header_dictionary = {\n",
    "        'Accept' : 'application/json',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "rdf_format = 'nquads'\n",
    "#rdf_format = 'ntriples'\n",
    "\n",
    "data = '''\n",
    "    {\n",
    "      \"source\" : \"s3://'''+ s3_bucket_name + '/' + s3_file_key + '''\",\n",
    "      \"format\" : \"'''  + rdf_format + '''\",\n",
    "      \"iamRoleArn\" : \"arn:aws:iam::555751041262:role/neptuneloadfroms3\",\n",
    "      \"region\" : \"us-east-1\",\n",
    "      \"failOnError\" : \"FALSE\",\n",
    "      \"parallelism\" : \"MEDIUM\",\n",
    "      \"updateSingleCardinalityProperties\" : \"FALSE\",\n",
    "      \"queueRequest\" : \"TRUE\"\n",
    "    }'''\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Send request to load\n",
    "response = requests.post(loader_endpoint_url + '/loader', data=data.encode('utf-8'), headers=loader_request_header_dictionary)\n",
    "data = response.json()\n",
    "print(json.dumps(data, indent = 2))\n",
    "load_id = data['payload']['loadId']\n",
    "\n",
    "# Check status of load once per second\n",
    "completed = False\n",
    "while not completed:\n",
    "    response = requests.get(loader_endpoint_url + '/loader/' + load_id)\n",
    "    data = response.json()\n",
    "    #print(json.dumps(data, indent = 2))\n",
    "    print(data['payload']['overallStatus']['status'])\n",
    "    if data['payload']['overallStatus']['status'] == 'LOAD_COMPLETED' or data['payload']['overallStatus']['status'] == 'LOAD_FAILED':\n",
    "        completed = True\n",
    "    sleep(1)\n",
    "elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "print('time to load:', int(elapsed_time), 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting n-triples to n-quads to specify graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(local_directory)\n",
    "file_list.remove('.DS_Store')\n",
    "\n",
    "# Note: assumes all files are n-triples serialization with .nt file extensions.\n",
    "for file_name in file_list:\n",
    "    file_name_root = parse_filename(file_name)[0]\n",
    "    print('converting:', file_name)\n",
    "    output_filename = file_name_root + '.nq'\n",
    "    graph_string = ' <http://' + file_name_root + '> .'\n",
    "\n",
    "    output_file_object = open(local_directory + output_filename, 'wt', encoding='utf-8')\n",
    "    input_file_object = open(local_directory + file_name, 'rt', encoding='utf-8')\n",
    "\n",
    "    for line in input_file_object:\n",
    "        line_text = line.strip() # remove trailing newline\n",
    "        line_text = line_text[:-1] + graph_string # remove period at end.\n",
    "        print(line_text, file=output_file_object)\n",
    "\n",
    "    output_file_object.close()\n",
    "    input_file_object.close()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f96c65e2c1d4fcba82e9525c1be2fd15c6a14102f9c31bd3457b5f48c526190"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
