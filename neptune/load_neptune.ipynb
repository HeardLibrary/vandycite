{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loader script for Neptune\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (c) 2022 Vanderbilt University, except Sparqler class: (c) Steven J. Baskauf (same license)\n",
    "# This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "# Date: 2022-06-06\n",
    "\n",
    "# ----------------\n",
    "# Configuration\n",
    "# ----------------\n",
    "import requests\n",
    "import json\n",
    "from time import sleep\n",
    "import datetime\n",
    "import os\n",
    "from os.path import exists\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# AWS Python SDK\n",
    "import boto3\n",
    "# import botocore  # not used\n",
    "\n",
    "import csv\n",
    "from pathlib import Path\n",
    "import sys\n",
    "import re # regex\n",
    "import urllib.parse\n",
    "\n",
    "# Global variables\n",
    "loader_endpoint_url = 'https://triplestore1.cluster-cml0hq81gymg.us-east-1.neptune.amazonaws.com:8182/sparql'\n",
    "reader_endpoint_url = 'https://5j6diw4i0h.execute-api.us-east-1.amazonaws.com/sparql'\n",
    "local_upload_directory = '/Users/baskausj/triplestore_upload/'\n",
    "s3_bucket_name = 'triplestore-upload'\n",
    "utc_offset = '-06:00'\n",
    "\n",
    "# See https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3.html#uploads\n",
    "# local_file_path = local_directory + local_filename\n",
    "# s3_file_key = local_filename\n",
    "# s3_file_key = s3_iiif_project_directory + '/' + subdirectory + '/' + local_filename\n",
    "\n",
    "def parse_filename(filename):\n",
    "    \"\"\"Separates and returns the root of a filename and the extension as a tuple.\"\"\"\n",
    "    pieces = filename.split('.')\n",
    "    file_name_root = '.'.join(pieces[:-1])\n",
    "    extension = pieces[len(pieces)-1]\n",
    "    return file_name_root, extension\n",
    "\n",
    "def csv_read(path, **kwargs):\n",
    "    \"\"\"Loads a CSV table into a Pandas DataFrame with all cells as strings and blank cells as empty strings\n",
    "    \n",
    "    Keyword argument:\n",
    "    rows -- the number of rows of the table to return when used for testing. When omitted, all rows are returned.\n",
    "    \"\"\"\n",
    "    dataframe = pd.read_csv(path, na_filter=False, dtype = str)\n",
    "    if 'rows' in kwargs:\n",
    "        return dataframe.head(kwargs['rows']).copy(deep=True)\n",
    "    else:\n",
    "        return dataframe\n",
    "\n",
    "class Sparqler:\n",
    "    \"\"\"Build SPARQL queries of various sorts\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    useragent : str\n",
    "        Required if using the Wikidata Query Service, otherwise optional.\n",
    "        Use the form: appname/v.v (URL; mailto:email@domain.com)\n",
    "        See https://meta.wikimedia.org/wiki/User-Agent_policy\n",
    "    endpoint: URL\n",
    "        Defaults to Wikidata Query Service if not provided.\n",
    "    method: str\n",
    "        Possible values are \"post\" (default) or \"get\". Use \"get\" if read-only query endpoint.\n",
    "        Must be \"post\" for update endpoint.\n",
    "    sleep: float\n",
    "        Number of seconds to wait between queries. Defaults to 0.1\n",
    "        \n",
    "    Required modules:\n",
    "    -------------\n",
    "    import requests\n",
    "    from time import sleep\n",
    "    \"\"\"\n",
    "    def __init__(self, **kwargs):\n",
    "        # attributes for all methods\n",
    "        try:\n",
    "            self.http_method = kwargs['method']\n",
    "        except:\n",
    "            self.http_method = 'post' # default to POST\n",
    "        try:\n",
    "            self.endpoint = kwargs['endpoint']\n",
    "        except:\n",
    "            self.endpoint = 'https://query.wikidata.org/sparql' # default to Wikidata endpoint\n",
    "        try:\n",
    "            self.useragent = kwargs['useragent']\n",
    "        except:\n",
    "            if self.endpoint == 'https://query.wikidata.org/sparql':\n",
    "                print('You must provide a value for the useragent argument when using the Wikidata Query Service.')\n",
    "                print()\n",
    "                raise KeyboardInterrupt # Use keyboard interrupt instead of sys.exit() because it works in Jupyter notebooks\n",
    "            else:\n",
    "                self.useragent = ''\n",
    "        try:\n",
    "            self.sleep = kwargs['sleep']\n",
    "        except:\n",
    "            self.sleep = 0.1 # default throtting of 0.1 seconds\n",
    "\n",
    "        self.requestheader = {}\n",
    "        if self.useragent:\n",
    "            self.requestheader['User-Agent'] = self.useragent\n",
    "        \n",
    "        if self.http_method == 'post':\n",
    "            self.requestheader['Content-Type'] = 'application/x-www-form-urlencoded'\n",
    "\n",
    "    def query(self, query_string, **kwargs):\n",
    "        \"\"\"Sends a SPARQL query to the endpoint.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        form : str\n",
    "            The SPARQL query form.\n",
    "            Possible values are: \"select\" (default), \"ask\", \"construct\", and \"describe\".\n",
    "        mediatype: str\n",
    "            The response media type (MIME type) of the query results.\n",
    "            Some possible values for \"select\" and \"ask\" are: \"application/sparql-results+json\" (default) and \"application/sparql-results+xml\".\n",
    "            Some possible values for \"construct\" and \"describe\" are: \"text/turtle\" (default) and \"application/rdf+xml\".\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        default: list of str\n",
    "            The graphs to be merged to form the default graph. List items must be URIs in string form.\n",
    "            If omitted, no graphs will be specified and default graph composition will be controlled by FROM clauses\n",
    "            in the query itself. \n",
    "            See https://www.w3.org/TR/sparql11-query/#namedGraphs and https://www.w3.org/TR/sparql11-protocol/#dataset\n",
    "            for details.\n",
    "        named: list of str\n",
    "            Graphs that may be specified by IRI in a query. List items must be URIs in string form.\n",
    "            If omitted, named graphs will be specified by FROM NAMED clauses in the query itself.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        If the form is \"select\" and mediatype is \"application/json\", a list of dictionaries containing the data.\n",
    "        If the form is \"ask\" and mediatype is \"application/json\", a boolean is returned.\n",
    "        If the mediatype is \"application/json\" and an error occurs, None is returned.\n",
    "        For other forms and mediatypes, the raw output is returned.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        To get UTF-8 text in the SPARQL queries to work properly, send URL-encoded text rather than raw text.\n",
    "        That is done automatically by the requests module for GET. I guess it also does it for POST when the\n",
    "        data are sent as a dict with the urlencoded header. \n",
    "        See SPARQL 1.1 protocol notes at https://www.w3.org/TR/sparql11-protocol/#query-operation        \n",
    "        \"\"\"\n",
    "        try:\n",
    "            query_form = kwargs['form']\n",
    "        except:\n",
    "            query_form = 'select' # default to SELECT query form\n",
    "        try:\n",
    "            media_type = kwargs['mediatype']\n",
    "        except:\n",
    "            #if query_form == 'construct' or query_form == 'describe':\n",
    "            if query_form == 'construct':\n",
    "                media_type = 'text/turtle'\n",
    "            else:\n",
    "                media_type = 'application/sparql-results+json' # default for SELECT and ASK query forms\n",
    "        self.requestheader['Accept'] = media_type\n",
    "        try:\n",
    "            verbose = kwargs['verbose']\n",
    "        except:\n",
    "            verbose = False # default to no printouts\n",
    "            \n",
    "        # Build the payload dictionary (query and graph data) to be sent to the endpoint\n",
    "        payload = {'query' : query_string}\n",
    "        try:\n",
    "            payload['default-graph-uri'] = kwargs['default']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            payload['named-graph-uri'] = kwargs['named']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if verbose:\n",
    "            print('querying SPARQL endpoint')\n",
    "\n",
    "        start_time = datetime.datetime.now()\n",
    "        if self.http_method == 'post':\n",
    "            response = requests.post(self.endpoint, data=payload, headers=self.requestheader)\n",
    "        else:\n",
    "            response = requests.get(self.endpoint, params=payload, headers=self.requestheader)\n",
    "        elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        self.response = response.text\n",
    "        sleep(self.sleep) # Throttle as a courtesy to avoid hitting the endpoint too fast.\n",
    "\n",
    "        if verbose:\n",
    "            print('done retrieving data in', int(elapsed_time), 's')\n",
    "\n",
    "        if query_form == 'construct' or query_form == 'describe':\n",
    "            return response.text\n",
    "        else:\n",
    "            if media_type != 'application/sparql-results+json':\n",
    "                return response.text\n",
    "            else:\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                except:\n",
    "                    return None # Returns no value if an error. \n",
    "\n",
    "                if query_form == 'select':\n",
    "                    # Extract the values from the response JSON\n",
    "                    results = data['results']['bindings']\n",
    "                else:\n",
    "                    results = data['boolean'] # True or False result from ASK query \n",
    "                return results           \n",
    "\n",
    "    def update(self, request_string, **kwargs):\n",
    "        \"\"\"Sends a SPARQL update to the endpoint.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        mediatype : str\n",
    "            The response media type (MIME type) from the endpoint after the update.\n",
    "            Default is \"application/json\"; probably no need to use anything different.\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        default: list of str\n",
    "            The graphs to be merged to form the default graph. List items must be URIs in string form.\n",
    "            If omitted, no graphs will be specified and default graph composition will be controlled by USING\n",
    "            clauses in the query itself. \n",
    "            See https://www.w3.org/TR/sparql11-update/#deleteInsert\n",
    "            and https://www.w3.org/TR/sparql11-protocol/#update-operation for details.\n",
    "        named: list of str\n",
    "            Graphs that may be specified by IRI in the graph pattern. List items must be URIs in string form.\n",
    "            If omitted, named graphs will be specified by USING NAMED clauses in the query itself.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            media_type = kwargs['mediatype']\n",
    "        except:\n",
    "            media_type = 'application/json' # default response type after update\n",
    "        self.requestheader['Accept'] = media_type\n",
    "        try:\n",
    "            verbose = kwargs['verbose']\n",
    "        except:\n",
    "            verbose = False # default to no printouts\n",
    "        \n",
    "        # Build the payload dictionary (update request and graph data) to be sent to the endpoint\n",
    "        payload = {'update' : request_string}\n",
    "        try:\n",
    "            payload['using-graph-uri'] = kwargs['default']\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        try:\n",
    "            payload['using-named-graph-uri'] = kwargs['named']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        if verbose:\n",
    "            print('beginning update')\n",
    "            \n",
    "        start_time = datetime.datetime.now()\n",
    "        response = requests.post(self.endpoint, data=payload, headers=self.requestheader)\n",
    "        elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "        self.response = response.text\n",
    "        sleep(self.sleep) # Throttle as a courtesy to avoid hitting the endpoint too fast.\n",
    "\n",
    "        if verbose:\n",
    "            print('done updating data in', int(elapsed_time), 's')\n",
    "\n",
    "        if media_type != 'application/json':\n",
    "            return response.text\n",
    "        else:\n",
    "            try:\n",
    "                data = response.json()\n",
    "            except:\n",
    "                return None # Returns no value if an error converting to JSON (e.g. plain text) \n",
    "            return data           \n",
    "\n",
    "    def load(self, file_location, graph_uri, **kwargs):\n",
    "        \"\"\"Loads an RDF document into a specified graph.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        s3 : str\n",
    "            Name of an AWS S3 bucket containing the file. Omit load a generic URL.\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        \n",
    "        Notes\n",
    "        -----\n",
    "        The triplestore may or may not rely on receiving a correct Content-Type header with the file to\n",
    "        determine the type of serialization. Blazegraph requires it, AWS Neptune does not and apparently\n",
    "        interprets serialization based on the file extension.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            s3 = kwargs['s3']\n",
    "        except:\n",
    "            s3 = ''\n",
    "        try:\n",
    "            verbose = kwargs['verbose']\n",
    "        except:\n",
    "            verbose = False # default to no printouts\n",
    "\n",
    "        if s3:\n",
    "            request_string = 'LOAD <https://' + s3 + '.s3.amazonaws.com/' + file_location + '> INTO GRAPH <' + graph_uri + '>'\n",
    "        else:\n",
    "            request_string = 'LOAD <' + file_location + '> INTO GRAPH <' + graph_uri + '>'\n",
    "        \n",
    "        if verbose:\n",
    "            print('Loading file:', file_location, ' into graph: ', graph_uri)\n",
    "        data = self.update(request_string, verbose=verbose)\n",
    "        return data\n",
    "\n",
    "    def drop(self, graph_uri, **kwargs):\n",
    "        \"\"\"Drop a specified graph.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            verbose = kwargs['verbose']\n",
    "        except:\n",
    "            verbose = False # default to no printouts\n",
    "\n",
    "        request_string = 'DROP GRAPH <' + graph_uri + '>'\n",
    "\n",
    "        if verbose:\n",
    "            print('Deleting graph:', graph_uri)\n",
    "        data = self.update(request_string, verbose=verbose)\n",
    "        return data\n",
    "\n",
    "# -----------------\n",
    "# Begin main script\n",
    "# -----------------\n",
    "sve = Sparqler(endpoint=reader_endpoint_url, method='get', sleep=0)\n",
    "neptune = Sparqler(endpoint=loader_endpoint_url, sleep=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## List graphs loaded in Neptune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://AATOut_1Subjects\n",
      "http://AATOut_2Terms\n",
      "http://AATOut_AssociativeRels\n",
      "http://AATOut_ContribRels\n",
      "http://AATOut_Contribs\n",
      "http://AATOut_HierarchicalRels\n",
      "http://AATOut_LCSHAlignment\n",
      "http://AATOut_Lang_sameAs\n",
      "http://AATOut_Notations\n",
      "http://AATOut_ObsoleteSubjects\n",
      "http://AATOut_OrderedCollections\n",
      "http://AATOut_ScopeNotes\n",
      "http://AATOut_SemanticLinks\n",
      "http://AATOut_SourceRels\n",
      "http://AATOut_Sources\n",
      "http://AATOut_WikidataCoref\n",
      "http://nomenclature_2022-02-02\n",
      "http://syriaca.org/bibl#graph\n",
      "https://sparql.vanderbilt.edu/graphs\n"
     ]
    }
   ],
   "source": [
    "query_string = '''select distinct ?graph where {\n",
    "graph ?graph {?s ?o ?p.}\n",
    "}\n",
    "order by ?graph'''\n",
    "\n",
    "data = sve.query(query_string)\n",
    "#print(json.dumps(data, indent=2))\n",
    "\n",
    "for graph in data:\n",
    "    print(graph['graph']['value'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use AWS SDK to upload files to the public S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading to s3: bibl-pelagios.rdf\n",
      "Upload complete in 6.516015 s.\n"
     ]
    }
   ],
   "source": [
    "# ----------------\n",
    "# Upload RDF triples to S3 bucket\n",
    "# ----------------\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# The upload directory should contain only RDF serializations that are to be uploaded.\n",
    "file_list = os.listdir(local_upload_directory)\n",
    "try:\n",
    "    file_list.remove('.DS_Store') # Macs sometimes generate this hidden file - try to remove from upload list\n",
    "except:\n",
    "    pass\n",
    "\n",
    "for file_name in file_list:\n",
    "    #file_name_root = parse_filename(file_name)[0]\n",
    "    local_file_path = local_upload_directory + file_name\n",
    "    s3_file_key = file_name\n",
    "\n",
    "    print('Uploading to s3:', file_name)\n",
    "    start_time = datetime.datetime.now()\n",
    "    s3.upload_file(local_file_path, s3_bucket_name, s3_file_key)\n",
    "    elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "    print('Upload complete in', elapsed_time, 's.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect and test the SSH tunnel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run this cell until we figure out how to make this command complete. The cell never stops executing and\n",
    "# that locks up the rest of the notebook. For now, run it in a separate terminal window (left open).\n",
    "\n",
    "# The EC2 server acting as the SSH tunnel has the IP address: 35.173.230.91\n",
    "\n",
    "# The following lines need to be added to the ~/.ssh/config file:\n",
    "\n",
    "\"\"\"\n",
    "host neptune\n",
    " ForwardAgent yes\n",
    " User ec2-user\n",
    " HostName 35.173.230.91\n",
    " IdentitiesOnly yes\n",
    " IdentityFile ~/NeptuneSSHtunnel.pem\n",
    " LocalForward 8182 triplestore1.cluster-cml0hq81gymg.us-east-1.neptune.amazonaws.com:8182\n",
    "\"\"\"\n",
    "\n",
    "# and the NeptuneSSHtunnel.pem file needs to be in the home directory\n",
    "\n",
    "# Start up SSH tunnel\n",
    "#os.system('ssh neptune -N') # uncomment if we get it to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'acceptedQueryCount': 56, 'runningQueryCount': 0, 'queries': []}\n"
     ]
    }
   ],
   "source": [
    "# Run this to make sure SSH tunnel is working\n",
    "try:\n",
    "    response = requests.get(loader_endpoint_url + '/status')\n",
    "    print(response.json())\n",
    "except Exception as e:\n",
    "    print('error', e.args[0])\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load files in S3 into triplestore\n",
    "\n",
    "For reference, Neptune's conformance with SPARQL is described at https://docs.aws.amazon.com/neptune/latest/userguide/feature-sparql-compliance.html . Most specifically, triples are always associated with a graph, even if one is not explicitly specified. See https://docs.aws.amazon.com/neptune/latest/userguide/feature-sparql-compliance.html#sparql-default-graph . The fallback URI `http://aws.amazon.com/neptune/vocab/v01/DefaultNamedGraph` is used.\n",
    "\n",
    "Setup and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate Neptune SPARQL object\n",
    "neptune = Sparqler(endpoint=loader_endpoint_url, sleep=0)\n",
    "\n",
    "# Load the query prefix section from a text file\n",
    "with open('prefixes.txt', 'rt', encoding='utf-8') as file_object:\n",
    "    prefixes_text = file_object.read()\n",
    "    \n",
    "# Load the named graph data CSV configuration\n",
    "if exists('named_graphs_config.yml'):\n",
    "    with open('named_graphs_config.yml') as file_object:\n",
    "        config_data = yaml.safe_load(file_object)\n",
    "else:\n",
    "    print('Must have a named_graphs_config.yml file for this script to operate.')\n",
    "    print()\n",
    "    raise KeyboardInterrupt\n",
    "\n",
    "# Load the data about named graphs to be updated\n",
    "named_graphs_df = csv_read('named_graphs.csv')\n",
    "\n",
    "# Load the data relating the graph names to the datafiles that contain the serializations\n",
    "graph_file_associations_df = csv_read('graph_file_associations.csv')\n",
    "\n",
    "# Check that all of the files exist before moving on\n",
    "for index, file in graph_file_associations_df.iterrows():\n",
    "    if not exists(local_upload_directory + file['filename']):\n",
    "        print(file['filename'], 'does not exist in the directory:', local_upload_directory)\n",
    "        print('Put it there, or adjust the graph_file_associations.csv file, and try again.')\n",
    "        print()\n",
    "        raise KeyboardInterrupt\n",
    "\n",
    "#    else:\n",
    "#        print(file['filename'], 'exists')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the triples from the S3 bucket into the triplestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading file: bibl-pelagios.rdf  into graph:  http://syriaca.org/bibl#graph\n",
      "beginning update\n",
      "done updating data in 7 s\n",
      "[\n",
      "  {\n",
      "    \"type\": \"UpdateEvent\",\n",
      "    \"totalElapsedMillis\": 2288,\n",
      "    \"elapsedMillis\": 2287,\n",
      "    \"connFlush\": 0,\n",
      "    \"batchResolve\": 0,\n",
      "    \"whereClause\": 0,\n",
      "    \"deleteClause\": 0,\n",
      "    \"insertClause\": 0\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Commit\",\n",
      "    \"totalElapsedMillis\": 7243\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "for index, file in graph_file_associations_df.iterrows():\n",
    "    graph_name = file['sd:name']\n",
    "    data = neptune.load(file['filename'], graph_name, s3=s3_bucket_name, verbose=True)\n",
    "    print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update the metadata about graphs to reflect the changes\n",
    "\n",
    "If there are old triples in the graphs metadata associated with the named graphs being updated, they are deleted from the <https://sparql.vanderbilt.edu/graphs> graph. See https://www.w3.org/TR/sparql11-update/#deleteWhere for reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete where {\n",
      "graph <https://sparql.vanderbilt.edu/graphs> {\n",
      "<http://syriaca.org/bibl#graph> ?o ?p.\n",
      "}}\n",
      "\n",
      "beginning update\n",
      "done updating data in 0 s\n",
      "[\n",
      "  {\n",
      "    \"type\": \"UpdateEvent\",\n",
      "    \"totalElapsedMillis\": 9,\n",
      "    \"elapsedMillis\": 8,\n",
      "    \"connFlush\": 0,\n",
      "    \"batchResolve\": 0,\n",
      "    \"whereClause\": 7,\n",
      "    \"deleteClause\": 0,\n",
      "    \"insertClause\": 0\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Commit\",\n",
      "    \"totalElapsedMillis\": 10\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Note that attempting to delete triples that don't exist or from a graph that doesn't exist has no effect\n",
    "# and generates no errors. \n",
    "\n",
    "\"\"\"Generates a request string of the form:\n",
    "delete where {\n",
    "graph <https://sparql.vanderbilt.edu/graphs> {\n",
    "<http://syriaca.org/bibl#graph> ?o ?p.\n",
    "\n",
    "...\n",
    "\n",
    "}}\n",
    "\"\"\"\n",
    "\n",
    "# NOTE: prefixes not needed since the subject IRI is unabbreviated.\n",
    "request_string = '''delete where {\n",
    "graph <https://sparql.vanderbilt.edu/graphs> {\\n'''\n",
    "for index, graph in named_graphs_df.iterrows():\n",
    "    request_string += '<' + graph['sd:name'] + '> ?o ?p.\\n'\n",
    "request_string += '}}\\n'    \n",
    "print(request_string)\n",
    "        \n",
    "data = neptune.update(request_string, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the new metadata for the service description and load it into the <https://sparql.vanderbilt.edu/graphs> graph. See https://www.w3.org/TR/sparql11-update/#insertData for reference. Note: unlike the examples at https://www.w3.org/TR/sparql11-service-description/#example-turtle and https://www.w3.org/TR/void/#sparql-sd , we don't use a blank node for the sd:NamedGraph instance. This makes it easier to manage linking it to the graph collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
      "prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
      "prefix xsd: <http://www.w3.org/2001/XMLSchema#>\n",
      "prefix sd: <http://www.w3.org/ns/sparql-service-description#>\n",
      "prefix void: <http://rdfs.org/ns/void#>\n",
      "prefix dc: <http://purl.org/dc/elements/1.1/>\n",
      "prefix dcterms: <http://purl.org/dc/terms/>\n",
      "prefix tdwgutility: <http://rs.tdwg.org/dwc/terms/attributes/>\n",
      "insert data {\n",
      "graph <https://sparql.vanderbilt.edu/graphs> {\n",
      "<http://syriaca.org/bibl#graph> dcterms:modified \"2022-06-06T18:04:54.026168-06:00\"^^xsd:dateTime.\n",
      "<http://syriaca.org/bibl#graph> sd:name <http://syriaca.org/bibl#graph>.\n",
      "<http://syriaca.org/bibl#graph> sd:graph <http://syriaca.org/bibl>.\n",
      "<http://syriaca.org/bibl#graph> dcterms:issued \"2018-03-06\"^^xsd:date.\n",
      "<http://syriaca.org/bibl#graph> dc:publisher \"syriaca.org\".\n",
      "<http://syriaca.org/bibl#graph> rdf:type sd:NamedGraph.\n",
      "<http://syriaca.org/bibl#graph> dcterms:isPartOf <http://syriaca.org/>.\n",
      "<http://syriaca.org/bibl#graph> tdwgutility:status \"production\".\n",
      "}}\n",
      "\n",
      "beginning update\n",
      "done updating data in 1 s\n",
      "[\n",
      "  {\n",
      "    \"type\": \"UpdateEvent\",\n",
      "    \"totalElapsedMillis\": 2,\n",
      "    \"elapsedMillis\": 0,\n",
      "    \"connFlush\": 0,\n",
      "    \"batchResolve\": 0,\n",
      "    \"whereClause\": 0,\n",
      "    \"deleteClause\": 0,\n",
      "    \"insertClause\": 0\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Commit\",\n",
      "    \"totalElapsedMillis\": 8\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Generates a request string of the form:\n",
    "\n",
    "prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "prefix xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "prefix sd: <http://www.w3.org/ns/sparql-service-description#>\n",
    "prefix dc: <http://purl.org/dc/elements/1.1/>\n",
    "prefix dcterms: <http://purl.org/dc/terms/>\n",
    "prefix tdwgutility: <http://rs.tdwg.org/dwc/terms/attributes/>\n",
    "insert data {\n",
    "graph <https://sparql.vanderbilt.edu/graphs> {\n",
    "<http://syriaca.org/bibl#graph> dcterms:modified \"2022-06-06T14:40:05.430286-06:00\"^^xsd:dateTime.\n",
    "<http://syriaca.org/bibl#graph> sd:name <http://syriaca.org/bibl#graph>.\n",
    "<http://syriaca.org/bibl#graph> sd:graph <http://syriaca.org/bibl>.\n",
    "<http://syriaca.org/bibl#graph> dcterms:issued \"2018-03-06\"^^xsd:date.\n",
    "<http://syriaca.org/bibl#graph> dc:publisher \"syriaca.org\".\n",
    "<http://syriaca.org/bibl#graph> rdf:type sd:NamedGraph.\n",
    "<http://syriaca.org/bibl#graph> dcterms:isPartOf <http://syriaca.org/>.\n",
    "<http://syriaca.org/bibl#graph> tdwgutility:status \"production\".\n",
    "\n",
    "...\n",
    "\n",
    "}}\n",
    "\"\"\"\n",
    "dcterms_modified = datetime.datetime.now().isoformat() + utc_offset\n",
    "\n",
    "request_string = prefixes_text + '''insert data {\n",
    "graph <https://sparql.vanderbilt.edu/graphs> {\\n'''\n",
    "for index, graph in named_graphs_df.iterrows():\n",
    "    named_graph_iri = graph['sd:name']\n",
    "    request_string += '<' + named_graph_iri + '> dcterms:modified \"' + dcterms_modified + '\"^^xsd:dateTime.\\n'\n",
    "    for column in config_data:\n",
    "        triple_pattern = '<' + named_graph_iri + '> ' + column['column_header'] + ' '\n",
    "        if column['object_type'] == 'iri':\n",
    "            triple_pattern += '<' + graph[column['column_header']] + '>'\n",
    "        elif column['object_type'] == 'curie':\n",
    "            triple_pattern += graph[column['column_header']]\n",
    "        elif column['object_type'] == 'literal':\n",
    "            triple_pattern += '\"' + graph[column['column_header']] + '\"'\n",
    "            if 'datatype' in column:\n",
    "                triple_pattern += '^^' + column['datatype']\n",
    "            if 'lang' in column:\n",
    "                triple_pattern += '@' + column['lang']\n",
    "        triple_pattern += '.\\n'\n",
    "        request_string += triple_pattern\n",
    "request_string += '}}\\n'    \n",
    "print(request_string)\n",
    "        \n",
    "data = neptune.update(request_string, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to add the named graph to the GraphCollection if it isn't there already and update the modified dateTime for the https://sparql.vanderbilt.edu/graphs graph. This uses the WITH clause to carry out both the insert and delete sequentially. See https://www.w3.org/TR/sparql11-update/#deleteInsert for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix dcterms: <http://purl.org/dc/terms/>\n",
      "\n",
      "with <https://sparql.vanderbilt.edu/graphs> \n",
      "delete {\n",
      "<https://sparql.vanderbilt.edu/graphs> dcterms:modified ?dateTime.\n",
      "}\n",
      "insert {\n",
      "<https://sparql.vanderbilt.edu/graphs> dcterms:modified \"2022-06-06T18:05:02.249377-06:00\"^^xsd:dateTime.\n",
      "}\n",
      "where {\n",
      "<https://sparql.vanderbilt.edu/graphs> dcterms:modified ?dateTime.\n",
      "}\n",
      "beginning update\n",
      "done updating data in 0 s\n",
      "[\n",
      "  {\n",
      "    \"type\": \"UpdateEvent\",\n",
      "    \"totalElapsedMillis\": 23,\n",
      "    \"elapsedMillis\": 21,\n",
      "    \"connFlush\": 0,\n",
      "    \"batchResolve\": 0,\n",
      "    \"whereClause\": 18,\n",
      "    \"deleteClause\": 1,\n",
      "    \"insertClause\": 0\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Commit\",\n",
      "    \"totalElapsedMillis\": 26\n",
      "  }\n",
      "]\n",
      "\n",
      "prefix sd: <http://www.w3.org/ns/sparql-service-description#>\n",
      "delete where {\n",
      "graph <https://sparql.vanderbilt.edu/graphs> {\n",
      "<https://sparql.vanderbilt.edu/graphs#collection> sd:namedGraph <http://syriaca.org/bibl#graph>.\n",
      "}}\n",
      "\n",
      "beginning update\n",
      "done updating data in 0 s\n",
      "[\n",
      "  {\n",
      "    \"type\": \"UpdateEvent\",\n",
      "    \"totalElapsedMillis\": 7,\n",
      "    \"elapsedMillis\": 6,\n",
      "    \"connFlush\": 0,\n",
      "    \"batchResolve\": 0,\n",
      "    \"whereClause\": 5,\n",
      "    \"deleteClause\": 0,\n",
      "    \"insertClause\": 0\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Commit\",\n",
      "    \"totalElapsedMillis\": 7\n",
      "  }\n",
      "]\n",
      "\n",
      "prefix sd: <http://www.w3.org/ns/sparql-service-description#>\n",
      "insert data {\n",
      "graph <https://sparql.vanderbilt.edu/graphs> {\n",
      "<https://sparql.vanderbilt.edu/graphs#collection> sd:namedGraph <http://syriaca.org/bibl#graph>.\n",
      "}}\n",
      "\n",
      "beginning update\n",
      "done updating data in 0 s\n",
      "[\n",
      "  {\n",
      "    \"type\": \"UpdateEvent\",\n",
      "    \"totalElapsedMillis\": 1,\n",
      "    \"elapsedMillis\": 0,\n",
      "    \"connFlush\": 0,\n",
      "    \"batchResolve\": 0,\n",
      "    \"whereClause\": 0,\n",
      "    \"deleteClause\": 0,\n",
      "    \"insertClause\": 0\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Commit\",\n",
      "    \"totalElapsedMillis\": 5\n",
      "  }\n",
      "]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Update the modified time for the graph of graphs data \n",
    "dcterms_modified = datetime.datetime.now().isoformat() + utc_offset\n",
    "request_string = '''prefix dcterms: <http://purl.org/dc/terms/>\n",
    "\n",
    "with <https://sparql.vanderbilt.edu/graphs> \n",
    "delete {\n",
    "<https://sparql.vanderbilt.edu/graphs> dcterms:modified ?dateTime.\n",
    "}\n",
    "insert {\n",
    "<https://sparql.vanderbilt.edu/graphs> dcterms:modified \"''' + dcterms_modified + '''\"^^xsd:dateTime.\n",
    "}\n",
    "where {\n",
    "<https://sparql.vanderbilt.edu/graphs> dcterms:modified ?dateTime.\n",
    "}'''\n",
    "\n",
    "print(request_string)\n",
    "\n",
    "data = neptune.update(request_string, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n",
    "print()\n",
    "\n",
    "# Delete the links from the GraphCollection to the named graphs so that the triples won't get duplicated when added\n",
    "# NOTE: this may not be necessary since it doesn't appear that duplicates get generated if we don't delete. \n",
    "# But it take negligible time and doesn't hurt anything.\n",
    "request_string = '''prefix sd: <http://www.w3.org/ns/sparql-service-description#>\n",
    "delete where {\n",
    "graph <https://sparql.vanderbilt.edu/graphs> {\\n'''\n",
    "for index, graph in named_graphs_df.iterrows():\n",
    "    request_string += '<https://sparql.vanderbilt.edu/graphs#collection> sd:namedGraph <' + graph['sd:name'] + '>.\\n'\n",
    "request_string += '}}\\n'    \n",
    "print(request_string)\n",
    "        \n",
    "data = neptune.update(request_string, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n",
    "print()\n",
    "\n",
    "# Now insert the linking triples\n",
    "request_string = '''prefix sd: <http://www.w3.org/ns/sparql-service-description#>\n",
    "insert data {\n",
    "graph <https://sparql.vanderbilt.edu/graphs> {\\n'''\n",
    "for index, graph in named_graphs_df.iterrows():\n",
    "    request_string += '<https://sparql.vanderbilt.edu/graphs#collection> sd:namedGraph <' + graph['sd:name'] + '>.\\n'\n",
    "request_string += '}}\\n'    \n",
    "print(request_string)\n",
    "        \n",
    "data = neptune.update(request_string, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
      "prefix xsd: <http://www.w3.org/2001/XMLSchema#>\n",
      "prefix dc: <http://purl.org/dc/elements/1.1/>\n",
      "prefix dcterms: <http://purl.org/dc/terms/>\n",
      "prefix sd: <http://www.w3.org/ns/sparql-service-description#>\n",
      "prefix tdwgutility: <http://rs.tdwg.org/dwc/terms/attributes/>\n",
      "\n",
      "insert data {\n",
      "graph <https://sparql.vanderbilt.edu/graphs> {\n",
      "<https://sparql.vanderbilt.edu/graphs> dcterms:modified \"2022-06-06T18:02:45.642038-06:00\"^^xsd:dateTime;\n",
      "    sd:name <https://sparql.vanderbilt.edu/graphs>;\n",
      "    dc:publisher \"Vanderbilt Heard Libraries\";\n",
      "    a sd:NamedGraph;\n",
      "    tdwgutility:status \"production\".\n",
      "<https://sparql.vanderbilt.edu/sparql#service> a sd:Service;\n",
      "    sd:endpoint <https://sparql.vanderbilt.edu/sparql>;\n",
      "    sd:availableGraphs <https://sparql.vanderbilt.edu/graphs#collection>.\n",
      "<https://sparql.vanderbilt.edu/graphs#collection> a sd:GraphCollection;\n",
      "    sd:namedGraph <https://sparql.vanderbilt.edu/graphs>.\n",
      "}}\n",
      "\n",
      "beginning update\n",
      "done updating data in 0 s\n",
      "[\n",
      "  {\n",
      "    \"type\": \"UpdateEvent\",\n",
      "    \"totalElapsedMillis\": 1,\n",
      "    \"elapsedMillis\": 0,\n",
      "    \"connFlush\": 0,\n",
      "    \"batchResolve\": 0,\n",
      "    \"whereClause\": 0,\n",
      "    \"deleteClause\": 0,\n",
      "    \"insertClause\": 0\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Commit\",\n",
      "    \"totalElapsedMillis\": 12\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# One-time generation of service description metadata\n",
    "\n",
    "dcterms_modified = datetime.datetime.now().isoformat() + utc_offset\n",
    "\n",
    "request_string = '''prefix rdf: <http://www.w3.org/1999/02/22-rdf-syntax-ns#>\n",
    "prefix xsd: <http://www.w3.org/2001/XMLSchema#>\n",
    "prefix dc: <http://purl.org/dc/elements/1.1/>\n",
    "prefix dcterms: <http://purl.org/dc/terms/>\n",
    "prefix sd: <http://www.w3.org/ns/sparql-service-description#>\n",
    "prefix tdwgutility: <http://rs.tdwg.org/dwc/terms/attributes/>\n",
    "\n",
    "insert data {\n",
    "graph <https://sparql.vanderbilt.edu/graphs> {\n",
    "<https://sparql.vanderbilt.edu/graphs> dcterms:modified \"''' + dcterms_modified + '''\"^^xsd:dateTime;\n",
    "    sd:name <https://sparql.vanderbilt.edu/graphs>;\n",
    "    dc:publisher \"Vanderbilt Heard Libraries\";\n",
    "    a sd:NamedGraph;\n",
    "    tdwgutility:status \"production\".\n",
    "<https://sparql.vanderbilt.edu/sparql#service> a sd:Service;\n",
    "    sd:endpoint <https://sparql.vanderbilt.edu/sparql>;\n",
    "    sd:availableGraphs <https://sparql.vanderbilt.edu/graphs#collection>.\n",
    "<https://sparql.vanderbilt.edu/graphs#collection> a sd:GraphCollection;\n",
    "    sd:namedGraph <https://sparql.vanderbilt.edu/graphs>.\n",
    "}}\n",
    "'''\n",
    "\n",
    "print(request_string)\n",
    "\n",
    "data = neptune.update(request_string, verbose=True)\n",
    "print(json.dumps(data, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code for dropping graphs\n",
    "\n",
    "(under development)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"type\": \"UpdateEvent\",\n",
      "    \"totalElapsedMillis\": 7,\n",
      "    \"elapsedMillis\": 2,\n",
      "    \"connFlush\": 0,\n",
      "    \"batchResolve\": 0,\n",
      "    \"whereClause\": 0,\n",
      "    \"deleteClause\": 0,\n",
      "    \"insertClause\": 0\n",
      "  },\n",
      "  {\n",
      "    \"type\": \"Commit\",\n",
      "    \"totalElapsedMillis\": 11\n",
      "  }\n",
      "]\n",
      "time to delete: 0 s\n"
     ]
    }
   ],
   "source": [
    "# POST SPARQL Update DROP GRAPH command\n",
    "\n",
    "query_string = 'DROP GRAPH <' + graph_iri + '>'\n",
    "start_time = datetime.datetime.now()\n",
    "response = requests.post(loader_endpoint_url + '/sparql', data=query_string.encode('utf-8'), headers=update_request_header_dictionary)\n",
    "#print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "data = response.json()\n",
    "print(json.dumps(data, indent = 2))\n",
    "\n",
    "elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "print('time to delete:', int(elapsed_time), 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DO NOT RUN this cell! It will delete everything in the triplestore\n",
    "\n",
    "Actually, I've left it with an error so that it will fail if it's accidentally run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST SPARQL Update DROP ALL command\n",
    "# Warning! This deletes all triples for all graphs !!!!\n",
    "\n",
    "query_string = 'DROP ALL'\n",
    "start_time = datetime.datetime.now()\n",
    "response = requests.post(loader_endpoint_url + '/sparql', data=query_string.encode('utf-8'), headers=update_request_header_dictionary)\n",
    "#print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "data = response.json()\n",
    "print(json.dumps(data, indent = 2))\n",
    "\n",
    "elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "print('time to load:', int(elapsed_time), 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obsolete development code\n",
    "\n",
    "Left for historical reference\n",
    "\n",
    "## Code to test the Neptune-specific loader\n",
    "\n",
    "There did not seem to be any real benefit to using this loader with respect to speed. It also does not have any mechanism for specifying the graph into which triples should be loaded, so the only way to load them into a specific graph is to use the n-quads format. See following cell to convert from n-triples to n-quads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST loader command\n",
    "\n",
    "loader_request_header_dictionary = {\n",
    "        'Accept' : 'application/json',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "rdf_format = 'nquads'\n",
    "#rdf_format = 'ntriples'\n",
    "\n",
    "data = '''\n",
    "    {\n",
    "      \"source\" : \"s3://'''+ s3_bucket_name + '/' + s3_file_key + '''\",\n",
    "      \"format\" : \"'''  + rdf_format + '''\",\n",
    "      \"iamRoleArn\" : \"arn:aws:iam::555751041262:role/neptuneloadfroms3\",\n",
    "      \"region\" : \"us-east-1\",\n",
    "      \"failOnError\" : \"FALSE\",\n",
    "      \"parallelism\" : \"MEDIUM\",\n",
    "      \"updateSingleCardinalityProperties\" : \"FALSE\",\n",
    "      \"queueRequest\" : \"TRUE\"\n",
    "    }'''\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Send request to load\n",
    "response = requests.post(loader_endpoint_url + '/loader', data=data.encode('utf-8'), headers=loader_request_header_dictionary)\n",
    "data = response.json()\n",
    "print(json.dumps(data, indent = 2))\n",
    "load_id = data['payload']['loadId']\n",
    "\n",
    "# Check status of load once per second\n",
    "completed = False\n",
    "while not completed:\n",
    "    response = requests.get(loader_endpoint_url + '/loader/' + load_id)\n",
    "    data = response.json()\n",
    "    #print(json.dumps(data, indent = 2))\n",
    "    print(data['payload']['overallStatus']['status'])\n",
    "    if data['payload']['overallStatus']['status'] == 'LOAD_COMPLETED' or data['payload']['overallStatus']['status'] == 'LOAD_FAILED':\n",
    "        completed = True\n",
    "    sleep(1)\n",
    "elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "print('time to load:', int(elapsed_time), 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting n-triples to n-quads to specify graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(local_directory)\n",
    "file_list.remove('.DS_Store')\n",
    "\n",
    "# Note: assumes all files are n-triples serialization with .nt file extensions.\n",
    "for file_name in file_list:\n",
    "    file_name_root = parse_filename(file_name)[0]\n",
    "    print('converting:', file_name)\n",
    "    output_filename = file_name_root + '.nq'\n",
    "    graph_string = ' <http://' + file_name_root + '> .'\n",
    "\n",
    "    output_file_object = open(local_directory + output_filename, 'wt', encoding='utf-8')\n",
    "    input_file_object = open(local_directory + file_name, 'rt', encoding='utf-8')\n",
    "\n",
    "    for line in input_file_object:\n",
    "        line_text = line.strip() # remove trailing newline\n",
    "        line_text = line_text[:-1] + graph_string # remove period at end.\n",
    "        print(line_text, file=output_file_object)\n",
    "\n",
    "    output_file_object.close()\n",
    "    input_file_object.close()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
