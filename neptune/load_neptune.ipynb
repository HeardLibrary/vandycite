{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# Configuration\n",
    "# ----------------\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "import sys\n",
    "import re # regex\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "\n",
    "# AWS Python SDK\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "loader_endpoint_url = 'https://triplestore1.cluster-cml0hq81gymg.us-east-1.neptune.amazonaws.com:8182'\n",
    "\n",
    "#local_filename = 'nomenclature_2022-02-02.jsonld'\n",
    "local_filename = 'AATOut_2Terms.nq'\n",
    "local_directory = '/Users/baskausj/triplestore_upload/'\n",
    "\n",
    "s3_bucket_name = 'triplestore-upload'\n",
    "\n",
    "# See https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3.html#uploads\n",
    "local_file_path = local_directory + local_filename\n",
    "s3_file_key = local_filename\n",
    "# s3_file_key = s3_iiif_project_directory + '/' + subdirectory + '/' + local_filename\n",
    "\n",
    "graph_iri = 'http://aatterms'\n",
    "\n",
    "update_request_header_dictionary = {\n",
    "        'Accept' : 'application/json',\n",
    "        'Content-Type': 'application/sparql-update'\n",
    "    }\n",
    "\n",
    "def parse_filename(filename):\n",
    "    pieces = filename.split('.')\n",
    "    file_name_root = '.'.join(pieces[:-1])\n",
    "    extension = pieces[len(pieces)-1]\n",
    "    return file_name_root, extension\n",
    "                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# Upload RDF triples to s3 bucket\n",
    "# ----------------\n",
    "# NOTE: assuming they are n-triples, change extension if not.\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "file_list = os.listdir(local_directory)\n",
    "file_list.remove('.DS_Store')\n",
    "\n",
    "for file_name in file_list:\n",
    "    file_name_root = parse_filename(file_name)[0]\n",
    "    local_file_path = local_directory + file_name\n",
    "    s3_file_key = local_filename\n",
    "\n",
    "    print('Uploading to s3:', file_name)\n",
    "    s3.upload_file(local_file_path, s3_bucket_name, s3_file_key)\n",
    "    print('Upload complete')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't run this cell until we figure out how to make this command complete. The cell never stops executing and\n",
    "# that locks up the rest of the notebook.\n",
    "\n",
    "# Start up SSH tunnel\n",
    "os.system('ssh neptune -N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this to make sure SSH tunnel is working\n",
    "try:\n",
    "    response = requests.get(loader_endpoint_url + '/status')\n",
    "    print(response.json())\n",
    "except Exception as e:\n",
    "    print('error', e.args[0])\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST SPARQL Update LOAD command\n",
    "# NOTE: assuming they are n-triples, change extension if not.\n",
    "\n",
    "file_list = os.listdir(local_directory)\n",
    "file_list.remove('.DS_Store')\n",
    "\n",
    "total_start_time = datetime.datetime.now()\n",
    "for file_name in file_list:\n",
    "    file_name_root = parse_filename(file_name)[0]\n",
    "    s3_file_key = file_name\n",
    "    graph_iri = 'http://' + file_name_root\n",
    "\n",
    "    print('Loading into Neptune:', file_name)\n",
    "\n",
    "    query_string = 'LOAD <https://' + s3_bucket_name + '.s3.amazonaws.com/' + s3_file_key + '> INTO GRAPH <' + graph_iri + '>'\n",
    "    start_time = datetime.datetime.now()\n",
    "    response = requests.post(loader_endpoint_url + '/sparql', data=query_string.encode('utf-8'), headers=update_request_header_dictionary)\n",
    "    #print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "    data = response.json()\n",
    "    print(json.dumps(data, indent = 2))\n",
    "\n",
    "    elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "    print('time to load:', int(elapsed_time), 's')\n",
    "    print()\n",
    "    \n",
    "total_elapsed_time = (datetime.datetime.now() - total_start_time).total_seconds()\n",
    "print(total_elapsed_time)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST SPARQL Update DROP GRAPH command\n",
    "\n",
    "query_string = 'DROP GRAPH <' + graph_iri + '>'\n",
    "start_time = datetime.datetime.now()\n",
    "response = requests.post(loader_endpoint_url + '/sparql', data=query_string.encode('utf-8'), headers=update_request_header_dictionary)\n",
    "#print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "data = response.json()\n",
    "print(json.dumps(data, indent = 2))\n",
    "\n",
    "elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "print('time to delete:', int(elapsed_time), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST SPARQL Update DROP ALL command\n",
    "# Warning! This deletes all triples for all graphs !!!!\n",
    "\n",
    "query_string = 'DROP ALL'\n",
    "start_time = datetime.datetime.now()\n",
    "response = requests.post(loader_endpoint_url + '/sparql', data=query_string.encode('utf-8'), headers=update_request_header_dictionary)\n",
    "#print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "data = response.json()\n",
    "print(json.dumps(data, indent = 2))\n",
    "\n",
    "elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "print('time to load:', int(elapsed_time), 's')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST loader command\n",
    "\n",
    "loader_request_header_dictionary = {\n",
    "        'Accept' : 'application/json',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "rdf_format = 'nquads'\n",
    "#rdf_format = 'ntriples'\n",
    "\n",
    "data = '''\n",
    "    {\n",
    "      \"source\" : \"s3://'''+ s3_bucket_name + '/' + s3_file_key + '''\",\n",
    "      \"format\" : \"'''  + rdf_format + '''\",\n",
    "      \"iamRoleArn\" : \"arn:aws:iam::555751041262:role/neptuneloadfroms3\",\n",
    "      \"region\" : \"us-east-1\",\n",
    "      \"failOnError\" : \"FALSE\",\n",
    "      \"parallelism\" : \"MEDIUM\",\n",
    "      \"updateSingleCardinalityProperties\" : \"FALSE\",\n",
    "      \"queueRequest\" : \"TRUE\"\n",
    "    }'''\n",
    "\n",
    "start_time = datetime.datetime.now()\n",
    "\n",
    "# Send request to load\n",
    "response = requests.post(loader_endpoint_url + '/loader', data=data.encode('utf-8'), headers=loader_request_header_dictionary)\n",
    "data = response.json()\n",
    "print(json.dumps(data, indent = 2))\n",
    "load_id = data['payload']['loadId']\n",
    "\n",
    "# Check status of load once per second\n",
    "completed = False\n",
    "while not completed:\n",
    "    response = requests.get(loader_endpoint_url + '/loader/' + load_id)\n",
    "    data = response.json()\n",
    "    #print(json.dumps(data, indent = 2))\n",
    "    print(data['payload']['overallStatus']['status'])\n",
    "    if data['payload']['overallStatus']['status'] == 'LOAD_COMPLETED' or data['payload']['overallStatus']['status'] == 'LOAD_FAILED':\n",
    "        completed = True\n",
    "    sleep(1)\n",
    "elapsed_time = (datetime.datetime.now() - start_time).total_seconds()\n",
    "print('time to load:', int(elapsed_time), 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting n-triples to n-quads to specify graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = os.listdir(local_directory)\n",
    "file_list.remove('.DS_Store')\n",
    "\n",
    "# Note: assumes all files are n-triples serialization with .nt file extensions.\n",
    "for file_name in file_list:\n",
    "    file_name_root = parse_filename(file_name)[0]\n",
    "    print('converting:', file_name)\n",
    "    output_filename = file_name_root + '.nq'\n",
    "    graph_string = ' <http://' + file_name_root + '> .'\n",
    "\n",
    "    output_file_object = open(local_directory + output_filename, 'wt', encoding='utf-8')\n",
    "    input_file_object = open(local_directory + file_name, 'rt', encoding='utf-8')\n",
    "\n",
    "    for line in input_file_object:\n",
    "        line_text = line.strip() # remove trailing newline\n",
    "        line_text = line_text[:-1] + graph_string # remove period at end.\n",
    "        print(line_text, file=output_file_object)\n",
    "\n",
    "    output_file_object.close()\n",
    "    input_file_object.close()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
