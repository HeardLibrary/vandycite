{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publoader.ipynb, a Python script for uploading files and data to Wikimedia Commons using the API.\n",
    "# version 0.1.0\n",
    "\n",
    "# (c) 2023 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "# ----------------\n",
    "# Module imports\n",
    "# ----------------\n",
    "\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "import yaml\n",
    "import sys\n",
    "#import csv\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import requests_cache\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from langdetect import detect_langs\n",
    "import re # regex\n",
    "import logging # See https://docs.python.org/3/howto/logging.html\n",
    "\n",
    "# module located in same directory as script\n",
    "import mapping_functions\n",
    "\n",
    "# Set up cache for HTTP requests\n",
    "requests_cache.install_cache('wqs_cache', backend='sqlite', expire_after=300, allowable_methods=['GET', 'POST'])\n",
    "\n",
    "# ------------------------\n",
    "# Utility functions\n",
    "# ------------------------\n",
    "\n",
    "def calculate_todays_date() -> str:\n",
    "    \"\"\"Generate the current UTC xsd:date.\"\"\"\n",
    "    whole_time_string_z = datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "def extract_local_name(iri: str) -> str:\n",
    "    \"\"\"Extract the local name part of an IRI, e.g. a Q ID from a Wikidata IRI.\n",
    "    \n",
    "    Note:\n",
    "    -----\n",
    "    Expected IRI pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    \"\"\"\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[-1]\n",
    "\n",
    "def include_reference_url(url: str, full_works: pd.DataFrame) -> str:\n",
    "    \"\"\"Determine whether a documentation URL is suitable to be used as a reference URL or full text available.\"\"\"\n",
    "    url_pattern = \"^https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)$\"\n",
    "    url_inclusion_strings = [\n",
    "    'doi',\n",
    "    'jstor',\n",
    "    #'oxfordjournals.org/content',\n",
    "    'article',\n",
    "    'academia.edu',\n",
    "    'content',\n",
    "    'proquest.com/docview',\n",
    "    'handle'\n",
    "    ]\n",
    "    \n",
    "    url_exclusion_strings = [\n",
    "    'login',\n",
    "    'proxy',\n",
    "    #'search.proquest.com',\n",
    "    'worldcat',\n",
    "    'wp-content',\n",
    "    'site.ebrary.com',\n",
    "    'cro3.org/',\n",
    "    'worldbookonline.com/pl/infofinder'\n",
    "    ]\n",
    "\n",
    "    url = url.lower() # convert to all lowercase\n",
    "    \n",
    "    # Exclude invalid URLs\n",
    "    if re.match(url_pattern, url) is None:\n",
    "        return ''\n",
    "\n",
    "    # If the URL matches one of the pre-screened URLs, use it\n",
    "    matched_series = full_works.loc[full_works['Url']==url, 'Url']\n",
    "    # matched_series will be a Series composed of all values in the Url column that match. There should be 1 or 0.\n",
    "    if len(matched_series) == 1:\n",
    "        return url\n",
    "    \n",
    "    # Exclude any URLs containing strings that indicate a login is required\n",
    "    for screening_string in url_exclusion_strings:\n",
    "        if screening_string in url:\n",
    "            return ''\n",
    "        \n",
    "    # Must contain one of the strings that indicate metadata and possible acces\n",
    "    for screening_string in url_inclusion_strings:\n",
    "        if screening_string in url:\n",
    "            return url\n",
    "        \n",
    "    return ''\n",
    "\n",
    "def set_description(string: str, work_types: List[dict]) -> str:\n",
    "    \"\"\"Match the type string with possible types for the data source and return the description.\"\"\"\n",
    "    if string == '':\n",
    "        return ''\n",
    "    \n",
    "    for work_type in work_types:\n",
    "        if string == work_type['type_string']:\n",
    "            return work_type['description']\n",
    "\n",
    "    print('No description, did not find datatype for type:', string)\n",
    "    logging.warning('No description, did not find datatype for type: ' + string)\n",
    "    return ''\n",
    "\n",
    "def title_if_no_lowercase(string: str) -> str:\n",
    "    \"\"\"Change to titlecase only if there are no lowercase letters in the string.\"\"\"\n",
    "    lower = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    is_lower = False\n",
    "    for letter in string:\n",
    "        if letter in lower:\n",
    "            is_lower = True\n",
    "    if is_lower:\n",
    "        return string\n",
    "    else:\n",
    "        return string.title()\n",
    "\n",
    "def fix_all_caps(name_pieces: List[str]) -> List[str]:\n",
    "    \"\"\"Correct the capitalization for a list of name parts that are in all caps.\"\"\"\n",
    "    clean_pieces = []\n",
    "    for piece in name_pieces:\n",
    "        # Special handing for names starting with apostrophe-based prefixes\n",
    "        apostrophe_list = [\"van't\", \"'t\", \"O'\", \"D'\", \"d'\", \"N'\"]\n",
    "        apostrophe_prefix = ''\n",
    "        for possible_apostrophe_prefix in apostrophe_list:\n",
    "            if possible_apostrophe_prefix in piece:\n",
    "                # Remove prefix\n",
    "                piece = piece.replace(possible_apostrophe_prefix, '')\n",
    "                apostrophe_prefix = possible_apostrophe_prefix\n",
    "        \n",
    "        # Special handling for name parts that are lowercase\n",
    "        lower_case_list = ['von', 'de', 'van', 'la', 'der']\n",
    "        if piece.lower() in lower_case_list:\n",
    "            piece = piece.lower()\n",
    "        else:\n",
    "            # Special handling for hyphenated names; doesn't work for an edge case with more than 2 hyphens\n",
    "            if '-' in piece:\n",
    "                halves = piece.split('-')\n",
    "                piece = title_if_no_lowercase(halves[0]) + '-' + title_if_no_lowercase(halves[1])\n",
    "            else:\n",
    "                piece = title_if_no_lowercase(piece)\n",
    "        \n",
    "        # put any apostrophe prefix back on the front\n",
    "        if apostrophe_prefix:\n",
    "            piece = apostrophe_prefix + piece\n",
    "        \n",
    "        clean_pieces.append(piece)\n",
    "    return clean_pieces\n",
    "  \n",
    "def extract_name_pieces(name: str) -> Tuple[List[str], str]:\n",
    "    \"\"\"Extract parts of names. Recognize typical male suffixes. Fix ALL CAPS if present.\"\"\"\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods, sometimes periods are close up with no spaces\n",
    "    name = name.replace('.', ' ')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "        \n",
    "    # Fix stupid situation where name is written in ALL CAPS\n",
    "    pieces = fix_all_caps(pieces)\n",
    "    return pieces, suffix\n",
    "    \n",
    "def extract_identifier_from_extra(extra_field: str, id_name: str) -> str:\n",
    "    \"\"\"Extract a specified identifier (book DOI, article PMID) from the Zotero export Extra field.\"\"\"\n",
    "    identifier = ''\n",
    "    tokens = extra_field.split(' ')\n",
    "    for token_index in range(len(tokens)):\n",
    "        if tokens[token_index] == id_name + ':': # match the tag for the desired ID\n",
    "            # The identifer is the next token after the tag\n",
    "            identifier = tokens[token_index + 1]\n",
    "            break\n",
    "    return identifier\n",
    "\n",
    "def search_name_at_wikidata(name: str, user_agent: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Carry out a search of labels in languages that use Latin characters, and other commonly used languages.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of dictionaries providing the Q IDs and names that match the passed-in name.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    See https://doi.org/10.1145/3233391.3233965 for reference.\n",
    "    \"\"\"\n",
    "    language_codes = [\n",
    "        'en',\n",
    "        'es',\n",
    "        'pt',\n",
    "        'fr',\n",
    "        'it',\n",
    "        'nl',\n",
    "        'de',\n",
    "        'da',\n",
    "        'et',\n",
    "        'hu',\n",
    "        'ga',\n",
    "        'ro',\n",
    "        'sk',\n",
    "        'sl',\n",
    "        'zu',\n",
    "        'tr',\n",
    "        'sv',\n",
    "        'zh',\n",
    "        'ru',\n",
    "        'ja',\n",
    "        'ar',\n",
    "        'pl',\n",
    "        'uk',\n",
    "        'ca',\n",
    "        'cs',\n",
    "        'la',\n",
    "        'nb',\n",
    "        'hu',\n",
    "        'he',\n",
    "        'eo',\n",
    "        'fi',\n",
    "        'ko'\n",
    "      ]\n",
    "    name_list = generate_name_alternatives(name)\n",
    "    alternatives = ''\n",
    "    for language_code in language_codes:\n",
    "        for alternative in name_list:\n",
    "            # get rid of quotes, which will break the query\n",
    "            alternative = alternative.replace('\"', '')\n",
    "            alternative = alternative.replace(\"'\", '')\n",
    "            alternatives += '\"' + alternative + '\"@' + language_code + '\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    \n",
    "    wdqs = Sparqler(useragent=user_agent)\n",
    "    statements = wdqs.query(query)\n",
    "    sleep(settings['sparql_sleep'])\n",
    "\n",
    "    results = []\n",
    "    for statement in statements:\n",
    "        wikidata_iri = statement['item']['value']\n",
    "        if 'label' in statement:\n",
    "            name = statement['label']['value']\n",
    "        else:\n",
    "            name = ''\n",
    "        qnumber = extract_local_name(wikidata_iri)\n",
    "        results.append({'qid': qnumber, 'name': name})\n",
    "    return results\n",
    "\n",
    "def search_wikidata_occ_emp_aff(qid: str, default_language: str, user_agent: str) -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"Search Wikidata by Q ID for occupation, employer, and affiliation claims.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Lists of occupations, employers, and affiliations.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "\n",
    "    query_string = '''select distinct ?occupation ?employer ?affiliation where {\n",
    "        optional {\n",
    "            wd:'''+ qid + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = \"'''+ default_language + '''\")            \n",
    "            }\n",
    "\n",
    "        optional {\n",
    "            wd:'''+ qid + ''' wdt:P108 ?employerId.\n",
    "            ?employerId rdfs:label ?employer.\n",
    "            FILTER(lang(?employer) = \"'''+ default_language + '''\")            \n",
    "            }\n",
    "\n",
    "        optional {\n",
    "            wd:'''+ qid + ''' wdt:P1416 ?affiliationId.\n",
    "            ?affiliationId rdfs:label ?affiliation.\n",
    "            FILTER(lang(?affiliation) = \"'''+ default_language + '''\")            \n",
    "            }\n",
    "        }'''\n",
    "    #print(query_string)\n",
    "    \n",
    "    wdqs = Sparqler(useragent=user_agent)\n",
    "    statements = wdqs.query(query_string)\n",
    "    sleep(settings['sparql_sleep'])\n",
    "    #print(statements)\n",
    "    \n",
    "    # pull all possible occupations\n",
    "    occupationList = []\n",
    "    employerList = []\n",
    "    affiliationList = []\n",
    "    for statement in statements:\n",
    "        if 'occupation' in statement:\n",
    "            occupationList.append(statement['occupation']['value'])\n",
    "        if 'employer' in statement:\n",
    "            employerList.append(statement['employer']['value'])\n",
    "        if 'affiliation' in statement:\n",
    "            affiliationList.append(statement['affiliation']['value'])\n",
    "    occupationList = list(set(occupationList))\n",
    "    employerList = list(set(employerList))\n",
    "    affiliationList = list(set(affiliationList))\n",
    "    #print(occupationList)\n",
    "    #print(employerList)\n",
    "    #print(affiliationList)\n",
    "    \n",
    "    return occupationList, employerList, affiliationList \n",
    "\n",
    "\n",
    "def find_surname_givens(name: str) -> Dict[str, str]:\n",
    "    \"\"\"Extract surname and given names from a full name string and remove typical male suffixes.\"\"\"\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', ' ')\n",
    "    name = name.replace(',', ' ')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Get rid of empty pieces formed from extra spaces\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "        \n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "def generate_name_alternatives(name: str) -> List[str]:\n",
    "    \"\"\"Generate permutations of names and initials (with and without periods) for a label and alias query.\"\"\"\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods, sometimes periods are close up with no spaces\n",
    "    name = name.replace('.', ' ')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "        \n",
    "    # Fix stupid situation where name is written in ALL CAPS\n",
    "    pieces = fix_all_caps(pieces)        \n",
    "\n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        # make sure first character is alphabetic\n",
    "        # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "        # typical cases are like (Kit) or \"Kit\"\n",
    "        if not piece[0:1].isalpha():\n",
    "            piece = piece[1:len(piece)] # remove the first non-alphabetic character\n",
    "        if len(piece) > 0:\n",
    "            initials.append(piece[0:1])\n",
    "        \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += pieces[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # full name with suffix\n",
    "    if suffix != '':\n",
    "        name_version = ''\n",
    "        for piece_number in range(0, len(pieces)-1):\n",
    "            name_version += pieces[piece_number] + ' '\n",
    "        name_version += pieces[len(pieces)-1] + suffix\n",
    "        alternatives.append(name_version)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    name_version = pieces[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    name_version = pieces[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first and last name only\n",
    "    name_version = pieces[0] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial and last name only\n",
    "    name_version = initials[0] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    name_version = initials[0] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial no period and all other names\n",
    "    name_version = initials[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += pieces[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial with period and all other names\n",
    "    name_version = initials[0] + '. '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += pieces[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials with last name\n",
    "    name_version = initials[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += initials[piece_number]\n",
    "    name_version += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def screen_qids(qids: List[str], screens: List[Dict[str, str]], default_language: str, user_agent: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Screen Q IDs based on criteria saved in the screens.yaml configuration file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of dictionaries providing labels and descriptions that match the queried Q IDs.\n",
    "    \"\"\"\n",
    "    qid_values =''\n",
    "    for qid in qids:\n",
    "        qid_values += 'wd:' + qid + '\\n'\n",
    "\n",
    "    graph_pattern = ''\n",
    "    first_screen = True\n",
    "    for screen in screens:\n",
    "        # Each requirement in a screen has an AND relationship (all must be satisfied)\n",
    "        subgraph_pattern = ''\n",
    "        for requirement in screen:\n",
    "\n",
    "            # Set the value if required or use a dummy variable if any value is allowed\n",
    "            if requirement['entity'] is None:\n",
    "                value = '?var' + requirement['property'] # add the property string to the variable to guarantee uniqueness\n",
    "            elif re.match(r'Q\\d+', requirement['entity']): # regex to match Q IDs\n",
    "                value = 'wd:' + requirement['entity']\n",
    "            else: # if not nothing or a Q ID, assume it's a string literal\n",
    "                if requirement['lang'] is None:\n",
    "                    value = '\"' + requirement['entity'] + '\"'\n",
    "                else:\n",
    "                    value = '\"' + requirement['entity'] + '\"@' + requirement['lang']\n",
    "\n",
    "            # Set the property (label, description, or P value)\n",
    "            if requirement['property'] == 'label':\n",
    "                property = 'rdfs:label'\n",
    "            elif requirement['property'] == 'description':\n",
    "                property = 'schema:description'\n",
    "            else:\n",
    "                property = 'wdt:' + requirement['property']\n",
    "\n",
    "            # Place the value in either the subject or object position in the triple\n",
    "            if requirement['position'] == 'object':\n",
    "                triple_pattern = '?qid ' + property + ' ' + value + '.'\n",
    "            else:\n",
    "                triple_pattern = value + ' ' + property + ' ?qid.'\n",
    "\n",
    "            # Add filters if needed\n",
    "            if requirement['filter_type'] == '<' or requirement['filter_type'] == '>': \n",
    "                # note: string comparison only e.g. for datetimes, needs modification for actual numbers\n",
    "                triple_pattern += '\\nFILTER (STR(?var' + requirement['property'] + ') ' + requirement['filter_type'] + ' \"' + requirement['filter_string'] + '\")'\n",
    "\n",
    "            if requirement['filter_type'] == 'in': \n",
    "                # note: string comparison only\n",
    "                triple_pattern += '\\nFILTER (CONTAINS(?var' + requirement['property'] + ', \"' + requirement['filter_string'] + '\"))'\n",
    "\n",
    "            # Use MINUS if you want to exclude items that fit the pattern.\n",
    "            if requirement['require'] == 'exclude':\n",
    "                triple_pattern = 'minus {' + triple_pattern + '}'\n",
    "\n",
    "            triple_pattern += '\\n'\n",
    "            #print(triple_pattern)\n",
    "            subgraph_pattern += triple_pattern\n",
    "\n",
    "        # Now attach the subgraph pattern to any previous subgraph patterns using UNION to great an OR relationship\n",
    "        subgraph_pattern = '{\\n' + subgraph_pattern + '}\\n' # create a subgraph pattern so that several can be UNIONed\n",
    "        if first_screen: # The first subgraph pattern doesn't need the UNION inserted\n",
    "            first_screen = False\n",
    "        else:\n",
    "            graph_pattern = graph_pattern + 'UNION\\n'\n",
    "        graph_pattern += subgraph_pattern \n",
    "\n",
    "    query_string = '''\n",
    "    select distinct ?qid ?label ?description where {\n",
    "      VALUES ?qid\n",
    "      {\n",
    "      ''' + qid_values + '''}\n",
    "    ''' + graph_pattern + '''\n",
    "    \n",
    "    ?qid rdfs:label ?label.\n",
    "    FILTER(lang(?label)=\"'''+ default_language + '''\")\n",
    "    \n",
    "    OPTIONAL {\n",
    "    ?qid schema:description ?description.\n",
    "    FILTER(lang(?description)=\"'''+ default_language + '''\")\n",
    "    }\n",
    "      }\n",
    "    '''\n",
    "    #print(query_string)\n",
    "    \n",
    "    wdqs = Sparqler(useragent=user_agent)\n",
    "    results = wdqs.query(query_string)\n",
    "    sleep(settings['sparql_sleep'])\n",
    "\n",
    "    return_list = []\n",
    "    for result in results:\n",
    "        out_dict = {\n",
    "            'qid': extract_local_name(result['qid']['value']),\n",
    "            'label': result['label']['value']\n",
    "            }\n",
    "        if 'description' in result:\n",
    "            out_dict['description'] = result['description']['value']\n",
    "        else:\n",
    "            out_dict['description'] = ''           \n",
    "        return_list.append(out_dict)\n",
    "    return return_list\n",
    "\n",
    "def work_in_wikidata_status(label: str, doi: str, pmid: str, existing_works_df: pd.DataFrame, settings: Dict[str, Any], verbose: bool = False) -> Tuple[str, str]:\n",
    "    \"\"\"Search by DOI, PubMed ID, and label for a work in the list of pre-existing Wikidata items.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    If a fuzzy match has a high score, accept as a match. \n",
    "    For intermediate range scores, flag as a case where one label is a subtitle of another.\n",
    "    \"\"\"\n",
    "    if doi and doi.upper() in list(existing_works_df.loc[:, 'doi']):\n",
    "        if verbose:\n",
    "            print('DOI found in existing works')\n",
    "        temp_series = existing_works_df.loc[existing_works_df['doi']==doi, 'qid'].copy()\n",
    "        qid = temp_series.iloc[0]\n",
    "        return 'found DOI', qid\n",
    "    elif pmid and pmid in list(existing_works_df.loc[:, 'pmid']):\n",
    "        if verbose:\n",
    "            print('PubMed ID found in existing works')\n",
    "        temp_series = existing_works_df.loc[existing_works_df['pmid']==pmid, 'qid'].copy()\n",
    "        qid = temp_series.iloc[0]\n",
    "        return 'found PubMed ID', qid\n",
    "    else:\n",
    "        # NOTE: although calculating the fuzz.WRatio is labor intensive, these checks must be done\n",
    "        # sequentially, since we don't want the search for a nearly exact match to be stopped if a\n",
    "        # stupid partial match is found first.\n",
    "        for index, work in existing_works_df.iterrows():\n",
    "            w_ratio = fuzz.WRatio(work['label'], label)\n",
    "\n",
    "            # Test for nearly exact title match\n",
    "            if w_ratio > settings['existing_work_fuzzy_match_cutoff']:\n",
    "                if verbose:\n",
    "                    print('fuzzy label match: ' + str(w_ratio))\n",
    "                    print('test:', label)\n",
    "                    print('wikidata:', extract_local_name(work['qid']), work['label'])\n",
    "                return 'fuzzy label match', work['qid']\n",
    "                \n",
    "        for index, work in existing_works_df.iterrows():\n",
    "            w_ratio = fuzz.WRatio(work['label'], label)\n",
    "            # Test for meaningful subtitle match\n",
    "            if w_ratio > settings['existing_work_subtitle_fuzzy_match_cutoff']:\n",
    "                # NOTE: sometimes a work will have an acceptable score, because a long title matches a single\n",
    "                # word title or vice versa. Those cases are nearly always bad matches and should not be\n",
    "                # considered possible subtitle matches.\n",
    "                if len(work['label'].split(' ')) != 1 and len(label.split(' ')) != 1:\n",
    "                    if verbose:\n",
    "                        print('Warning!!! Possible partial title: ' + str(w_ratio))\n",
    "                        print('test:', label)\n",
    "                        print('wikidata:', extract_local_name(work['qid']), work['label'])\n",
    "                        logging.warning('Possible partial title: ' + str(w_ratio) + ' match, ' + extract_local_name(work['qid']) + ' ' + work['label'])\n",
    "                    return 'possible partial title', work['qid']\n",
    "                else: # keep looking for matches among existing works\n",
    "                    print('skipped over possible partial title: ' + str(w_ratio) + ' match, ' + extract_local_name(work['qid']) + ' ' + work['label'])\n",
    "                    continue\n",
    "\n",
    "    if verbose:\n",
    "        print('Not found')\n",
    "    return 'not found', ''\n",
    "\n",
    "def find_containing_book(isbn: str, label: str, default_language: str, user_agent: str) -> Tuple[bool, str]:\n",
    "    \"\"\"SPARQL query for a book by ISBN or label.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of\n",
    "    First item a boolean, True if an error condition such as more than one book match or non-book type.\n",
    "    Second item is the Q ID of the matching book or empty string if no match (not an error condition).\n",
    "    \"\"\"\n",
    "    book_error = False\n",
    "    found = False\n",
    "    \n",
    "    if isbn != '':\n",
    "        query_string = '''SELECT DISTINCT ?item\n",
    "    WHERE \n",
    "    {\n",
    "      BIND (\"''' + isbn + '''\" AS ?isbn)\n",
    "      {?item wdt:P212 ?isbn.} # ISBN-13\n",
    "      union\n",
    "      {?item wdt:P957 ?isbn.} # ISBN-10\n",
    "    }\n",
    "    '''\n",
    "        wdqs = Sparqler(useragent=user_agent)\n",
    "        results = wdqs.query(query_string)\n",
    "        sleep(settings['sparql_sleep'])\n",
    "\n",
    "        if len(results) > 1:\n",
    "            print('More than one book matches the ISBN')\n",
    "            logging.warning('More than one book matches the ISBN')\n",
    "            book_error = True\n",
    "            found = True\n",
    "            book_qid = ''\n",
    "        elif len(results) == 1:\n",
    "            found = True\n",
    "            book_qid = extract_local_name(results[0]['item']['value'])\n",
    "            \n",
    "    if not found:\n",
    "        query_string = '''SELECT DISTINCT ?item ?type WHERE {\n",
    "?item rdfs:label \"''' + label + '\"@' + default_language + '''.\n",
    "?item wdt:P31 ?type.\n",
    "}'''\n",
    "        wdqs = Sparqler(useragent=user_agent)\n",
    "        results = wdqs.query(query_string)\n",
    "        sleep(settings['sparql_sleep'])\n",
    "        if (results is None) or (len(results) == 0):\n",
    "            book_qid = ''\n",
    "        else:\n",
    "            type_list = [extract_local_name(result['type']['value']) for result in results]\n",
    "            qids_list = list(set([extract_local_name(result['item']['value']) for result in results]))\n",
    "            if len(qids_list) > 1:\n",
    "                book_error = True\n",
    "                print('Label for published_in matches multiple items:', qids_list)\n",
    "                logging.warning('Label for published_in matches multiple items: ' + str(qids_list))\n",
    "                book_qid = ''\n",
    "            else:\n",
    "                result_qid = qids_list[0]\n",
    "                if 'Q1711593' in type_list: # edited volume\n",
    "                    book_qid = result_qid\n",
    "                else:\n",
    "                    book_error = True\n",
    "                    print('Possible published_in', result_qid, 'not edited volume but has types', type_list)\n",
    "                    logging.warning('Possible published_in ' + result_qid + ' not edited volume but has types ' + str(type_list))\n",
    "                    book_qid = ''\n",
    "    return book_error, book_qid\n",
    "\n",
    "def create_skipped_dict(wikidata_status: str, work_data: pd.Series, mapping: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"Create a row dictionary for works whose processing is skipped for various reasons.\"\"\"\n",
    "    row_dict = {}\n",
    "    row_dict['key'] = work_data[mapping['constants']['unique_identifier_column']]\n",
    "    row_dict['reason'] = wikidata_status\n",
    "    # look up remaining information from the works DataFrame\n",
    "    row_dict['item_type'] = work_data[mapping['constants']['description_code_column']]\n",
    "    # !!!! Idiosyncratic to Zotero dump\n",
    "    if row_dict['item_type'] == 'bookSection':\n",
    "        row_dict['isbn'] = work_data['parent_isbn']\n",
    "    else:\n",
    "        row_dict['isbn'] = work_data['ISBN']\n",
    "    row_dict['issn'] = work_data['ISSN']\n",
    "    row_dict['publication_title'] = work_data['Publication Title']\n",
    "    row_dict['publication_year'] = work_data['Publication Year']\n",
    "    row_dict['author'] = work_data['Author']\n",
    "    row_dict['title'] = work_data['Title']\n",
    "    return row_dict\n",
    "\n",
    "# ------------------------\n",
    "# SPARQL query class\n",
    "# ------------------------\n",
    "\n",
    "# This is a condensed version of the more full-featured script at \n",
    "# https://github.com/HeardLibrary/digital-scholarship/blob/master/code/wikidata/sparqler.py\n",
    "# It includes only the method for the query form.\n",
    "\n",
    "class Sparqler:\n",
    "    \n",
    "    def __init__(self, method: str = 'post', endpoint: str = 'https://query.wikidata.org/sparql', useragent: Optional[str] = None, sleep: float = 0.1):\n",
    "        \"\"\"Build SPARQL queries of various sorts\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        useragent: str\n",
    "            Required if using the Wikidata Query Service, otherwise optional.\n",
    "            Use the form: appname/v.v (URL; mailto:email@domain.com)\n",
    "            See https://meta.wikimedia.org/wiki/User-Agent_policy\n",
    "        endpoint: URL\n",
    "            Defaults to Wikidata Query Service if not provided.\n",
    "        method: str\n",
    "            Possible values are \"post\" (default) or \"get\". Use \"get\" if read-only query endpoint.\n",
    "            Must be \"post\" for update endpoint.\n",
    "        sleep: float\n",
    "            Number of seconds to wait between queries. Defaults to 0.1\n",
    "\n",
    "        Required modules\n",
    "        ----------------\n",
    "        requests, datetime, time\n",
    "        \"\"\"\n",
    "        # attributes for all methods\n",
    "        self.http_method = method\n",
    "        self.endpoint = endpoint\n",
    "        if useragent is None:\n",
    "            if self.endpoint == 'https://query.wikidata.org/sparql':\n",
    "                print('You must provide a value for the useragent argument when using the Wikidata Query Service.')\n",
    "                print()\n",
    "                raise KeyboardInterrupt # Use keyboard interrupt instead of sys.exit() because it works in Jupyter notebooks\n",
    "        self.sleep = sleep\n",
    "\n",
    "        self.requestheader = {}\n",
    "        if useragent:\n",
    "            self.requestheader['User-Agent'] = useragent\n",
    "        \n",
    "        if self.http_method == 'post':\n",
    "            self.requestheader['Content-Type'] = 'application/x-www-form-urlencoded'\n",
    "\n",
    "    def query(self, query_string: str, form: str = 'select', verbose: bool = False, **kwargs):\n",
    "        \"\"\"Send a SPARQL query to the endpoint.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        form : str\n",
    "            The SPARQL query form.\n",
    "            Possible values are: \"select\" (default), \"ask\", \"construct\", and \"describe\".\n",
    "        mediatype: str\n",
    "            The response media type (MIME type) of the query results.\n",
    "            Some possible values for \"select\" and \"ask\" are: \"application/sparql-results+json\" (default) and \"application/sparql-results+xml\".\n",
    "            Some possible values for \"construct\" and \"describe\" are: \"text/turtle\" (default) and \"application/rdf+xml\".\n",
    "            See https://docs.aws.amazon.com/neptune/latest/userguide/sparql-media-type-support.html#sparql-serialization-formats-neptune-output\n",
    "            for response serializations supported by Neptune.\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        default: list of str\n",
    "            The graphs to be merged to form the default graph. List items must be URIs in string form.\n",
    "            If omitted, no graphs will be specified and default graph composition will be controlled by FROM clauses\n",
    "            in the query itself. \n",
    "            See https://www.w3.org/TR/sparql11-query/#namedGraphs and https://www.w3.org/TR/sparql11-protocol/#dataset\n",
    "            for details.\n",
    "        named: list of str\n",
    "            Graphs that may be specified by IRI in a query. List items must be URIs in string form.\n",
    "            If omitted, named graphs will be specified by FROM NAMED clauses in the query itself.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        If the form is \"select\" and mediatype is \"application/json\", a list of dictionaries containing the data.\n",
    "        If the form is \"ask\" and mediatype is \"application/json\", a boolean is returned.\n",
    "        If the mediatype is \"application/json\" and an error occurs, None is returned.\n",
    "        For other forms and mediatypes, the raw output is returned.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        To get UTF-8 text in the SPARQL queries to work properly, send URL-encoded text rather than raw text.\n",
    "        That is done automatically by the requests module for GET. I guess it also does it for POST when the\n",
    "        data are sent as a dict with the urlencoded header. \n",
    "        See SPARQL 1.1 protocol notes at https://www.w3.org/TR/sparql11-protocol/#query-operation        \n",
    "        \"\"\"\n",
    "        query_form = form\n",
    "        if 'mediatype' in kwargs:\n",
    "            media_type = kwargs['mediatype']\n",
    "        else:\n",
    "            if query_form == 'construct' or query_form == 'describe':\n",
    "            #if query_form == 'construct':\n",
    "                media_type = 'text/turtle'\n",
    "            else:\n",
    "                media_type = 'application/sparql-results+json' # default for SELECT and ASK query forms\n",
    "        self.requestheader['Accept'] = media_type\n",
    "            \n",
    "        # Build the payload dictionary (query and graph data) to be sent to the endpoint\n",
    "        payload = {'query' : query_string}\n",
    "        if 'default' in kwargs:\n",
    "            payload['default-graph-uri'] = kwargs['default']\n",
    "        \n",
    "        if 'named' in kwargs:\n",
    "            payload['named-graph-uri'] = kwargs['named']\n",
    "\n",
    "        if verbose:\n",
    "            print('querying SPARQL endpoint')\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        if self.http_method == 'post':\n",
    "            response = requests.post(self.endpoint, data=payload, headers=self.requestheader)\n",
    "        else:\n",
    "            response = requests.get(self.endpoint, params=payload, headers=self.requestheader)\n",
    "        #print('from cache:', response.from_cache) # uncomment if you want to see if cached data are used\n",
    "        elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "        self.response = response.text\n",
    "        sleep(self.sleep) # Throttle as a courtesy to avoid hitting the endpoint too fast.\n",
    "\n",
    "        if verbose:\n",
    "            print('done retrieving data in', int(elapsed_time), 's')\n",
    "\n",
    "        if query_form == 'construct' or query_form == 'describe':\n",
    "            return response.text\n",
    "        else:\n",
    "            if media_type != 'application/sparql-results+json':\n",
    "                return response.text\n",
    "            else:\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                except:\n",
    "                    return None # Returns no value if an error. \n",
    "\n",
    "                if query_form == 'select':\n",
    "                    # Extract the values from the response JSON\n",
    "                    results = data['results']['bindings']\n",
    "                else:\n",
    "                    results = data['boolean'] # True or False result from ASK query \n",
    "                return results           \n",
    "\n",
    "# ---------------------------\n",
    "# Major processes functions\n",
    "# ---------------------------\n",
    "\n",
    "def evaluate_function(work_data: pd.Series, column_map: Dict[str, str], settings: Dict[str, Any]) -> Any:\n",
    "    \"\"\"Create argument list and pass to the function specified in the column mapping data.\"\"\"\n",
    "    args_list = []\n",
    "    if 'in_col_label' in column_map:\n",
    "        in_value = work_data[column_map['in_col_label']]\n",
    "        args_list.append(in_value)\n",
    "    # If use_settings key is present, it must have a True value to pass the settings\n",
    "    if 'use_settings' in column_map and column_map['use_settings']:\n",
    "        args_list.append(settings)\n",
    "    # Add any necessary globally defined reference DataFrames to the argument list\n",
    "    if 'reference_dfs' in column_map:\n",
    "        for df_name in column_map['reference_dfs']:\n",
    "            args_list.append(VARIABLES[df_name.upper()])\n",
    "    args = tuple(args_list)\n",
    "\n",
    "    my_function = MODULE_FUNCTIONS[column_map['mapping_function']]\n",
    "    return my_function(*args)\n",
    "\n",
    "def extract_metadata(mapping: Dict[str, Any], work_data: pd.Series, settings: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"Step through output fields, map them to source data columns, and transform input data for output row.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mapping: complex structure\n",
    "        Maps column headers (\"out_col_label\") in the destination table to column headers (\"in_col_label\") \n",
    "        in the source table.\n",
    "        The \"mapping_function\" key indicates the function used to determine the value to be used in the \n",
    "        output row of the destination table.\n",
    "    work_data: pd.Series\n",
    "        A row of data from the source data table with column headers as the keys.\n",
    "    settings: complex structure\n",
    "        Configuration data\n",
    "    \"\"\"\n",
    "    out_dict = {'qid': '', 'unique_identifier': work_data[mapping['constants']['unique_identifier_column']]}\n",
    "    out_dict['label_' + settings['default_language']] = work_data[mapping['constants']['label_column']]\n",
    "    out_dict['description_' + settings['default_language']] = set_description(work_data[mapping['constants']['description_code_column']], settings['work_types'])\n",
    "\n",
    "    for out_property in config['outfiles'][0]['prop_list']:\n",
    "        \n",
    "        # Find the mapping variable that matches the config property\n",
    "        for prop in mapping['properties']:\n",
    "            if prop['out_col_label'] == out_property['variable']:\n",
    "                break\n",
    "    \n",
    "        out_field = out_property['variable']\n",
    "        out_dict[out_field + '_uuid'] = ''\n",
    "        \n",
    "        # If a function requires some data structure for input, its mapping must include the string that is \n",
    "        # the name of the data structure object needed by the function as an item in the reference_dfs list.\n",
    "        \n",
    "        # Functions not needing additional data will have\n",
    "        # only three arguments and None will be passed into the constructor functions as the data_structure argument.\n",
    "        \n",
    "        # NOTE: the data structure will be a global variable and be defined in the main script.\n",
    "        if 'structure_name_string' in prop:\n",
    "            data_structure = eval(prop['structure_name_string'])\n",
    "        else:\n",
    "            data_structure = None\n",
    "\n",
    "        output_value = evaluate_function(work_data, prop, settings)\n",
    "        if output_value == '':\n",
    "            no_value = True\n",
    "        else:\n",
    "            no_value = False\n",
    "\n",
    "        # Populate the values-related columns\n",
    "        if out_property['value_type'] == 'date':\n",
    "            out_dict[out_field + '_nodeId'] = ''\n",
    "            out_dict[out_field + '_val'] = output_value\n",
    "            out_dict[out_field + '_prec'] = ''\n",
    "\n",
    "        elif out_property['value_type'] == 'quantity':\n",
    "            out_dict[out_field + '_nodeId'] = ''\n",
    "            out_dict[out_field + '_val'] = output_value\n",
    "            if no_value:\n",
    "                out_dict[out_field + '_unit'] = ''\n",
    "            else:\n",
    "                out_dict[out_field + '_unit'] = prop['quantity_unit']\n",
    "\n",
    "        # This is not actually implemented and will generate an error if used\n",
    "        elif out_property['value_type'] == 'globecoordinate':\n",
    "            out_dict[out_field + '_nodeId'] = ''\n",
    "            out_dict[out_field + '_val'] = output_value\n",
    "            if no_value:\n",
    "                out_dict[out_field + '_long'] = ''\n",
    "                out_dict[out_field + '_prec'] = ''\n",
    "            else:\n",
    "                out_dict[out_field + '_long'] = work_data[out_field + '_long']\n",
    "                out_dict[out_field + '_prec'] = work_data[out_field + '_prec']\n",
    "\n",
    "        else:\n",
    "            out_dict[out_field] = output_value\n",
    "\n",
    "        # Populate the qualifier columns\n",
    "        for qualifier in out_property['qual']:\n",
    "            if no_value:\n",
    "                qual_output_value = ''\n",
    "            else:\n",
    "                # Find the mapping variable that matches the config property\n",
    "                for qual in prop['qual']:\n",
    "                    if qual['out_col_label'] == qualifier['variable']:\n",
    "                        break\n",
    "                        \n",
    "                qual_output_value = evaluate_function(work_data, qual, settings)\n",
    "\n",
    "            qual_field = out_field + '_' + qualifier['variable']\n",
    "            # To my knowledge, dates are the only complex types used as qualifiers (no quantities or globecoordinates).\n",
    "            if qualifier['value_type'] == 'date':\n",
    "                out_dict[qual_field + '_nodeId'] = ''\n",
    "                out_dict[qual_field + '_val'] = qual_output_value\n",
    "                out_dict[qual_field + '_prec'] = ''\n",
    "            else:\n",
    "                out_dict[qual_field] = qual_output_value\n",
    "                \n",
    "        # Populate the reference columns\n",
    "        # There's only a hash ID column if there's at least one reference.\n",
    "        if len(out_property['ref']) > 0:\n",
    "            out_dict[out_field + '_ref1_hash'] = ''\n",
    "            \n",
    "        for reference in out_property['ref']:\n",
    "            if no_value:\n",
    "                ref_output_value = ''\n",
    "            else:\n",
    "                # Find the mapping variable that matches the config property\n",
    "                for ref in prop['ref']:\n",
    "                    if ref['out_col_label'] == reference['variable']:\n",
    "                        break\n",
    "\n",
    "                ref_output_value = evaluate_function(work_data, ref, settings)\n",
    "\n",
    "            ref_field = out_field + '_ref1_' + reference['variable']\n",
    "            # To my knowledge, dates are the only complex types used as qualifiers (no quantities or globecoordinates).\n",
    "            if reference['value_type'] == 'date':\n",
    "                out_dict[ref_field + '_nodeId'] = ''\n",
    "                out_dict[ref_field + '_val'] = ref_output_value\n",
    "                out_dict[ref_field + '_prec'] = ''\n",
    "            else:\n",
    "                out_dict[ref_field] = ref_output_value\n",
    "                    \n",
    "    #print(out_dict)\n",
    "    return out_dict\n",
    "\n",
    "def disambiguate_agents(authors: List[Dict[str, str]], pmid: str, coauthors: pd.DataFrame, settings: Dict[str, Any], user_agent: str) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    \"\"\"Find possible Wikidata Q ID matches from agent strings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    When a positive ID is made, returns Q IDs, series ordinal, stated_as to use for author/editor/translator\n",
    "    statements.\n",
    "    When no positive ID is made, a list of possible matches is also included for each author string.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Use a wide variety of data and tricks to come up with possible matches.\n",
    "    This includes fuzzy matching against department names and querying Wikidata labels and aliases with\n",
    "    many variations of the name string.\n",
    "    \"\"\"\n",
    "    max_pmids_to_check = 10\n",
    "    # If there is a PubMed ID for the article, retrieve the author info\n",
    "    if pmid != '':\n",
    "        pubmed_author_info = retrieve_pubmed_data(pmid)\n",
    "        print('retrieved data from PubMed ID', pmid)\n",
    "        for author_index in range(len(pubmed_author_info)):\n",
    "            pubmed_author_info[author_index]['name'] = pubmed_author_info[author_index]['forename'] + ' ' + pubmed_author_info[author_index]['surname']\n",
    "    else:\n",
    "        print('no PubMed data')\n",
    "\n",
    "    # Augment CrossRef data with PubMed data. Typically the PubMed data is more likely to have the affiliations\n",
    "    # Names are generally very similar, but vary with added or missing periods on initials and suffixes\n",
    "    if pmid != '':\n",
    "        for author_index in range(len(authors)):\n",
    "            found = False\n",
    "            crossref_name = authors[author_index]['givenName'] + ' ' + authors[author_index]['familyName']\n",
    "            #print(crossref_name)\n",
    "            for pubmed_author in pubmed_author_info:\n",
    "                ratio = fuzz.ratio(pubmed_author['name'], crossref_name)\n",
    "                #print(ratio, pubmed_author['name'])\n",
    "                if ratio > 87: # had to drop down to this level because some people with missing \"Jr\" weren't matching\n",
    "                    found = True\n",
    "                    result_string = 'fuzzy label match: ' + str(ratio) + pubmed_author['name'] + ' / ' + crossref_name\n",
    "                    #print(result_string)\n",
    "                    break\n",
    "            if not found:\n",
    "                print('Did not find a match in the PubMed data for', crossref_name)\n",
    "            else:\n",
    "                #print(pubmed_author)\n",
    "                #print(authors[author_index])\n",
    "\n",
    "                # If there is a PubMed affiliation and no affiliation in the CrossRef data, add the PubMed affiliation\n",
    "                if pubmed_author['affiliation'] != '':\n",
    "                    if len(authors[author_index]['affiliation']) == 0:\n",
    "                        authors[author_index]['affiliation'].append(pubmed_author['affiliation'])\n",
    "\n",
    "                # If there is an ORCID in PubMed and no ORCID in the CrossRef data, add the ORCID to CrossRef data\n",
    "                # Not sure how often this happens since I think maybe usually of one has it, the other does, too.\n",
    "                if pubmed_author['orcid'] != '':\n",
    "                    if authors[author_index]['orcid'] == '':\n",
    "                        authors[author_index]['orcid'] = pubmed_author['orcid']\n",
    "\n",
    "                #print(authors[author_index])\n",
    "\n",
    "            #print()\n",
    "    #print(json.dumps(pubmed_author_info, indent=2))\n",
    "\n",
    "    # Perform screening operations on authors to try to determine their Q IDs\n",
    "    found_qid_values = []\n",
    "    not_found_author_list = []\n",
    "    author_count = 1\n",
    "    for author in authors:\n",
    "        print(author_count)\n",
    "        found = False\n",
    "        \n",
    "        # First eliminate the case where all of the name pieces are empty\n",
    "        if (author['givenName'] + ' ' + author['familyName']).strip() == '':\n",
    "            break\n",
    "            \n",
    "        # Record stated_as\n",
    "        stated_as = (author['givenName'] + ' ' + author['familyName']).strip()\n",
    "            \n",
    "        # Fix case where names are stupidly in all caps\n",
    "        name_pieces = author['givenName'].strip().split(' ')\n",
    "        author['givenName'] = ' '.join(fix_all_caps(name_pieces))\n",
    "        name_pieces = author['familyName'].strip().split(' ')\n",
    "        author['familyName'] = ' '.join(fix_all_caps(name_pieces))\n",
    "        \n",
    "        # Screen for exact match to Wikidata labels\n",
    "        for index, researcher in RESEARCHERS.iterrows():\n",
    "            if researcher['label_en'] == author['givenName'] + ' ' + author['familyName']:\n",
    "                found = True\n",
    "                result_string = 'researcher exact label match: ' + researcher['qid'] + ' ' + researcher['label_en']\n",
    "                name = researcher['label_en']\n",
    "                qid = researcher['qid']\n",
    "                break\n",
    "        if not found:\n",
    "            # screen for exact match to alternate names\n",
    "            for index, altname in ALTNAMES.iterrows():\n",
    "                if altname['altLabel'] == author['givenName'] + ' ' + author['familyName']:\n",
    "                    found = True\n",
    "                    result_string = 'researcher altname match: ' + altname['qid'] + ' ' + altname['altLabel']\n",
    "                    name = altname['altLabel']\n",
    "                    qid = altname['qid']\n",
    "                    break\n",
    "            if not found:\n",
    "                # If the researcher has an ORCID, see if it's at Wikidata\n",
    "                if author['orcid'] != '':\n",
    "                    hit = searchWikidataForQIdByOrcid(author['orcid'])\n",
    "                    if hit != {}:\n",
    "                        found = True\n",
    "                        result_string = 'Wikidata ORCID search: ' + hit['qid'] + ' ' + hit['label'] + ' / ' + hit['description']\n",
    "                        name = hit['label']\n",
    "                        qid = hit['qid']\n",
    "\n",
    "                if not found:\n",
    "                    # screen for fuzzy match to Wikidata-derived labels\n",
    "                    for index, researcher in RESEARCHERS.iterrows():\n",
    "                        # Require the surname to match the label surname exactly\n",
    "                        split_names = find_surname_givens(researcher['label_en']) # returns False if no family name\n",
    "                        if split_names: # skip names that don't have 2 parts !!! also misses non-English labels!\n",
    "                            if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                                w_ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], researcher['label_en'])\n",
    "                                #w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], researcher['label_en'])\n",
    "                                if w_ratio > 90:\n",
    "                                    found = True\n",
    "                                    result_string = 'fuzzy label match: ' + str(w_ratio) + ' ' + researcher['qid'] + ' ' + researcher['label_en'] + ' / ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                    name = researcher['label_en']\n",
    "                                    qid = researcher['qid']\n",
    "                                    break\n",
    "                    if not found:\n",
    "                        # screen for fuzzy match to alternate names\n",
    "                        for index, altname in ALTNAMES.iterrows():\n",
    "                            split_names = find_surname_givens(altname['altLabel'])\n",
    "                            if split_names: # skip names that don't have 2 parts\n",
    "                                if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                                    w_ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], altname['altLabel'])\n",
    "                                    #w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], altname['altLabel'])\n",
    "                                    if w_ratio > 90:\n",
    "                                        found = True\n",
    "                                        result_string = 'researcher altname fuzzy match: ' + str(w_ratio) + ' ' + altname['qid'] + ' ' + altname['altLabel'] + ' / ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                        name = altname['altLabel']\n",
    "                                        qid = altname['qid']\n",
    "                                        break\n",
    "                        if not found:\n",
    "                            name = author['givenName'] + ' ' + author['familyName']\n",
    "                            print('Searching Wikidata for', name)\n",
    "                            print('researcher known affiliations: ', author['affiliation'])\n",
    "                            print()\n",
    "                            hits = search_name_at_wikidata(name, user_agent)\n",
    "                            #print(hits)\n",
    "\n",
    "                            qids = []\n",
    "                            for hit in hits:\n",
    "                                qids.append(hit['qid'])\n",
    "                            return_list = screen_qids(qids, screens, settings['default_language'], user_agent) # screens is a global variable loaded at the start\n",
    "                            #print(return_list)\n",
    "\n",
    "                            for hit in return_list:\n",
    "                                # Check each possible name match to the list of known co-authors/co-editors\n",
    "                                # If there is a match, then use that Q ID and quit trying to match.\n",
    "                                if hit['qid'] in list(coauthors.index):\n",
    "                                    found = True\n",
    "                                    qid = hit['qid']\n",
    "                                    result_string = 'Match with known coauthor'\n",
    "                                    \n",
    "                            if not found:\n",
    "                                # Save discovered data to return if not matched\n",
    "                                discovered_data = []\n",
    "                                for hit in return_list:                                \n",
    "                                    hit_data = hit\n",
    "                                    split_names = find_surname_givens(hit['label'])\n",
    "\n",
    "                                    # Require the surname to match the Wikidata label surname exactly\n",
    "                                    # This prevents a high fraction of fuzzy matches where the last names are similar but not the same\n",
    "                                    if split_names: # skip names that don't have 2 parts\n",
    "                                        if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                                            #print(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print(hit)\n",
    "                                            w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('w_ratio:', w_ratio)\n",
    "                                            #ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('ratio:', ratio)\n",
    "                                            #partial_ratio = fuzz.partial_ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('partial_ratio:', partial_ratio)\n",
    "                                            #token_sort_ratio = fuzz.token_sort_ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('token_sort_ratio:', token_sort_ratio)\n",
    "                                            #token_set_ratio = fuzz.token_set_ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('token_set_ratio:', token_set_ratio)\n",
    "\n",
    "                                            # This screen requires a high degree of similarity between the overall ORCID names and Wikidata labels\n",
    "                                            if w_ratio > 80:\n",
    "                                                print('Wikidata search fuzzy match:', w_ratio, author['givenName'] + ' ' + author['familyName'], ' / ', 'https://www.wikidata.org/wiki/'+ hit['qid'], hit['label'])\n",
    "                                                print('Wikidata description: ', hit['description'])\n",
    "\n",
    "                                                # Here we need to check Wikidata employer and affiliation and fuzzy match against known affiliations\n",
    "                                                occupations, employers, affiliations = search_wikidata_occ_emp_aff(hit['qid'], settings['default_language'], user_agent)\n",
    "                                                print('occupations:', occupations)\n",
    "                                                hit_data['occupations'] = occupations\n",
    "                                                print('employers:', employers)\n",
    "                                                hit_data['employers'] = employers\n",
    "                                                print('affiliations', affiliations)\n",
    "                                                hit_data['affiliations'] = affiliations\n",
    "                                                print()\n",
    "\n",
    "                                                # Perform a check of the employer to make sure we didn't miss somebody in the earlier\n",
    "                                                # string matching\n",
    "                                                for employer in employers:\n",
    "                                                    if 'Vanderbilt University' in employer: # catch university and med center\n",
    "                                                        found = True\n",
    "                                                        result_string = 'Match Vanderbilt employer in Wikidata: ' + hit['qid'] + ' ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                                        qid = hit['qid']\n",
    "\n",
    "                                                # If the author doesn't have any known affiliations, there is no point in checking PubMed\n",
    "                                                if author['affiliation'] != []:\n",
    "                                                    # Search Wikidata for articles written by this match\n",
    "                                                    articles_in_wikidata = search_wikidata_article(hit['qid'])\n",
    "                                                    #print(articles_in_wikidata)\n",
    "\n",
    "                                                    # Step through articles with PubMed IDs found in Wikidata and see if the author affiliation or ORCID matches any of the articles\n",
    "                                                    check = 0\n",
    "                                                    for article_in_wikidata in articles_in_wikidata:\n",
    "                                                        if article_in_wikidata['pmid'] != '':\n",
    "                                                            check += 1\n",
    "                                                            if check > max_pmids_to_check:\n",
    "                                                                print('More articles, but stopping after checking', max_pmids_to_check)\n",
    "                                                                break # break out of article-checking loop\n",
    "                                                            print('Checking article, PMID:', article_in_wikidata['pmid'], article_in_wikidata['title'])\n",
    "                                                            pubmed_match = identified_in_pubmed(article_in_wikidata['pmid'], author['givenName'] + ' ' + author['familyName'], author['affiliation'], author['orcid'])\n",
    "                                                            if not pubmed_match:\n",
    "                                                                #print('no match')\n",
    "                                                                print()\n",
    "                                                            else:\n",
    "                                                                found = True\n",
    "                                                                result_string = 'PubMed affilation match: ' + hit['qid'] + ' ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                                                qid = hit['qid']\n",
    "                                                                break # break out of article-checking loop\n",
    "\n",
    "                                                if found:\n",
    "                                                    break # break out of hit list loop\n",
    "                                                print()\n",
    "                                                # If none of the matching criteria are met, save the data for future use\n",
    "                                                discovered_data.append(hit_data)\n",
    "\n",
    "        if not found:\n",
    "            not_found_author_list.append({'name_string': author['givenName'] + ' ' + author['familyName'], 'series_ordinal': author_count, 'possible_matches': discovered_data})\n",
    "            print('not found:', author['givenName'] + ' ' + author['familyName'])\n",
    "\n",
    "        else:\n",
    "            found_qid_values.append({'qid': qid, 'stated_as': stated_as, 'series_ordinal': author_count})\n",
    "            print(result_string)\n",
    "            for index, department in DEPARTMENTS.iterrows():\n",
    "                if qid == department['qid']:\n",
    "                    for lindex, department_label in DEPARTMENT_LABELS.iterrows():\n",
    "                        if department_label['qid'] == department['affiliation']:\n",
    "                            print(department_label['label_en'])\n",
    "                            break\n",
    "        print()\n",
    "        author_count += 1\n",
    "\n",
    "    print()\n",
    "    return found_qid_values, not_found_author_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accounting\n",
    "\n",
    "We would like to know two things:\n",
    "1. What publications are in Wikidata that we didn't put there? We might potentially want to monitor their metadata. (Categories B + C below)\n",
    "2. What publications from the Zotero database dump are not yet in Wikidata? We cannot assume that it's all the ones we didn't put in, because they may have been uploaded by others. (Category D below)\n",
    "\n",
    "![categories diagram](venn_diagram.png)\n",
    "\n",
    "## Categories\n",
    "\n",
    "| description | name | in Zotero | in Wikidata | uploaded by us |\n",
    "| ----------- | -------- | --------- | ----------- | -------------- |\n",
    "| A. uploaded to Wikidata by us | works | x | x | x |\n",
    "| B. Wikidata in Zotero not by us | wd_zo_not_us | x | x |  |\n",
    "| C. Wikidata not Zotero | wd_not_zo |  | x |  |\n",
    "| D. In Zotero but not in Wikidata | not_wikidata | x |  |  |\n",
    "\n",
    "## Calculation methods\n",
    "\n",
    "**Uploaded by us (A)**\n",
    "\n",
    "already present in works.csv; have qids and Zotero IDs\n",
    "\n",
    "1030 works\n",
    "\n",
    "\n",
    "**All publications in Wikidata not by us (B + C)** \n",
    "\n",
    "existing_works_in_wikidat.csv minus works -> wd_not_us\n",
    "\n",
    "Use qid as unique identifier\n",
    "\n",
    "988 works\n",
    "\n",
    "\n",
    "**Zotero pubs in Wikidata, but not by us (B)**\n",
    "\n",
    "wd_not_us intersection divinity_publications_2022-12-01.csv -> wd_zo_not_us\n",
    "\n",
    "Must be done by fuzzy string matching since no common identifier. Assign a qid (already have Zotero ID).\n",
    "\n",
    "567 works\n",
    "\n",
    "\n",
    "**Wikidata pubs not in Zotero (C)**\n",
    "\n",
    "existing_works_in_wikidata.csv minus { works.csv (A) union wd_zo_not_us (B) } -> wd_not_zo\n",
    "\n",
    "Remove from existing works all that now have a Zotero ID.\n",
    "\n",
    "423 works\n",
    "\n",
    "**Zotero pubs not in Wikidata (D)**\n",
    "\n",
    "divinity_publications_2022-12-01.csv minus { works.csv (A) union wd_zo_not_us (B) } -> not_wikidata.csv\n",
    "\n",
    "Remove from div pubs all that have qids \n",
    "\n",
    "4183 works\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load program settings, mappings, screens, and configurations\n",
    "with open('settings.yaml', 'r') as file_object:\n",
    "    settings = yaml.safe_load(file_object)\n",
    "\n",
    "# Load the source file from the Zotero export\n",
    "zotero = pd.read_csv(settings['data_file_path'] + settings['source_data_filename'], na_filter=False, dtype = str)\n",
    "zotero = zotero.set_index('Key')\n",
    "\n",
    "# Load the works uploaded by us\n",
    "works = pd.read_csv(settings['data_file_path'] + 'works.csv', na_filter=False, dtype = str)\n",
    "works = works.set_index('qid')\n",
    "\n",
    "# Load the works known to be in Wikidata (via SPARQL query)\n",
    "existing_works_in_wikidata = pd.read_csv(settings['temporary_files_path'] + 'existing_works_in_wikidata.csv', na_filter=False, dtype = str)\n",
    "existing_works_in_wikidata = existing_works_in_wikidata.set_index('qid')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#zotero = zotero.iloc[170:180]\n",
    "zotero\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_wo_qids = list(existing_works_in_wikidata.index.values)\n",
    "print('existing works', len(ex_wo_qids))\n",
    "print('non-redundant', len(list(set(ex_wo_qids))))\n",
    "existing_works_in_wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Publications uploaded by us (A)\n",
    "wo_up_qids = list(works.index.values)\n",
    "print('existing works', len(wo_up_qids))\n",
    "print('non-redundant', len(list(set(wo_up_qids))))\n",
    "works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Later:\n",
    "\n",
    "10 works that we have on record as having uploaded aren't being found as works in WD. \n",
    "\n",
    "1. In some cases these were probably merged as duplicates of works already in Wikidata, so they should have been deleted from the works uploaded list. (Found one of these and deleted.)\n",
    "2. In other cases, there might have been duplicates within the Zotero database itself (with one note yet uploaded, for example). They should have been deleted from the Zotero database list, though.\n",
    "3. Another possibility is that we may have uploaded them but none of their agents were actually linked as Vanderbilt faculty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find all publications in Wikidata not by us (B + C)\n",
    "# existing_works_in_wikidata minus works -> wd_not_us\n",
    "# Use qid as unique identifier\n",
    "works_qids = list(works.index.values)\n",
    "print('uploaded', len(works_qids))\n",
    "print('non-redundant', len(list(set(works_qids))))\n",
    "print('difference', len(ex_wo_qids)-len(works_qids))\n",
    "wd_not_us = existing_works_in_wikidata.loc[~existing_works_in_wikidata.index.isin(works_qids)].copy()\n",
    "wd_not_us = wd_not_us.reset_index()\n",
    "wd_not_us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zotero pubs in Wikidata, but not by us (B)\n",
    "# wd_not_us intersection divinity_publications_2022-12-01.csv -> wd_zo_not_us\n",
    "# Must be done by fuzzy string matching since no common identifier. Assign a qid (already have ).\n",
    "\n",
    "# Create an empty DataFrame to add \n",
    "wd_zo_not_us = pd.DataFrame()\n",
    "\n",
    "count = 0\n",
    "for index, work_data in zotero.iterrows():\n",
    "    count += 1\n",
    "    print(count, index, work_data['Title'])\n",
    "    doi = mapping_functions.clean_doi(work_data['DOI'])\n",
    "    pmid = mapping_functions.extract_pmid_from_extra(work_data['Extra'])\n",
    "    wikidata_status, qid = work_in_wikidata_status(work_data['Title'], doi, pmid, wd_not_us, settings, verbose=True)\n",
    "    \n",
    "    # Try to match agents if partial title or if a single word label match\n",
    "    if wikidata_status == 'possible partial title' or (wikidata_status == 'fuzzy label match' and len(work_data['Title'].strip().split(' '))==1):\n",
    "        \n",
    "        # Extract the agent family names from the agents in the Zotero output\n",
    "        agent_family_names = []\n",
    "        for agent_type in mapping_agents['sources']:\n",
    "            agents = mapping_functions.extract_names_from_list(work_data[agent_type['in_col_label']], settings)\n",
    "            for agent in agents:\n",
    "                agent_family_names.append(agent['familyName'])\n",
    "                \n",
    "        # Query Wikidata to get all of the agent names for the possibly matching work\n",
    "        wdqs = Sparqler(useragent=user_agent)\n",
    "        query_string = '''select distinct ?agentLabel where {\n",
    "  BIND (wd:''' + qid + ''' AS ?work)\n",
    "  {?work wdt:P50 ?author.\n",
    "   ?author rdfs:label ?agentLabel.}\n",
    "  union\n",
    "  {?work wdt:P98 ?editor.\n",
    "   ?editor rdfs:label ?agentLabel.}\n",
    "  union\n",
    "  {?work wdt:P655 ?translator.\n",
    "   ?translator rdfs:label ?agentLabel.}\n",
    "  filter(lang(?agentLabel) = 'en')\n",
    "  }'''\n",
    "        query_results = wdqs.query(query_string)\n",
    "        sleep(settings['sparql_sleep'])\n",
    "\n",
    "        # Check each name from Wikidata to see if any of the Zotero family names is within the Wikidata name\n",
    "        found = False\n",
    "        for result in query_results:\n",
    "            name = result['agentLabel']['value']\n",
    "            for agent_family_name in agent_family_names:\n",
    "                if agent_family_name in name:\n",
    "                    found = True\n",
    "                    wikidata_status = 'good partial title with name match ' + name\n",
    "                    break\n",
    "            if found:\n",
    "                break\n",
    "        if not found:\n",
    "            wikidata_status = 'bad partial title no agent name match'\n",
    "            qid = '' \n",
    "\n",
    "    print(wikidata_status, qid)\n",
    "    print()\n",
    "    \n",
    "    # \n",
    "    if qid != '':\n",
    "        zotero.loc[index, 'qid'] = qid # Add the Q ID to a new column in the Zotero DataFrame\n",
    "        work_data['Key'] = index # Add index ('Key') back into the row series so it doesn't get lost\n",
    "        work_data['qid'] = qid # Add the Q ID to the row series\n",
    "        wd_zo_not_us= wd_zo_not_us.append(work_data, ignore_index=True) # Add the modified row to the new DF\n",
    "    else:\n",
    "        zotero.loc[index, 'qid'] = ''\n",
    "\n",
    "wd_zo_not_us.to_csv(settings['data_file_path'] + 'wd_zo_not_us.csv', index = False)\n",
    "zotero.to_csv(settings['data_file_path'] + 'zotero.csv')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important note:\n",
    "\n",
    "After this step, the results need to be checked carefully. In some cases, the original Zotero output had duplicates and those need to be de-duplicated, both in the results and the original data.\n",
    "\n",
    "In other cases, there were errors that resulted in multiple works being matched to the same Q ID. Those needed to be resolved as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zotero = pd.read_csv(settings['data_file_path'] + 'zotero.csv', na_filter=False, dtype = str)\n",
    "print(len(zotero))\n",
    "\n",
    "wd_zo_not_us = pd.read_csv(settings['data_file_path'] + 'wd_zo_not_us.csv', na_filter=False, dtype = str)\n",
    "print(len(wd_zo_not_us))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wikidata pubs not in Zotero (C)\n",
    "# existing_works_in_wikidat.csv minus { works.csv (A) union wd_zo_not_us (B) } -> wd_not_zo\n",
    "# Remove from existing works all that now have a Zotero ID.\n",
    "\n",
    "# Get the union of Zotero pubs in Wikidata\n",
    "works_qids_list = list(works.index.values)\n",
    "print('works_qids_list (A)', len(works_qids_list))\n",
    "print('unique A', len(list(set(works_qids_list))))\n",
    "wd_zo_not_us_qids_list = list(wd_zo_not_us['qid'])\n",
    "print('wd_zo_not_us_qids_list (B)', len(wd_zo_not_us_qids_list))\n",
    "print('unique B', len(list(set(wd_zo_not_us_qids_list))))\n",
    "print('B+C', len(wd_not_us))\n",
    "print('A + B', len(works_qids_list + wd_zo_not_us_qids_list))\n",
    "zotero_pubs_in_wikidata_list = list(set(works_qids_list + wd_zo_not_us_qids_list))\n",
    "\n",
    "a_plus_b = works_qids_list + wd_zo_not_us_qids_list\n",
    "duplicates = [qid for qid in a_plus_b if a_plus_b.count(qid) > 1]\n",
    "unique_duplicates = list(set(duplicates))\n",
    "print(unique_duplicates)\n",
    "\n",
    "print('non-redudundant zotero_pubs_in_wikidata_list (A+B)', len(zotero_pubs_in_wikidata_list))\n",
    "#print(zotero_pubs_in_wikidata_list)\n",
    "\n",
    "# Remove those pubs from the total list of works in Wikidata\n",
    "print('existing_works_in_wikidata', len(existing_works_in_wikidata))\n",
    "wd_not_zo = existing_works_in_wikidata.loc[~existing_works_in_wikidata.index.isin(zotero_pubs_in_wikidata_list)].copy()\n",
    "print('wd_not_zo (C)', len(wd_not_zo))\n",
    "wd_not_zo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zotero pubs not in Wikidata (D)\n",
    "\n",
    "# divinity_publications_2022-12-01.csv minus { works.csv (A) union wd_zo_not_us (B) } -> not_wikidata.csv\n",
    "# Alternatively: remove from div pubs all that have qids \n",
    "works_key_list = list(works['unique_identifier'])\n",
    "print(len(works_key_list))\n",
    "wd_zo_not_us_key_list = list(wd_zo_not_us['Key'])\n",
    "print(len(wd_zo_not_us_key_list))\n",
    "\n",
    "not_wikidata = zotero.loc[~zotero.index.isin(works_key_list + wd_zo_not_us_key_list)].copy()\n",
    "print('not_wikidata (D)', len(not_wikidata))\n",
    "not_wikidata.to_csv(settings['data_file_path'] + 'not_wikidata.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
