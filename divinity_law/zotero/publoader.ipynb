{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publoader.ipynb, a Python script for uploading files and data to Wikimedia Commons using the API.\n",
    "\n",
    "# (c) 2022 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "# ----------------\n",
    "# Global variables\n",
    "# ----------------\n",
    "\n",
    "script_version = '0.0.1'\n",
    "default_language = 'en'\n",
    "precision_cutoff = 0.95\n",
    "phrase_length_cutoff = 2\n",
    "sparql_sleep = 0.1 # minimal delay between SPARQL queries\n",
    "\n",
    "# The user_agent string identifies this application to Wikimedia APIs.\n",
    "# If you modify this script, you need to change the user-agent string to something else!\n",
    "user_agent = 'PubLoader/' + script_version + ' (mailto:steve.baskauf@vanderbilt.edu)'\n",
    "\n",
    "# !!! Need to set up an error log!\n",
    "\n",
    "# ----------------\n",
    "# Module imports\n",
    "# ----------------\n",
    "\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from langdetect import detect_langs\n",
    "\n",
    "language_qid = {\n",
    "    'en': 'Q1860',\n",
    "    'de': 'Q188',\n",
    "    'fr': 'Q150',\n",
    "    'es': 'Q1321',\n",
    "    'it': 'Q652',\n",
    "    'nl': 'Q7411',\n",
    "    'zh': 'Q7850',\n",
    "    'no': 'Q9043',\n",
    "    'ar': 'Q13955',\n",
    "    'he': 'Q9288',\n",
    "    'pt': 'Q5146'\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "# List of known work types used by CrossRef\n",
    "work_types = [\n",
    "    {\n",
    "    'type_string': 'journal-article',\n",
    "    'qid': 'Q18918145', # academic journal article, alternatively Q13442814 scholarly article\n",
    "    'description': 'journal article'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'book',\n",
    "    'qid': 'Q3331189', # \"version, edition, or translation\"\n",
    "    'description': 'book'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'book-chapter',\n",
    "    'qid': 'Q21481766', # \"academic chapter\"\n",
    "    'description': 'academic book chapter'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'monograph',\n",
    "    'qid': 'Q193495', # monograph\n",
    "    'description': 'monograph'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'reference-book',\n",
    "    'qid': 'Q5292', # encyclopedia\n",
    "    'description': 'encyclopedia'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'reference-entry',\n",
    "    'qid': 'Q13433827', # encyclopedia article, some are handbook articles\n",
    "    'description': 'encyclopedia article'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'dataset',\n",
    "    'qid': 'Q13433827', # also used for encyclopedia articles\n",
    "    'description': 'encyclopedia article'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'other',\n",
    "    'qid': 'Q55915575', # \n",
    "    'description': 'scholarly work'\n",
    "    }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# List of known work types used by Zotero\n",
    "work_types = [\n",
    "    {\n",
    "    'type_string': 'journalArticle',\n",
    "    'qid': 'Q18918145', # academic journal article, alternatively Q13442814 scholarly article\n",
    "    'description': 'journal article'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'book',\n",
    "    'qid': 'Q3331189', # \"version, edition, or translation\"\n",
    "    'description': 'book'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'bookSection',\n",
    "    'qid': 'Q21481766', # \"academic chapter\"\n",
    "    'description': 'academic book chapter'\n",
    "    }\n",
    "]\n",
    "\n",
    "# ------------------------\n",
    "# Utility functions\n",
    "# ------------------------\n",
    "\n",
    "def extract_local_name(iri):\n",
    "    \"\"\"Extract the local name part of an IRI, e.g. a Q ID from a Wikidata IRI\"\"\"\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[-1]\n",
    "\n",
    "def roman_integer_value(r):\n",
    "    \"\"\"Return value of Roman numeral symbol.\n",
    "    \n",
    "    Note:\n",
    "    -----\n",
    "    Code from https://www.geeksforgeeks.org/python-program-for-converting-roman-numerals-to-decimal-lying-between-1-to-3999/\"\"\"    \n",
    "    if (r == 'I'):\n",
    "        return 1\n",
    "    if (r == 'V'):\n",
    "        return 5\n",
    "    if (r == 'X'):\n",
    "        return 10\n",
    "    if (r == 'L'):\n",
    "        return 50\n",
    "    if (r == 'C'):\n",
    "        return 100\n",
    "    if (r == 'D'):\n",
    "        return 500\n",
    "    if (r == 'M'):\n",
    "        return 1000\n",
    "    return -1\n",
    "\n",
    "def roman_to_decimal(numeral):\n",
    "    \"\"\"Convert Roman numerals to integers.\n",
    "    \n",
    "    Note:\n",
    "    -----\n",
    "    Code from https://www.geeksforgeeks.org/python-program-for-converting-roman-numerals-to-decimal-lying-between-1-to-3999/\"\"\"\n",
    "    str = numeral.upper()\n",
    "    res = 0\n",
    "    i = 0\n",
    "\n",
    "    while (i < len(str)):\n",
    "\n",
    "        # Getting value of symbol s[i]\n",
    "        s1 = roman_integer_value(str[i])\n",
    "        \n",
    "        # Return a negative number if error.\n",
    "        if s1 < 0:\n",
    "            return -1\n",
    "\n",
    "        if (i + 1 < len(str)):\n",
    "\n",
    "            # Getting value of symbol s[i + 1]\n",
    "            s2 = roman_integer_value(str[i + 1])\n",
    "            \n",
    "            # Return a negative number if error.\n",
    "            if s2 < 0:\n",
    "                return -1\n",
    "\n",
    "            # Comparing both values\n",
    "            if (s1 >= s2):\n",
    "\n",
    "                # Value of current symbol is greater\n",
    "                # or equal to the next symbol\n",
    "                res = res + s1\n",
    "                i = i + 1\n",
    "            else:\n",
    "\n",
    "                # Value of current symbol is greater\n",
    "                # or equal to the next symbol\n",
    "                res = res + s2 - s1\n",
    "                i = i + 2\n",
    "        else:\n",
    "            res = res + s1\n",
    "            i = i + 1\n",
    "\n",
    "    return res\n",
    "\n",
    "# ------------------------\n",
    "# SPARQL query class\n",
    "# ------------------------\n",
    "\n",
    "# This is a condensed version of the more full-featured script at \n",
    "# https://github.com/HeardLibrary/digital-scholarship/blob/master/code/wikidata/sparqler.py\n",
    "# It includes only the method for the query form.\n",
    "\n",
    "class Sparqler:\n",
    "    \"\"\"Build SPARQL queries of various sorts\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    useragent : str\n",
    "        Required if using the Wikidata Query Service, otherwise optional.\n",
    "        Use the form: appname/v.v (URL; mailto:email@domain.com)\n",
    "        See https://meta.wikimedia.org/wiki/User-Agent_policy\n",
    "    endpoint: URL\n",
    "        Defaults to Wikidata Query Service if not provided.\n",
    "    method: str\n",
    "        Possible values are \"post\" (default) or \"get\". Use \"get\" if read-only query endpoint.\n",
    "        Must be \"post\" for update endpoint.\n",
    "    sleep: float\n",
    "        Number of seconds to wait between queries. Defaults to 0.1\n",
    "        \n",
    "    Required modules:\n",
    "    -------------\n",
    "    requests, datetime, time\n",
    "    \"\"\"\n",
    "    def __init__(self, method='post', endpoint='https://query.wikidata.org/sparql', useragent=None, sleep=0.1):\n",
    "        # attributes for all methods\n",
    "        self.http_method = method\n",
    "        self.endpoint = endpoint\n",
    "        if useragent is None:\n",
    "            if self.endpoint == 'https://query.wikidata.org/sparql':\n",
    "                print('You must provide a value for the useragent argument when using the Wikidata Query Service.')\n",
    "                print()\n",
    "                raise KeyboardInterrupt # Use keyboard interrupt instead of sys.exit() because it works in Jupyter notebooks\n",
    "        self.sleep = sleep\n",
    "\n",
    "        self.requestheader = {}\n",
    "        if useragent:\n",
    "            self.requestheader['User-Agent'] = useragent\n",
    "        \n",
    "        if self.http_method == 'post':\n",
    "            self.requestheader['Content-Type'] = 'application/x-www-form-urlencoded'\n",
    "\n",
    "    def query(self, query_string, form='select', verbose=False, **kwargs):\n",
    "        \"\"\"Send a SPARQL query to the endpoint.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        form : str\n",
    "            The SPARQL query form.\n",
    "            Possible values are: \"select\" (default), \"ask\", \"construct\", and \"describe\".\n",
    "        mediatype: str\n",
    "            The response media type (MIME type) of the query results.\n",
    "            Some possible values for \"select\" and \"ask\" are: \"application/sparql-results+json\" (default) and \"application/sparql-results+xml\".\n",
    "            Some possible values for \"construct\" and \"describe\" are: \"text/turtle\" (default) and \"application/rdf+xml\".\n",
    "            See https://docs.aws.amazon.com/neptune/latest/userguide/sparql-media-type-support.html#sparql-serialization-formats-neptune-output\n",
    "            for response serializations supported by Neptune.\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        default: list of str\n",
    "            The graphs to be merged to form the default graph. List items must be URIs in string form.\n",
    "            If omitted, no graphs will be specified and default graph composition will be controlled by FROM clauses\n",
    "            in the query itself. \n",
    "            See https://www.w3.org/TR/sparql11-query/#namedGraphs and https://www.w3.org/TR/sparql11-protocol/#dataset\n",
    "            for details.\n",
    "        named: list of str\n",
    "            Graphs that may be specified by IRI in a query. List items must be URIs in string form.\n",
    "            If omitted, named graphs will be specified by FROM NAMED clauses in the query itself.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        If the form is \"select\" and mediatype is \"application/json\", a list of dictionaries containing the data.\n",
    "        If the form is \"ask\" and mediatype is \"application/json\", a boolean is returned.\n",
    "        If the mediatype is \"application/json\" and an error occurs, None is returned.\n",
    "        For other forms and mediatypes, the raw output is returned.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        To get UTF-8 text in the SPARQL queries to work properly, send URL-encoded text rather than raw text.\n",
    "        That is done automatically by the requests module for GET. I guess it also does it for POST when the\n",
    "        data are sent as a dict with the urlencoded header. \n",
    "        See SPARQL 1.1 protocol notes at https://www.w3.org/TR/sparql11-protocol/#query-operation        \n",
    "        \"\"\"\n",
    "        query_form = form\n",
    "        if 'mediatype' in kwargs:\n",
    "            media_type = kwargs['mediatype']\n",
    "        else:\n",
    "            if query_form == 'construct' or query_form == 'describe':\n",
    "            #if query_form == 'construct':\n",
    "                media_type = 'text/turtle'\n",
    "            else:\n",
    "                media_type = 'application/sparql-results+json' # default for SELECT and ASK query forms\n",
    "        self.requestheader['Accept'] = media_type\n",
    "            \n",
    "        # Build the payload dictionary (query and graph data) to be sent to the endpoint\n",
    "        payload = {'query' : query_string}\n",
    "        if 'default' in kwargs:\n",
    "            payload['default-graph-uri'] = kwargs['default']\n",
    "        \n",
    "        if 'named' in kwargs:\n",
    "            payload['named-graph-uri'] = kwargs['named']\n",
    "\n",
    "        if verbose:\n",
    "            print('querying SPARQL endpoint')\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        if self.http_method == 'post':\n",
    "            response = requests.post(self.endpoint, data=payload, headers=self.requestheader)\n",
    "        else:\n",
    "            response = requests.get(self.endpoint, params=payload, headers=self.requestheader)\n",
    "        elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "        self.response = response.text\n",
    "        sleep(self.sleep) # Throttle as a courtesy to avoid hitting the endpoint too fast.\n",
    "\n",
    "        if verbose:\n",
    "            print('done retrieving data in', int(elapsed_time), 's')\n",
    "\n",
    "        if query_form == 'construct' or query_form == 'describe':\n",
    "            return response.text\n",
    "        else:\n",
    "            if media_type != 'application/sparql-results+json':\n",
    "                return response.text\n",
    "            else:\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                except:\n",
    "                    return None # Returns no value if an error. \n",
    "\n",
    "                if query_form == 'select':\n",
    "                    # Extract the values from the response JSON\n",
    "                    results = data['results']['bindings']\n",
    "                else:\n",
    "                    results = data['boolean'] # True or False result from ASK query \n",
    "                return results           \n",
    "\n",
    "# ------------------------\n",
    "# mapping functions\n",
    "# ------------------------\n",
    "\n",
    "def identity(value):\n",
    "    \"\"\"Return the value argument with any leading and trailing whitespace removed.\"\"\"\n",
    "    return value.strip()\n",
    "\n",
    "def set_instance_of(string):\n",
    "    \"\"\"Match the type string with possible types for the data source and return the type Q ID.\"\"\"\n",
    "    if string == '':\n",
    "        return ''\n",
    "\n",
    "    for work_type in work_types:\n",
    "        if string == work_type['type_string']:\n",
    "            return work_type['qid']\n",
    "\n",
    "    print('Did not find datatype for type:', string)\n",
    "    return ''\n",
    "\n",
    "def set_description(string):\n",
    "    \"\"\"Match the type string with possible types for the data source and return the description.\"\"\"\n",
    "    if string == '':\n",
    "        return ''\n",
    "    \n",
    "    for work_type in work_types:\n",
    "        if string == work_type['type_string']:\n",
    "            return work_type['description']\n",
    "\n",
    "    print('Did not find datatype for type:', string)\n",
    "    return ''\n",
    "\n",
    "def detect_language(string):\n",
    "    \"\"\"Detect the language of the label.\"\"\"\n",
    "    try:\n",
    "        lang_list = detect_langs(string)\n",
    "        lang_string = str(lang_list[0])\n",
    "        confidence = float(lang_string[3:])\n",
    "        lang = lang_string[:2]\n",
    "    except: #exceptions occur when no info to decide, e.g. numbers\n",
    "        lang = 'zxx'\n",
    "        confidence = float(0)\n",
    "    if confidence < precision_cutoff:\n",
    "        print('Warning: language confidence below', precision_cutoff, ':', confidence)\n",
    "    if lang in language_qid:\n",
    "        return language_qid[lang]\n",
    "    else:\n",
    "        print('Warning: detected language', lang, 'not in list of known languages.')\n",
    "        return ''\n",
    "\n",
    "def calculate_pages(range):\n",
    "    \"\"\"Calculate the number of pages from the page range.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    Supports properly formatted Roman numerals and doesn't care about whitespace.\"\"\"\n",
    "    numbers = range.split('-')\n",
    "    \n",
    "    # If there is only a single number or an empty cell, return the empty string.\n",
    "    if len(numbers) < 2:\n",
    "        return ''\n",
    "    # Edge case where it isn't a well-formed range and has multiple hyphens\n",
    "    if len(numbers) > 2:\n",
    "        return ''\n",
    "    \n",
    "    # Step through the two numbers to try to convert them from Roman numerals if not integers.\n",
    "    for index, number in enumerate(numbers):\n",
    "        number = number.strip()\n",
    "        if not number.isnumeric():\n",
    "            numbers[index] = roman_to_decimal(number)\n",
    "            \n",
    "            # Will return -1 error if it contains characters not valid for Roman numerals \n",
    "            if numbers[index] < 0:\n",
    "                return ''\n",
    "    \n",
    "    number_pages = int(numbers[1]) - int(numbers[0]) + 1 # Need to add one since first page in range counts\n",
    "    if number_pages < 1:\n",
    "        return ''\n",
    "    return str(number_pages)\n",
    "    \n",
    "    return value\n",
    "\n",
    "def clean_doi(value):\n",
    "    \"\"\"Turn DOI into uppercase and remove leading and trailing whitespace.\"\"\"\n",
    "    cleaned_value = value.upper().strip()\n",
    "    return cleaned_value\n",
    "\n",
    "def disambiguate_published_in(value):\n",
    "    \"\"\"Use the value in the ISSN column to try to find the containing work.\n",
    "    \n",
    "    Note:\n",
    "    -----\n",
    "    For journal articles, this performs a legitimate WQS search for the journal title using the ISSN.\n",
    "    For book chapters, the ISSN column may contain the Q ID of the containing book, inserted there during\n",
    "    a pre-processing step (a hack, but typically books would not have an ISSN and this column would be empty).\"\"\"\n",
    "    if value == '':\n",
    "        return value\n",
    "    \n",
    "    # The value is a Q ID and was determined during a pre-processing step (i.e. for book chapters)\n",
    "    if value[0] == 'Q':\n",
    "        return value\n",
    "\n",
    "    # Look up the ISSN from CrossRef in Wikidata\n",
    "    # Build query string\n",
    "    query_string = '''select distinct ?container ?containerLabel where {\n",
    "      ?container wdt:P236 \"''' + value + '''\".\n",
    "      optional {\n",
    "      ?container rdfs:label ?containerLabel.\n",
    "      filter(lang(?containerLabel)=\"''' + default_language + '''\")\n",
    "      }\n",
    "    }'''\n",
    "    #print(query_string)\n",
    "\n",
    "    wdqs = Sparqler(useragent=user_agent)\n",
    "    query_results = wdqs.query(query_string)\n",
    "    sleep(sparql_sleep)\n",
    "\n",
    "    # !!!!!!!!!!!!!!!!!! Enable this code when the error log is set up\n",
    "    \"\"\"\n",
    "    if len(query_results) > 1:\n",
    "        print('Warning! More than one container in Wikidata matched the ISSN ', file=log_object)\n",
    "        print(query_results, '\\n', file=log_object)\n",
    "    \"\"\"\n",
    "    # Extract Q ID from SPARQL query results. If there is more than one result, the last one will be used for the Q ID\n",
    "    for result in query_results:\n",
    "        container_qid = extract_local_name(result['container']['value'])\n",
    "        # Skipping this since container name isn't passed into the function.\n",
    "        \"\"\"\n",
    "        journal_name = result['containerLabel']['value']\n",
    "        if journal_name != crossref_results['journal_title']:\n",
    "            # NOTE: did empirical testing to see which kind of fuzzy matching worked best\n",
    "            #ratio = fuzz.ratio(journal_name, crossref_results['journal_title'])\n",
    "            #partial_ratio = fuzz.partial_ratio(journal_name, crossref_results['journal_title'])\n",
    "            #sort_ratio = fuzz.token_sort_ratio(journal_name, crossref_results['journal_title'])\n",
    "            #set_ratio = fuzz.token_set_ratio(journal_name, crossref_results['journal_title'])\n",
    "            w_ratio = fuzz.WRatio(journal_name, crossref_results['journal_title'])\n",
    "            #print('name similarity ratio', ratio)\n",
    "            #print('partial ratio', partial_ratio)\n",
    "            #print('sort_ratio', sort_ratio)\n",
    "            #print('set_ratio', set_ratio)\n",
    "            if w_ratio < 99:\n",
    "                print('article:', crossref_results['label_' + default_language], 'w_ratio:', w_ratio, 'Warning: Wikidata journal: \"' + journal_name + '\"', journal_qid, 'does not match CrossRef journal title: \"' + crossref_results['journal_title'] + '\"\\n', file=log_object)\n",
    "        #print('article:', crossref_results['label_' + default_language], 'journal:', journal_qid, journal_name)\n",
    "        \"\"\"\n",
    "    return container_qid\n",
    "\n",
    "def isbn10(string):\n",
    "    test = string.replace('-', '')\n",
    "    if len(test) == 10:\n",
    "        return string\n",
    "    return ''\n",
    "\n",
    "def isbn13(string):\n",
    "    test = string.replace('-', '')\n",
    "    if len(test) == 13:\n",
    "        return string\n",
    "    return ''\n",
    "\n",
    "def disambiguate_publisher(name_string):\n",
    "    \"\"\"Look up the publisher Q ID from a list derived from a SPARQL query https://w.wiki/4pbi\"\"\"\n",
    "    # Set publisher Q ID to empty string if there's no publisher string\n",
    "    if name_string == '':\n",
    "        return ''\n",
    "    \n",
    "    best_match_score = 0\n",
    "    best_match = ''\n",
    "    best_match_label = ''\n",
    "    for qid, publisher in publishers.iterrows():  # The publishers DataFrame is a global variable\n",
    "        w_ratio = fuzz.WRatio(name_string, publisher['label'])\n",
    "        if w_ratio > best_match_score:\n",
    "            best_match = qid\n",
    "            best_match_label = publisher['label']\n",
    "            best_match_score = w_ratio\n",
    "            \n",
    "    if best_match_score < 98:\n",
    "        print('w_ratio:', best_match_score, 'Warning: poor match of: \"' + best_match_label + '\"', best_match, 'to stated publisher: \"' + name_string + '\"\\n')\n",
    "        #print('w_ratio:', best_match_score, 'Warning: poor match of: \"' + best_match_label + '\"', best_match, 'to stated publisher: \"' + name_string + '\"\\n', file=log_object)\n",
    "    return best_match\n",
    "\n",
    "def disambiguate_place_of_publication(value):\n",
    "    \"\"\"Return the value argument unchanged.\"\"\"\n",
    "    return value\n",
    "\n",
    "def today():\n",
    "    \"\"\"Generate the current UTC xsd:date\"\"\"\n",
    "    whole_time_string_z = datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "def set_reference(url):\n",
    "    return url\n",
    "\n",
    "# ---------------------------\n",
    "# Major processes functions\n",
    "# ---------------------------\n",
    "\n",
    "def extract_metadata(mapping, work_data):\n",
    "    out_dict = {'qid': ''}\n",
    "\n",
    "    for out_property in config['outfiles'][0]['prop_list']:\n",
    "        \n",
    "        # Find the mapping variable that matches the config property\n",
    "        for prop in mapping['properties']:\n",
    "            if prop['variable'] == out_property['variable']:\n",
    "                break\n",
    "    \n",
    "        out_field = out_property['variable']\n",
    "        out_dict[out_field + '_uuid'] = ''\n",
    "        \n",
    "        # The mapping function may not require an argument. In that case, there's no source column.\n",
    "        if 'source' in prop:\n",
    "            # If the source data CSV doesn't have any column named according to mappings, the output for that\n",
    "            # variable is an empty string.\n",
    "            if prop['source'] in work_data:\n",
    "                if work_data[prop['source']] == '':\n",
    "                    output_value = ''\n",
    "                    no_value = True\n",
    "                else:\n",
    "                    expression = prop['value'] + \"('''\" + work_data[prop['source']] + \"''')\"\n",
    "                    output_value = eval(expression)\n",
    "                    no_value = False\n",
    "            else:\n",
    "                output_value = ''\n",
    "                no_value = True\n",
    "        # Case where there's no argument passed to mapping function\n",
    "        else:\n",
    "            expression = prop['value'] + '()'\n",
    "            output_value = eval(expression)\n",
    "            if output_value == '':\n",
    "                no_value = True\n",
    "            else:\n",
    "                no_value = False\n",
    "\n",
    "        # Populate the values-related columns\n",
    "        if out_property['value_type'] == 'date':\n",
    "            out_dict[out_field + '_nodeId'] = ''\n",
    "            out_dict[out_field + '_val'] = output_value\n",
    "            out_dict[out_field + '_prec'] = ''\n",
    "\n",
    "        elif out_property['value_type'] == 'quantity':\n",
    "            out_dict[out_field + '_nodeId'] = ''\n",
    "            out_dict[out_field + '_val'] = output_value\n",
    "            if no_value:\n",
    "                out_dict[out_field + '_unit'] = ''\n",
    "            else:\n",
    "                out_dict[out_field + '_unit'] = prop['quantity_unit']\n",
    "\n",
    "        # This is not actually implemented and will generate an error if used\n",
    "        elif out_property['value_type'] == 'globecoordinate':\n",
    "            out_dict[out_field + '_nodeId'] = ''\n",
    "            out_dict[out_field + '_val'] = output_value\n",
    "            if no_value:\n",
    "                out_dict[out_field + '_long'] = ''\n",
    "                out_dict[out_field + '_prec'] = ''\n",
    "            else:\n",
    "                out_dict[out_field + '_long'] = work_data[out_field + '_long']\n",
    "                out_dict[out_field + '_prec'] = work_data[out_field + '_prec']\n",
    "\n",
    "        else:\n",
    "            out_dict[out_field] = output_value\n",
    "\n",
    "        # Populate the qualifier columns\n",
    "        for qualifier in out_property['qual']:\n",
    "            if no_value:\n",
    "                no_qual_value = True\n",
    "            else:\n",
    "                # Find the mapping variable that matches the config property\n",
    "                for qual in prop['qual']:\n",
    "                    if qual['variable'] == qualifier['variable']:\n",
    "                        break\n",
    "\n",
    "                # Skip reading a value from a source column if the function doesn't need input.\n",
    "                if 'source' in qual:\n",
    "                    if work_data[qual['source']] == '':\n",
    "                        no_qual_value = True\n",
    "                    else:\n",
    "                        no_qual_value = False\n",
    "                        expression = qual['value'] + \"('''\" + work_data[qual['source']] + \"''')\"\n",
    "                        qual_output_value = eval(expression)\n",
    "                else:\n",
    "                    no_qual_value = False\n",
    "                    expression = qual['value'] + '()'\n",
    "                    qual_output_value = eval(expression) # If this evalutes as empty string, result is same as no_ref_value = True\n",
    "\n",
    "\n",
    "            qual_field = out_field + '_' + qualifier['variable']\n",
    "            # To my knowledge, dates are the only complex types used as qualifiers (no quantities or globecoordinates).\n",
    "            if qualifier['value_type'] == 'date':\n",
    "                out_dict[qual_field + '_nodeId'] = ''\n",
    "                if no_qual_value:\n",
    "                    out_dict[qual_field + '_val'] = ''\n",
    "                else:\n",
    "                    out_dict[qual_field + '_val'] = qual_output_value\n",
    "                out_dict[qual_field + '_prec'] = ''\n",
    "            else:\n",
    "                if no_qual_value:\n",
    "                    out_dict[qual_field] = ''\n",
    "                else:\n",
    "                    out_dict[qual_field] = qual_output_value\n",
    "                \n",
    "        # Populate the reference columns\n",
    "        # There's only a hash ID column if there's at least one reference.\n",
    "        if len(out_property['ref']) > 0:\n",
    "            out_dict[out_field + '_ref1_hash'] = ''\n",
    "            \n",
    "        for reference in out_property['ref']:\n",
    "            if no_value:\n",
    "                no_ref_value = True\n",
    "            else:\n",
    "                # Find the mapping variable that matches the config property\n",
    "                for ref in prop['ref']:\n",
    "                    if ref['variable'] == reference['variable']:\n",
    "                        break\n",
    "\n",
    "                # Some functions like today() don't need input from the source table, and therefore \n",
    "                # skip reading a value from a source column.\n",
    "                if 'source' in ref:\n",
    "                    if work_data[ref['source']] == '':\n",
    "                        no_ref_value = True\n",
    "                    else:\n",
    "                        no_ref_value = False\n",
    "                        expression = ref['value'] + \"('''\" + work_data[ref['source']] + \"''')\"\n",
    "                        ref_output_value = eval(expression)\n",
    "                else:\n",
    "                    no_ref_value = False\n",
    "                    expression = ref['value'] + '()'\n",
    "                    ref_output_value = eval(expression) # If this evalutes as empty string, result is same as no_ref_value = True\n",
    "\n",
    "            ref_field = out_field + '_ref1_' + reference['variable']\n",
    "            # To my knowledge, dates are the only complex types used as qualifiers (no quantities or globecoordinates).\n",
    "            if reference['value_type'] == 'date':\n",
    "                out_dict[ref_field + '_nodeId'] = ''\n",
    "                if no_ref_value:\n",
    "                    out_dict[ref_field + '_val'] = ''\n",
    "                else:\n",
    "                    out_dict[ref_field + '_val'] = ref_output_value\n",
    "                out_dict[ref_field + '_prec'] = ''\n",
    "            else:\n",
    "                if no_ref_value:\n",
    "                    out_dict[ref_field] = ''\n",
    "                else:\n",
    "                    out_dict[ref_field] = ref_output_value\n",
    "                    \n",
    "    #print(out_dict)\n",
    "    return out_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "publishers = pd.read_csv('publishers.csv', na_filter=False, dtype = str)\n",
    "publishers.set_index('qid', inplace=True)\n",
    "\n",
    "source_data = 'output_examples_div_pubs.csv'\n",
    "#source_data = 'output-example_baldwinbookschaptersarticles.csv'\n",
    "works = pd.read_csv(source_data, na_filter=False, dtype = str)\n",
    "\n",
    "with open('config.yaml', 'r') as file_object:\n",
    "    config = yaml.safe_load(file_object)\n",
    "\n",
    "with open('mapping.yaml', 'r') as file_object:\n",
    "    mapping = yaml.safe_load(file_object)\n",
    "\n",
    "works = works.iloc[1970:1988]\n",
    "\n",
    "works_list = []\n",
    "for index, work_data in works.iterrows():\n",
    "    print(work_data['Title'])\n",
    "    row = extract_metadata(mapping, work_data)\n",
    "    works_list.append(row)\n",
    "out_frame = pd.DataFrame(works_list)\n",
    "\n",
    "out_frame.to_csv('test_works_to_write.csv', index = False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
