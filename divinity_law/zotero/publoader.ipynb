{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publoader.ipynb, a Python script for uploading files and data to Wikimedia Commons using the API.\n",
    "# version 0.0.1\n",
    "\n",
    "# (c) 2022 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "# ----------------\n",
    "# Module imports\n",
    "# ----------------\n",
    "\n",
    "import yaml\n",
    "import sys\n",
    "#import csv\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import requests_cache\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from langdetect import detect_langs\n",
    "import re # regex\n",
    "import logging # See https://docs.python.org/3/howto/logging.html\n",
    "\n",
    "# module located in same directory as script\n",
    "import mapping_functions\n",
    "\n",
    "# Set up cache for HTTP requests\n",
    "requests_cache.install_cache('wqs_cache', backend='sqlite', expire_after=300, allowable_methods=['GET', 'POST'])\n",
    "\n",
    "# ------------------------\n",
    "# Utility functions\n",
    "# ------------------------\n",
    "\n",
    "def calculate_todays_date():\n",
    "    \"\"\"Generate the current UTC xsd:date\"\"\"\n",
    "    whole_time_string_z = datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "def extract_local_name(iri):\n",
    "    \"\"\"Extract the local name part of an IRI, e.g. a Q ID from a Wikidata IRI\"\"\"\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[-1]\n",
    "\n",
    "def include_reference_url(url, full_works):\n",
    "    \"\"\"Returned strings are suitable to use for references. Currently it's assumed that the criteria are the\n",
    "    same for full work available.\"\"\"\n",
    "    url_pattern = \"^https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)$\"\n",
    "    url_inclusion_strings = [\n",
    "    'doi',\n",
    "    'jstor',\n",
    "    #'oxfordjournals.org/content',\n",
    "    'article',\n",
    "    'academia.edu',\n",
    "    'content',\n",
    "    'proquest.com/docview',\n",
    "    'handle'\n",
    "    ]\n",
    "    \n",
    "    url_exclusion_strings = [\n",
    "    'login',\n",
    "    'proxy',\n",
    "    #'search.proquest.com',\n",
    "    'worldcat',\n",
    "    'wp-content',\n",
    "    'site.ebrary.com',\n",
    "    'cro3.org/',\n",
    "    'worldbookonline.com/pl/infofinder'\n",
    "    ]\n",
    "\n",
    "    url = url.lower() # convert to all lowercase\n",
    "    \n",
    "    # Exclude invalid URLs\n",
    "    if re.match(url_pattern, url) is None:\n",
    "        return ''\n",
    "\n",
    "    # If the URL matches one of the pre-screened URLs, use it\n",
    "    matched_series = full_works.loc[full_works['Url']==url, 'Url']\n",
    "    # matched_series will be a Series composed of all values in the Url column that match. There should be 1 or 0.\n",
    "    if len(matched_series) == 1:\n",
    "        return url\n",
    "    \n",
    "    # Exclude any URLs containing strings that indicate a login is required\n",
    "    for screening_string in url_exclusion_strings:\n",
    "        if screening_string in url:\n",
    "            return ''\n",
    "        \n",
    "    # Must contain one of the strings that indicate metadata and possible acces\n",
    "    for screening_string in url_inclusion_strings:\n",
    "        if screening_string in url:\n",
    "            return url\n",
    "        \n",
    "    return ''\n",
    "\n",
    "def set_description(string, work_types):\n",
    "    \"\"\"Match the type string with possible types for the data source and return the description.\"\"\"\n",
    "    if string == '':\n",
    "        return ''\n",
    "    \n",
    "    for work_type in work_types:\n",
    "        if string == work_type['type_string']:\n",
    "            return work_type['description']\n",
    "\n",
    "    print('Did not find datatype for type:', string)\n",
    "    logging.warning('Did not find datatype for type: ' + string)\n",
    "    return ''\n",
    "\n",
    "def title_if_no_lowercase(string):\n",
    "    \"\"\"Change to titlecase only if there are no lowercase letters in the string.\"\"\"\n",
    "    lower = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    is_lower = False\n",
    "    for letter in string:\n",
    "        if letter in lower:\n",
    "            is_lower = True\n",
    "    if is_lower:\n",
    "        return string\n",
    "    else:\n",
    "        return string.title()\n",
    "\n",
    "def fix_all_caps(name_pieces):\n",
    "    \"\"\"Input is a list of name strings from name split by spaces\"\"\"\n",
    "    clean_pieces = []\n",
    "    for piece in name_pieces:\n",
    "        # Special handing for names starting with apostrophe-based prefixes\n",
    "        apostrophe_list = [\"van't\", \"'t\", \"O'\", \"D'\", \"d'\", \"N'\"]\n",
    "        apostrophe_prefix = ''\n",
    "        for possible_apostrophe_prefix in apostrophe_list:\n",
    "            if possible_apostrophe_prefix in piece:\n",
    "                # Remove prefix\n",
    "                piece = piece.replace(possible_apostrophe_prefix, '')\n",
    "                apostrophe_prefix = possible_apostrophe_prefix\n",
    "        \n",
    "        # Special handling for name parts that are lowercase\n",
    "        lower_case_list = ['von', 'de', 'van', 'la', 'der']\n",
    "        if piece.lower() in lower_case_list:\n",
    "            piece = piece.lower()\n",
    "        else:\n",
    "            # Special handling for hyphenated names; doesn't work for an edge case with more than 2 hyphens\n",
    "            if '-' in piece:\n",
    "                halves = piece.split('-')\n",
    "                piece = title_if_no_lowercase(halves[0]) + '-' + title_if_no_lowercase(halves[1])\n",
    "            else:\n",
    "                piece = title_if_no_lowercase(piece)\n",
    "        \n",
    "        # put any apostrophe prefix back on the front\n",
    "        if apostrophe_prefix:\n",
    "            piece = apostrophe_prefix + piece\n",
    "        \n",
    "        clean_pieces.append(piece)\n",
    "    return clean_pieces\n",
    "  \n",
    "def extract_name_pieces(name):\n",
    "    \"\"\"Extract parts of names. Recognize typical male suffixes. Fix ALL CAPS if present.\"\"\"\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods, sometimes periods are close up with no spaces\n",
    "    name = name.replace('.', ' ')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "        \n",
    "    # Fix stupid situation where name is written in ALL CAPS\n",
    "    pieces = fix_all_caps(pieces)\n",
    "    return pieces, suffix\n",
    "    \n",
    "def extract_names_from_list(names_string, settings):\n",
    "    \"\"\"Extract multiple authors from a character-separated list in a single string.\"\"\"\n",
    "    if names_string == '':\n",
    "        return []\n",
    "    \n",
    "    names_list = names_string.split(settings['names_separator'])\n",
    "    \n",
    "    output_list = []\n",
    "    # If names are last name first\n",
    "    if settings['name_part_separator']:\n",
    "        for name in names_list:\n",
    "            pieces = name.split(settings['name_part_separator'])\n",
    "            # Keep removing empty strings until there aren't any more\n",
    "            while '' in pieces:\n",
    "                pieces.remove('')\n",
    "            if len(pieces) == 1: # an error, name wasn't reversed\n",
    "                print('Name error:', names_string)\n",
    "                logging.warning('Name error: ' + names_string)\n",
    "                surname_pieces = []\n",
    "                given_pieces = []\n",
    "                suffix = ''\n",
    "            elif len(pieces) == 2: # no Jr.\n",
    "                surname_pieces, suffix = extract_name_pieces(pieces[0].strip())\n",
    "                given_pieces, dummy = extract_name_pieces(pieces[1].strip())\n",
    "            elif len(pieces) == 3: # has Jr.\n",
    "                # Note Jr. is handled inconsistently, sometimes placed after entire name, sometimes after surname\n",
    "                if 'Jr' in pieces[2]:\n",
    "                    surname_pieces, suffix = extract_name_pieces(pieces[0].strip() + ', ' + pieces[2].strip())\n",
    "                    given_pieces, dummy = extract_name_pieces(pieces[1].strip())\n",
    "                else:\n",
    "                    surname_pieces, suffix = extract_name_pieces(pieces[0].strip() + ', ' + pieces[1].strip())\n",
    "                    given_pieces, dummy = extract_name_pieces(pieces[2].strip())                    \n",
    "            else:\n",
    "                print('Name error:', names_string)\n",
    "                logging.warning('Name error: ' + names_string)\n",
    "                surname_pieces = []\n",
    "                given_pieces = []\n",
    "                suffix = ''\n",
    "                \n",
    "            surname = ' '.join(surname_pieces)\n",
    "            given = ' '.join(given_pieces)\n",
    "            output_list.append({'orcid': '', 'givenName': given, 'familyName': surname, 'suffix': suffix, 'affiliation': []})\n",
    "    else:\n",
    "        pass # need to write code for case where they aren't reversed\n",
    "        \n",
    "    \n",
    "    return output_list\n",
    "\n",
    "def extract_identifier_from_extra(extra_field, id_name):\n",
    "    \"\"\"Extract an identifier from the Zotero export from the Extra field. May contain the DOI of book \n",
    "    sections or PMID of articles.\"\"\"\n",
    "    identifier = ''\n",
    "    tokens = extra_field.split(' ')\n",
    "    for token_index in range(len(tokens)):\n",
    "        if tokens[token_index] == id_name + ':': # match the tag for the desired ID\n",
    "            # The identifer is the next token after the tag\n",
    "            identifier = tokens[token_index + 1]\n",
    "            break\n",
    "    return identifier\n",
    "\n",
    "def search_name_at_wikidata(name, user_agent):\n",
    "    \"\"\"Carry out a search for most languages that use Latin characters, plus some other commonly used \n",
    "    languages. See https://doi.org/10.1145/3233391.3233965 for reference.\"\"\"\n",
    "    language_codes = [\n",
    "        'en',\n",
    "        'es',\n",
    "        'pt',\n",
    "        'fr',\n",
    "        'it',\n",
    "        'nl',\n",
    "        'de',\n",
    "        'da',\n",
    "        'et',\n",
    "        'hu',\n",
    "        'ga',\n",
    "        'ro',\n",
    "        'sk',\n",
    "        'sl',\n",
    "        'zu',\n",
    "        'tr',\n",
    "        'sv',\n",
    "        'zh',\n",
    "        'ru',\n",
    "        'ja',\n",
    "        'ar',\n",
    "        'pl',\n",
    "        'uk',\n",
    "        'ca',\n",
    "        'cs',\n",
    "        'la',\n",
    "        'nb',\n",
    "        'hu',\n",
    "        'he',\n",
    "        'eo',\n",
    "        'fi',\n",
    "        'ko'\n",
    "      ]\n",
    "    name_list = generate_name_alternatives(name)\n",
    "    alternatives = ''\n",
    "    for language_code in language_codes:\n",
    "        for alternative in name_list:\n",
    "            # get rid of quotes, which will break the query\n",
    "            alternative = alternative.replace('\"', '')\n",
    "            alternative = alternative.replace(\"'\", '')\n",
    "            alternatives += '\"' + alternative + '\"@' + language_code + '\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    \n",
    "    wdqs = Sparqler(useragent=user_agent)\n",
    "    statements = wdqs.query(query)\n",
    "    sleep(settings['sparql_sleep'])\n",
    "\n",
    "    results = []\n",
    "    for statement in statements:\n",
    "        wikidata_iri = statement['item']['value']\n",
    "        if 'label' in statement:\n",
    "            name = statement['label']['value']\n",
    "        else:\n",
    "            name = ''\n",
    "        qnumber = extract_local_name(wikidata_iri)\n",
    "        results.append({'qid': qnumber, 'name': name})\n",
    "    return results\n",
    "\n",
    "# returns lists of occupations, employers, and affiliations for a person with Wikidata ID qid\n",
    "def search_wikidata_occ_emp_aff(qid, default_language, user_agent):\n",
    "    \"\"\"Search Wikidata for occupation, employer, and affiliation claims. Return a \n",
    "    (occupation, employer, affiliation) tuple of lists.\"\"\"\n",
    "    results_list = []\n",
    "\n",
    "    query_string = '''select distinct ?occupation ?employer ?affiliation where {\n",
    "        optional {\n",
    "            wd:'''+ qid + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = \"'''+ default_language + '''\")            \n",
    "            }\n",
    "\n",
    "        optional {\n",
    "            wd:'''+ qid + ''' wdt:P108 ?employerId.\n",
    "            ?employerId rdfs:label ?employer.\n",
    "            FILTER(lang(?employer) = \"'''+ default_language + '''\")            \n",
    "            }\n",
    "\n",
    "        optional {\n",
    "            wd:'''+ qid + ''' wdt:P1416 ?affiliationId.\n",
    "            ?affiliationId rdfs:label ?affiliation.\n",
    "            FILTER(lang(?affiliation) = \"'''+ default_language + '''\")            \n",
    "            }\n",
    "        }'''\n",
    "    #print(query_string)\n",
    "    \n",
    "    wdqs = Sparqler(useragent=user_agent)\n",
    "    statements = wdqs.query(query_string)\n",
    "    sleep(settings['sparql_sleep'])\n",
    "    #print(statements)\n",
    "    \n",
    "    # pull all possible occupations\n",
    "    occupationList = []\n",
    "    employerList = []\n",
    "    affiliationList = []\n",
    "    for statement in statements:\n",
    "        if 'occupation' in statement:\n",
    "            occupationList.append(statement['occupation']['value'])\n",
    "        if 'employer' in statement:\n",
    "            employerList.append(statement['employer']['value'])\n",
    "        if 'affiliation' in statement:\n",
    "            affiliationList.append(statement['affiliation']['value'])\n",
    "    occupationList = list(set(occupationList))\n",
    "    employerList = list(set(employerList))\n",
    "    affiliationList = list(set(affiliationList))\n",
    "    #print(occupationList)\n",
    "    #print(employerList)\n",
    "    #print(affiliationList)\n",
    "    \n",
    "    return occupationList, employerList, affiliationList \n",
    "\n",
    "\n",
    "def find_surname_givens(name):\n",
    "    \"\"\"Find the surname and given names (as a single string) from a name string. Remove typical male suffixes.\"\"\"\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', ' ')\n",
    "    name = name.replace(',', ' ')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Get rid of empty pieces formed from extra spaces\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "        \n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "def generate_name_alternatives(name):\n",
    "    \"\"\"Generate many permutations of names and initials, with and without periods, to be queried\n",
    "    against Wikidata labels and aliases.\"\"\"\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods, sometimes periods are close up with no spaces\n",
    "    name = name.replace('.', ' ')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "        \n",
    "    # Fix stupid situation where name is written in ALL CAPS\n",
    "    pieces = fix_all_caps(pieces)        \n",
    "\n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        # make sure first character is alphabetic\n",
    "        # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "        # typical cases are like (Kit) or \"Kit\"\n",
    "        if not piece[0:1].isalpha():\n",
    "            piece = piece[1:len(piece)] # remove the first non-alphabetic character\n",
    "        if len(piece) > 0:\n",
    "            initials.append(piece[0:1])\n",
    "        \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += pieces[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # full name with suffix\n",
    "    if suffix != '':\n",
    "        name_version = ''\n",
    "        for piece_number in range(0, len(pieces)-1):\n",
    "            name_version += pieces[piece_number] + ' '\n",
    "        name_version += pieces[len(pieces)-1] + suffix\n",
    "        alternatives.append(name_version)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    name_version = pieces[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    name_version = pieces[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first and last name only\n",
    "    name_version = pieces[0] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial and last name only\n",
    "    name_version = initials[0] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    name_version = initials[0] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial no period and all other names\n",
    "    name_version = initials[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += pieces[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial with period and all other names\n",
    "    name_version = initials[0] + '. '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += pieces[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials with last name\n",
    "    name_version = initials[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += initials[piece_number]\n",
    "    name_version += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def screen_qids(qids, screens, default_language, user_agent):\n",
    "    \"\"\"Screen Q IDs based on criteria saved in a configuration file (probably called screens.yaml . \n",
    "    Return a list of dicts containing Q ID, label, and description.\"\"\"\n",
    "    qid_values =''\n",
    "    for qid in qids:\n",
    "        qid_values += 'wd:' + qid + '\\n'\n",
    "\n",
    "    graph_pattern = ''\n",
    "    first_screen = True\n",
    "    for screen in screens:\n",
    "        # Each requirement in a screen has an AND relationship (all must be satisfied)\n",
    "        subgraph_pattern = ''\n",
    "        for requirement in screen:\n",
    "\n",
    "            # Set the value if required or use a dummy variable if any value is allowed\n",
    "            if requirement['entity'] is None:\n",
    "                value = '?var' + requirement['property'] # add the property string to the variable to guarantee uniqueness\n",
    "            elif re.match(r'Q\\d+', requirement['entity']): # regex to match Q IDs\n",
    "                value = 'wd:' + requirement['entity']\n",
    "            else: # if not nothing or a Q ID, assume it's a string literal\n",
    "                if requirement['lang'] is None:\n",
    "                    value = '\"' + requirement['entity'] + '\"'\n",
    "                else:\n",
    "                    value = '\"' + requirement['entity'] + '\"@' + requirement['lang']\n",
    "\n",
    "            # Set the property (label, description, or P value)\n",
    "            if requirement['property'] == 'label':\n",
    "                property = 'rdfs:label'\n",
    "            elif requirement['property'] == 'description':\n",
    "                property = 'schema:description'\n",
    "            else:\n",
    "                property = 'wdt:' + requirement['property']\n",
    "\n",
    "            # Place the value in either the subject or object position in the triple\n",
    "            if requirement['position'] == 'object':\n",
    "                triple_pattern = '?qid ' + property + ' ' + value + '.'\n",
    "            else:\n",
    "                triple_pattern = value + ' ' + property + ' ?qid.'\n",
    "\n",
    "            # Add filters if needed\n",
    "            if requirement['filter_type'] == '<' or requirement['filter_type'] == '>': \n",
    "                # note: string comparison only e.g. for datetimes, needs modification for actual numbers\n",
    "                triple_pattern += '\\nFILTER (STR(?var' + requirement['property'] + ') ' + requirement['filter_type'] + ' \"' + requirement['filter_string'] + '\")'\n",
    "\n",
    "            if requirement['filter_type'] == 'in': \n",
    "                # note: string comparison only\n",
    "                triple_pattern += '\\nFILTER (CONTAINS(?var' + requirement['property'] + ', \"' + requirement['filter_string'] + '\"))'\n",
    "\n",
    "            # Use MINUS if you want to exclude items that fit the pattern.\n",
    "            if requirement['require'] == 'exclude':\n",
    "                triple_pattern = 'minus {' + triple_pattern + '}'\n",
    "\n",
    "            triple_pattern += '\\n'\n",
    "            #print(triple_pattern)\n",
    "            subgraph_pattern += triple_pattern\n",
    "\n",
    "        # Now attach the subgraph pattern to any previous subgraph patterns using UNION to great an OR relationship\n",
    "        subgraph_pattern = '{\\n' + subgraph_pattern + '}\\n' # create a subgraph pattern so that several can be UNIONed\n",
    "        if first_screen: # The first subgraph pattern doesn't need the UNION inserted\n",
    "            first_screen = False\n",
    "        else:\n",
    "            graph_pattern = graph_pattern + 'UNION\\n'\n",
    "        graph_pattern += subgraph_pattern \n",
    "\n",
    "    query_string = '''\n",
    "    select distinct ?qid ?label ?description where {\n",
    "      VALUES ?qid\n",
    "      {\n",
    "      ''' + qid_values + '''}\n",
    "    ''' + graph_pattern + '''\n",
    "    \n",
    "    ?qid rdfs:label ?label.\n",
    "    FILTER(lang(?label)=\"'''+ default_language + '''\")\n",
    "    \n",
    "    OPTIONAL {\n",
    "    ?qid schema:description ?description.\n",
    "    FILTER(lang(?description)=\"'''+ default_language + '''\")\n",
    "    }\n",
    "      }\n",
    "    '''\n",
    "    #print(query_string)\n",
    "    \n",
    "    wdqs = Sparqler(useragent=user_agent)\n",
    "    results = wdqs.query(query_string)\n",
    "    sleep(settings['sparql_sleep'])\n",
    "\n",
    "    return_list = []\n",
    "    for result in results:\n",
    "        out_dict = {\n",
    "            'qid': extract_local_name(result['qid']['value']),\n",
    "            'label': result['label']['value']\n",
    "            }\n",
    "        if 'description' in result:\n",
    "            out_dict['description'] = result['description']['value']\n",
    "        else:\n",
    "            out_dict['description'] = ''           \n",
    "        return_list.append(out_dict)\n",
    "    return return_list\n",
    "\n",
    "def work_in_wikidata_status(label, doi, pmid, existing_works_df, settings, verbose=False):\n",
    "    \"\"\"Search for a work in the list of pre-existing Wikidata items by DOI, PubMed ID, and label. \n",
    "    If a fuzzy match has a high score, accept as a match. For intermediate range scores, flag as\n",
    "    a case where one label is a subtitle of another.\"\"\"\n",
    "    if doi and doi.upper() in list(existing_works_df.loc[:, 'doi']):\n",
    "        if verbose:\n",
    "            print('DOI found in existing works')\n",
    "        return 'found DOI'\n",
    "    elif pmid and pmid in list(existing_works_df.loc[:, 'pmid']):\n",
    "        if verbose:\n",
    "            print('PubMed ID found in existing works')\n",
    "        return 'found PubMed ID'\n",
    "    else:\n",
    "        # NOTE: although calculating the fuzz.WRatio is labor intensive, these checks must be done\n",
    "        # sequentially, since we don't want the search for a nearly exact match to be stopped if a\n",
    "        # stupid partial match is found first.\n",
    "        for index, work in existing_works_df.iterrows():\n",
    "            w_ratio = fuzz.WRatio(work['label'], label)\n",
    "\n",
    "            # Test for nearly exact title match\n",
    "            if w_ratio > settings['existing_work_fuzzy_match_cutoff']:\n",
    "                if verbose:\n",
    "                    print('fuzzy label match: ' + str(w_ratio))\n",
    "                    print('test:', label)\n",
    "                    print('wikidata:', work['label'])\n",
    "                return 'fuzzy label match'\n",
    "                \n",
    "        for index, work in existing_works_df.iterrows():\n",
    "            w_ratio = fuzz.WRatio(work['label'], label)\n",
    "            # Test for meaningful subtitle match\n",
    "            if w_ratio > settings['existing_work_subtitle_fuzzy_match_cutoff']:\n",
    "                if verbose:\n",
    "                    print('Warning!!! Possible partial title: ' + str(w_ratio))\n",
    "                    print('test:', label)\n",
    "                    print('wikidata:', work['label'])\n",
    "                    logging.warning('Possible partial title: ' + str(w_ratio) + ' match, ' + extract_local_name(work['qid']) + ' ' + work['label'])\n",
    "                return 'possible partial title'\n",
    "        \n",
    "    if verbose:\n",
    "        print('Not found')\n",
    "    return 'not found'\n",
    "\n",
    "def find_containing_book(isbn, label, default_language, user_agent):\n",
    "    \"\"\"Search for a book by ISBN or label. Handles minimal error conditions.\"\"\"\n",
    "    book_error = False\n",
    "    found = False\n",
    "    \n",
    "    if isbn != '':\n",
    "        query_string = '''SELECT DISTINCT ?item\n",
    "    WHERE \n",
    "    {\n",
    "      BIND (\"''' + isbn + '''\" AS ?isbn)\n",
    "      {?item wdt:P212 ?isbn.} # ISBN-13\n",
    "      union\n",
    "      {?item wdt:P957 ?isbn.} # ISBN-10\n",
    "    }\n",
    "    '''\n",
    "        wdqs = Sparqler(useragent=user_agent)\n",
    "        results = wdqs.query(query_string)\n",
    "        sleep(settings['sparql_sleep'])\n",
    "\n",
    "        if len(results) > 1:\n",
    "            print('More than one book matches the ISBN')\n",
    "            logging.warning('More than one book matches the ISBN')\n",
    "            book_error = True\n",
    "            found = True\n",
    "            book_qid = ''\n",
    "        elif len(results) == 1:\n",
    "            found = True\n",
    "            book_qid = extract_local_name(results[0]['item']['value'])\n",
    "            \n",
    "    if not found:\n",
    "        query_string = '''SELECT DISTINCT ?item ?type WHERE {\n",
    "?item rdfs:label \"''' + label + '\"@' + default_language + '''.\n",
    "?item wdt:P31 ?type.\n",
    "}'''\n",
    "        wdqs = Sparqler(useragent=user_agent)\n",
    "        results = wdqs.query(query_string)\n",
    "        sleep(settings['sparql_sleep'])\n",
    "        if (results is None) or (len(results) == 0):\n",
    "            book_qid = ''\n",
    "        else:\n",
    "            type_list = [extract_local_name(result['type']['value']) for result in results]\n",
    "            qids_list = list(set([extract_local_name(result['item']['value']) for result in results]))\n",
    "            if len(qids_list) > 1:\n",
    "                book_error = True\n",
    "                print('Label for published_in matches multiple items:', qids_list)\n",
    "                logging.warning('Label for published_in matches multiple items: ' + str(qids_list))\n",
    "                book_qid = ''\n",
    "            else:\n",
    "                result_qid = qids_list[0]\n",
    "                if 'Q1711593' in type_list: # edited volume\n",
    "                    book_qid = result_qid\n",
    "                else:\n",
    "                    book_error = True\n",
    "                    print('Possible published_in', result_qid, 'not edited volume but has types', type_list)\n",
    "                    logging.warning('Possible published_in ' + result_qid + ' not edited volume but has types ' + str(type_list))\n",
    "                    book_qid = ''\n",
    "    return book_error, book_qid\n",
    "\n",
    "# ------------------------\n",
    "# SPARQL query class\n",
    "# ------------------------\n",
    "\n",
    "# This is a condensed version of the more full-featured script at \n",
    "# https://github.com/HeardLibrary/digital-scholarship/blob/master/code/wikidata/sparqler.py\n",
    "# It includes only the method for the query form.\n",
    "\n",
    "class Sparqler:\n",
    "    \"\"\"Build SPARQL queries of various sorts\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    useragent : str\n",
    "        Required if using the Wikidata Query Service, otherwise optional.\n",
    "        Use the form: appname/v.v (URL; mailto:email@domain.com)\n",
    "        See https://meta.wikimedia.org/wiki/User-Agent_policy\n",
    "    endpoint: URL\n",
    "        Defaults to Wikidata Query Service if not provided.\n",
    "    method: str\n",
    "        Possible values are \"post\" (default) or \"get\". Use \"get\" if read-only query endpoint.\n",
    "        Must be \"post\" for update endpoint.\n",
    "    sleep: float\n",
    "        Number of seconds to wait between queries. Defaults to 0.1\n",
    "        \n",
    "    Required modules:\n",
    "    -------------\n",
    "    requests, datetime, time\n",
    "    \"\"\"\n",
    "    def __init__(self, method='post', endpoint='https://query.wikidata.org/sparql', useragent=None, sleep=0.1):\n",
    "        # attributes for all methods\n",
    "        self.http_method = method\n",
    "        self.endpoint = endpoint\n",
    "        if useragent is None:\n",
    "            if self.endpoint == 'https://query.wikidata.org/sparql':\n",
    "                print('You must provide a value for the useragent argument when using the Wikidata Query Service.')\n",
    "                print()\n",
    "                raise KeyboardInterrupt # Use keyboard interrupt instead of sys.exit() because it works in Jupyter notebooks\n",
    "        self.sleep = sleep\n",
    "\n",
    "        self.requestheader = {}\n",
    "        if useragent:\n",
    "            self.requestheader['User-Agent'] = useragent\n",
    "        \n",
    "        if self.http_method == 'post':\n",
    "            self.requestheader['Content-Type'] = 'application/x-www-form-urlencoded'\n",
    "\n",
    "    def query(self, query_string, form='select', verbose=False, **kwargs):\n",
    "        \"\"\"Send a SPARQL query to the endpoint.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        form : str\n",
    "            The SPARQL query form.\n",
    "            Possible values are: \"select\" (default), \"ask\", \"construct\", and \"describe\".\n",
    "        mediatype: str\n",
    "            The response media type (MIME type) of the query results.\n",
    "            Some possible values for \"select\" and \"ask\" are: \"application/sparql-results+json\" (default) and \"application/sparql-results+xml\".\n",
    "            Some possible values for \"construct\" and \"describe\" are: \"text/turtle\" (default) and \"application/rdf+xml\".\n",
    "            See https://docs.aws.amazon.com/neptune/latest/userguide/sparql-media-type-support.html#sparql-serialization-formats-neptune-output\n",
    "            for response serializations supported by Neptune.\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        default: list of str\n",
    "            The graphs to be merged to form the default graph. List items must be URIs in string form.\n",
    "            If omitted, no graphs will be specified and default graph composition will be controlled by FROM clauses\n",
    "            in the query itself. \n",
    "            See https://www.w3.org/TR/sparql11-query/#namedGraphs and https://www.w3.org/TR/sparql11-protocol/#dataset\n",
    "            for details.\n",
    "        named: list of str\n",
    "            Graphs that may be specified by IRI in a query. List items must be URIs in string form.\n",
    "            If omitted, named graphs will be specified by FROM NAMED clauses in the query itself.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        If the form is \"select\" and mediatype is \"application/json\", a list of dictionaries containing the data.\n",
    "        If the form is \"ask\" and mediatype is \"application/json\", a boolean is returned.\n",
    "        If the mediatype is \"application/json\" and an error occurs, None is returned.\n",
    "        For other forms and mediatypes, the raw output is returned.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        To get UTF-8 text in the SPARQL queries to work properly, send URL-encoded text rather than raw text.\n",
    "        That is done automatically by the requests module for GET. I guess it also does it for POST when the\n",
    "        data are sent as a dict with the urlencoded header. \n",
    "        See SPARQL 1.1 protocol notes at https://www.w3.org/TR/sparql11-protocol/#query-operation        \n",
    "        \"\"\"\n",
    "        query_form = form\n",
    "        if 'mediatype' in kwargs:\n",
    "            media_type = kwargs['mediatype']\n",
    "        else:\n",
    "            if query_form == 'construct' or query_form == 'describe':\n",
    "            #if query_form == 'construct':\n",
    "                media_type = 'text/turtle'\n",
    "            else:\n",
    "                media_type = 'application/sparql-results+json' # default for SELECT and ASK query forms\n",
    "        self.requestheader['Accept'] = media_type\n",
    "            \n",
    "        # Build the payload dictionary (query and graph data) to be sent to the endpoint\n",
    "        payload = {'query' : query_string}\n",
    "        if 'default' in kwargs:\n",
    "            payload['default-graph-uri'] = kwargs['default']\n",
    "        \n",
    "        if 'named' in kwargs:\n",
    "            payload['named-graph-uri'] = kwargs['named']\n",
    "\n",
    "        if verbose:\n",
    "            print('querying SPARQL endpoint')\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        if self.http_method == 'post':\n",
    "            response = requests.post(self.endpoint, data=payload, headers=self.requestheader)\n",
    "        else:\n",
    "            response = requests.get(self.endpoint, params=payload, headers=self.requestheader)\n",
    "        #print('from cache:', response.from_cache) # uncomment if you want to see if cached data are used\n",
    "        elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "        self.response = response.text\n",
    "        sleep(self.sleep) # Throttle as a courtesy to avoid hitting the endpoint too fast.\n",
    "\n",
    "        if verbose:\n",
    "            print('done retrieving data in', int(elapsed_time), 's')\n",
    "\n",
    "        if query_form == 'construct' or query_form == 'describe':\n",
    "            return response.text\n",
    "        else:\n",
    "            if media_type != 'application/sparql-results+json':\n",
    "                return response.text\n",
    "            else:\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                except:\n",
    "                    return None # Returns no value if an error. \n",
    "\n",
    "                if query_form == 'select':\n",
    "                    # Extract the values from the response JSON\n",
    "                    results = data['results']['bindings']\n",
    "                else:\n",
    "                    results = data['boolean'] # True or False result from ASK query \n",
    "                return results           \n",
    "\n",
    "# ------------------------\n",
    "# mapping functions\n",
    "# ------------------------\n",
    "\n",
    "def identity(value, settings):\n",
    "    \"\"\"Return the value argument with any leading and trailing whitespace removed.\"\"\"\n",
    "    return value.strip()\n",
    "\n",
    "def set_instance_of(string, settings):\n",
    "    \"\"\"Match the type string with possible types for the data source and return the type Q ID.\"\"\"\n",
    "    global error_log_string\n",
    "    if string == '':\n",
    "        return ''\n",
    "\n",
    "    for work_type in settings['work_types']:\n",
    "        if string == work_type['type_string']:\n",
    "            return work_type['qid']\n",
    "\n",
    "    print('Did not find datatype for type:', string)\n",
    "    error_log_string += 'Did not find datatype for type:' + string + '\\n'\n",
    "    return ''\n",
    "\n",
    "def detect_language(string, settings):\n",
    "    \"\"\"Detect the language of the label and return the Wikidata Q ID for it.\"\"\"\n",
    "    global error_log_string\n",
    "    if string == '':\n",
    "        return ''\n",
    "    try:\n",
    "        lang_list = detect_langs(string)\n",
    "        lang_string = str(lang_list[0])\n",
    "        confidence = float(lang_string[3:])\n",
    "        lang = lang_string[:2]\n",
    "    except: #exceptions occur when no info to decide, e.g. numbers\n",
    "        lang = 'zxx'\n",
    "        confidence = float(0)\n",
    "    if confidence < settings['language_precision_cutoff']:\n",
    "        print('Warning: language confidence for', lang, 'below', settings['language_precision_cutoff'], ':', confidence)\n",
    "        error_log_string += 'Warning: language confidence for ' + lang + ' below ' + str(settings['language_precision_cutoff']) + ': ' + str(confidence) + '\\n'\n",
    "    if lang in settings['language_qid']:\n",
    "        return settings['language_qid'][lang]\n",
    "    else:\n",
    "        print('Warning: detected language', lang, 'not in list of known languages.')\n",
    "        error_log_string += 'Warning: detected language ' + lang + ' not in list of known languages.\\n'\n",
    "        return ''\n",
    "\n",
    "def title_en(string, settings):\n",
    "    \"\"\"Detect the language of the label and return the language code for it.\"\"\"\n",
    "    if string == '':\n",
    "        return ''\n",
    "    try:\n",
    "        lang_list = detect_langs(string)\n",
    "        lang_string = str(lang_list[0])\n",
    "        confidence = float(lang_string[3:])\n",
    "        lang = lang_string[:2]\n",
    "    except: #exceptions occur when no info to decide, e.g. numbers\n",
    "        lang = 'zxx'\n",
    "        confidence = float(0)\n",
    "    if lang == 'en':\n",
    "        return string\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# The following function is needed by the calculate_pages function\n",
    "def roman_to_decimal(numeral):\n",
    "    \"\"\"Convert Roman numerals to integers.\n",
    "    \n",
    "    Note:\n",
    "    -----\n",
    "    Code from https://www.geeksforgeeks.org/python-program-for-converting-roman-numerals-to-decimal-lying-between-1-to-3999/\"\"\"\n",
    "\n",
    "    def roman_integer_value(r):\n",
    "        \"\"\"Return value of Roman numeral symbol.\n",
    "\n",
    "        Note:\n",
    "        -----\n",
    "        Code from https://www.geeksforgeeks.org/python-program-for-converting-roman-numerals-to-decimal-lying-between-1-to-3999/\"\"\"    \n",
    "        if (r == 'I'):\n",
    "            return 1\n",
    "        if (r == 'V'):\n",
    "            return 5\n",
    "        if (r == 'X'):\n",
    "            return 10\n",
    "        if (r == 'L'):\n",
    "            return 50\n",
    "        if (r == 'C'):\n",
    "            return 100\n",
    "        if (r == 'D'):\n",
    "            return 500\n",
    "        if (r == 'M'):\n",
    "            return 1000\n",
    "        return -1\n",
    "\n",
    "    str = numeral.upper()\n",
    "    res = 0\n",
    "    i = 0\n",
    "\n",
    "    while (i < len(str)):\n",
    "\n",
    "        # Getting value of symbol s[i]\n",
    "        s1 = roman_integer_value(str[i])\n",
    "        \n",
    "        # Return a negative number if error.\n",
    "        if s1 < 0:\n",
    "            return -1\n",
    "\n",
    "        if (i + 1 < len(str)):\n",
    "\n",
    "            # Getting value of symbol s[i + 1]\n",
    "            s2 = roman_integer_value(str[i + 1])\n",
    "            \n",
    "            # Return a negative number if error.\n",
    "            if s2 < 0:\n",
    "                return -1\n",
    "\n",
    "            # Comparing both values\n",
    "            if (s1 >= s2):\n",
    "\n",
    "                # Value of current symbol is greater\n",
    "                # or equal to the next symbol\n",
    "                res = res + s1\n",
    "                i = i + 1\n",
    "            else:\n",
    "\n",
    "                # Value of current symbol is greater\n",
    "                # or equal to the next symbol\n",
    "                res = res + s2 - s1\n",
    "                i = i + 2\n",
    "        else:\n",
    "            res = res + s1\n",
    "            i = i + 1\n",
    "\n",
    "    return res\n",
    "\n",
    "def calculate_pages(range, settings):\n",
    "    \"\"\"Calculate the number of pages from the page range.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    Supports properly formatted Roman numerals and doesn't care about whitespace.\"\"\"\n",
    "    if range == '':\n",
    "        return ''\n",
    "    numbers = range.split('-')\n",
    "    \n",
    "    # If there is only a single number or an empty cell, return the empty string.\n",
    "    if len(numbers) < 2:\n",
    "        return ''\n",
    "    # Edge case where it isn't a well-formed range and has multiple hyphens\n",
    "    if len(numbers) > 2:\n",
    "        return ''\n",
    "    \n",
    "    # Step through the two numbers to try to convert them from Roman numerals if not integers.\n",
    "    for index, number in enumerate(numbers):\n",
    "        number = number.strip()\n",
    "        if not number.isnumeric():\n",
    "            numbers[index] = roman_to_decimal(number)\n",
    "            \n",
    "            # Will return -1 error if it contains characters not valid for Roman numerals \n",
    "            if numbers[index] < 0:\n",
    "                return ''\n",
    "    \n",
    "    number_pages = int(numbers[1]) - int(numbers[0]) + 1 # Need to add one since first page in range counts\n",
    "    if number_pages < 1:\n",
    "        return ''\n",
    "    return str(number_pages)\n",
    "\n",
    "def clean_doi(value, settings):\n",
    "    \"\"\"Turn DOI into uppercase and remove leading and trailing whitespace.\"\"\"\n",
    "    cleaned_value = value.upper().strip()\n",
    "    return cleaned_value\n",
    "\n",
    "def extract_pmid_from_extra(extra_field, settings):\n",
    "    \"\"\"Extract the PubMed ID from the Extra field in the Zotero export.\"\"\"\n",
    "    identifier = ''\n",
    "    tokens = extra_field.split(' ')\n",
    "    for token_index in range(len(tokens)):\n",
    "        if tokens[token_index] == 'PMID:': # match the tag for PMID\n",
    "            # The identifer is the next token after the tag\n",
    "            identifier = tokens[token_index + 1]\n",
    "            break\n",
    "    return identifier\n",
    "\n",
    "def disambiguate_published_in(value, settings):\n",
    "    \"\"\"Use the value in the ISSN column to try to find the containing work.\n",
    "\n",
    "    Note:\n",
    "    -----\n",
    "    For journal articles, this performs a legitimate WQS search for the journal title using the ISSN.\n",
    "    For book chapters, the ISSN column may contain the Q ID of the containing book, inserted there during\n",
    "    a pre-processing step (a hack, but typically books would not have an ISSN and this column would be empty).\"\"\"\n",
    "\n",
    "    global error_log_string\n",
    "    if value == '':\n",
    "        return value\n",
    "    \n",
    "    # The value is a Q ID and was determined during a pre-processing step (i.e. for book chapters)\n",
    "    if value[0] == 'Q':\n",
    "        return value\n",
    "\n",
    "    # Look up the ISSN in Wikidata\n",
    "    # Build query string\n",
    "    query_string = '''select distinct ?container ?containerLabel where {\n",
    "      ?container wdt:P236 \"''' + value + '''\".\n",
    "      optional {\n",
    "      ?container rdfs:label ?containerLabel.\n",
    "      filter(lang(?containerLabel)=\"en\")\n",
    "      }\n",
    "    }'''\n",
    "    #print(query_string)\n",
    "\n",
    "    user_agent = 'PubLoader/' + settings['script_version'] + ' (mailto:' + settings['operator_email_address'] + ')'\n",
    "    wdqs = Sparqler(useragent=user_agent)\n",
    "    query_results = wdqs.query(query_string)\n",
    "    sleep(settings['sparql_sleep'])\n",
    "    \n",
    "    if len(query_results) == 0:\n",
    "        return ''\n",
    "\n",
    "    if len(query_results) > 1:\n",
    "        print('Warning! More than one container in Wikidata matched the ISSN ')\n",
    "        error_log_string += 'Warning! More than one container in Wikidata matched the ISSN\\n'\n",
    "        print(query_results, '\\n')\n",
    "        error_log_string += str(query_results) + '\\n'\n",
    "\n",
    "    # Extract Q ID from SPARQL query results. If there is more than one result, the last one will be used for the Q ID\n",
    "    for result in query_results:\n",
    "        container_qid = result['container']['value'].split('/')[-1] # extract the local name from the IRI\n",
    "\n",
    "    return container_qid\n",
    "\n",
    "def isbn10(string, settings):\n",
    "    \"\"\"Check whether the ISBN value has 10 characters or not.\"\"\"\n",
    "    test = string.replace('-', '')\n",
    "    if len(test) == 10:\n",
    "        return string\n",
    "    return ''\n",
    "\n",
    "def isbn13(string, settings):\n",
    "    \"\"\"Check whether the ISBN value has 13 characters or not.\"\"\"\n",
    "    test = string.replace('-', '')\n",
    "    if len(test) == 13:\n",
    "        return string\n",
    "    return ''\n",
    "\n",
    "def disambiguate_publisher(name_string, settings, publishers):\n",
    "    \"\"\"Look up the publisher Q ID from a list derived from a SPARQL query https://w.wiki/4pbi\"\"\"\n",
    "    # Set publisher Q ID to empty string if there's no publisher string\n",
    "    global error_log_string\n",
    "    if name_string == '':\n",
    "        return ''\n",
    "    \n",
    "    best_match_score = 0\n",
    "    best_match = ''\n",
    "    best_match_label = ''\n",
    "    for qid, publisher in publishers.iterrows():\n",
    "        w_ratio = fuzz.WRatio(name_string, publisher['label'])\n",
    "        if w_ratio > best_match_score:\n",
    "            best_match = qid\n",
    "            best_match_label = publisher['label']\n",
    "            best_match_score = w_ratio\n",
    "            \n",
    "    if best_match_score < 98:\n",
    "        print('w_ratio:', best_match_score, 'Warning: poor match of: \"' + best_match_label + '\"', best_match, 'to stated publisher: \"' + name_string + '\"\\n')\n",
    "        error_log_string += 'w_ratio: ' + str(best_match_score) + ' Warning: poor match of: \"' + best_match_label + '\" ' + best_match + ' to stated publisher: \"' + name_string + '\"\\n'\n",
    "        \n",
    "    return best_match\n",
    "\n",
    "def disambiguate_place_of_publication(value, settings, publisher_locations):\n",
    "    \"\"\"Look up place of publication Q ID from a list derived from query https://w.wiki/63Ap\n",
    "    If there is a single match, the Q ID is returned.\n",
    "    If there are no matches, the string is returned unprocessed.\n",
    "    If there are multiple matches, a dict with possible values is returned.\"\"\"\n",
    "    global error_log_string\n",
    "    if value == '':\n",
    "        return ''\n",
    "    \n",
    "    if 'New York' in value:\n",
    "        return 'Q60'\n",
    "    \n",
    "    if 'New Brunswick' in value:\n",
    "        return 'Q138338'\n",
    "    \n",
    "    if 'California' in value:\n",
    "        value = value.replace('California', 'CA')\n",
    "    \n",
    "    if 'Calif' in value:\n",
    "        value = value.replace('Calif', 'CA')\n",
    "        \n",
    "    if 'Massachusetts' in value:\n",
    "        value = value.replace('Massachusetts', 'MA')\n",
    "        \n",
    "    if 'Cambridge' in value:\n",
    "        if 'Cambridge, M' in value:\n",
    "            return 'Q49111'\n",
    "        else:\n",
    "            return 'Q350'\n",
    "    \n",
    "    location_list = []\n",
    "    for qid, location in publisher_locations.iterrows():\n",
    "        if location['label'] in value:\n",
    "            location_list.append({'qid': qid, 'label': location['label']})\n",
    "    if len(location_list) == 0:\n",
    "        error_log_string += value + ' not found in place list.\\n'\n",
    "        return value\n",
    "    \n",
    "    elif len(location_list) == 1:\n",
    "        return location_list[0]['qid']\n",
    "    else:\n",
    "        error_log_string += 'Multiple matches found in place list.' + str(location_list) + '\\n'\n",
    "        return location_list\n",
    "\n",
    "def today(settings):\n",
    "    \"\"\"Generate the current UTC xsd:date\"\"\"\n",
    "    whole_time_string_z = datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "def set_reference(input_url, settings, full_works):\n",
    "    \"\"\"Screen any URL that is present in the field for suitability as the reference URL value.\"\"\"\n",
    "    url = include_reference_url(input_url, full_works) # Screen for suitable URLs\n",
    "    if url != '':\n",
    "        return url\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def set_stated_in(input_url, settings, full_works):\n",
    "    \"\"\"If no URL is present, set a fixed value to be used as the stated_in value.\"\"\"\n",
    "    url = include_reference_url(input_url, full_works) # Screen for suitable URLs\n",
    "    if url == '':\n",
    "        return 'Q114403967' # Vanderbilt Divinity publications database\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# ---------------------------\n",
    "# Major processes functions\n",
    "# ---------------------------\n",
    "\n",
    "def build_function(function_name_string, passed_value_string, settings, data_structure):\n",
    "    \"\"\"Construct a function with or without arguments from text string data in the configuration file.\n",
    "    settings is a list of string arguments from configuration values to be passed following the value.\"\"\"\n",
    "    if data_structure is not None:\n",
    "        data_structure_string = ', data_structure'\n",
    "    else:\n",
    "        data_structure_string = ''\n",
    "\n",
    "    if len(passed_value_string) == 0:\n",
    "        expression = 'mapping_functions.' + function_name_string + \"('', settings\" + data_structure_string + ')'\n",
    "    else:        \n",
    "        # Hack for cases where the data string is enclosed in single quotes\n",
    "        if passed_value_string[0] == \"'\" or passed_value_string[-1] == \"'\":\n",
    "            expression = 'mapping_functions.' + function_name_string + '(\"\"\"' + passed_value_string + '\"\"\", settings' + data_structure_string + ')'\n",
    "        else:\n",
    "            expression = 'mapping_functions.' + function_name_string + \"('''\" + passed_value_string + \"''', settings\" + data_structure_string + ')'\n",
    "    output_value = eval(expression)\n",
    "    return output_value\n",
    "\n",
    "def evaluate_function(prop, work_data, settings, data_structure):\n",
    "    \"\"\"Evaluate a mapping function based on data from the source spreadsheet after building the function\n",
    "    from the configuration string data.\"\"\"\n",
    "    # The mapping function may not require an argument. In that case, there's no source column.\n",
    "    if 'in_col_label' in prop:\n",
    "        # If the source data CSV doesn't have any column named according to mappings, the output for that\n",
    "        # variable is an empty string.\n",
    "        if prop['in_col_label'] in work_data:\n",
    "            output_value = build_function(prop['mapping_function'], work_data[prop['in_col_label']], settings, data_structure)\n",
    "            if output_value == '':\n",
    "                no_value = True\n",
    "            else:\n",
    "                no_value = False\n",
    "        else:\n",
    "            output_value = ''\n",
    "            no_value = True\n",
    "    # Case where there's no argument passed to mapping function\n",
    "    else:\n",
    "        expression = 'mapping_functions.' + prop['mapping_function'] + '(settings)'\n",
    "        output_value = eval(expression)\n",
    "        if output_value == '':\n",
    "            no_value = True\n",
    "        else:\n",
    "            no_value = False\n",
    "    return no_value, output_value\n",
    "\n",
    "def extract_metadata(mapping, work_data, settings):\n",
    "    \"\"\"Steps through fields described in the config file, maps them to columns in the source data, and\n",
    "    uses processing functions to transform the input data to forms required in the output table.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mapping : complex structure\n",
    "        Maps column headers (\"out_col_label\") in the destination table to column headers (\"in_col_label\") in the source table.\n",
    "        The \"mapping_function\" key indicates the function used to determine the value to be used in the destination table.\n",
    "    work_data : dict\n",
    "        A row of data from the source data table with column headers as the keys.\n",
    "    \"\"\"\n",
    "    out_dict = {'qid': '', 'unique_identifier': work_data[mapping['constants']['unique_identifier_column']]}\n",
    "    out_dict['label_' + settings['default_language']] = work_data[mapping['constants']['label_column']]\n",
    "    out_dict['description_' + settings['default_language']] = set_description(work_data[mapping['constants']['description_code_column']], settings['work_types'])\n",
    "\n",
    "    for out_property in config['outfiles'][0]['prop_list']:\n",
    "        \n",
    "        # Find the mapping variable that matches the config property\n",
    "        for prop in mapping['properties']:\n",
    "            if prop['out_col_label'] == out_property['variable']:\n",
    "                break\n",
    "    \n",
    "        out_field = out_property['variable']\n",
    "        out_dict[out_field + '_uuid'] = ''\n",
    "        \n",
    "        # If a function requires some data structure for input, its mapping must include a structure_name_string\n",
    "        # object whose value is a string that is the name of the data structure object needed by the function.\n",
    "        # In the function, that object is the fourth argument. Functions not needing additional data will have\n",
    "        # only three arguments and None will be passed into the constructor functions as the data_structure argument.\n",
    "        # NOTE: the data structure must be a global variable and therefore be defined in the main script.\n",
    "        if 'structure_name_string' in prop:\n",
    "            data_structure = eval(prop['structure_name_string'])\n",
    "        else:\n",
    "            data_structure = None\n",
    "\n",
    "        no_value, output_value = evaluate_function(prop, work_data, settings, data_structure)\n",
    "\n",
    "        # Populate the values-related columns\n",
    "        if out_property['value_type'] == 'date':\n",
    "            out_dict[out_field + '_nodeId'] = ''\n",
    "            out_dict[out_field + '_val'] = output_value\n",
    "            out_dict[out_field + '_prec'] = ''\n",
    "\n",
    "        elif out_property['value_type'] == 'quantity':\n",
    "            out_dict[out_field + '_nodeId'] = ''\n",
    "            out_dict[out_field + '_val'] = output_value\n",
    "            if no_value:\n",
    "                out_dict[out_field + '_unit'] = ''\n",
    "            else:\n",
    "                out_dict[out_field + '_unit'] = prop['quantity_unit']\n",
    "\n",
    "        # This is not actually implemented and will generate an error if used\n",
    "        elif out_property['value_type'] == 'globecoordinate':\n",
    "            out_dict[out_field + '_nodeId'] = ''\n",
    "            out_dict[out_field + '_val'] = output_value\n",
    "            if no_value:\n",
    "                out_dict[out_field + '_long'] = ''\n",
    "                out_dict[out_field + '_prec'] = ''\n",
    "            else:\n",
    "                out_dict[out_field + '_long'] = work_data[out_field + '_long']\n",
    "                out_dict[out_field + '_prec'] = work_data[out_field + '_prec']\n",
    "\n",
    "        else:\n",
    "            out_dict[out_field] = output_value\n",
    "\n",
    "        # Populate the qualifier columns\n",
    "        for qualifier in out_property['qual']:\n",
    "            if no_value:\n",
    "                no_qual_value = True\n",
    "            else:\n",
    "                # Find the mapping variable that matches the config property\n",
    "                for qual in prop['qual']:\n",
    "                    if qual['out_col_label'] == qualifier['variable']:\n",
    "                        break\n",
    "\n",
    "                if 'structure_name_string' in qual:\n",
    "                    data_structure = eval(qual['structure_name_string'])\n",
    "                    data_structure_string = ', data_structure'\n",
    "                else:\n",
    "                    data_structure_string = ''\n",
    "                    data_structure = None\n",
    "                \n",
    "                # Skip reading a value from a source column if the function doesn't need input.\n",
    "                if 'in_col_label' in qual:\n",
    "                    expression = 'mapping_functions.' + qual['mapping_function'] + \"('''\" + work_data[qual['in_col_label']] + \"''', settings\" + data_structure_string + ')'\n",
    "                    qual_output_value = eval(expression)\n",
    "                    if qual_output_value == '':\n",
    "                        no_qual_value = True\n",
    "                    else:\n",
    "                        no_qual_value = False\n",
    "                else:\n",
    "                    no_qual_value = False\n",
    "                    expression = 'mapping_functions.' + qual['mapping_function'] + '(settings)'\n",
    "                    qual_output_value = eval(expression) # If this evalutes as empty string, result is same as no_ref_value = True\n",
    "\n",
    "\n",
    "            qual_field = out_field + '_' + qualifier['variable']\n",
    "            # To my knowledge, dates are the only complex types used as qualifiers (no quantities or globecoordinates).\n",
    "            if qualifier['value_type'] == 'date':\n",
    "                out_dict[qual_field + '_nodeId'] = ''\n",
    "                if no_qual_value:\n",
    "                    out_dict[qual_field + '_val'] = ''\n",
    "                else:\n",
    "                    out_dict[qual_field + '_val'] = qual_output_value\n",
    "                out_dict[qual_field + '_prec'] = ''\n",
    "            else:\n",
    "                if no_qual_value:\n",
    "                    out_dict[qual_field] = ''\n",
    "                else:\n",
    "                    out_dict[qual_field] = qual_output_value\n",
    "                \n",
    "        # Populate the reference columns\n",
    "        # There's only a hash ID column if there's at least one reference.\n",
    "        if len(out_property['ref']) > 0:\n",
    "            out_dict[out_field + '_ref1_hash'] = ''\n",
    "            \n",
    "        for reference in out_property['ref']:\n",
    "            if no_value:\n",
    "                no_ref_value = True\n",
    "            else:\n",
    "                # Find the mapping variable that matches the config property\n",
    "                for ref in prop['ref']:\n",
    "                    if ref['out_col_label'] == reference['variable']:\n",
    "                        break\n",
    "\n",
    "                # Some functions like today() don't need input from the source table, and therefore \n",
    "                # skip reading a value from a source column.\n",
    "                if 'structure_name_string' in ref:\n",
    "                    data_structure = eval(ref['structure_name_string'])\n",
    "                    data_structure_string = ', data_structure'\n",
    "                else:\n",
    "                    data_structure_string = ''\n",
    "                    data_structure = None\n",
    "                \n",
    "                if 'in_col_label' in ref:\n",
    "                    expression = 'mapping_functions.' + ref['mapping_function'] + \"('''\" + work_data[ref['in_col_label']] + \"''', settings\" + data_structure_string + ')'\n",
    "                    ref_output_value = eval(expression)\n",
    "                    if ref_output_value == '':\n",
    "                        no_ref_value = True\n",
    "                    else:\n",
    "                        no_ref_value = False\n",
    "                else:\n",
    "                    no_ref_value = False\n",
    "                    expression = 'mapping_functions.' + ref['mapping_function'] + '(settings)'\n",
    "                    ref_output_value = eval(expression) # If this evalutes as empty string, result is same as no_ref_value = True\n",
    "\n",
    "            ref_field = out_field + '_ref1_' + reference['variable']\n",
    "            # To my knowledge, dates are the only complex types used as qualifiers (no quantities or globecoordinates).\n",
    "            if reference['value_type'] == 'date':\n",
    "                out_dict[ref_field + '_nodeId'] = ''\n",
    "                if no_ref_value:\n",
    "                    out_dict[ref_field + '_val'] = ''\n",
    "                else:\n",
    "                    out_dict[ref_field + '_val'] = ref_output_value\n",
    "                out_dict[ref_field + '_prec'] = ''\n",
    "            else:\n",
    "                if no_ref_value:\n",
    "                    out_dict[ref_field] = ''\n",
    "                else:\n",
    "                    out_dict[ref_field] = ref_output_value\n",
    "                    \n",
    "    #print(out_dict)\n",
    "    return out_dict\n",
    "\n",
    "def disambiguate_agents(authors, pmid, coauthors, settings, user_agent):\n",
    "    \"\"\"Use a wide variety of data and tricks to come up with possible Wikidata matches for agent strings. \n",
    "    This includes fuzzy matching against department names and querying Wikidata labels and aliases with\n",
    "    many variations of the name string.\n",
    "    Returns data (Q IDs, series ordinal, stated as) that can be used for author/editor/translator statements \n",
    "    when a positive ID is made. When no positive ID is made, a list of possible matches is included for each\n",
    "    author string.\"\"\"\n",
    "\n",
    "    max_pmids_to_check = 10\n",
    "    # If there is a PubMed ID for the article, retrieve the author info\n",
    "    if pmid != '':\n",
    "        pubmed_author_info = retrieve_pubmed_data(pmid)\n",
    "        print('retrieved data from PubMed ID', pmid)\n",
    "        for author_index in range(len(pubmed_author_info)):\n",
    "            pubmed_author_info[author_index]['name'] = pubmed_author_info[author_index]['forename'] + ' ' + pubmed_author_info[author_index]['surname']\n",
    "    else:\n",
    "        print('no PubMed data')\n",
    "\n",
    "    # Augment CrossRef data with PubMed data. Typically the PubMed data is more likely to have the affiliations\n",
    "    # Names are generally very similar, but vary with added or missing periods on initials and suffixes\n",
    "    if pmid != '':\n",
    "        for author_index in range(len(authors)):\n",
    "            found = False\n",
    "            crossref_name = authors[author_index]['givenName'] + ' ' + authors[author_index]['familyName']\n",
    "            #print(crossref_name)\n",
    "            for pubmed_author in pubmed_author_info:\n",
    "                ratio = fuzz.ratio(pubmed_author['name'], crossref_name)\n",
    "                #print(ratio, pubmed_author['name'])\n",
    "                if ratio > 87: # had to drop down to this level because some people with missing \"Jr\" weren't matching\n",
    "                    found = True\n",
    "                    result_string = 'fuzzy label match: ' + str(ratio) + pubmed_author['name'] + ' / ' + crossref_name\n",
    "                    #print(result_string)\n",
    "                    break\n",
    "            if not found:\n",
    "                print('Did not find a match in the PubMed data for', crossref_name)\n",
    "            else:\n",
    "                #print(pubmed_author)\n",
    "                #print(authors[author_index])\n",
    "\n",
    "                # If there is a PubMed affiliation and no affiliation in the CrossRef data, add the PubMed affiliation\n",
    "                if pubmed_author['affiliation'] != '':\n",
    "                    if len(authors[author_index]['affiliation']) == 0:\n",
    "                        authors[author_index]['affiliation'].append(pubmed_author['affiliation'])\n",
    "\n",
    "                # If there is an ORCID in PubMed and no ORCID in the CrossRef data, add the ORCID to CrossRef data\n",
    "                # Not sure how often this happens since I think maybe usually of one has it, the other does, too.\n",
    "                if pubmed_author['orcid'] != '':\n",
    "                    if authors[author_index]['orcid'] == '':\n",
    "                        authors[author_index]['orcid'] = pubmed_author['orcid']\n",
    "\n",
    "                #print(authors[author_index])\n",
    "\n",
    "            #print()\n",
    "    #print(json.dumps(pubmed_author_info, indent=2))\n",
    "\n",
    "    # Perform screening operations on authors to try to determine their Q IDs\n",
    "    found_qid_values = []\n",
    "    not_found_author_list = []\n",
    "    author_count = 1\n",
    "    for author in authors:\n",
    "        print(author_count)\n",
    "        found = False\n",
    "        \n",
    "        # First eliminate the case where all of the name pieces are empty\n",
    "        if (author['givenName'] + ' ' + author['familyName']).strip() == '':\n",
    "            break\n",
    "            \n",
    "        # Record stated_as\n",
    "        stated_as = (author['givenName'] + ' ' + author['familyName']).strip()\n",
    "            \n",
    "        # Fix case where names are stupidly in all caps\n",
    "        name_pieces = author['givenName'].strip().split(' ')\n",
    "        author['givenName'] = ' '.join(fix_all_caps(name_pieces))\n",
    "        name_pieces = author['familyName'].strip().split(' ')\n",
    "        author['familyName'] = ' '.join(fix_all_caps(name_pieces))\n",
    "        \n",
    "        # Screen for exact match to Wikidata labels\n",
    "        for index, researcher in researchers.iterrows():\n",
    "            if researcher['label_en'] == author['givenName'] + ' ' + author['familyName']:\n",
    "                found = True\n",
    "                result_string = 'researcher exact label match: ' + researcher['qid'] + ' ' + researcher['label_en']\n",
    "                name = researcher['label_en']\n",
    "                qid = researcher['qid']\n",
    "                break\n",
    "        if not found:\n",
    "            # screen for exact match to alternate names\n",
    "            for index, altname in altnames.iterrows():\n",
    "                if altname['altLabel'] == author['givenName'] + ' ' + author['familyName']:\n",
    "                    found = True\n",
    "                    result_string = 'researcher altname match: ' + altname['qid'] + ' ' + altname['altLabel']\n",
    "                    name = altname['altLabel']\n",
    "                    qid = altname['qid']\n",
    "                    break\n",
    "            if not found:\n",
    "                # If the researcher has an ORCID, see if it's at Wikidata\n",
    "                if author['orcid'] != '':\n",
    "                    hit = searchWikidataForQIdByOrcid(author['orcid'])\n",
    "                    if hit != {}:\n",
    "                        found = True\n",
    "                        result_string = 'Wikidata ORCID search: ' + hit['qid'] + ' ' + hit['label'] + ' / ' + hit['description']\n",
    "                        name = hit['label']\n",
    "                        qid = hit['qid']\n",
    "\n",
    "                if not found:\n",
    "                    # screen for fuzzy match to Wikidata-derived labels\n",
    "                    for index, researcher in researchers.iterrows():\n",
    "                        # Require the surname to match the label surname exactly\n",
    "                        split_names = find_surname_givens(researcher['label_en']) # returns False if no family name\n",
    "                        if split_names: # skip names that don't have 2 parts !!! also misses non-English labels!\n",
    "                            if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                                w_ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], researcher['label_en'])\n",
    "                                #w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], researcher['label_en'])\n",
    "                                if w_ratio > 90:\n",
    "                                    found = True\n",
    "                                    result_string = 'fuzzy label match: ' + str(w_ratio) + ' ' + researcher['qid'] + ' ' + researcher['label_en'] + ' / ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                    name = researcher['label_en']\n",
    "                                    qid = researcher['qid']\n",
    "                                    break\n",
    "                    if not found:\n",
    "                        # screen for fuzzy match to alternate names\n",
    "                        for index, altname in altnames.iterrows():\n",
    "                            split_names = find_surname_givens(altname['altLabel'])\n",
    "                            if split_names: # skip names that don't have 2 parts\n",
    "                                if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                                    w_ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], altname['altLabel'])\n",
    "                                    #w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], altname['altLabel'])\n",
    "                                    if w_ratio > 90:\n",
    "                                        found = True\n",
    "                                        result_string = 'researcher altname fuzzy match: ' + str(w_ratio) + ' ' + altname['qid'] + ' ' + altname['altLabel'] + ' / ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                        name = altname['altLabel']\n",
    "                                        qid = altname['qid']\n",
    "                                        break\n",
    "                        if not found:\n",
    "                            name = author['givenName'] + ' ' + author['familyName']\n",
    "                            print('Searching Wikidata for', name)\n",
    "                            print('researcher known affiliations: ', author['affiliation'])\n",
    "                            print()\n",
    "                            hits = search_name_at_wikidata(name, user_agent)\n",
    "                            #print(hits)\n",
    "\n",
    "                            qids = []\n",
    "                            for hit in hits:\n",
    "                                qids.append(hit['qid'])\n",
    "                            return_list = screen_qids(qids, screens, settings['default_language'], user_agent) # screens is a global variable loaded at the start\n",
    "                            #print(return_list)\n",
    "\n",
    "                            for hit in return_list:\n",
    "                                # Check each possible name match to the list of known co-authors/co-editors\n",
    "                                # If there is a match, then use that Q ID and quit trying to match.\n",
    "                                if hit['qid'] in list(coauthors.index):\n",
    "                                    found = True\n",
    "                                    qid = hit['qid']\n",
    "                                    result_string = 'Match with known coauthor'\n",
    "                                    \n",
    "                            if not found:\n",
    "                                # Save discovered data to return if not matched\n",
    "                                discovered_data = []\n",
    "                                for hit in return_list:                                \n",
    "                                    hit_data = hit\n",
    "                                    split_names = find_surname_givens(hit['label'])\n",
    "\n",
    "                                    # Require the surname to match the Wikidata label surname exactly\n",
    "                                    # This prevents a high fraction of fuzzy matches where the last names are similar but not the same\n",
    "                                    if split_names: # skip names that don't have 2 parts\n",
    "                                        if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                                            #print(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print(hit)\n",
    "                                            w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('w_ratio:', w_ratio)\n",
    "                                            #ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('ratio:', ratio)\n",
    "                                            #partial_ratio = fuzz.partial_ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('partial_ratio:', partial_ratio)\n",
    "                                            #token_sort_ratio = fuzz.token_sort_ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('token_sort_ratio:', token_sort_ratio)\n",
    "                                            #token_set_ratio = fuzz.token_set_ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('token_set_ratio:', token_set_ratio)\n",
    "\n",
    "                                            # This screen requires a high degree of similarity between the overall ORCID names and Wikidata labels\n",
    "                                            if w_ratio > 80:\n",
    "                                                print('Wikidata search fuzzy match:', w_ratio, author['givenName'] + ' ' + author['familyName'], ' / ', 'https://www.wikidata.org/wiki/'+ hit['qid'], hit['label'])\n",
    "                                                print('Wikidata description: ', hit['description'])\n",
    "\n",
    "                                                # Here we need to check Wikidata employer and affiliation and fuzzy match against known affiliations\n",
    "                                                occupations, employers, affiliations = search_wikidata_occ_emp_aff(hit['qid'], settings['default_language'], user_agent)\n",
    "                                                print('occupations:', occupations)\n",
    "                                                hit_data['occupations'] = occupations\n",
    "                                                print('employers:', employers)\n",
    "                                                hit_data['employers'] = employers\n",
    "                                                print('affiliations', affiliations)\n",
    "                                                hit_data['affiliations'] = affiliations\n",
    "                                                print()\n",
    "\n",
    "                                                # Perform a check of the employer to make sure we didn't miss somebody in the earlier\n",
    "                                                # string matching\n",
    "                                                for employer in employers:\n",
    "                                                    if 'Vanderbilt University' in employer: # catch university and med center\n",
    "                                                        found = True\n",
    "                                                        result_string = 'Match Vanderbilt employer in Wikidata: ' + hit['qid'] + ' ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                                        qid = hit['qid']\n",
    "\n",
    "                                                # If the author doesn't have any known affiliations, there is no point in checking PubMed\n",
    "                                                if author['affiliation'] != []:\n",
    "                                                    # Search Wikidata for articles written by this match\n",
    "                                                    articles_in_wikidata = search_wikidata_article(hit['qid'])\n",
    "                                                    #print(articles_in_wikidata)\n",
    "\n",
    "                                                    # Step through articles with PubMed IDs found in Wikidata and see if the author affiliation or ORCID matches any of the articles\n",
    "                                                    check = 0\n",
    "                                                    for article_in_wikidata in articles_in_wikidata:\n",
    "                                                        if article_in_wikidata['pmid'] != '':\n",
    "                                                            check += 1\n",
    "                                                            if check > max_pmids_to_check:\n",
    "                                                                print('More articles, but stopping after checking', max_pmids_to_check)\n",
    "                                                                break # break out of article-checking loop\n",
    "                                                            print('Checking article, PMID:', article_in_wikidata['pmid'], article_in_wikidata['title'])\n",
    "                                                            pubmed_match = identified_in_pubmed(article_in_wikidata['pmid'], author['givenName'] + ' ' + author['familyName'], author['affiliation'], author['orcid'])\n",
    "                                                            if not pubmed_match:\n",
    "                                                                #print('no match')\n",
    "                                                                print()\n",
    "                                                            else:\n",
    "                                                                found = True\n",
    "                                                                result_string = 'PubMed affilation match: ' + hit['qid'] + ' ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                                                qid = hit['qid']\n",
    "                                                                break # break out of article-checking loop\n",
    "\n",
    "                                                if found:\n",
    "                                                    break # break out of hit list loop\n",
    "                                                print()\n",
    "                                                # If none of the matching criteria are met, save the data for future use\n",
    "                                                discovered_data.append(hit_data)\n",
    "\n",
    "        if not found:\n",
    "            not_found_author_list.append({'name_string': author['givenName'] + ' ' + author['familyName'], 'series_ordinal': author_count, 'possible_matches': discovered_data})\n",
    "            print('not found:', author['givenName'] + ' ' + author['familyName'])\n",
    "\n",
    "        else:\n",
    "            found_qid_values.append({'qid': qid, 'stated_as': stated_as, 'series_ordinal': author_count})\n",
    "            print(result_string)\n",
    "            for index, department in departments.iterrows():\n",
    "                if qid == department['qid']:\n",
    "                    for lindex, department_label in department_labels.iterrows():\n",
    "                        if department_label['qid'] == department['affiliation']:\n",
    "                            print(department_label['label_en'])\n",
    "                            break\n",
    "        print()\n",
    "        author_count += 1\n",
    "\n",
    "    print()\n",
    "    return found_qid_values, not_found_author_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Idiosyncratic steps that need to be done between the Zotero output and running the \"standardized\" script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step involves re-setting the Url column to use the screened URLs if the Zotero output title matches\n",
    "# the title in the screened full work CSV.\n",
    "\n",
    "# Set up log for warnings\n",
    "logging.basicConfig(filename='warnings.log', filemode='w', format='%(message)s', level=logging.WARNING)\n",
    "\n",
    "with open('settings.yaml', 'r') as file_object:\n",
    "    settings = yaml.safe_load(file_object)\n",
    "\n",
    "# The user_agent string identifies this application to Wikimedia APIs.\n",
    "# If you modify this script, you need to change the user-agent string to something else!\n",
    "user_agent = 'PubLoader/' + settings['script_version'] + ' (mailto:' + settings['operator_email_address'] + ')'\n",
    "    \n",
    "# These are the pre-screened \"full works available\" URLs that Charlotte prepared.\n",
    "full_works = pd.read_csv('full_work_div_pub.csv', na_filter=False, dtype = str)\n",
    "full_works = full_works.set_index('Title')\n",
    "\n",
    "source_data = settings['file_path'] + settings['source_data_filename']\n",
    "works = pd.read_csv(source_data, na_filter=False, dtype = str)\n",
    "works = works.iloc[1950:2000]\n",
    "\n",
    "for label, work_series in works.iterrows():\n",
    "    try:\n",
    "        # Find the row(s) in the full_works DataFrame that matches the series. There should be only one.\n",
    "        # Create a series of URL values for those rows. Since there should be only one, get the 0th value.\n",
    "        new_url = full_works.loc[full_works.index==work_series['Title'], 'Url'][0]\n",
    "        # Set a new value for the Url column in the works DataFrame using the looked-up URL.\n",
    "        works.loc[label, 'Url'] = new_url\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if work_series['Item Type'] == 'bookSection':\n",
    "        works.loc[label, 'parent_isbn'] = work_series['ISBN']\n",
    "        works.loc[label, 'ISBN'] = ''\n",
    "    else:\n",
    "        works.loc[label, 'parent_isbn'] = ''\n",
    "        \n",
    "    # Extract the DOI for book chapters from the Extra field \n",
    "    # and put it in the appropriate column.\n",
    "    if work_series['DOI'] == '':\n",
    "        works.loc[label, 'DOI'] = extract_identifier_from_extra(work_series['Extra'], 'DOI')\n",
    "\n",
    "works.to_csv(settings['file_path'] + 'preprocessed.csv', index = False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main routine\n",
    "\n",
    "NOTES: \n",
    "- Before continuing on after this step, you need to correct any of the publication locations that weren't determined.\n",
    "- It also would be a good idea to run Author Disambiguator on authors before retrieving the existing works, since if they are only listed under author name string, they won't get picked up and you might end up generating duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loading data from files')\n",
    "publishers = pd.read_csv(settings['file_path'] + 'publishers.csv', na_filter=False, dtype = str)\n",
    "publishers = publishers.set_index('qid')\n",
    "\n",
    "publisher_locations = pd.read_csv(settings['file_path'] + 'publisher_locations.csv', na_filter=False, dtype = str)\n",
    "publisher_locations = publisher_locations.set_index('qid')\n",
    "\n",
    "researchers = pd.read_csv(settings['reference_file_path'] + 'researchers.csv', na_filter=False, dtype = str)\n",
    "altnames = pd.read_csv(settings['reference_file_path'] + 'vanderbilt_wikidata_altlabels.csv', na_filter=False, dtype = str)\n",
    "departments = pd.read_csv(settings['reference_file_path'] + 'departments.csv', na_filter=False, dtype = str)\n",
    "department_labels = pd.read_csv(settings['reference_file_path'] + 'department_labels.csv', na_filter=False, dtype = str)\n",
    "\n",
    "works = pd.read_csv(settings['file_path'] + 'preprocessed.csv', na_filter=False, dtype = str)\n",
    "\n",
    "with open(settings['file_path'] + 'config.yaml', 'r') as file_object:\n",
    "    config = yaml.safe_load(file_object)\n",
    "\n",
    "with open(settings['file_path'] + 'mapping.yaml', 'r') as file_object:\n",
    "    mapping = yaml.safe_load(file_object)\n",
    "\n",
    "with open(settings['file_path'] + 'mapping_agents.yaml', 'r') as file_object:\n",
    "    mapping_agents = yaml.safe_load(file_object)\n",
    "    \n",
    "# screens.yaml is a configuration file that defines the kinds of screens to be performed on potential agent Q ID matches from Wikidata\n",
    "with open(settings['file_path'] + 'screens.yaml', 'r') as file_object:\n",
    "    screens = yaml.safe_load(file_object)\n",
    "\n",
    "print('retrieving existing works from Wikidata')\n",
    "query_string = '''select distinct ?work ?workLabel ?doi ?pmid where {\n",
    "  {?author wdt:P1416 wd:Q7914452.} # Div school\n",
    "  union\n",
    "  {?author wdt:P1416 wd:Q114065689.} # graduate department of religion\n",
    "\n",
    "  {?work wdt:P50 ?author.} # author\n",
    "  union\n",
    "  {?work wdt:P98 ?author.} # editor\n",
    "\n",
    "  optional {\n",
    "    ?work rdfs:label ?workLabel.\n",
    "    filter(lang(?workLabel)=\"''' + settings['default_language'] + '''\")\n",
    "    }\n",
    "\n",
    "  optional {?work wdt:P356 ?doi.}\n",
    "  optional {?work wdt:P698 ?pmid.}  \n",
    "  }\n",
    "'''\n",
    "\n",
    "wdqs = Sparqler(useragent=user_agent)\n",
    "query_results = wdqs.query(query_string)\n",
    "sleep(settings['sparql_sleep'])\n",
    "\n",
    "found_works = []\n",
    "for result in query_results:\n",
    "    work_dict = {}\n",
    "    #work_dict['qid'] = extract_local_name(result['work']['value'])\n",
    "    work_dict['qid'] = result['work']['value']\n",
    "    if 'workLabel' in result:\n",
    "        work_dict['label'] = result['workLabel']['value']\n",
    "    else:\n",
    "        work_dict['label'] = ''\n",
    "    if 'doi' in result:\n",
    "        work_dict['doi'] = result['doi']['value'].upper() # valid DOIs are all upper case, but could be some bad ones\n",
    "    else:\n",
    "        work_dict['doi'] = ''\n",
    "    if 'pmid' in result:\n",
    "        work_dict['pmid'] = result['pmid']['value']\n",
    "    else:\n",
    "        work_dict['pmid'] = ''\n",
    "    found_works.append(work_dict)\n",
    "existing_works_df = pd.DataFrame(found_works) # Note: qids are full IRIs\n",
    "existing_works_df = existing_works_df.sort_values(by=['qid'])\n",
    "existing_works_df.to_csv(settings['file_path'] + 'existing_works_in_wikidata.csv', index = False)\n",
    "\n",
    "print('retrieving author/editor data from Wikidata')\n",
    "existing_works_qids_list = list(existing_works_df.loc[:, 'qid']) # Generate a list of work Q IDs from the qid column\n",
    "existing_works_qids_string = '>\\n<'.join(existing_works_qids_list) # Join the list into a string with one Q ID per line\n",
    "existing_works_qids_string = '<' + existing_works_qids_string + '>'\n",
    "\n",
    "query_string = '''\n",
    "select distinct ?agent ?label ?orcid where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + existing_works_qids_string + '''}\n",
    "  \n",
    "{?value wdt:P50 ?agent.}\n",
    "union\n",
    "{?value wdt:P98 ?agent.}\n",
    "\n",
    "?agent rdfs:label ?label.\n",
    "FILTER(lang(?label)=\"''' + settings['default_language'] + '''\")\n",
    "\n",
    "optional {?agent wdt:P496 ?orcid.}\n",
    "\n",
    "MINUS # remove Vanderbilt Div people\n",
    "{\n",
    "  {?agent wdt:P1416 wd:Q7914452.} # Div school\n",
    "  union\n",
    "  {?agent wdt:P1416 wd:Q114065689.} # graduate department of religion\n",
    "}\n",
    "  }\n",
    "'''\n",
    "#print(query_string)\n",
    "\n",
    "wdqs = Sparqler(useragent=user_agent)\n",
    "query_results = wdqs.query(query_string)\n",
    "sleep(settings['sparql_sleep'])\n",
    "\n",
    "coauthors = []\n",
    "for result in query_results:\n",
    "    author_dict = {}\n",
    "    author_dict['qid'] = extract_local_name(result['agent']['value'])\n",
    "    if 'label' in result:\n",
    "        author_dict['label'] = result['label']['value']\n",
    "    else:\n",
    "        author_dict['label'] = ''\n",
    "    if 'orcid' in result:\n",
    "        author_dict['orcid'] = result['orcid']['value']\n",
    "    else:\n",
    "        author_dict['orcid'] = ''\n",
    "    coauthors.append(author_dict)\n",
    "coauthors = pd.DataFrame(coauthors) # NOTE: Q IDs don't include Wikidata namespace\n",
    "coauthors = coauthors.sort_values(by=['qid'])\n",
    "coauthors.to_csv(settings['file_path'] + 'coauthors_from_wikidata.csv', index = False)\n",
    "\n",
    "print('done retrieving author/editor data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: before running this for the first time, you need to generate `works.csv` with only the column headers and no data rows. This file can be created using the convert_yaml_to_metadata_schema.py if the config.yaml file is correctly set up. The column headers also need to be saved as `works_questionable_subtitles.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initiate error logging\n",
    "error_log_object = open(settings['log_path'] + 'log_error.txt', 'at', encoding='utf-8') # direct output to log_object to log file instead of sys.stdout\n",
    "skipped_log_object = open(settings['log_path'] + 'log_skipped.tsv', 'at', encoding='utf-8')\n",
    "found_log_object = open(settings['log_path'] + 'log_found.tsv', 'at', encoding='utf-8')\n",
    "\n",
    "# Load existing data for works written/to be written\n",
    "works_df = pd.read_csv(settings['file_path'] + 'works.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# Load existing data for works with uncertain subtitle status\n",
    "works_subtitle_df = pd.read_csv(settings['file_path'] + 'works_questionable_subtitles.csv', na_filter=False, dtype = str)\n",
    "\n",
    "agents_list = []\n",
    "for index, work_data in works.iterrows():\n",
    "    print()\n",
    "    print(work_data[mapping['constants']['label_column']])\n",
    "    \n",
    "    # Use the mappings to extract and process the main metadata from the source columns\n",
    "    row = extract_metadata(mapping, work_data, settings)\n",
    "    \n",
    "    # Check whether the work is already in Wikidata\n",
    "    wikidata_status = work_in_wikidata_status(row['label_en'], row['doi'], row['pmid'], existing_works_df, settings, verbose=True)\n",
    "    if wikidata_status == 'not found' or wikidata_status == 'possible partial title':\n",
    "        agents_dict = {'unique_identifier': work_data[mapping['constants']['unique_identifier_column']]}\n",
    "\n",
    "        # For each agent type (author, editor, etc.) extract the name information\n",
    "        for agent_type in mapping_agents['sources']:            \n",
    "            if 'structure_name_string' in agent_type:\n",
    "                data_structure = eval(agent_type['structure_name_string'])\n",
    "            else:\n",
    "                data_structure = None\n",
    "            \n",
    "            source_column = agent_type['in_col_label']\n",
    "            agent_structured_data = build_function(agent_type['mapping_function'], work_data[source_column], settings, data_structure)\n",
    "            agents_dict[agent_type['out_col_label']] = json.dumps(agent_structured_data)\n",
    "\n",
    "        # Get the reference values for that work\n",
    "        has_values = False\n",
    "        for reference_type in mapping_agents['ref']:\n",
    "            if 'structure_name_string' in reference_type:\n",
    "                data_structure = eval(reference_type['structure_name_string'])\n",
    "            else:\n",
    "                data_structure = None\n",
    "\n",
    "            no_value, output_value = evaluate_function(reference_type, work_data, settings, data_structure)\n",
    "            if no_value:\n",
    "                agents_dict[reference_type['out_col_label']] = ''\n",
    "            else:\n",
    "                has_values = True\n",
    "                agents_dict[reference_type['out_col_label']] = output_value\n",
    "                \n",
    "        # Special handling for book chapters; need to find the containing book to use for published_in\n",
    "        if row['instance_of'] == 'Q21481766': # Q ID for academic chapter\n",
    "            book_error, book_qid = find_containing_book(work_data['parent_isbn'], row['label_' + settings['default_language']], settings['default_language'], user_agent)\n",
    "            if not book_error:\n",
    "                row['published_in'] = book_qid\n",
    "\n",
    "        # Do not add the work to the list if there is no author or editor information\n",
    "        if not has_values:\n",
    "            print('Warning! No agents associated with this work. Not added to output files.')\n",
    "            print(row['unique_identifier'] + '\\t' + 'No agents associated with this work.\\t' + row['label_' + settings['default_language']].replace('\"',''), file=skipped_log_object)\n",
    "            continue\n",
    "            \n",
    "        # Do not add the work to the list if it's a journal article or book chapter and has no published_in value\n",
    "        if (row['instance_of'] in settings['contained_types']) and (row['published_in'] == ''):\n",
    "            print('Warning! Article or chapter without published_in. Not added to output files.')\n",
    "            print(row['unique_identifier'] + '\\t' + 'Article or chapter without published_in.\\t' + row['label_' + settings['default_language']].replace('\"',''), file=skipped_log_object)\n",
    "            continue\n",
    "            \n",
    "        # Do not add the work unless it is one of the known work types\n",
    "        # Warning already given in the processing function\n",
    "        if row['instance_of'] == '':\n",
    "            print('Warning! Unknown work type. Not added to output files.')\n",
    "            print(row['unique_identifier'] + '\\t' + 'Unknown work type.\\t' + row['label_' + settings['default_language']].replace('\"',''), file=skipped_log_object)\n",
    "            continue\n",
    "        \n",
    "        # Append dict to end of works DataFrame\n",
    "        if wikidata_status == 'not found':\n",
    "            works_df = works_df.append(row, ignore_index=True)\n",
    "        else: # status 'possible partial title'\n",
    "            works_subtitle_df = works_subtitle_df.append(row, ignore_index=True)\n",
    "\n",
    "        # Append agents data to the agents list\n",
    "        agents_list.append(agents_dict)\n",
    "        \n",
    "        # Log any errors that occurred\n",
    "        print(row['unique_identifier'] + ' ' + row['label_' + settings['default_language']], file=error_log_object)\n",
    "        \n",
    "        # Read the warnings log\n",
    "        with open('warnings.log', 'rt') as file_object:\n",
    "            warnings_text = file_object.read()\n",
    "        if warnings_text == '':\n",
    "            print('No errors occurred.', file=error_log_object)\n",
    "        else:\n",
    "            print(warnings_text, file=error_log_object)\n",
    "        print('', file=error_log_object)\n",
    "        \n",
    "        # Clear the warnings log\n",
    "        with open('warnings.log', 'w'):\n",
    "            pass\n",
    "        \n",
    "    # Log cases where work is found in Wikidata\n",
    "    else:\n",
    "        label = row['label_' + settings['default_language']]\n",
    "        label = label.replace('\"', '') # get rid of quotes that will mess up the TSV\n",
    "        label = label.replace('\\t', '') # get rid of tabs that will mess up the TSV\n",
    "        print(row['unique_identifier'] + '\\t' + wikidata_status + '\\t' + label, file=found_log_object)\n",
    "\n",
    "    print()\n",
    "    \n",
    "agents_frame = pd.DataFrame(agents_list)\n",
    "\n",
    "works_df.to_csv(settings['file_path'] + 'works.csv', index = False)\n",
    "works_subtitle_df.to_csv(settings['file_path'] + 'works_questionable_subtitles.csv', index = False)\n",
    "agents_frame.to_csv(settings['file_path'] + 'stored_retrieved_agents.csv', index = False)\n",
    "\n",
    "error_log_object.close()\n",
    "skipped_log_object.close()\n",
    "found_log_object.close()\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interlude\n",
    "\n",
    "After running the code above, the VanderBot script must be run on the `works.csv` file.\n",
    "\n",
    "Before running this script the first time, empty agents files (`author.csv`, `editor.csv`, `translator.csv`, etc.) must be created with appropriate column headers but no data rows. These files can be created using the `convert_yaml_to_metadata_schema.py` if the `config.yaml` file is correctly set up. \n",
    "\n",
    "The following code must be run, and then run the VanderBot script again to add the author and author string data.\n",
    "\n",
    "NOTE: The coauthor screen is really effective at decreasing the amount of searching that needs to be done in Wikidata. So after a first pass at running this code, one can examine the `unidentified_people.json` file to find the obvious matches (people listed as theologians, people with unusualy names that match exactly, etc.), then add them to the `coauthors_from_wikidata.csv` file. Even though they aren't actually coauthors yet, they will become coauthors as soon as the agents data generated here are uploaded to Wikidata, and if this code cell is then re-run, those matched people will automatically get put correctly into the `author.csv` or `editor.csv` file. This is much simpler than trying to manually move them from `author_strings.csv` or to manually enter them in `editor.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_response = input('Write to file? (y/<cr>) ')\n",
    "if user_response == '':\n",
    "    allow_writing_agents = False\n",
    "else:\n",
    "    allow_writing_agents = True\n",
    "\n",
    "today_date = calculate_todays_date()\n",
    "\n",
    "# Create a dictionary to hold a DataFrame for each agent type to append data to as it is generated,\n",
    "# and load it with existing agents data.\n",
    "agents_dict = {}\n",
    "for agent_type in mapping_agents['sources']:\n",
    "    agents_dict[agent_type['out_col_label']] = pd.read_csv(settings['file_path'] + agent_type['out_col_label'] + '.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# Load existing author strings data\n",
    "author_strings_df = pd.read_csv(settings['file_path'] + 'author_strings.csv', na_filter=False, dtype = str)\n",
    "unidentified = []\n",
    "\n",
    "# Open the file containing known co-authors/co-editors\n",
    "coauthors = pd.read_csv(settings['file_path'] + 'coauthors_from_wikidata.csv', na_filter=False, dtype = str)\n",
    "coauthors = coauthors.set_index('qid')\n",
    "\n",
    "# Open the file containing the stored data about authors and editors retrieved from the data source\n",
    "stored_retrieved_agents = pd.read_csv(settings['file_path'] + 'stored_retrieved_agents.csv', na_filter=False, dtype = str)\n",
    "stored_retrieved_agents = stored_retrieved_agents.set_index('unique_identifier')\n",
    "\n",
    "# Open the works items file after upload in order to get the Q IDs for the newly written works\n",
    "processed_works = pd.read_csv(settings['file_path'] + 'works.csv', na_filter=False, dtype = str)\n",
    "processed_works = processed_works.set_index('unique_identifier')\n",
    "\n",
    "# The source data file needs to be opened only to support the hack for the Vanderbilt Divinity School database\n",
    "# that decides whether or not to include series ordinal\n",
    "raw_works = pd.read_csv(settings['file_path'] + 'preprocessed.csv', na_filter=False, dtype = str)\n",
    "raw_works = raw_works.set_index('Key')\n",
    "\n",
    "for work_unique_identifier, work in processed_works.iterrows():\n",
    "    # The processed_works DataFrame will have rows for works whose agents have previously been written.\n",
    "    # In those cases, the unique identifier won't be in the retrieved_agents list, so they should be skipped.\n",
    "    if work_unique_identifier not in list(stored_retrieved_agents.index):\n",
    "        continue\n",
    "        \n",
    "    qid = work['qid']\n",
    "    doi = work['doi']\n",
    "    pmid = work['pmid']\n",
    "    print(qid, work_unique_identifier)\n",
    "    unidentified_for_work = {'qid': 'https://wikidata.org/entity/' + qid, 'unique_identifier': work_unique_identifier}\n",
    "    \n",
    "    # NOTE: in order for this lookup to work, the unique_identifier for the work must actually be unique in the\n",
    "    # processed works table.    \n",
    "    work_agents = stored_retrieved_agents.loc[work_unique_identifier] # result is a Series if unique\n",
    "    \n",
    "    unidentifieds_exist = False\n",
    "    for agent_type in mapping_agents['sources']: # agent_type includes authors, editors, translators, etc.\n",
    "        agent_type_name = agent_type['out_col_label']\n",
    "        print('disamblguating', agent_type_name)\n",
    "\n",
    "        # Disambiguate agents against existing Wikidata people items\n",
    "        agents = json.loads(work_agents[agent_type_name])\n",
    "        found_agent_qids, author_name_strings = disambiguate_agents(agents, pmid, coauthors, settings, user_agent)\n",
    "\n",
    "        # Add data about unidentified people with possible Q ID matches to the list for further work.\n",
    "        if len(author_name_strings) != 0:\n",
    "            unidentifieds_exist = True\n",
    "        unidentified_for_work[agent_type_name] = author_name_strings\n",
    "        \n",
    "        suppress_series_ordinal = False\n",
    "        # Don't use series ordinal if there is only one agent in the category\n",
    "        if len(agents) <= 1:\n",
    "            suppress_series_ordinal = True\n",
    "        if mapping_agents['constants']['suppress_series_ordinal']:\n",
    "            suppress_series_ordinal = True\n",
    "        # Special hack for Vanderbilt Divinity database. Order of agents is not reliable if manually entered.\n",
    "        # Order of agents is reliable if retrieved automatically from a cataloging source \n",
    "        # (i.e. has a value in the \"Library Catalog\" field).\n",
    "        if raw_works.loc[work_unique_identifier]['Library Catalog'] == '':\n",
    "            suppress_series_ordinal = True\n",
    "\n",
    "        # Create a list of dictionaries\n",
    "        for agent in found_agent_qids:\n",
    "            out_dict = {}\n",
    "            out_dict['qid'] = qid\n",
    "            out_dict['label_' + settings['default_language']] = work['label_' + settings['default_language']]\n",
    "            out_dict[agent_type_name + '_uuid'] = ''\n",
    "            out_dict[agent_type_name] = agent['qid']\n",
    "            \n",
    "            out_dict[agent_type_name + '_stated_as'] = agent['stated_as']\n",
    "\n",
    "            if suppress_series_ordinal:\n",
    "                out_dict[agent_type_name + '_series_ordinal'] = ''\n",
    "            else:\n",
    "                out_dict[agent_type_name + '_series_ordinal'] = agent['series_ordinal']\n",
    "                        \n",
    "            # Loop through all of the reference types specified in the agents mapping file\n",
    "            out_dict[agent_type_name + '_ref1_hash'] = ''\n",
    "            for reference_type in mapping_agents['ref']:\n",
    "                out_dict[agent_type_name + '_ref1_' + reference_type['out_col_label']] = work_agents[reference_type['out_col_label']]\n",
    "            out_dict[agent_type_name + '_ref1_retrieved_nodeId'] = ''\n",
    "            out_dict[agent_type_name + '_ref1_retrieved_val'] = today_date\n",
    "            out_dict[agent_type_name + '_ref1_retrieved_prec'] = ''\n",
    "            \n",
    "            # Append dict to end of DataFrame for that particular agent type\n",
    "            agents_dict[agent_type_name] = agents_dict[agent_type_name].append(out_dict, ignore_index=True)\n",
    "            \n",
    "        # Save after each work in case of script crash\n",
    "        if allow_writing_agents:\n",
    "            if len(found_agent_qids) > 0:\n",
    "                agents_dict[agent_type_name].to_csv(settings['file_path'] + agent_type_name + '.csv', index = False)\n",
    "\n",
    "        # Special treatment for authors since only authors have a \"name string property\"\n",
    "        if agent_type_name == 'author':\n",
    "            for author in author_name_strings:\n",
    "                out_dict = {}\n",
    "                out_dict['qid'] = qid\n",
    "                out_dict['label_' + settings['default_language']] = work['label_' + settings['default_language']]\n",
    "                out_dict['author_string_uuid'] = ''\n",
    "                out_dict['author_string'] = author['name_string']\n",
    "            \n",
    "                if suppress_series_ordinal:\n",
    "                    out_dict['author_string_series_ordinal'] = ''\n",
    "                else:\n",
    "                    out_dict['author_string_series_ordinal'] = author['series_ordinal']\n",
    "                    \n",
    "                out_dict['author_string_ref1_hash'] = ''\n",
    "                for reference_type in mapping_agents['ref']:\n",
    "                    out_dict['author_string_ref1_' + reference_type['out_col_label']] = work_agents[reference_type['out_col_label']]\n",
    "                out_dict['author_string_ref1_retrieved_nodeId'] = ''\n",
    "                out_dict['author_string_ref1_retrieved_val'] = today_date\n",
    "                out_dict['author_string_ref1_retrieved_prec'] = ''\n",
    "                \n",
    "                # Append dict to end of DataFrame\n",
    "                author_strings_df = author_strings_df.append(out_dict, ignore_index=True)\n",
    "                \n",
    "            #  Save after each work in case of script crash\n",
    "            if allow_writing_agents:\n",
    "                if len(author_name_strings) > 0:\n",
    "                    author_strings_df.to_csv(settings['file_path'] + 'author_strings.csv', index = False)\n",
    "\n",
    "    if unidentifieds_exist:\n",
    "        unidentified.append(unidentified_for_work)\n",
    "        \n",
    "    # Save the potential author and editor matches in a file\n",
    "    # Save after each work in case of crash; maybe later just write at end\n",
    "    with open(settings['file_path'] + 'unidentified_people.json', 'wt', encoding='utf-8') as file_object:\n",
    "        file_object.write(json.dumps(unidentified, indent=2))\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code for extracting data from skipped log and joining it with fields from the original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('settings.yaml', 'r') as file_object:\n",
    "    settings = yaml.safe_load(file_object)\n",
    "    \n",
    "source_data = settings['file_path'] + settings['source_data_filename']\n",
    "works = pd.read_csv(source_data, na_filter=False, dtype = str)\n",
    "works = works.set_index('Key')\n",
    "\n",
    "skipped = pd.read_csv('log_skipped.csv', na_filter=False, dtype = str)\n",
    "skipped = skipped.set_index('Key')\n",
    "\n",
    "skipped_merge = pd.merge(skipped, works, on=['Key'], how='inner')\n",
    "out = skipped_merge.loc[:, 'reason':'ISSN']\n",
    "out = out.drop(['title', 'Divinity Faculty'], axis='columns')\n",
    "out.to_csv('skipped_works.csv', index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
