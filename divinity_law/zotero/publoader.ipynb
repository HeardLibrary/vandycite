{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publoader.ipynb, a Python script for uploading files and data to Wikimedia Commons using the API.\n",
    "# version 0.1.0\n",
    "\n",
    "# (c) 2023 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "# ----------------\n",
    "# Module imports\n",
    "# ----------------\n",
    "\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "import yaml\n",
    "import sys\n",
    "#import csv\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import requests_cache\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from langdetect import detect_langs\n",
    "import re # regex\n",
    "import logging # See https://docs.python.org/3/howto/logging.html\n",
    "\n",
    "# module located in same directory as script\n",
    "import mapping_functions\n",
    "\n",
    "# Set up cache for HTTP requests\n",
    "requests_cache.install_cache('wqs_cache', backend='sqlite', expire_after=300, allowable_methods=['GET', 'POST'])\n",
    "\n",
    "# ------------------------\n",
    "# Utility functions\n",
    "# ------------------------\n",
    "\n",
    "def calculate_todays_date() -> str:\n",
    "    \"\"\"Generate the current UTC xsd:date.\"\"\"\n",
    "    whole_time_string_z = datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "def extract_local_name(iri: str) -> str:\n",
    "    \"\"\"Extract the local name part of an IRI, e.g. a Q ID from a Wikidata IRI.\n",
    "    \n",
    "    Note:\n",
    "    -----\n",
    "    Expected IRI pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    \"\"\"\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[-1]\n",
    "\n",
    "def include_reference_url(url: str, full_works: pd.DataFrame) -> str:\n",
    "    \"\"\"Determine whether a documentation URL is suitable to be used as a reference URL or full text available.\"\"\"\n",
    "    url_pattern = \"^https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)$\"\n",
    "    url_inclusion_strings = [\n",
    "    'doi',\n",
    "    'jstor',\n",
    "    #'oxfordjournals.org/content',\n",
    "    'article',\n",
    "    'academia.edu',\n",
    "    'content',\n",
    "    'proquest.com/docview',\n",
    "    'handle'\n",
    "    ]\n",
    "    \n",
    "    url_exclusion_strings = [\n",
    "    'login',\n",
    "    'proxy',\n",
    "    #'search.proquest.com',\n",
    "    'worldcat',\n",
    "    'wp-content',\n",
    "    'site.ebrary.com',\n",
    "    'cro3.org/',\n",
    "    'worldbookonline.com/pl/infofinder'\n",
    "    ]\n",
    "\n",
    "    url = url.lower() # convert to all lowercase\n",
    "    \n",
    "    # Exclude invalid URLs\n",
    "    if re.match(url_pattern, url) is None:\n",
    "        return ''\n",
    "\n",
    "    # If the URL matches one of the pre-screened URLs, use it\n",
    "    matched_series = full_works.loc[full_works['Url']==url, 'Url']\n",
    "    # matched_series will be a Series composed of all values in the Url column that match. There should be 1 or 0.\n",
    "    if len(matched_series) == 1:\n",
    "        return url\n",
    "    \n",
    "    # Exclude any URLs containing strings that indicate a login is required\n",
    "    for screening_string in url_exclusion_strings:\n",
    "        if screening_string in url:\n",
    "            return ''\n",
    "        \n",
    "    # Must contain one of the strings that indicate metadata and possible acces\n",
    "    for screening_string in url_inclusion_strings:\n",
    "        if screening_string in url:\n",
    "            return url\n",
    "        \n",
    "    return ''\n",
    "\n",
    "def set_description(string: str, work_types: List[dict]) -> str:\n",
    "    \"\"\"Match the type string with possible types for the data source and return the description.\"\"\"\n",
    "    if string == '':\n",
    "        return ''\n",
    "    \n",
    "    for work_type in work_types:\n",
    "        if string == work_type['type_string']:\n",
    "            return work_type['description']\n",
    "\n",
    "    print('No description, did not find datatype for type:', string)\n",
    "    logging.warning('No description, did not find datatype for type: ' + string)\n",
    "    return ''\n",
    "\n",
    "def title_if_no_lowercase(string: str) -> str:\n",
    "    \"\"\"Change to titlecase only if there are no lowercase letters in the string.\"\"\"\n",
    "    lower = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    is_lower = False\n",
    "    for letter in string:\n",
    "        if letter in lower:\n",
    "            is_lower = True\n",
    "    if is_lower:\n",
    "        return string\n",
    "    else:\n",
    "        return string.title()\n",
    "\n",
    "def fix_all_caps(name_pieces: List[str]) -> List[str]:\n",
    "    \"\"\"Correct the capitalization for a list of name parts that are in all caps.\"\"\"\n",
    "    clean_pieces = []\n",
    "    for piece in name_pieces:\n",
    "        # Special handing for names starting with apostrophe-based prefixes\n",
    "        apostrophe_list = [\"van't\", \"'t\", \"O'\", \"D'\", \"d'\", \"N'\"]\n",
    "        apostrophe_prefix = ''\n",
    "        for possible_apostrophe_prefix in apostrophe_list:\n",
    "            if possible_apostrophe_prefix in piece:\n",
    "                # Remove prefix\n",
    "                piece = piece.replace(possible_apostrophe_prefix, '')\n",
    "                apostrophe_prefix = possible_apostrophe_prefix\n",
    "        \n",
    "        # Special handling for name parts that are lowercase\n",
    "        lower_case_list = ['von', 'de', 'van', 'la', 'der']\n",
    "        if piece.lower() in lower_case_list:\n",
    "            piece = piece.lower()\n",
    "        else:\n",
    "            # Special handling for hyphenated names; doesn't work for an edge case with more than 2 hyphens\n",
    "            if '-' in piece:\n",
    "                halves = piece.split('-')\n",
    "                piece = title_if_no_lowercase(halves[0]) + '-' + title_if_no_lowercase(halves[1])\n",
    "            else:\n",
    "                piece = title_if_no_lowercase(piece)\n",
    "        \n",
    "        # put any apostrophe prefix back on the front\n",
    "        if apostrophe_prefix:\n",
    "            piece = apostrophe_prefix + piece\n",
    "        \n",
    "        clean_pieces.append(piece)\n",
    "    return clean_pieces\n",
    "  \n",
    "def extract_name_pieces(name: str) -> Tuple[List[str], str]:\n",
    "    \"\"\"Extract parts of names. Recognize typical male suffixes. Fix ALL CAPS if present.\"\"\"\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods, sometimes periods are close up with no spaces\n",
    "    name = name.replace('.', ' ')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "        \n",
    "    # Fix stupid situation where name is written in ALL CAPS\n",
    "    pieces = fix_all_caps(pieces)\n",
    "    return pieces, suffix\n",
    "    \n",
    "def extract_identifier_from_extra(extra_field: str, id_name: str) -> str:\n",
    "    \"\"\"Extract a specified identifier (book DOI, article PMID) from the Zotero export Extra field.\"\"\"\n",
    "    identifier = ''\n",
    "    tokens = extra_field.split(' ')\n",
    "    for token_index in range(len(tokens)):\n",
    "        if tokens[token_index] == id_name + ':': # match the tag for the desired ID\n",
    "            # The identifer is the next token after the tag\n",
    "            identifier = tokens[token_index + 1]\n",
    "            break\n",
    "    return identifier\n",
    "\n",
    "def search_name_at_wikidata(name: str, user_agent: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Carry out a search of labels in languages that use Latin characters, and other commonly used languages.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of dictionaries providing the Q IDs and names that match the passed-in name.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    See https://doi.org/10.1145/3233391.3233965 for reference.\n",
    "    \"\"\"\n",
    "    language_codes = [\n",
    "        'en',\n",
    "        'es',\n",
    "        'pt',\n",
    "        'fr',\n",
    "        'it',\n",
    "        'nl',\n",
    "        'de',\n",
    "        'da',\n",
    "        'et',\n",
    "        'hu',\n",
    "        'ga',\n",
    "        'ro',\n",
    "        'sk',\n",
    "        'sl',\n",
    "        'zu',\n",
    "        'tr',\n",
    "        'sv',\n",
    "        'zh',\n",
    "        'ru',\n",
    "        'ja',\n",
    "        'ar',\n",
    "        'pl',\n",
    "        'uk',\n",
    "        'ca',\n",
    "        'cs',\n",
    "        'la',\n",
    "        'nb',\n",
    "        'hu',\n",
    "        'he',\n",
    "        'eo',\n",
    "        'fi',\n",
    "        'ko'\n",
    "      ]\n",
    "    name_list = generate_name_alternatives(name)\n",
    "    alternatives = ''\n",
    "    for language_code in language_codes:\n",
    "        for alternative in name_list:\n",
    "            # get rid of quotes, which will break the query\n",
    "            alternative = alternative.replace('\"', '')\n",
    "            alternative = alternative.replace(\"'\", '')\n",
    "            alternatives += '\"' + alternative + '\"@' + language_code + '\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    \n",
    "    wdqs = Sparqler(useragent=user_agent)\n",
    "    statements = wdqs.query(query)\n",
    "    sleep(settings['sparql_sleep'])\n",
    "\n",
    "    results = []\n",
    "    for statement in statements:\n",
    "        wikidata_iri = statement['item']['value']\n",
    "        if 'label' in statement:\n",
    "            name = statement['label']['value']\n",
    "        else:\n",
    "            name = ''\n",
    "        qnumber = extract_local_name(wikidata_iri)\n",
    "        results.append({'qid': qnumber, 'name': name})\n",
    "    return results\n",
    "\n",
    "def search_wikidata_occ_emp_aff(qid: str, default_language: str, user_agent: str) -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"Search Wikidata by Q ID for occupation, employer, and affiliation claims.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Lists of occupations, employers, and affiliations.\n",
    "    \"\"\"\n",
    "    results_list = []\n",
    "\n",
    "    query_string = '''select distinct ?occupation ?employer ?affiliation where {\n",
    "        optional {\n",
    "            wd:'''+ qid + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = \"'''+ default_language + '''\")            \n",
    "            }\n",
    "\n",
    "        optional {\n",
    "            wd:'''+ qid + ''' wdt:P108 ?employerId.\n",
    "            ?employerId rdfs:label ?employer.\n",
    "            FILTER(lang(?employer) = \"'''+ default_language + '''\")            \n",
    "            }\n",
    "\n",
    "        optional {\n",
    "            wd:'''+ qid + ''' wdt:P1416 ?affiliationId.\n",
    "            ?affiliationId rdfs:label ?affiliation.\n",
    "            FILTER(lang(?affiliation) = \"'''+ default_language + '''\")            \n",
    "            }\n",
    "        }'''\n",
    "    #print(query_string)\n",
    "    \n",
    "    wdqs = Sparqler(useragent=user_agent)\n",
    "    statements = wdqs.query(query_string)\n",
    "    sleep(settings['sparql_sleep'])\n",
    "    #print(statements)\n",
    "    \n",
    "    # pull all possible occupations\n",
    "    occupationList = []\n",
    "    employerList = []\n",
    "    affiliationList = []\n",
    "    for statement in statements:\n",
    "        if 'occupation' in statement:\n",
    "            occupationList.append(statement['occupation']['value'])\n",
    "        if 'employer' in statement:\n",
    "            employerList.append(statement['employer']['value'])\n",
    "        if 'affiliation' in statement:\n",
    "            affiliationList.append(statement['affiliation']['value'])\n",
    "    occupationList = list(set(occupationList))\n",
    "    employerList = list(set(employerList))\n",
    "    affiliationList = list(set(affiliationList))\n",
    "    #print(occupationList)\n",
    "    #print(employerList)\n",
    "    #print(affiliationList)\n",
    "    \n",
    "    return occupationList, employerList, affiliationList \n",
    "\n",
    "\n",
    "def find_surname_givens(name: str) -> Dict[str, str]:\n",
    "    \"\"\"Extract surname and given names from a full name string and remove typical male suffixes.\"\"\"\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', ' ')\n",
    "    name = name.replace(',', ' ')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Get rid of empty pieces formed from extra spaces\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "        \n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "def generate_name_alternatives(name: str) -> List[str]:\n",
    "    \"\"\"Generate permutations of names and initials (with and without periods) for a label and alias query.\"\"\"\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods, sometimes periods are close up with no spaces\n",
    "    name = name.replace('.', ' ')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "        \n",
    "    # Fix stupid situation where name is written in ALL CAPS\n",
    "    pieces = fix_all_caps(pieces)        \n",
    "\n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        # make sure first character is alphabetic\n",
    "        # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "        # typical cases are like (Kit) or \"Kit\"\n",
    "        if not piece[0:1].isalpha():\n",
    "            piece = piece[1:len(piece)] # remove the first non-alphabetic character\n",
    "        if len(piece) > 0:\n",
    "            initials.append(piece[0:1])\n",
    "        \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += pieces[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # full name with suffix\n",
    "    if suffix != '':\n",
    "        name_version = ''\n",
    "        for piece_number in range(0, len(pieces)-1):\n",
    "            name_version += pieces[piece_number] + ' '\n",
    "        name_version += pieces[len(pieces)-1] + suffix\n",
    "        alternatives.append(name_version)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    name_version = pieces[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    name_version = pieces[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first and last name only\n",
    "    name_version = pieces[0] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial and last name only\n",
    "    name_version = initials[0] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    name_version = initials[0] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial no period and all other names\n",
    "    name_version = initials[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += pieces[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial with period and all other names\n",
    "    name_version = initials[0] + '. '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += pieces[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials with last name\n",
    "    name_version = initials[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += initials[piece_number]\n",
    "    name_version += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def screen_qids(qids: List[str], screens: List[Dict[str, str]], default_language: str, user_agent: str) -> List[Dict[str, str]]:\n",
    "    \"\"\"Screen Q IDs based on criteria saved in the screens.yaml configuration file.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    A list of dictionaries providing labels and descriptions that match the queried Q IDs.\n",
    "    \"\"\"\n",
    "    qid_values =''\n",
    "    for qid in qids:\n",
    "        qid_values += 'wd:' + qid + '\\n'\n",
    "\n",
    "    graph_pattern = ''\n",
    "    first_screen = True\n",
    "    for screen in screens:\n",
    "        # Each requirement in a screen has an AND relationship (all must be satisfied)\n",
    "        subgraph_pattern = ''\n",
    "        for requirement in screen:\n",
    "\n",
    "            # Set the value if required or use a dummy variable if any value is allowed\n",
    "            if requirement['entity'] is None:\n",
    "                value = '?var' + requirement['property'] # add the property string to the variable to guarantee uniqueness\n",
    "            elif re.match(r'Q\\d+', requirement['entity']): # regex to match Q IDs\n",
    "                value = 'wd:' + requirement['entity']\n",
    "            else: # if not nothing or a Q ID, assume it's a string literal\n",
    "                if requirement['lang'] is None:\n",
    "                    value = '\"' + requirement['entity'] + '\"'\n",
    "                else:\n",
    "                    value = '\"' + requirement['entity'] + '\"@' + requirement['lang']\n",
    "\n",
    "            # Set the property (label, description, or P value)\n",
    "            if requirement['property'] == 'label':\n",
    "                property = 'rdfs:label'\n",
    "            elif requirement['property'] == 'description':\n",
    "                property = 'schema:description'\n",
    "            else:\n",
    "                property = 'wdt:' + requirement['property']\n",
    "\n",
    "            # Place the value in either the subject or object position in the triple\n",
    "            if requirement['position'] == 'object':\n",
    "                triple_pattern = '?qid ' + property + ' ' + value + '.'\n",
    "            else:\n",
    "                triple_pattern = value + ' ' + property + ' ?qid.'\n",
    "\n",
    "            # Add filters if needed\n",
    "            if requirement['filter_type'] == '<' or requirement['filter_type'] == '>': \n",
    "                # note: string comparison only e.g. for datetimes, needs modification for actual numbers\n",
    "                triple_pattern += '\\nFILTER (STR(?var' + requirement['property'] + ') ' + requirement['filter_type'] + ' \"' + requirement['filter_string'] + '\")'\n",
    "\n",
    "            if requirement['filter_type'] == 'in': \n",
    "                # note: string comparison only\n",
    "                triple_pattern += '\\nFILTER (CONTAINS(?var' + requirement['property'] + ', \"' + requirement['filter_string'] + '\"))'\n",
    "\n",
    "            # Use MINUS if you want to exclude items that fit the pattern.\n",
    "            if requirement['require'] == 'exclude':\n",
    "                triple_pattern = 'minus {' + triple_pattern + '}'\n",
    "\n",
    "            triple_pattern += '\\n'\n",
    "            #print(triple_pattern)\n",
    "            subgraph_pattern += triple_pattern\n",
    "\n",
    "        # Now attach the subgraph pattern to any previous subgraph patterns using UNION to great an OR relationship\n",
    "        subgraph_pattern = '{\\n' + subgraph_pattern + '}\\n' # create a subgraph pattern so that several can be UNIONed\n",
    "        if first_screen: # The first subgraph pattern doesn't need the UNION inserted\n",
    "            first_screen = False\n",
    "        else:\n",
    "            graph_pattern = graph_pattern + 'UNION\\n'\n",
    "        graph_pattern += subgraph_pattern \n",
    "\n",
    "    query_string = '''\n",
    "    select distinct ?qid ?label ?description where {\n",
    "      VALUES ?qid\n",
    "      {\n",
    "      ''' + qid_values + '''}\n",
    "    ''' + graph_pattern + '''\n",
    "    \n",
    "    ?qid rdfs:label ?label.\n",
    "    FILTER(lang(?label)=\"'''+ default_language + '''\")\n",
    "    \n",
    "    OPTIONAL {\n",
    "    ?qid schema:description ?description.\n",
    "    FILTER(lang(?description)=\"'''+ default_language + '''\")\n",
    "    }\n",
    "      }\n",
    "    '''\n",
    "    #print(query_string)\n",
    "    \n",
    "    wdqs = Sparqler(useragent=user_agent)\n",
    "    results = wdqs.query(query_string)\n",
    "    sleep(settings['sparql_sleep'])\n",
    "\n",
    "    return_list = []\n",
    "    for result in results:\n",
    "        out_dict = {\n",
    "            'qid': extract_local_name(result['qid']['value']),\n",
    "            'label': result['label']['value']\n",
    "            }\n",
    "        if 'description' in result:\n",
    "            out_dict['description'] = result['description']['value']\n",
    "        else:\n",
    "            out_dict['description'] = ''           \n",
    "        return_list.append(out_dict)\n",
    "    return return_list\n",
    "\n",
    "def work_in_wikidata_status(label: str, doi: str, pmid: str, existing_works_df: pd.DataFrame, settings: Dict[str, Any], verbose: bool = False) -> Tuple[str, str]:\n",
    "    \"\"\"Search by DOI, PubMed ID, and label for a work in the list of pre-existing Wikidata items.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    If a fuzzy match has a high score, accept as a match. \n",
    "    For intermediate range scores, flag as a case where one label is a subtitle of another.\n",
    "    \"\"\"\n",
    "    if doi and doi.upper() in list(existing_works_df.loc[:, 'doi']):\n",
    "        if verbose:\n",
    "            print('DOI found in existing works')\n",
    "        temp_series = existing_works_df.loc[existing_works_df['doi']==doi, 'qid'].copy()\n",
    "        qid = temp_series.iloc[0]\n",
    "        return 'found DOI', qid\n",
    "    elif pmid and pmid in list(existing_works_df.loc[:, 'pmid']):\n",
    "        if verbose:\n",
    "            print('PubMed ID found in existing works')\n",
    "        temp_series = existing_works_df.loc[existing_works_df['pmid']==pmid, 'qid'].copy()\n",
    "        qid = temp_series.iloc[0]\n",
    "        return 'found PubMed ID', qid\n",
    "    else:\n",
    "        # NOTE: although calculating the fuzz.WRatio is labor intensive, these checks must be done\n",
    "        # sequentially, since we don't want the search for a nearly exact match to be stopped if a\n",
    "        # stupid partial match is found first.\n",
    "        for index, work in existing_works_df.iterrows():\n",
    "            w_ratio = fuzz.WRatio(work['label'], label)\n",
    "\n",
    "            # Test for nearly exact title match\n",
    "            if w_ratio > settings['existing_work_fuzzy_match_cutoff']:\n",
    "                if verbose:\n",
    "                    print('fuzzy label match: ' + str(w_ratio))\n",
    "                    print('test:', label)\n",
    "                    print('wikidata:', extract_local_name(work['qid']), work['label'])\n",
    "                return 'fuzzy label match', work['qid']\n",
    "                \n",
    "        for index, work in existing_works_df.iterrows():\n",
    "            w_ratio = fuzz.WRatio(work['label'], label)\n",
    "            # Test for meaningful subtitle match\n",
    "            if w_ratio > settings['existing_work_subtitle_fuzzy_match_cutoff']:\n",
    "                # NOTE: sometimes a work will have an acceptable score, because a long title matches a single\n",
    "                # word title or vice versa. Those cases are nearly always bad matches and should not be\n",
    "                # considered possible subtitle matches.\n",
    "                if len(work['label'].split(' ')) != 1 and len(label.split(' ')) != 1:\n",
    "                    if verbose:\n",
    "                        print('Warning!!! Possible partial title: ' + str(w_ratio))\n",
    "                        print('test:', label)\n",
    "                        print('wikidata:', extract_local_name(work['qid']), work['label'])\n",
    "                        logging.warning('Possible partial title: ' + str(w_ratio) + ' match, ' + extract_local_name(work['qid']) + ' ' + work['label'])\n",
    "                    return 'possible partial title', work['qid']\n",
    "                else: # keep looking for matches among existing works\n",
    "                    print('skipped over possible partial title: ' + str(w_ratio) + ' match, ' + extract_local_name(work['qid']) + ' ' + work['label'])\n",
    "                    continue\n",
    "\n",
    "    if verbose:\n",
    "        print('Not found')\n",
    "    return 'not found', ''\n",
    "\n",
    "def find_containing_book(isbn: str, label: str, default_language: str, user_agent: str) -> Tuple[bool, str]:\n",
    "    \"\"\"SPARQL query for a book by ISBN or label.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple of\n",
    "    First item a boolean, True if an error condition such as more than one book match or non-book type.\n",
    "    Second item is the Q ID of the matching book or empty string if no match (not an error condition).\n",
    "    \"\"\"\n",
    "    book_error = False\n",
    "    found = False\n",
    "    \n",
    "    if isbn != '':\n",
    "        query_string = '''SELECT DISTINCT ?item\n",
    "    WHERE \n",
    "    {\n",
    "      BIND (\"''' + isbn + '''\" AS ?isbn)\n",
    "      {?item wdt:P212 ?isbn.} # ISBN-13\n",
    "      union\n",
    "      {?item wdt:P957 ?isbn.} # ISBN-10\n",
    "    }\n",
    "    '''\n",
    "        wdqs = Sparqler(useragent=user_agent)\n",
    "        results = wdqs.query(query_string)\n",
    "        sleep(settings['sparql_sleep'])\n",
    "\n",
    "        if len(results) > 1:\n",
    "            print('More than one book matches the ISBN')\n",
    "            logging.warning('More than one book matches the ISBN')\n",
    "            book_error = True\n",
    "            found = True\n",
    "            book_qid = ''\n",
    "        elif len(results) == 1:\n",
    "            found = True\n",
    "            book_qid = extract_local_name(results[0]['item']['value'])\n",
    "            \n",
    "    if not found:\n",
    "        query_string = '''SELECT DISTINCT ?item ?type WHERE {\n",
    "?item rdfs:label \"''' + label + '\"@' + default_language + '''.\n",
    "?item wdt:P31 ?type.\n",
    "}'''\n",
    "        wdqs = Sparqler(useragent=user_agent)\n",
    "        results = wdqs.query(query_string)\n",
    "        sleep(settings['sparql_sleep'])\n",
    "        if (results is None) or (len(results) == 0):\n",
    "            book_qid = ''\n",
    "        else:\n",
    "            type_list = [extract_local_name(result['type']['value']) for result in results]\n",
    "            qids_list = list(set([extract_local_name(result['item']['value']) for result in results]))\n",
    "            if len(qids_list) > 1:\n",
    "                book_error = True\n",
    "                print('Label for published_in matches multiple items:', qids_list)\n",
    "                logging.warning('Label for published_in matches multiple items: ' + str(qids_list))\n",
    "                book_qid = ''\n",
    "            else:\n",
    "                result_qid = qids_list[0]\n",
    "                if 'Q1711593' in type_list: # edited volume\n",
    "                    book_qid = result_qid\n",
    "                else:\n",
    "                    book_error = True\n",
    "                    print('Possible published_in', result_qid, 'not edited volume but has types', type_list)\n",
    "                    logging.warning('Possible published_in ' + result_qid + ' not edited volume but has types ' + str(type_list))\n",
    "                    book_qid = ''\n",
    "    return book_error, book_qid\n",
    "\n",
    "def create_skipped_dict(wikidata_status: str, work_data: pd.Series, mapping: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"Create a row dictionary for works whose processing is skipped for various reasons.\"\"\"\n",
    "    row_dict = {}\n",
    "    row_dict['key'] = work_data[mapping['constants']['unique_identifier_column']]\n",
    "    row_dict['reason'] = wikidata_status\n",
    "    # look up remaining information from the works DataFrame\n",
    "    row_dict['item_type'] = work_data[mapping['constants']['description_code_column']]\n",
    "    # !!!! Idiosyncratic to Zotero dump\n",
    "    if row_dict['item_type'] == 'bookSection':\n",
    "        row_dict['isbn'] = work_data['parent_isbn']\n",
    "    else:\n",
    "        row_dict['isbn'] = work_data['ISBN']\n",
    "    row_dict['issn'] = work_data['ISSN']\n",
    "    row_dict['publication_title'] = work_data['Publication Title']\n",
    "    row_dict['publication_year'] = work_data['Publication Year']\n",
    "    row_dict['author'] = work_data['Author']\n",
    "    row_dict['title'] = work_data['Title']\n",
    "    return row_dict\n",
    "\n",
    "# ------------------------\n",
    "# SPARQL query class\n",
    "# ------------------------\n",
    "\n",
    "# This is a condensed version of the more full-featured script at \n",
    "# https://github.com/HeardLibrary/digital-scholarship/blob/master/code/wikidata/sparqler.py\n",
    "# It includes only the method for the query form.\n",
    "\n",
    "class Sparqler:\n",
    "    \n",
    "    def __init__(self, method: str = 'post', endpoint: str = 'https://query.wikidata.org/sparql', useragent: Optional[str] = None, sleep: float = 0.1):\n",
    "        \"\"\"Build SPARQL queries of various sorts\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        useragent: str\n",
    "            Required if using the Wikidata Query Service, otherwise optional.\n",
    "            Use the form: appname/v.v (URL; mailto:email@domain.com)\n",
    "            See https://meta.wikimedia.org/wiki/User-Agent_policy\n",
    "        endpoint: URL\n",
    "            Defaults to Wikidata Query Service if not provided.\n",
    "        method: str\n",
    "            Possible values are \"post\" (default) or \"get\". Use \"get\" if read-only query endpoint.\n",
    "            Must be \"post\" for update endpoint.\n",
    "        sleep: float\n",
    "            Number of seconds to wait between queries. Defaults to 0.1\n",
    "\n",
    "        Required modules\n",
    "        ----------------\n",
    "        requests, datetime, time\n",
    "        \"\"\"\n",
    "        # attributes for all methods\n",
    "        self.http_method = method\n",
    "        self.endpoint = endpoint\n",
    "        if useragent is None:\n",
    "            if self.endpoint == 'https://query.wikidata.org/sparql':\n",
    "                print('You must provide a value for the useragent argument when using the Wikidata Query Service.')\n",
    "                print()\n",
    "                raise KeyboardInterrupt # Use keyboard interrupt instead of sys.exit() because it works in Jupyter notebooks\n",
    "        self.sleep = sleep\n",
    "\n",
    "        self.requestheader = {}\n",
    "        if useragent:\n",
    "            self.requestheader['User-Agent'] = useragent\n",
    "        \n",
    "        if self.http_method == 'post':\n",
    "            self.requestheader['Content-Type'] = 'application/x-www-form-urlencoded'\n",
    "\n",
    "    def query(self, query_string: str, form: str = 'select', verbose: bool = False, **kwargs):\n",
    "        \"\"\"Send a SPARQL query to the endpoint.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        form : str\n",
    "            The SPARQL query form.\n",
    "            Possible values are: \"select\" (default), \"ask\", \"construct\", and \"describe\".\n",
    "        mediatype: str\n",
    "            The response media type (MIME type) of the query results.\n",
    "            Some possible values for \"select\" and \"ask\" are: \"application/sparql-results+json\" (default) and \"application/sparql-results+xml\".\n",
    "            Some possible values for \"construct\" and \"describe\" are: \"text/turtle\" (default) and \"application/rdf+xml\".\n",
    "            See https://docs.aws.amazon.com/neptune/latest/userguide/sparql-media-type-support.html#sparql-serialization-formats-neptune-output\n",
    "            for response serializations supported by Neptune.\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        default: list of str\n",
    "            The graphs to be merged to form the default graph. List items must be URIs in string form.\n",
    "            If omitted, no graphs will be specified and default graph composition will be controlled by FROM clauses\n",
    "            in the query itself. \n",
    "            See https://www.w3.org/TR/sparql11-query/#namedGraphs and https://www.w3.org/TR/sparql11-protocol/#dataset\n",
    "            for details.\n",
    "        named: list of str\n",
    "            Graphs that may be specified by IRI in a query. List items must be URIs in string form.\n",
    "            If omitted, named graphs will be specified by FROM NAMED clauses in the query itself.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        If the form is \"select\" and mediatype is \"application/json\", a list of dictionaries containing the data.\n",
    "        If the form is \"ask\" and mediatype is \"application/json\", a boolean is returned.\n",
    "        If the mediatype is \"application/json\" and an error occurs, None is returned.\n",
    "        For other forms and mediatypes, the raw output is returned.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        To get UTF-8 text in the SPARQL queries to work properly, send URL-encoded text rather than raw text.\n",
    "        That is done automatically by the requests module for GET. I guess it also does it for POST when the\n",
    "        data are sent as a dict with the urlencoded header. \n",
    "        See SPARQL 1.1 protocol notes at https://www.w3.org/TR/sparql11-protocol/#query-operation        \n",
    "        \"\"\"\n",
    "        query_form = form\n",
    "        if 'mediatype' in kwargs:\n",
    "            media_type = kwargs['mediatype']\n",
    "        else:\n",
    "            if query_form == 'construct' or query_form == 'describe':\n",
    "            #if query_form == 'construct':\n",
    "                media_type = 'text/turtle'\n",
    "            else:\n",
    "                media_type = 'application/sparql-results+json' # default for SELECT and ASK query forms\n",
    "        self.requestheader['Accept'] = media_type\n",
    "            \n",
    "        # Build the payload dictionary (query and graph data) to be sent to the endpoint\n",
    "        payload = {'query' : query_string}\n",
    "        if 'default' in kwargs:\n",
    "            payload['default-graph-uri'] = kwargs['default']\n",
    "        \n",
    "        if 'named' in kwargs:\n",
    "            payload['named-graph-uri'] = kwargs['named']\n",
    "\n",
    "        if verbose:\n",
    "            print('querying SPARQL endpoint')\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        if self.http_method == 'post':\n",
    "            response = requests.post(self.endpoint, data=payload, headers=self.requestheader)\n",
    "        else:\n",
    "            response = requests.get(self.endpoint, params=payload, headers=self.requestheader)\n",
    "        #print('from cache:', response.from_cache) # uncomment if you want to see if cached data are used\n",
    "        elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "        self.response = response.text\n",
    "        sleep(self.sleep) # Throttle as a courtesy to avoid hitting the endpoint too fast.\n",
    "\n",
    "        if verbose:\n",
    "            print('done retrieving data in', int(elapsed_time), 's')\n",
    "\n",
    "        if query_form == 'construct' or query_form == 'describe':\n",
    "            return response.text\n",
    "        else:\n",
    "            if media_type != 'application/sparql-results+json':\n",
    "                return response.text\n",
    "            else:\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                except:\n",
    "                    return None # Returns no value if an error. \n",
    "\n",
    "                if query_form == 'select':\n",
    "                    # Extract the values from the response JSON\n",
    "                    results = data['results']['bindings']\n",
    "                else:\n",
    "                    results = data['boolean'] # True or False result from ASK query \n",
    "                return results           \n",
    "\n",
    "# ---------------------------\n",
    "# Major processes functions\n",
    "# ---------------------------\n",
    "\n",
    "def evaluate_function(work_data: pd.Series, column_map: Dict[str, str], settings: Dict[str, Any]) -> Any:\n",
    "    \"\"\"Create argument list and pass to the function specified in the column mapping data.\"\"\"\n",
    "    args_list = []\n",
    "    if 'in_col_label' in column_map:\n",
    "        in_value = work_data[column_map['in_col_label']]\n",
    "        args_list.append(in_value)\n",
    "    # If use_settings key is present, it must have a True value to pass the settings\n",
    "    if 'use_settings' in column_map and column_map['use_settings']:\n",
    "        args_list.append(settings)\n",
    "    # Add any necessary globally defined reference DataFrames to the argument list\n",
    "    if 'reference_dfs' in column_map:\n",
    "        for df_name in column_map['reference_dfs']:\n",
    "            args_list.append(VARIABLES[df_name.upper()])\n",
    "    args = tuple(args_list)\n",
    "\n",
    "    my_function = MODULE_FUNCTIONS[column_map['mapping_function']]\n",
    "    return my_function(*args)\n",
    "\n",
    "def extract_metadata(mapping: Dict[str, Any], work_data: pd.Series, settings: Dict[str, Any]) -> Dict[str, str]:\n",
    "    \"\"\"Step through output fields, map them to source data columns, and transform input data for output row.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mapping: complex structure\n",
    "        Maps column headers (\"out_col_label\") in the destination table to column headers (\"in_col_label\") \n",
    "        in the source table.\n",
    "        The \"mapping_function\" key indicates the function used to determine the value to be used in the \n",
    "        output row of the destination table.\n",
    "    work_data: pd.Series\n",
    "        A row of data from the source data table with column headers as the keys.\n",
    "    settings: complex structure\n",
    "        Configuration data\n",
    "    \"\"\"\n",
    "    out_dict = {'qid': '', 'unique_identifier': work_data[mapping['constants']['unique_identifier_column']]}\n",
    "    out_dict['label_' + settings['default_language']] = work_data[mapping['constants']['label_column']]\n",
    "    out_dict['description_' + settings['default_language']] = set_description(work_data[mapping['constants']['description_code_column']], settings['work_types'])\n",
    "\n",
    "    for out_property in config['outfiles'][0]['prop_list']:\n",
    "        \n",
    "        # Find the mapping variable that matches the config property\n",
    "        for prop in mapping['properties']:\n",
    "            if prop['out_col_label'] == out_property['variable']:\n",
    "                break\n",
    "    \n",
    "        out_field = out_property['variable']\n",
    "        out_dict[out_field + '_uuid'] = ''\n",
    "        \n",
    "        # If a function requires some data structure for input, its mapping must include the string that is \n",
    "        # the name of the data structure object needed by the function as an item in the reference_dfs list.\n",
    "        \n",
    "        # Functions not needing additional data will have\n",
    "        # only three arguments and None will be passed into the constructor functions as the data_structure argument.\n",
    "        \n",
    "        # NOTE: the data structure will be a global variable and be defined in the main script.\n",
    "        if 'structure_name_string' in prop:\n",
    "            data_structure = eval(prop['structure_name_string'])\n",
    "        else:\n",
    "            data_structure = None\n",
    "\n",
    "        output_value = evaluate_function(work_data, prop, settings)\n",
    "        if output_value == '':\n",
    "            no_value = True\n",
    "        else:\n",
    "            no_value = False\n",
    "\n",
    "        # Populate the values-related columns\n",
    "        if out_property['value_type'] == 'date':\n",
    "            out_dict[out_field + '_nodeId'] = ''\n",
    "            out_dict[out_field + '_val'] = output_value\n",
    "            out_dict[out_field + '_prec'] = ''\n",
    "\n",
    "        elif out_property['value_type'] == 'quantity':\n",
    "            out_dict[out_field + '_nodeId'] = ''\n",
    "            out_dict[out_field + '_val'] = output_value\n",
    "            if no_value:\n",
    "                out_dict[out_field + '_unit'] = ''\n",
    "            else:\n",
    "                out_dict[out_field + '_unit'] = prop['quantity_unit']\n",
    "\n",
    "        # This is not actually implemented and will generate an error if used\n",
    "        elif out_property['value_type'] == 'globecoordinate':\n",
    "            out_dict[out_field + '_nodeId'] = ''\n",
    "            out_dict[out_field + '_val'] = output_value\n",
    "            if no_value:\n",
    "                out_dict[out_field + '_long'] = ''\n",
    "                out_dict[out_field + '_prec'] = ''\n",
    "            else:\n",
    "                out_dict[out_field + '_long'] = work_data[out_field + '_long']\n",
    "                out_dict[out_field + '_prec'] = work_data[out_field + '_prec']\n",
    "\n",
    "        else:\n",
    "            out_dict[out_field] = output_value\n",
    "\n",
    "        # Populate the qualifier columns\n",
    "        for qualifier in out_property['qual']:\n",
    "            if no_value:\n",
    "                qual_output_value = ''\n",
    "            else:\n",
    "                # Find the mapping variable that matches the config property\n",
    "                for qual in prop['qual']:\n",
    "                    if qual['out_col_label'] == qualifier['variable']:\n",
    "                        break\n",
    "                        \n",
    "                qual_output_value = evaluate_function(work_data, qual, settings)\n",
    "\n",
    "            qual_field = out_field + '_' + qualifier['variable']\n",
    "            # To my knowledge, dates are the only complex types used as qualifiers (no quantities or globecoordinates).\n",
    "            if qualifier['value_type'] == 'date':\n",
    "                out_dict[qual_field + '_nodeId'] = ''\n",
    "                out_dict[qual_field + '_val'] = qual_output_value\n",
    "                out_dict[qual_field + '_prec'] = ''\n",
    "            else:\n",
    "                out_dict[qual_field] = qual_output_value\n",
    "                \n",
    "        # Populate the reference columns\n",
    "        # There's only a hash ID column if there's at least one reference.\n",
    "        if len(out_property['ref']) > 0:\n",
    "            out_dict[out_field + '_ref1_hash'] = ''\n",
    "            \n",
    "        for reference in out_property['ref']:\n",
    "            if no_value:\n",
    "                ref_output_value = ''\n",
    "            else:\n",
    "                # Find the mapping variable that matches the config property\n",
    "                for ref in prop['ref']:\n",
    "                    if ref['out_col_label'] == reference['variable']:\n",
    "                        break\n",
    "\n",
    "                ref_output_value = evaluate_function(work_data, ref, settings)\n",
    "\n",
    "            ref_field = out_field + '_ref1_' + reference['variable']\n",
    "            # To my knowledge, dates are the only complex types used as qualifiers (no quantities or globecoordinates).\n",
    "            if reference['value_type'] == 'date':\n",
    "                out_dict[ref_field + '_nodeId'] = ''\n",
    "                out_dict[ref_field + '_val'] = ref_output_value\n",
    "                out_dict[ref_field + '_prec'] = ''\n",
    "            else:\n",
    "                out_dict[ref_field] = ref_output_value\n",
    "                    \n",
    "    #print(out_dict)\n",
    "    return out_dict\n",
    "\n",
    "def disambiguate_agents(authors: List[Dict[str, str]], pmid: str, coauthors: pd.DataFrame, settings: Dict[str, Any], user_agent: str) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    \"\"\"Find possible Wikidata Q ID matches from agent strings.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    When a positive ID is made, returns Q IDs, series ordinal, stated_as to use for author/editor/translator\n",
    "    statements.\n",
    "    When no positive ID is made, a list of possible matches is also included for each author string.\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    Use a wide variety of data and tricks to come up with possible matches.\n",
    "    This includes fuzzy matching against department names and querying Wikidata labels and aliases with\n",
    "    many variations of the name string.\n",
    "    \"\"\"\n",
    "    max_pmids_to_check = 10\n",
    "    # If there is a PubMed ID for the article, retrieve the author info\n",
    "    if pmid != '':\n",
    "        pubmed_author_info = retrieve_pubmed_data(pmid)\n",
    "        print('retrieved data from PubMed ID', pmid)\n",
    "        for author_index in range(len(pubmed_author_info)):\n",
    "            pubmed_author_info[author_index]['name'] = pubmed_author_info[author_index]['forename'] + ' ' + pubmed_author_info[author_index]['surname']\n",
    "    else:\n",
    "        print('no PubMed data')\n",
    "\n",
    "    # Augment CrossRef data with PubMed data. Typically the PubMed data is more likely to have the affiliations\n",
    "    # Names are generally very similar, but vary with added or missing periods on initials and suffixes\n",
    "    if pmid != '':\n",
    "        for author_index in range(len(authors)):\n",
    "            found = False\n",
    "            crossref_name = authors[author_index]['givenName'] + ' ' + authors[author_index]['familyName']\n",
    "            #print(crossref_name)\n",
    "            for pubmed_author in pubmed_author_info:\n",
    "                ratio = fuzz.ratio(pubmed_author['name'], crossref_name)\n",
    "                #print(ratio, pubmed_author['name'])\n",
    "                if ratio > 87: # had to drop down to this level because some people with missing \"Jr\" weren't matching\n",
    "                    found = True\n",
    "                    result_string = 'fuzzy label match: ' + str(ratio) + pubmed_author['name'] + ' / ' + crossref_name\n",
    "                    #print(result_string)\n",
    "                    break\n",
    "            if not found:\n",
    "                print('Did not find a match in the PubMed data for', crossref_name)\n",
    "            else:\n",
    "                #print(pubmed_author)\n",
    "                #print(authors[author_index])\n",
    "\n",
    "                # If there is a PubMed affiliation and no affiliation in the CrossRef data, add the PubMed affiliation\n",
    "                if pubmed_author['affiliation'] != '':\n",
    "                    if len(authors[author_index]['affiliation']) == 0:\n",
    "                        authors[author_index]['affiliation'].append(pubmed_author['affiliation'])\n",
    "\n",
    "                # If there is an ORCID in PubMed and no ORCID in the CrossRef data, add the ORCID to CrossRef data\n",
    "                # Not sure how often this happens since I think maybe usually of one has it, the other does, too.\n",
    "                if pubmed_author['orcid'] != '':\n",
    "                    if authors[author_index]['orcid'] == '':\n",
    "                        authors[author_index]['orcid'] = pubmed_author['orcid']\n",
    "\n",
    "                #print(authors[author_index])\n",
    "\n",
    "            #print()\n",
    "    #print(json.dumps(pubmed_author_info, indent=2))\n",
    "\n",
    "    # Perform screening operations on authors to try to determine their Q IDs\n",
    "    found_qid_values = []\n",
    "    not_found_author_list = []\n",
    "    author_count = 1\n",
    "    for author in authors:\n",
    "        print(author_count)\n",
    "        found = False\n",
    "        \n",
    "        # First eliminate the case where all of the name pieces are empty\n",
    "        if (author['givenName'] + ' ' + author['familyName']).strip() == '':\n",
    "            break\n",
    "            \n",
    "        # Record stated_as\n",
    "        stated_as = (author['givenName'] + ' ' + author['familyName']).strip()\n",
    "            \n",
    "        # Fix case where names are stupidly in all caps\n",
    "        name_pieces = author['givenName'].strip().split(' ')\n",
    "        author['givenName'] = ' '.join(fix_all_caps(name_pieces))\n",
    "        name_pieces = author['familyName'].strip().split(' ')\n",
    "        author['familyName'] = ' '.join(fix_all_caps(name_pieces))\n",
    "        \n",
    "        # Screen for exact match to Wikidata labels\n",
    "        for index, researcher in RESEARCHERS.iterrows():\n",
    "            if researcher['label_en'] == author['givenName'] + ' ' + author['familyName']:\n",
    "                found = True\n",
    "                result_string = 'researcher exact label match: ' + researcher['qid'] + ' ' + researcher['label_en']\n",
    "                name = researcher['label_en']\n",
    "                qid = researcher['qid']\n",
    "                break\n",
    "        if not found:\n",
    "            # screen for exact match to alternate names\n",
    "            for index, altname in ALTNAMES.iterrows():\n",
    "                if altname['altLabel'] == author['givenName'] + ' ' + author['familyName']:\n",
    "                    found = True\n",
    "                    result_string = 'researcher altname match: ' + altname['qid'] + ' ' + altname['altLabel']\n",
    "                    name = altname['altLabel']\n",
    "                    qid = altname['qid']\n",
    "                    break\n",
    "            if not found:\n",
    "                # If the researcher has an ORCID, see if it's at Wikidata\n",
    "                if author['orcid'] != '':\n",
    "                    hit = searchWikidataForQIdByOrcid(author['orcid'])\n",
    "                    if hit != {}:\n",
    "                        found = True\n",
    "                        result_string = 'Wikidata ORCID search: ' + hit['qid'] + ' ' + hit['label'] + ' / ' + hit['description']\n",
    "                        name = hit['label']\n",
    "                        qid = hit['qid']\n",
    "\n",
    "                if not found:\n",
    "                    # screen for fuzzy match to Wikidata-derived labels\n",
    "                    for index, researcher in RESEARCHERS.iterrows():\n",
    "                        # Require the surname to match the label surname exactly\n",
    "                        split_names = find_surname_givens(researcher['label_en']) # returns False if no family name\n",
    "                        if split_names: # skip names that don't have 2 parts !!! also misses non-English labels!\n",
    "                            if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                                w_ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], researcher['label_en'])\n",
    "                                #w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], researcher['label_en'])\n",
    "                                if w_ratio > 90:\n",
    "                                    found = True\n",
    "                                    result_string = 'fuzzy label match: ' + str(w_ratio) + ' ' + researcher['qid'] + ' ' + researcher['label_en'] + ' / ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                    name = researcher['label_en']\n",
    "                                    qid = researcher['qid']\n",
    "                                    break\n",
    "                    if not found:\n",
    "                        # screen for fuzzy match to alternate names\n",
    "                        for index, altname in ALTNAMES.iterrows():\n",
    "                            split_names = find_surname_givens(altname['altLabel'])\n",
    "                            if split_names: # skip names that don't have 2 parts\n",
    "                                if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                                    w_ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], altname['altLabel'])\n",
    "                                    #w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], altname['altLabel'])\n",
    "                                    if w_ratio > 90:\n",
    "                                        found = True\n",
    "                                        result_string = 'researcher altname fuzzy match: ' + str(w_ratio) + ' ' + altname['qid'] + ' ' + altname['altLabel'] + ' / ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                        name = altname['altLabel']\n",
    "                                        qid = altname['qid']\n",
    "                                        break\n",
    "                        if not found:\n",
    "                            name = author['givenName'] + ' ' + author['familyName']\n",
    "                            print('Searching Wikidata for', name)\n",
    "                            print('researcher known affiliations: ', author['affiliation'])\n",
    "                            print()\n",
    "                            hits = search_name_at_wikidata(name, user_agent)\n",
    "                            #print(hits)\n",
    "\n",
    "                            qids = []\n",
    "                            for hit in hits:\n",
    "                                qids.append(hit['qid'])\n",
    "                            return_list = screen_qids(qids, screens, settings['default_language'], user_agent) # screens is a global variable loaded at the start\n",
    "                            #print(return_list)\n",
    "\n",
    "                            for hit in return_list:\n",
    "                                # Check each possible name match to the list of known co-authors/co-editors\n",
    "                                # If there is a match, then use that Q ID and quit trying to match.\n",
    "                                if hit['qid'] in list(coauthors.index):\n",
    "                                    found = True\n",
    "                                    qid = hit['qid']\n",
    "                                    result_string = 'Match with known coauthor'\n",
    "                                    \n",
    "                            if not found:\n",
    "                                # Save discovered data to return if not matched\n",
    "                                discovered_data = []\n",
    "                                for hit in return_list:                                \n",
    "                                    hit_data = hit\n",
    "                                    split_names = find_surname_givens(hit['label'])\n",
    "\n",
    "                                    # Require the surname to match the Wikidata label surname exactly\n",
    "                                    # This prevents a high fraction of fuzzy matches where the last names are similar but not the same\n",
    "                                    if split_names: # skip names that don't have 2 parts\n",
    "                                        if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                                            #print(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print(hit)\n",
    "                                            w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('w_ratio:', w_ratio)\n",
    "                                            #ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('ratio:', ratio)\n",
    "                                            #partial_ratio = fuzz.partial_ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('partial_ratio:', partial_ratio)\n",
    "                                            #token_sort_ratio = fuzz.token_sort_ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('token_sort_ratio:', token_sort_ratio)\n",
    "                                            #token_set_ratio = fuzz.token_set_ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('token_set_ratio:', token_set_ratio)\n",
    "\n",
    "                                            # This screen requires a high degree of similarity between the overall ORCID names and Wikidata labels\n",
    "                                            if w_ratio > 80:\n",
    "                                                print('Wikidata search fuzzy match:', w_ratio, author['givenName'] + ' ' + author['familyName'], ' / ', 'https://www.wikidata.org/wiki/'+ hit['qid'], hit['label'])\n",
    "                                                print('Wikidata description: ', hit['description'])\n",
    "\n",
    "                                                # Here we need to check Wikidata employer and affiliation and fuzzy match against known affiliations\n",
    "                                                occupations, employers, affiliations = search_wikidata_occ_emp_aff(hit['qid'], settings['default_language'], user_agent)\n",
    "                                                print('occupations:', occupations)\n",
    "                                                hit_data['occupations'] = occupations\n",
    "                                                print('employers:', employers)\n",
    "                                                hit_data['employers'] = employers\n",
    "                                                print('affiliations', affiliations)\n",
    "                                                hit_data['affiliations'] = affiliations\n",
    "                                                print()\n",
    "\n",
    "                                                # Perform a check of the employer to make sure we didn't miss somebody in the earlier\n",
    "                                                # string matching\n",
    "                                                for employer in employers:\n",
    "                                                    if 'Vanderbilt University' in employer: # catch university and med center\n",
    "                                                        found = True\n",
    "                                                        result_string = 'Match Vanderbilt employer in Wikidata: ' + hit['qid'] + ' ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                                        qid = hit['qid']\n",
    "\n",
    "                                                # If the author doesn't have any known affiliations, there is no point in checking PubMed\n",
    "                                                if author['affiliation'] != []:\n",
    "                                                    # Search Wikidata for articles written by this match\n",
    "                                                    articles_in_wikidata = search_wikidata_article(hit['qid'])\n",
    "                                                    #print(articles_in_wikidata)\n",
    "\n",
    "                                                    # Step through articles with PubMed IDs found in Wikidata and see if the author affiliation or ORCID matches any of the articles\n",
    "                                                    check = 0\n",
    "                                                    for article_in_wikidata in articles_in_wikidata:\n",
    "                                                        if article_in_wikidata['pmid'] != '':\n",
    "                                                            check += 1\n",
    "                                                            if check > max_pmids_to_check:\n",
    "                                                                print('More articles, but stopping after checking', max_pmids_to_check)\n",
    "                                                                break # break out of article-checking loop\n",
    "                                                            print('Checking article, PMID:', article_in_wikidata['pmid'], article_in_wikidata['title'])\n",
    "                                                            pubmed_match = identified_in_pubmed(article_in_wikidata['pmid'], author['givenName'] + ' ' + author['familyName'], author['affiliation'], author['orcid'])\n",
    "                                                            if not pubmed_match:\n",
    "                                                                #print('no match')\n",
    "                                                                print()\n",
    "                                                            else:\n",
    "                                                                found = True\n",
    "                                                                result_string = 'PubMed affilation match: ' + hit['qid'] + ' ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                                                qid = hit['qid']\n",
    "                                                                break # break out of article-checking loop\n",
    "\n",
    "                                                if found:\n",
    "                                                    break # break out of hit list loop\n",
    "                                                print()\n",
    "                                                # If none of the matching criteria are met, save the data for future use\n",
    "                                                discovered_data.append(hit_data)\n",
    "\n",
    "        if not found:\n",
    "            not_found_author_list.append({'name_string': author['givenName'] + ' ' + author['familyName'], 'series_ordinal': author_count, 'possible_matches': discovered_data})\n",
    "            print('not found:', author['givenName'] + ' ' + author['familyName'])\n",
    "\n",
    "        else:\n",
    "            found_qid_values.append({'qid': qid, 'stated_as': stated_as, 'series_ordinal': author_count})\n",
    "            print(result_string)\n",
    "            for index, department in DEPARTMENTS.iterrows():\n",
    "                if qid == department['qid']:\n",
    "                    for lindex, department_label in DEPARTMENT_LABELS.iterrows():\n",
    "                        if department_label['qid'] == department['affiliation']:\n",
    "                            print(department_label['label_en'])\n",
    "                            break\n",
    "        print()\n",
    "        author_count += 1\n",
    "\n",
    "    print()\n",
    "    return found_qid_values, not_found_author_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up log for warnings\n",
    "# This is a system file and hard to look at, so its data are harvested and put into a plain text log file later.\n",
    "logging.basicConfig(filename='warnings.log', filemode='w', format='%(message)s', level=logging.WARNING)\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "# Global variables and configuration settings\n",
    "VARIABLES = vars() # get dictionary of variable names: variables for main module\n",
    "MODULE_FUNCTIONS = vars(mapping_functions) # get dictionary of function names: functions from mapping_functions module\n",
    "\n",
    "print('loading data from files')\n",
    "\n",
    "# Load program settings, mappings, screens, and configurations\n",
    "with open('settings.yaml', 'r') as file_object:\n",
    "    settings = yaml.safe_load(file_object)\n",
    "\n",
    "with open(settings['data_file_path'] + 'config.yaml', 'r') as file_object:\n",
    "    config = yaml.safe_load(file_object)\n",
    "\n",
    "with open(settings['configuration_files_path'] + 'mapping.yaml', 'r') as file_object:\n",
    "    mapping = yaml.safe_load(file_object)\n",
    "\n",
    "with open(settings['configuration_files_path'] + 'mapping_agents.yaml', 'r') as file_object:\n",
    "    mapping_agents = yaml.safe_load(file_object)\n",
    "    \n",
    "# screens.yaml is a configuration file that defines the kinds of screens to be performed on potential agent Q ID matches from Wikidata\n",
    "with open(settings['configuration_files_path'] + 'screens.yaml', 'r') as file_object:\n",
    "    screens = yaml.safe_load(file_object)\n",
    "\n",
    "# The user_agent string identifies this application to Wikimedia APIs.\n",
    "# If you modify this script, you need to modify the settings to change the user-agent string to something else!\n",
    "user_agent = 'PubLoader/' + settings['script_version'] + ' (mailto:' + settings['operator_email_address'] + ')'\n",
    "\n",
    "# Extract names of globally available reference DataFrames from column mapping data\n",
    "\n",
    "# The publishers and publisher_location were previously generated from SPARQL queries and hand-maintained after that. \n",
    "# full_works are the pre-screened \"full works available\" URLs that Charlotte prepared.\n",
    "global_frame_names = []\n",
    "for column in mapping['properties']:\n",
    "    if 'reference_dfs' in column:\n",
    "        for df_name in column['reference_dfs']:\n",
    "            global_frame_names.append(df_name)\n",
    "    if 'qual' in column:\n",
    "        for qual_column in column['qual']:\n",
    "            if 'reference_dfs' in qual_column:\n",
    "                for df_name in qual_column['reference_dfs']:\n",
    "                    global_frame_names.append(df_name)\n",
    "    if 'ref' in column:\n",
    "        for ref_column in column['ref']:\n",
    "            if 'reference_dfs' in ref_column:\n",
    "                for df_name in ref_column['reference_dfs']:\n",
    "                    global_frame_names.append(df_name)\n",
    "global_frame_names = list(set(global_frame_names)) # deduplicate list of global DataFrames\n",
    "\n",
    "for frame_name in global_frame_names:\n",
    "    df = pd.read_csv(settings['reference_file_path'] + frame_name + '.csv', na_filter=False, dtype = str)\n",
    "    # If the dataframe has a Q ID column, set it as the index label for the rows.\n",
    "    if 'qid' in list(df.columns):\n",
    "        df = df.set_index('qid')\n",
    "    VARIABLES.__setitem__(frame_name.upper(), df) # set variable names to UPPER since they are global.\n",
    "\n",
    "# These are references that have been maintained manually over time and are used for agents disambiguation\n",
    "RESEARCHERS = pd.read_csv(settings['reference_file_path'] + 'researchers.csv', na_filter=False, dtype = str)\n",
    "ALTNAMES = pd.read_csv(settings['reference_file_path'] + 'vanderbilt_wikidata_altlabels.csv', na_filter=False, dtype = str)\n",
    "DEPARTMENTS = pd.read_csv(settings['reference_file_path'] + 'departments.csv', na_filter=False, dtype = str)\n",
    "DEPARTMENT_LABELS = pd.read_csv(settings['reference_file_path'] + 'department_labels.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "print('Running SPARQL queries to retrieve existing works and agents from Wikidata')\n",
    "\n",
    "wdqs = Sparqler(useragent=user_agent)\n",
    "\n",
    "query_string = '''select distinct ?work ?workLabel ?doi ?pmid where {\n",
    "  {?author wdt:P1416 wd:Q7914452.} # Div school\n",
    "  union\n",
    "  {?author wdt:P1416 wd:Q114065689.} # graduate department of religion\n",
    "\n",
    "  {?work wdt:P50 ?author.} # author\n",
    "  union\n",
    "  {?work wdt:P98 ?author.} # editor\n",
    "\n",
    "  optional {\n",
    "    ?work rdfs:label ?workLabel.\n",
    "    filter(lang(?workLabel)=\"''' + settings['default_language'] + '''\")\n",
    "    }\n",
    "\n",
    "  optional {?work wdt:P356 ?doi.}\n",
    "  optional {?work wdt:P698 ?pmid.}  \n",
    "  }\n",
    "'''\n",
    "query_results = wdqs.query(query_string)\n",
    "sleep(settings['sparql_sleep'])\n",
    "\n",
    "found_works = []\n",
    "for result in query_results:\n",
    "    work_dict = {}\n",
    "    work_dict['qid'] = extract_local_name(result['work']['value'])\n",
    "    if 'workLabel' in result:\n",
    "        work_dict['label'] = result['workLabel']['value']\n",
    "    else:\n",
    "        work_dict['label'] = ''\n",
    "    if 'doi' in result:\n",
    "        work_dict['doi'] = result['doi']['value'].upper() # valid DOIs are all upper case, but could be some bad ones\n",
    "    else:\n",
    "        work_dict['doi'] = ''\n",
    "    if 'pmid' in result:\n",
    "        work_dict['pmid'] = result['pmid']['value']\n",
    "    else:\n",
    "        work_dict['pmid'] = ''\n",
    "    found_works.append(work_dict)\n",
    "existing_works_df = pd.DataFrame(found_works) # Note: qids are full IRIs\n",
    "existing_works_df = existing_works_df.sort_values(by=['qid'])\n",
    "existing_works_df = existing_works_df.drop_duplicates(subset='qid') # consider only qid column for duplicates\n",
    "existing_works_df.to_csv(settings['temporary_files_path'] + 'existing_works_in_wikidata.csv', index = False)\n",
    "\n",
    "print('retrieving author/editor data from Wikidata')\n",
    "existing_works_qids_list = list(existing_works_df.loc[:, 'qid']) # Generate a list of work Q IDs from the qid column\n",
    "existing_works_qids_string = '>\\n<http://www.wikidata.org/entity/'.join(existing_works_qids_list) # Join the list into a string with one Q ID per line\n",
    "existing_works_qids_string = '<http://www.wikidata.org/entity/' + existing_works_qids_string + '>'\n",
    "\n",
    "query_string = '''\n",
    "select distinct ?agent ?label ?orcid where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + existing_works_qids_string + '''}\n",
    "  \n",
    "{?value wdt:P50 ?agent.}\n",
    "union\n",
    "{?value wdt:P98 ?agent.}\n",
    "\n",
    "?agent rdfs:label ?label.\n",
    "FILTER(lang(?label)=\"''' + settings['default_language'] + '''\")\n",
    "\n",
    "optional {?agent wdt:P496 ?orcid.}\n",
    "\n",
    "MINUS # remove Vanderbilt Div people\n",
    "{\n",
    "  {?agent wdt:P1416 wd:Q7914452.} # Div school\n",
    "  union\n",
    "  {?agent wdt:P1416 wd:Q114065689.} # graduate department of religion\n",
    "}\n",
    "  }\n",
    "'''\n",
    "query_results = wdqs.query(query_string)\n",
    "sleep(settings['sparql_sleep'])\n",
    "\n",
    "coauthors = []\n",
    "for result in query_results:\n",
    "    author_dict = {}\n",
    "    author_dict['qid'] = extract_local_name(result['agent']['value'])\n",
    "    if 'label' in result:\n",
    "        author_dict['label'] = result['label']['value']\n",
    "    else:\n",
    "        author_dict['label'] = ''\n",
    "    if 'orcid' in result:\n",
    "        author_dict['orcid'] = result['orcid']['value']\n",
    "    else:\n",
    "        author_dict['orcid'] = ''\n",
    "    coauthors.append(author_dict)\n",
    "coauthors = pd.DataFrame(coauthors) # NOTE: Q IDs don't include Wikidata namespace\n",
    "coauthors = coauthors.sort_values(by=['qid'])\n",
    "coauthors = coauthors.drop_duplicates(subset='qid') # consider only qid for duplicates\n",
    "coauthors.to_csv(settings['temporary_files_path'] + 'coauthors_from_wikidata.csv', index = False)\n",
    "print('done retrieving author/editor data')\n",
    "\n",
    "# --------------------------------------------\n",
    "\n",
    "# Load the source file from the Zotero export\n",
    "print('loading data to be processed')\n",
    "works = pd.read_csv(settings['data_file_path'] + settings['source_data_filename'], na_filter=False, dtype = str)\n",
    "works = works.iloc[2500:2600]\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Idiosyncratic steps that need to be done between the Zotero output and running the \"standardized\" script.\n",
    "\n",
    "This step involves re-setting the Url column to use the screened URLs if the Zotero output title matches the title in the screened full work CSV.\n",
    "\n",
    "It also involves moving the ISBN from the (work) ISBN column to a new column called `parent_isbn` if the work is a book chapter rather than a book. That's because for book chapters, the ISBN is for the containing book and not the chapter itself.\n",
    "\n",
    "For book chapters only, the `Extra` column has a key:value pair for the chapter DOI, instead of populating the DOI column itself. So if present, this value is extracted and moved to the actual DOI column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for label, work_series in works.iterrows():\n",
    "    try:\n",
    "        # Re-set the Url column to use the screened URLs if the Zotero output title matches\n",
    "        # the title in the screened full work CSV.\n",
    "\n",
    "        # Find the row(s) in the full_works DataFrame that matches the series. There should be only one.\n",
    "        # Create a series of URL values for those rows. Since there should be only one, get the 0th value.\n",
    "        new_url = FULL_WORKS.loc[FULL_WORKS.index==work_series['Title'], 'Url'][0]\n",
    "        # Set a new value for the Url column in the works DataFrame using the looked-up URL.\n",
    "        works.loc[label, 'Url'] = new_url\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    if work_series['Item Type'] == 'bookSection':\n",
    "        works.loc[label, 'parent_isbn'] = work_series['ISBN']\n",
    "        works.loc[label, 'ISBN'] = ''\n",
    "    else:\n",
    "        works.loc[label, 'parent_isbn'] = ''\n",
    "        \n",
    "    # Extract the DOI for book chapters from the Extra field \n",
    "    # and put it in the appropriate column.\n",
    "    if work_series['DOI'] == '':\n",
    "        works.loc[label, 'DOI'] = extract_identifier_from_extra(work_series['Extra'], 'DOI')\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main routine\n",
    "\n",
    "NOTES: \n",
    "- Before running this for the first time, you need to generate `works.csv` with only the column headers and no data rows. This file can be created using the convert_yaml_to_metadata_schema.py if the config.yaml file is correctly set up. The column headers also need to be saved as `works_questionable_subtitles.csv`.\n",
    "- It also would be a good idea to run Author Disambiguator on authors before retrieving the existing works, since if they are only listed under author name string, they won't get picked up and you might end up generating duplicates\n",
    "- After this step, you need to correct any of the publication locations that weren't determined. They will be logged as warnings.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initiate error logging\n",
    "error_log_object = open(settings['log_path'] + 'log_error.txt', 'at', encoding='utf-8') # direct output to log_object to log file instead of sys.stdout\n",
    "found_log_object = open(settings['log_path'] + 'log_found.tsv', 'at', encoding='utf-8')\n",
    "\n",
    "skipped_df = pd.read_csv(settings['log_path'] + 'log_skipped.csv', na_filter=False, dtype = str)\n",
    "\n",
    "\n",
    "# Load existing data for works written/to be written\n",
    "processed_works_df = pd.read_csv(settings['data_file_path'] + 'works.csv', na_filter=False, dtype = str)\n",
    "processed_works_unique_ids = list(processed_works_df['unique_identifier'])\n",
    "\n",
    "# Load existing data for works with uncertain subtitle status\n",
    "works_subtitle_df = pd.read_csv(settings['temporary_files_path'] + 'works_questionable_subtitles.csv', na_filter=False, dtype = str)\n",
    "\n",
    "agents_list = []\n",
    "for index, work_data in works.iterrows():\n",
    "    \n",
    "    # Clear the warnings log\n",
    "    with open('warnings.log', 'wt'):\n",
    "        pass\n",
    "\n",
    "    print()\n",
    "    print(work_data[mapping['constants']['label_column']])\n",
    "    \n",
    "    # Check if the work was already processed. If so, skip it\n",
    "    if work_data[mapping['constants']['unique_identifier_column']] in processed_works_unique_ids:\n",
    "        print('Work already processed')\n",
    "        continue\n",
    "    \n",
    "    # Use the mappings to extract and process the main metadata from the source columns\n",
    "    row = extract_metadata(mapping, work_data, settings)\n",
    "    \n",
    "    # Check whether the work is already in Wikidata\n",
    "    wikidata_status, dummy = work_in_wikidata_status(row['label_en'], row['doi'], row['pmid'], existing_works_df, settings, verbose=True)\n",
    "    if wikidata_status == 'not found' or wikidata_status == 'possible partial title':\n",
    "        agents_dict = {'unique_identifier': work_data[mapping['constants']['unique_identifier_column']]}\n",
    "\n",
    "        # For each agent type (author, editor, etc.) extract the name information\n",
    "        for agent_type in mapping_agents['sources']:\n",
    "            agent_structured_data = evaluate_function(work_data, agent_type, settings)\n",
    "            agents_dict[agent_type['out_col_label']] = json.dumps(agent_structured_data)\n",
    "\n",
    "        # Get the reference values for that agents of that work\n",
    "        has_agent_values = False\n",
    "        for reference_type in mapping_agents['ref']:\n",
    "            output_value = evaluate_function(work_data, reference_type, settings)\n",
    "            if output_value: # If no value, will be empty string and evaluate False\n",
    "                has_agent_values = True\n",
    "            agents_dict[reference_type['out_col_label']] = output_value\n",
    "                \n",
    "        # Special handling for book chapters; need to find the containing book to use for published_in\n",
    "        if row['instance_of'] == 'Q21481766': # Q ID for academic chapter\n",
    "            book_error, book_qid = find_containing_book(work_data['parent_isbn'], row['label_' + settings['default_language']], settings['default_language'], user_agent)\n",
    "            if not book_error:\n",
    "                row['published_in'] = book_qid\n",
    "\n",
    "        # Do not add the work to the list if there is no author or editor information\n",
    "        if not has_agent_values:\n",
    "            print('Warning! No agents associated with this work. Not added to output files.')\n",
    "            row_dict = create_skipped_dict('No agents associated with this work.', work_data, mapping)\n",
    "            skipped_df = skipped_df.append(row_dict, ignore_index=True)\n",
    "            continue\n",
    "            \n",
    "        # Do not add the work to the list if it's a journal article or book chapter and has no published_in value\n",
    "        if (row['instance_of'] in settings['contained_types']) and (row['published_in'] == ''):\n",
    "            print('Warning! Article or chapter without published_in. Not added to output files.')\n",
    "            row_dict = create_skipped_dict('Article or chapter without published_in.', work_data, mapping)\n",
    "            skipped_df = skipped_df.append(row_dict, ignore_index=True)\n",
    "            continue\n",
    "            \n",
    "        # Do not add the work unless it is one of the known work types\n",
    "        # Warning already given in the processing function\n",
    "        if row['instance_of'] == '':\n",
    "            print('Warning! Unknown work type. Not added to output files.')\n",
    "            row_dict = create_skipped_dict('Unknown work type.', work_data, mapping)\n",
    "            skipped_df = skipped_df.append(row_dict, ignore_index=True)\n",
    "            continue\n",
    "        \n",
    "        # Append dict to end of works DataFrame\n",
    "        if wikidata_status == 'not found':\n",
    "            processed_works_df = processed_works_df.append(row, ignore_index=True)\n",
    "        else: # status 'possible partial title'\n",
    "            works_subtitle_df = works_subtitle_df.append(row, ignore_index=True)\n",
    "\n",
    "        # Append agents data to the agents list\n",
    "        agents_list.append(agents_dict)\n",
    "        \n",
    "        # Log any errors that occurred\n",
    "        print(row['unique_identifier'] + ' ' + row['label_' + settings['default_language']], file=error_log_object)\n",
    "        \n",
    "        # Read the warnings log\n",
    "        # For some reason, the log is considered considered a binary file. So when it is read in as text, \n",
    "        # it contains many null characters. So they are removed from the string read from the file.\n",
    "        with open('warnings.log', 'rt') as file_object:\n",
    "            warnings_text = file_object.read().replace('\\0', '')\n",
    "        if warnings_text == '':\n",
    "            print('No errors occurred.', file=error_log_object)\n",
    "        else:\n",
    "            print(warnings_text, file=error_log_object)\n",
    "        print('', file=error_log_object)\n",
    "        \n",
    "    # Log cases where work is found in Wikidata\n",
    "    else:\n",
    "        label = row['label_' + settings['default_language']]\n",
    "        label = label.replace('\"', '') # get rid of quotes that will mess up the TSV\n",
    "        label = label.replace('\\t', '') # get rid of tabs that will mess up the TSV\n",
    "        print(row['unique_identifier'] + '\\t' + wikidata_status + '\\t' + label, file=found_log_object)\n",
    "    print()\n",
    "    \n",
    "agents_frame = pd.DataFrame(agents_list)\n",
    "\n",
    "processed_works_df.to_csv(settings['data_file_path'] + 'works.csv', index = False)\n",
    "works_subtitle_df.to_csv(settings['temporary_files_path'] + 'works_questionable_subtitles.csv', index = False)\n",
    "agents_frame.to_csv(settings['temporary_files_path'] + 'stored_retrieved_agents.csv', index = False)\n",
    "skipped_df.to_csv(settings['log_path'] + 'log_skipped.csv', index = False)\n",
    "\n",
    "error_log_object.close()\n",
    "found_log_object.close()\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interlude\n",
    "\n",
    "After running the code above, the VanderBot script must be run on the `works.csv` file.\n",
    "\n",
    "Before running this script the first time, empty agents files (`author.csv`, `editor.csv`, `translator.csv`, etc.) must be created with appropriate column headers but no data rows. These files can be created using the `convert_yaml_to_metadata_schema.py` if the `config.yaml` file is correctly set up. \n",
    "\n",
    "The following code must be run, and then run the VanderBot script again to add the author and author string data.\n",
    "\n",
    "NOTE: The coauthor screen is really effective at decreasing the amount of searching that needs to be done in Wikidata. So after a first pass at running this code, one can examine the `unidentified_people.json` file to find the obvious matches (people listed as theologians, people with unusualy names that match exactly, etc.), then add them to the `coauthors_from_wikidata.csv` file. Even though they aren't actually coauthors yet, they will become coauthors as soon as the agents data generated here are uploaded to Wikidata, and if this code cell is then re-run, those matched people will automatically get put correctly into the `author.csv` or `editor.csv` file. This is much simpler than trying to manually move them from `author_strings.csv` or to manually enter them in `editor.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_response = input('Write to file? (y/<cr>) ')\n",
    "if user_response == 'y':\n",
    "    allow_writing_agents = True\n",
    "else:\n",
    "    allow_writing_agents = False\n",
    "\n",
    "today_date = calculate_todays_date()\n",
    "\n",
    "# Create a dictionary to hold a DataFrame for each agent type to append data to as it is generated,\n",
    "# and load it with existing agents data.\n",
    "agents_dict = {}\n",
    "for agent_type in mapping_agents['sources']:\n",
    "    agents_dict[agent_type['out_col_label']] = pd.read_csv(settings['data_file_path'] + agent_type['out_col_label'] + '.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# Load existing author strings data\n",
    "author_strings_df = pd.read_csv(settings['data_file_path'] + 'author_strings.csv', na_filter=False, dtype = str)\n",
    "unidentified = []\n",
    "\n",
    "# Open the file containing known co-authors/co-editors\n",
    "coauthors = pd.read_csv(settings['temporary_files_path'] + 'coauthors_from_wikidata.csv', na_filter=False, dtype = str)\n",
    "coauthors = coauthors.set_index('qid')\n",
    "\n",
    "# Open the file containing the stored data about authors and editors retrieved from the data source\n",
    "stored_retrieved_agents = pd.read_csv(settings['temporary_files_path'] + 'stored_retrieved_agents.csv', na_filter=False, dtype = str)\n",
    "stored_retrieved_agents = stored_retrieved_agents.set_index('unique_identifier')\n",
    "\n",
    "# Open the works items file after upload in order to get the Q IDs for the newly written works\n",
    "processed_works = pd.read_csv(settings['data_file_path'] + 'works.csv', na_filter=False, dtype = str)\n",
    "processed_works = processed_works.set_index('unique_identifier')\n",
    "\n",
    "for work_unique_identifier, work in processed_works.iterrows():\n",
    "    # The processed_works DataFrame will have rows for works whose agents have previously been written.\n",
    "    # In those cases, the unique identifier won't be in the retrieved_agents list, so they should be skipped.\n",
    "    if work_unique_identifier not in list(stored_retrieved_agents.index):\n",
    "        continue\n",
    "        \n",
    "    qid = work['qid']\n",
    "    doi = work['doi']\n",
    "    pmid = work['pmid']\n",
    "    print(qid, work_unique_identifier)\n",
    "    unidentified_for_work = {'qid': 'https://wikidata.org/entity/' + qid, 'unique_identifier': work_unique_identifier}\n",
    "    \n",
    "    # NOTE: in order for this lookup to work, the unique_identifier for the work must actually be unique in the\n",
    "    # processed works table.    \n",
    "    work_agents = stored_retrieved_agents.loc[work_unique_identifier] # result is a Series if unique\n",
    "    \n",
    "    unidentifieds_exist = False\n",
    "    for agent_type in mapping_agents['sources']: # agent_type includes authors, editors, translators, etc.\n",
    "        agent_type_name = agent_type['out_col_label']\n",
    "        print('disamblguating', agent_type_name)\n",
    "\n",
    "        # Disambiguate agents against existing Wikidata people items\n",
    "        agents = json.loads(work_agents[agent_type_name])\n",
    "        found_agent_qids, author_name_strings = disambiguate_agents(agents, pmid, coauthors, settings, user_agent)\n",
    "\n",
    "        # Add data about unidentified people with possible Q ID matches to the list for further work.\n",
    "        if len(author_name_strings) != 0:\n",
    "            unidentifieds_exist = True\n",
    "        unidentified_for_work[agent_type_name] = author_name_strings\n",
    "        \n",
    "        suppress_series_ordinal = False\n",
    "        # Don't use series ordinal if there is only one agent in the category\n",
    "        if len(agents) <= 1:\n",
    "            suppress_series_ordinal = True\n",
    "        if mapping_agents['constants']['suppress_series_ordinal']:\n",
    "            suppress_series_ordinal = True\n",
    "\n",
    "        # Special hack for Vanderbilt Divinity database. Order of agents is not reliable if manually entered.\n",
    "        # Order of agents is reliable if retrieved automatically from a cataloging source \n",
    "        # (i.e. has a value in the \"Library Catalog\" field).\n",
    "        if works.loc[works['Key']==work_unique_identifier, 'Library Catalog'].reset_index().iloc[0]['Library Catalog'] == '':\n",
    "            suppress_series_ordinal = True\n",
    "\n",
    "        # Create a list of dictionaries\n",
    "        for agent in found_agent_qids:\n",
    "            out_dict = {}\n",
    "            out_dict['qid'] = qid\n",
    "            out_dict['label_' + settings['default_language']] = work['label_' + settings['default_language']]\n",
    "            out_dict[agent_type_name + '_uuid'] = ''\n",
    "            out_dict[agent_type_name] = agent['qid']\n",
    "            \n",
    "            out_dict[agent_type_name + '_stated_as'] = agent['stated_as']\n",
    "\n",
    "            if suppress_series_ordinal:\n",
    "                out_dict[agent_type_name + '_series_ordinal'] = ''\n",
    "            else:\n",
    "                out_dict[agent_type_name + '_series_ordinal'] = agent['series_ordinal']\n",
    "                        \n",
    "            # Loop through all of the reference types specified in the agents mapping file\n",
    "            out_dict[agent_type_name + '_ref1_hash'] = ''\n",
    "            for reference_type in mapping_agents['ref']:\n",
    "                out_dict[agent_type_name + '_ref1_' + reference_type['out_col_label']] = work_agents[reference_type['out_col_label']]\n",
    "            out_dict[agent_type_name + '_ref1_retrieved_nodeId'] = ''\n",
    "            out_dict[agent_type_name + '_ref1_retrieved_val'] = today_date\n",
    "            out_dict[agent_type_name + '_ref1_retrieved_prec'] = ''\n",
    "            \n",
    "            # Append dict to end of DataFrame for that particular agent type\n",
    "            agents_dict[agent_type_name] = agents_dict[agent_type_name].append(out_dict, ignore_index=True)\n",
    "            \n",
    "        # Save after each work in case of script crash\n",
    "        if allow_writing_agents:\n",
    "            if len(found_agent_qids) > 0:\n",
    "                agents_dict[agent_type_name].to_csv(settings['data_file_path'] + agent_type_name + '.csv', index = False)\n",
    "\n",
    "        # Special treatment for authors since only authors have a \"name string property\"\n",
    "        if agent_type_name == 'author':\n",
    "            for author in author_name_strings:\n",
    "                out_dict = {}\n",
    "                out_dict['qid'] = qid\n",
    "                out_dict['label_' + settings['default_language']] = work['label_' + settings['default_language']]\n",
    "                out_dict['author_string_uuid'] = ''\n",
    "                out_dict['author_string'] = author['name_string']\n",
    "            \n",
    "                if suppress_series_ordinal:\n",
    "                    out_dict['author_string_series_ordinal'] = ''\n",
    "                else:\n",
    "                    out_dict['author_string_series_ordinal'] = author['series_ordinal']\n",
    "                    \n",
    "                out_dict['author_string_ref1_hash'] = ''\n",
    "                for reference_type in mapping_agents['ref']:\n",
    "                    out_dict['author_string_ref1_' + reference_type['out_col_label']] = work_agents[reference_type['out_col_label']]\n",
    "                out_dict['author_string_ref1_retrieved_nodeId'] = ''\n",
    "                out_dict['author_string_ref1_retrieved_val'] = today_date\n",
    "                out_dict['author_string_ref1_retrieved_prec'] = ''\n",
    "                \n",
    "                # Append dict to end of DataFrame\n",
    "                author_strings_df = author_strings_df.append(out_dict, ignore_index=True)\n",
    "                \n",
    "            #  Save after each work in case of script crash\n",
    "            if allow_writing_agents:\n",
    "                if len(author_name_strings) > 0:\n",
    "                    author_strings_df.to_csv(settings['data_file_path'] + 'author_strings.csv', index = False)\n",
    "\n",
    "                    \n",
    "    if unidentifieds_exist:\n",
    "        unidentified.append(unidentified_for_work)\n",
    "        \n",
    "    # Save the potential author and editor matches in a file\n",
    "    # Save after each work in case of crash; maybe later just write at end\n",
    "    with open(settings['temporary_files_path'] + 'unidentified_people.json', 'wt', encoding='utf-8') as file_object:\n",
    "        file_object.write(json.dumps(unidentified, indent=2))\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
