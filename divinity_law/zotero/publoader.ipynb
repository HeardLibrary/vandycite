{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publoader.ipynb, a Python script for uploading files and data to Wikimedia Commons using the API.\n",
    "\n",
    "# (c) 2022 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "# ----------------\n",
    "# Global variables\n",
    "# ----------------\n",
    "\n",
    "script_version = '0.0.1'\n",
    "default_language = 'en'\n",
    "precision_cutoff = 0.95\n",
    "phrase_length_cutoff = 2\n",
    "\n",
    "# These cutoffs were empiracally determined. During testing, there were no errors above the match cutoff.\n",
    "# The subtitle cutoff probably caught all partial/subtitle cases, but there were also errors with the same\n",
    "# scores as some correct ones, so they need human review.\n",
    "existing_work_fuzzy_match_cutoff = 97\n",
    "existing_work_subtitle_fuzzy_match_cutoff = 86\n",
    "\n",
    "sparql_sleep = 0.1 # minimal delay between SPARQL queries\n",
    "names_separator = ';'\n",
    "name_part_separator = ',' # Set to empty string if names aren't reversed\n",
    "\n",
    "file_path = ''\n",
    "reference_file_path = '/users/baskausj/github/vandycite/divinity_law/'\n",
    "\n",
    "# The user_agent string identifies this application to Wikimedia APIs.\n",
    "# If you modify this script, you need to change the user-agent string to something else!\n",
    "user_agent = 'PubLoader/' + script_version + ' (mailto:steve.baskauf@vanderbilt.edu)'\n",
    "\n",
    "# !!! Need to set up an error log!\n",
    "\n",
    "# ----------------\n",
    "# Module imports\n",
    "# ----------------\n",
    "\n",
    "import yaml\n",
    "#import csv\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import requests_cache\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from langdetect import detect_langs\n",
    "import re # regex\n",
    "\n",
    "requests_cache.install_cache('wqs_cache', backend='sqlite', expire_after=300, allowable_methods=['GET', 'POST'])\n",
    "\n",
    "# These are the pre-screened \"full works available\" URLs that Charlotte prepared.\n",
    "full_works = pd.read_csv('full_work_div_pub.csv', na_filter=False, dtype = str)\n",
    "full_works = full_works.set_index('Title')\n",
    "\n",
    "language_qid = {\n",
    "    'en': 'Q1860',\n",
    "    'de': 'Q188',\n",
    "    'fr': 'Q150',\n",
    "    'es': 'Q1321',\n",
    "    'it': 'Q652',\n",
    "    'nl': 'Q7411',\n",
    "    'zh': 'Q7850',\n",
    "    'no': 'Q9043',\n",
    "    'ar': 'Q13955',\n",
    "    'he': 'Q9288',\n",
    "    'pt': 'Q5146'\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "# List of known work types used by CrossRef\n",
    "work_types = [\n",
    "    {\n",
    "    'type_string': 'journal-article',\n",
    "    'qid': 'Q18918145', # academic journal article, alternatively Q13442814 scholarly article\n",
    "    'description': 'journal article'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'book',\n",
    "    'qid': 'Q3331189', # \"version, edition, or translation\"\n",
    "    'description': 'book'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'book-chapter',\n",
    "    'qid': 'Q21481766', # \"academic chapter\"\n",
    "    'description': 'academic book chapter'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'monograph',\n",
    "    'qid': 'Q193495', # monograph\n",
    "    'description': 'monograph'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'reference-book',\n",
    "    'qid': 'Q5292', # encyclopedia\n",
    "    'description': 'encyclopedia'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'reference-entry',\n",
    "    'qid': 'Q13433827', # encyclopedia article, some are handbook articles\n",
    "    'description': 'encyclopedia article'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'dataset',\n",
    "    'qid': 'Q13433827', # also used for encyclopedia articles\n",
    "    'description': 'encyclopedia article'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'other',\n",
    "    'qid': 'Q55915575', # \n",
    "    'description': 'scholarly work'\n",
    "    }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# List of known work types used by Zotero\n",
    "work_types = [\n",
    "    {\n",
    "    'type_string': 'journalArticle',\n",
    "    'qid': 'Q18918145', # academic journal article, alternatively Q13442814 scholarly article\n",
    "    'description': 'journal article'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'book',\n",
    "    'qid': 'Q3331189', # \"version, edition, or translation\"\n",
    "    'description': 'book'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'bookSection',\n",
    "    'qid': 'Q21481766', # \"academic chapter\"\n",
    "    'description': 'academic book chapter'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'thesis',\n",
    "    'qid': 'Q1266946', # \"thesis\"\n",
    "    'description': 'thesis'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'encyclopediaArticle',\n",
    "    'qid': 'Q13433827', # \"encyclopedia article\"\n",
    "    'description': 'encyclopedia article'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'dictionaryEntry',\n",
    "    'qid': 'Q1580166', # \"dictionary entry\"\n",
    "    'description': 'dictionary entry'\n",
    "    }\n",
    "\n",
    "]\n",
    "\n",
    "url_exclusion_strings = [\n",
    "    'login',\n",
    "    'proxy',\n",
    "    #'search.proquest.com',\n",
    "    'worldcat',\n",
    "    'wp-content',\n",
    "    'site.ebrary.com',\n",
    "    'cro3.org/',\n",
    "    'worldbookonline.com/pl/infofinder'\n",
    "]\n",
    "\n",
    "url_inclusion_strings = [\n",
    "    'doi',\n",
    "    'jstor',\n",
    "    #'oxfordjournals.org/content',\n",
    "    'article',\n",
    "    'academia.edu',\n",
    "    'content',\n",
    "    'proquest.com/docview',\n",
    "    'handle'\n",
    "]\n",
    "\n",
    "# ------------------------\n",
    "# Utility functions\n",
    "# ------------------------\n",
    "\n",
    "def extract_local_name(iri):\n",
    "    \"\"\"Extract the local name part of an IRI, e.g. a Q ID from a Wikidata IRI\"\"\"\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[-1]\n",
    "\n",
    "url_pattern = \"^https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)$\"\n",
    "\n",
    "def include_reference_url(url):\n",
    "    \"\"\"Returned strings are suitable to use for references. Currently it's assumed that the criteria are the\n",
    "    same for full work available.\"\"\"\n",
    "    url = url.lower() # convert to all lowercase\n",
    "    \n",
    "    # Exclude invalid URLs\n",
    "    if re.match(url_pattern, url) is None:\n",
    "        return ''\n",
    "\n",
    "    # If the URL matches one of the pre-screened URLs, use it\n",
    "    matched_series = full_works.loc[full_works['Url']==url, 'Url']\n",
    "    # matched_series will be a Series composed of all values in the Url column that match. There should be 1 or 0.\n",
    "    if len(matched_series) == 1:\n",
    "        return url\n",
    "    \n",
    "    # Exclude any URLs containing strings that indicate a login is required\n",
    "    for screening_string in url_exclusion_strings:\n",
    "        if screening_string in url:\n",
    "            return ''\n",
    "        \n",
    "    # Must contain one of the strings that indicate metadata and possible acces\n",
    "    for screening_string in url_inclusion_strings:\n",
    "        if screening_string in url:\n",
    "            return url\n",
    "        \n",
    "    return ''\n",
    "\n",
    "def set_description(string):\n",
    "    \"\"\"Match the type string with possible types for the data source and return the description.\"\"\"\n",
    "    if string == '':\n",
    "        return ''\n",
    "    \n",
    "    for work_type in work_types:\n",
    "        if string == work_type['type_string']:\n",
    "            return work_type['description']\n",
    "\n",
    "    print('Did not find datatype for type:', string)\n",
    "    return ''\n",
    "\n",
    "def roman_integer_value(r):\n",
    "    \"\"\"Return value of Roman numeral symbol.\n",
    "    \n",
    "    Note:\n",
    "    -----\n",
    "    Code from https://www.geeksforgeeks.org/python-program-for-converting-roman-numerals-to-decimal-lying-between-1-to-3999/\"\"\"    \n",
    "    if (r == 'I'):\n",
    "        return 1\n",
    "    if (r == 'V'):\n",
    "        return 5\n",
    "    if (r == 'X'):\n",
    "        return 10\n",
    "    if (r == 'L'):\n",
    "        return 50\n",
    "    if (r == 'C'):\n",
    "        return 100\n",
    "    if (r == 'D'):\n",
    "        return 500\n",
    "    if (r == 'M'):\n",
    "        return 1000\n",
    "    return -1\n",
    "\n",
    "def roman_to_decimal(numeral):\n",
    "    \"\"\"Convert Roman numerals to integers.\n",
    "    \n",
    "    Note:\n",
    "    -----\n",
    "    Code from https://www.geeksforgeeks.org/python-program-for-converting-roman-numerals-to-decimal-lying-between-1-to-3999/\"\"\"\n",
    "    str = numeral.upper()\n",
    "    res = 0\n",
    "    i = 0\n",
    "\n",
    "    while (i < len(str)):\n",
    "\n",
    "        # Getting value of symbol s[i]\n",
    "        s1 = roman_integer_value(str[i])\n",
    "        \n",
    "        # Return a negative number if error.\n",
    "        if s1 < 0:\n",
    "            return -1\n",
    "\n",
    "        if (i + 1 < len(str)):\n",
    "\n",
    "            # Getting value of symbol s[i + 1]\n",
    "            s2 = roman_integer_value(str[i + 1])\n",
    "            \n",
    "            # Return a negative number if error.\n",
    "            if s2 < 0:\n",
    "                return -1\n",
    "\n",
    "            # Comparing both values\n",
    "            if (s1 >= s2):\n",
    "\n",
    "                # Value of current symbol is greater\n",
    "                # or equal to the next symbol\n",
    "                res = res + s1\n",
    "                i = i + 1\n",
    "            else:\n",
    "\n",
    "                # Value of current symbol is greater\n",
    "                # or equal to the next symbol\n",
    "                res = res + s2 - s1\n",
    "                i = i + 2\n",
    "        else:\n",
    "            res = res + s1\n",
    "            i = i + 1\n",
    "\n",
    "    return res\n",
    "\n",
    "def title_if_no_lowercase(string):\n",
    "    \"\"\"Changes to titlecase only if there are no lowercase letters in the string.\"\"\"\n",
    "    lower = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    is_lower = False\n",
    "    for letter in string:\n",
    "        if letter in lower:\n",
    "            is_lower = True\n",
    "    if is_lower:\n",
    "        return string\n",
    "    else:\n",
    "        return string.title()\n",
    "\n",
    "def fix_all_caps(name_pieces):\n",
    "    \"\"\"Input is a list of name strings from name split by spaces\"\"\"\n",
    "    clean_pieces = []\n",
    "    for piece in name_pieces:\n",
    "        # Special handing for names starting with apostrophe-based prefixes\n",
    "        apostrophe_list = [\"van't\", \"'t\", \"O'\", \"D'\", \"d'\", \"N'\"]\n",
    "        apostrophe_prefix = ''\n",
    "        for possible_apostrophe_prefix in apostrophe_list:\n",
    "            if possible_apostrophe_prefix in piece:\n",
    "                # Remove prefix\n",
    "                piece = piece.replace(possible_apostrophe_prefix, '')\n",
    "                apostrophe_prefix = possible_apostrophe_prefix\n",
    "        \n",
    "        # Special handling for name parts that are lowercase\n",
    "        lower_case_list = ['von', 'de', 'van', 'la', 'der']\n",
    "        if piece.lower() in lower_case_list:\n",
    "            piece = piece.lower()\n",
    "        else:\n",
    "            # Special handling for hyphenated names; doesn't work for an edge case with more than 2 hyphens\n",
    "            if '-' in piece:\n",
    "                halves = piece.split('-')\n",
    "                piece = title_if_no_lowercase(halves[0]) + '-' + title_if_no_lowercase(halves[1])\n",
    "            else:\n",
    "                piece = title_if_no_lowercase(piece)\n",
    "        \n",
    "        # put any apostrophe prefix back on the front\n",
    "        if apostrophe_prefix:\n",
    "            piece = apostrophe_prefix + piece\n",
    "        \n",
    "        clean_pieces.append(piece)\n",
    "    return clean_pieces\n",
    "  \n",
    "def extract_name_pieces(name):\n",
    "    \"\"\"add description here\"\"\"\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods, sometimes periods are close up with no spaces\n",
    "    name = name.replace('.', ' ')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "        \n",
    "    # Fix stupid situation where name is written in ALL CAPS\n",
    "    pieces = fix_all_caps(pieces)\n",
    "    return pieces, suffix\n",
    "    \n",
    "def extract_names_from_list(names_string):\n",
    "    \"\"\"Extract multiple authors from a character-separated list in a single string.\"\"\"\n",
    "    if names_string == '':\n",
    "        return []\n",
    "    \n",
    "    names_list = names_string.split(names_separator)\n",
    "    \n",
    "    output_list = []\n",
    "    # If names are last name first\n",
    "    if name_part_separator:\n",
    "        for name in names_list:\n",
    "            pieces = name.split(name_part_separator)\n",
    "            if len(pieces) == 1: # an error, name wasn't reversed\n",
    "                print('Name error:', names_string)\n",
    "            elif len(pieces) == 2: # no Jr.\n",
    "                surname_pieces, suffix = extract_name_pieces(pieces[0].strip())\n",
    "                given_pieces, dummy = extract_name_pieces(pieces[1].strip())\n",
    "            elif len(pieces) == 3: # has Jr.\n",
    "                # Note Jr. is handled inconsistently, sometimes placed after entire name, sometimes after surname\n",
    "                if 'Jr' in pieces[2]:\n",
    "                    surname_pieces, suffix = extract_name_pieces(pieces[0].strip() + ', ' + pieces[2].strip())\n",
    "                    given_pieces, dummy = extract_name_pieces(pieces[1].strip())\n",
    "                else:\n",
    "                    surname_pieces, suffix = extract_name_pieces(pieces[0].strip() + ', ' + pieces[1].strip())\n",
    "                    given_pieces, dummy = extract_name_pieces(pieces[2].strip())                    \n",
    "            else:\n",
    "                print('Name error:', names_string)\n",
    "                \n",
    "            surname = ' '.join(surname_pieces)\n",
    "            given = ' '.join(given_pieces)\n",
    "            output_list.append({'orcid': '', 'givenName': given, 'familyName': surname, 'suffix': suffix, 'affiliation': []})\n",
    "    else:\n",
    "        pass # need to write code for case where they aren't reversed\n",
    "        \n",
    "    \n",
    "    return output_list\n",
    "\n",
    "def search_name_at_wikidata(name):\n",
    "    # carry out search for most languages that use Latin characters, plus some other commonly used languages\n",
    "    # See https://doi.org/10.1145/3233391.3233965\n",
    "    language_codes = [\n",
    "        'en',\n",
    "        'es',\n",
    "        'pt',\n",
    "        'fr',\n",
    "        'it',\n",
    "        'nl',\n",
    "        'de',\n",
    "        'da',\n",
    "        'et',\n",
    "        'hu',\n",
    "        'ga',\n",
    "        'ro',\n",
    "        'sk',\n",
    "        'sl',\n",
    "        'zu',\n",
    "        'tr',\n",
    "        'sv',\n",
    "        'zh',\n",
    "        'ru',\n",
    "        'ja',\n",
    "        'ar',\n",
    "        'pl',\n",
    "        'uk',\n",
    "        'ca',\n",
    "        'cs',\n",
    "        'la',\n",
    "        'nb',\n",
    "        'hu',\n",
    "        'he',\n",
    "        'eo',\n",
    "        'fi',\n",
    "        'ko'\n",
    "      ]\n",
    "    name_list = generate_name_alternatives(name)\n",
    "    alternatives = ''\n",
    "    for language_code in language_codes:\n",
    "        for alternative in name_list:\n",
    "            # get rid of quotes, which will break the query\n",
    "            alternative = alternative.replace('\"', '')\n",
    "            alternative = alternative.replace(\"'\", '')\n",
    "            alternatives += '\"' + alternative + '\"@' + language_code + '\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    \n",
    "    wdqs = Sparqler(useragent=user_agent)\n",
    "    statements = wdqs.query(query)\n",
    "    sleep(sparql_sleep)\n",
    "\n",
    "    results = []\n",
    "    for statement in statements:\n",
    "        wikidata_iri = statement['item']['value']\n",
    "        if 'label' in statement:\n",
    "            name = statement['label']['value']\n",
    "        else:\n",
    "            name = ''\n",
    "        qnumber = extract_local_name(wikidata_iri)\n",
    "        results.append({'qid': qnumber, 'name': name})\n",
    "    return results\n",
    "\n",
    "# returns lists of occupations, employers, and affiliations for a person with Wikidata ID qid\n",
    "def search_wikidata_occ_emp_aff(qid):\n",
    "    results_list = []\n",
    "\n",
    "    query_string = '''select distinct ?occupation ?employer ?affiliation where {\n",
    "        optional {\n",
    "            wd:'''+ qid + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = \"'''+ default_language + '''\")            \n",
    "            }\n",
    "\n",
    "        optional {\n",
    "            wd:'''+ qid + ''' wdt:P108 ?employerId.\n",
    "            ?employerId rdfs:label ?employer.\n",
    "            FILTER(lang(?employer) = \"'''+ default_language + '''\")            \n",
    "            }\n",
    "\n",
    "        optional {\n",
    "            wd:'''+ qid + ''' wdt:P1416 ?affiliationId.\n",
    "            ?affiliationId rdfs:label ?affiliation.\n",
    "            FILTER(lang(?affiliation) = \"'''+ default_language + '''\")            \n",
    "            }\n",
    "        }'''\n",
    "    #print(query_string)\n",
    "    \n",
    "    wdqs = Sparqler(useragent=user_agent)\n",
    "    statements = wdqs.query(query_string)\n",
    "    sleep(sparql_sleep)\n",
    "    #print(statements)\n",
    "    \n",
    "    # pull all possible occupations\n",
    "    occupationList = []\n",
    "    employerList = []\n",
    "    affiliationList = []\n",
    "    for statement in statements:\n",
    "        if 'occupation' in statement:\n",
    "            occupationList.append(statement['occupation']['value'])\n",
    "        if 'employer' in statement:\n",
    "            employerList.append(statement['employer']['value'])\n",
    "        if 'affiliation' in statement:\n",
    "            affiliationList.append(statement['affiliation']['value'])\n",
    "    occupationList = list(set(occupationList))\n",
    "    employerList = list(set(employerList))\n",
    "    affiliationList = list(set(affiliationList))\n",
    "    #print(occupationList)\n",
    "    #print(employerList)\n",
    "    #print(affiliationList)\n",
    "    \n",
    "    return occupationList, employerList, affiliationList \n",
    "\n",
    "\n",
    "def find_surname_givens(name):\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', ' ')\n",
    "    name = name.replace(',', ' ')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Get rid of empty pieces formed from extra spaces\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "        \n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "def generate_name_alternatives(name):\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods, sometimes periods are close up with no spaces\n",
    "    name = name.replace('.', ' ')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "        \n",
    "    # Fix stupid situation where name is written in ALL CAPS\n",
    "    pieces = fix_all_caps(pieces)        \n",
    "\n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        # make sure first character is alphabetic\n",
    "        # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "        # typical cases are like (Kit) or \"Kit\"\n",
    "        if not piece[0:1].isalpha():\n",
    "            piece = piece[1:len(piece)] # remove the first non-alphabetic character\n",
    "        if len(piece) > 0:\n",
    "            initials.append(piece[0:1])\n",
    "        \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += pieces[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # full name with suffix\n",
    "    if suffix != '':\n",
    "        name_version = ''\n",
    "        for piece_number in range(0, len(pieces)-1):\n",
    "            name_version += pieces[piece_number] + ' '\n",
    "        name_version += pieces[len(pieces)-1] + suffix\n",
    "        alternatives.append(name_version)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    name_version = pieces[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    name_version = pieces[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first and last name only\n",
    "    name_version = pieces[0] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial and last name only\n",
    "    name_version = initials[0] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    name_version = initials[0] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial no period and all other names\n",
    "    name_version = initials[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += pieces[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial with period and all other names\n",
    "    name_version = initials[0] + '. '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += pieces[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials with last name\n",
    "    name_version = initials[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += initials[piece_number]\n",
    "    name_version += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def screen_qids(qids, screens):\n",
    "    qid_values =''\n",
    "    for qid in qids:\n",
    "        qid_values += 'wd:' + qid + '\\n'\n",
    "\n",
    "    graph_pattern = ''\n",
    "    first_screen = True\n",
    "    for screen in screens:\n",
    "        # Each requirement in a screen has an AND relationship (all must be satisfied)\n",
    "        subgraph_pattern = ''\n",
    "        for requirement in screen:\n",
    "\n",
    "            # Set the value if required or use a dummy variable if any value is allowed\n",
    "            if requirement['entity'] is None:\n",
    "                value = '?var' + requirement['property'] # add the property string to the variable to guarantee uniqueness\n",
    "            elif re.match(r'Q\\d+', requirement['entity']): # regex to match Q IDs\n",
    "                value = 'wd:' + requirement['entity']\n",
    "            else: # if not nothing or a Q ID, assume it's a string literal\n",
    "                if requirement['lang'] is None:\n",
    "                    value = '\"' + requirement['entity'] + '\"'\n",
    "                else:\n",
    "                    value = '\"' + requirement['entity'] + '\"@' + requirement['lang']\n",
    "\n",
    "            # Set the property (label, description, or P value)\n",
    "            if requirement['property'] == 'label':\n",
    "                property = 'rdfs:label'\n",
    "            elif requirement['property'] == 'description':\n",
    "                property = 'schema:description'\n",
    "            else:\n",
    "                property = 'wdt:' + requirement['property']\n",
    "\n",
    "            # Place the value in either the subject or object position in the triple\n",
    "            if requirement['position'] == 'object':\n",
    "                triple_pattern = '?qid ' + property + ' ' + value + '.'\n",
    "            else:\n",
    "                triple_pattern = value + ' ' + property + ' ?qid.'\n",
    "\n",
    "            # Add filters if needed\n",
    "            if requirement['filter_type'] == '<' or requirement['filter_type'] == '>': \n",
    "                # note: string comparison only e.g. for datetimes, needs modification for actual numbers\n",
    "                triple_pattern += '\\nFILTER (STR(?var' + requirement['property'] + ') ' + requirement['filter_type'] + ' \"' + requirement['filter_string'] + '\")'\n",
    "\n",
    "            if requirement['filter_type'] == 'in': \n",
    "                # note: string comparison only\n",
    "                triple_pattern += '\\nFILTER (CONTAINS(?var' + requirement['property'] + ', \"' + requirement['filter_string'] + '\"))'\n",
    "\n",
    "            # Use MINUS if you want to exclude items that fit the pattern.\n",
    "            if requirement['require'] == 'exclude':\n",
    "                triple_pattern = 'minus {' + triple_pattern + '}'\n",
    "\n",
    "            triple_pattern += '\\n'\n",
    "            #print(triple_pattern)\n",
    "            subgraph_pattern += triple_pattern\n",
    "\n",
    "        # Now attach the subgraph pattern to any previous subgraph patterns using UNION to great an OR relationship\n",
    "        subgraph_pattern = '{\\n' + subgraph_pattern + '}\\n' # create a subgraph pattern so that several can be UNIONed\n",
    "        if first_screen: # The first subgraph pattern doesn't need the UNION inserted\n",
    "            first_screen = False\n",
    "        else:\n",
    "            graph_pattern = graph_pattern + 'UNION\\n'\n",
    "        graph_pattern += subgraph_pattern \n",
    "\n",
    "    query_string = '''\n",
    "    select distinct ?qid ?label ?description where {\n",
    "      VALUES ?qid\n",
    "      {\n",
    "      ''' + qid_values + '''}\n",
    "    ''' + graph_pattern + '''\n",
    "    \n",
    "    ?qid rdfs:label ?label.\n",
    "    FILTER(lang(?label)=\"'''+ default_language + '''\")\n",
    "    \n",
    "    OPTIONAL {\n",
    "    ?qid schema:description ?description.\n",
    "    FILTER(lang(?description)=\"'''+ default_language + '''\")\n",
    "    }\n",
    "      }\n",
    "    '''\n",
    "    #print(query_string)\n",
    "    \n",
    "    wdqs = Sparqler(useragent=user_agent)\n",
    "    results = wdqs.query(query_string)\n",
    "    sleep(sparql_sleep)\n",
    "\n",
    "    return_list = []\n",
    "    for result in results:\n",
    "        out_dict = {\n",
    "            'qid': extract_local_name(result['qid']['value']),\n",
    "            'label': result['label']['value']\n",
    "            }\n",
    "        if 'description' in result:\n",
    "            out_dict['description'] = result['description']['value']\n",
    "        else:\n",
    "            out_dict['description'] = ''           \n",
    "        return_list.append(out_dict)\n",
    "    return return_list\n",
    "\n",
    "def work_not_found_in_wikidata(label, doi, pmid, existing_works_df, verbose=False):\n",
    "    \"\"\"!!! Need to add description of this.\"\"\"\n",
    "    if doi and doi.upper() in list(existing_works_df.loc[:, 'doi']):\n",
    "        if verbose:\n",
    "            print('DOI found in existing works')\n",
    "        return False\n",
    "    elif pmid and pmid in list(existing_works_df.loc[:, 'pmid']):\n",
    "        if verbose:\n",
    "            print('PubMed ID found in existing works')\n",
    "        return False\n",
    "    else:\n",
    "        found = False\n",
    "        for index, work in existing_works_df.iterrows():\n",
    "            w_ratio = fuzz.WRatio(work['label'], label)\n",
    "\n",
    "            # Test for nearly exact title match\n",
    "            if w_ratio > existing_work_fuzzy_match_cutoff:\n",
    "                found = True\n",
    "                if verbose:\n",
    "                    print('fuzzy label match: ' + str(w_ratio))\n",
    "                    print('test:', label)\n",
    "                    print('wikidata:', work['label'])\n",
    "                break\n",
    "                \n",
    "            # Test for meaningful subtitle match\n",
    "            if w_ratio > existing_work_subtitle_fuzzy_match_cutoff:\n",
    "                found = True\n",
    "                if verbose:\n",
    "                    print('possible partial title: ' + str(w_ratio))\n",
    "                    print('test:', label)\n",
    "                    print('wikidata:', work['label'])\n",
    "                    \n",
    "                # !!!! Need to log these!\n",
    "                \n",
    "                break\n",
    "\n",
    "        if found:\n",
    "            return False\n",
    "        \n",
    "    if verbose:\n",
    "        print('Not found')    \n",
    "    return True\n",
    "\n",
    "# ------------------------\n",
    "# SPARQL query class\n",
    "# ------------------------\n",
    "\n",
    "# This is a condensed version of the more full-featured script at \n",
    "# https://github.com/HeardLibrary/digital-scholarship/blob/master/code/wikidata/sparqler.py\n",
    "# It includes only the method for the query form.\n",
    "\n",
    "class Sparqler:\n",
    "    \"\"\"Build SPARQL queries of various sorts\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    useragent : str\n",
    "        Required if using the Wikidata Query Service, otherwise optional.\n",
    "        Use the form: appname/v.v (URL; mailto:email@domain.com)\n",
    "        See https://meta.wikimedia.org/wiki/User-Agent_policy\n",
    "    endpoint: URL\n",
    "        Defaults to Wikidata Query Service if not provided.\n",
    "    method: str\n",
    "        Possible values are \"post\" (default) or \"get\". Use \"get\" if read-only query endpoint.\n",
    "        Must be \"post\" for update endpoint.\n",
    "    sleep: float\n",
    "        Number of seconds to wait between queries. Defaults to 0.1\n",
    "        \n",
    "    Required modules:\n",
    "    -------------\n",
    "    requests, datetime, time\n",
    "    \"\"\"\n",
    "    def __init__(self, method='post', endpoint='https://query.wikidata.org/sparql', useragent=None, sleep=0.1):\n",
    "        # attributes for all methods\n",
    "        self.http_method = method\n",
    "        self.endpoint = endpoint\n",
    "        if useragent is None:\n",
    "            if self.endpoint == 'https://query.wikidata.org/sparql':\n",
    "                print('You must provide a value for the useragent argument when using the Wikidata Query Service.')\n",
    "                print()\n",
    "                raise KeyboardInterrupt # Use keyboard interrupt instead of sys.exit() because it works in Jupyter notebooks\n",
    "        self.sleep = sleep\n",
    "\n",
    "        self.requestheader = {}\n",
    "        if useragent:\n",
    "            self.requestheader['User-Agent'] = useragent\n",
    "        \n",
    "        if self.http_method == 'post':\n",
    "            self.requestheader['Content-Type'] = 'application/x-www-form-urlencoded'\n",
    "\n",
    "    def query(self, query_string, form='select', verbose=False, **kwargs):\n",
    "        \"\"\"Send a SPARQL query to the endpoint.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        form : str\n",
    "            The SPARQL query form.\n",
    "            Possible values are: \"select\" (default), \"ask\", \"construct\", and \"describe\".\n",
    "        mediatype: str\n",
    "            The response media type (MIME type) of the query results.\n",
    "            Some possible values for \"select\" and \"ask\" are: \"application/sparql-results+json\" (default) and \"application/sparql-results+xml\".\n",
    "            Some possible values for \"construct\" and \"describe\" are: \"text/turtle\" (default) and \"application/rdf+xml\".\n",
    "            See https://docs.aws.amazon.com/neptune/latest/userguide/sparql-media-type-support.html#sparql-serialization-formats-neptune-output\n",
    "            for response serializations supported by Neptune.\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        default: list of str\n",
    "            The graphs to be merged to form the default graph. List items must be URIs in string form.\n",
    "            If omitted, no graphs will be specified and default graph composition will be controlled by FROM clauses\n",
    "            in the query itself. \n",
    "            See https://www.w3.org/TR/sparql11-query/#namedGraphs and https://www.w3.org/TR/sparql11-protocol/#dataset\n",
    "            for details.\n",
    "        named: list of str\n",
    "            Graphs that may be specified by IRI in a query. List items must be URIs in string form.\n",
    "            If omitted, named graphs will be specified by FROM NAMED clauses in the query itself.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        If the form is \"select\" and mediatype is \"application/json\", a list of dictionaries containing the data.\n",
    "        If the form is \"ask\" and mediatype is \"application/json\", a boolean is returned.\n",
    "        If the mediatype is \"application/json\" and an error occurs, None is returned.\n",
    "        For other forms and mediatypes, the raw output is returned.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        To get UTF-8 text in the SPARQL queries to work properly, send URL-encoded text rather than raw text.\n",
    "        That is done automatically by the requests module for GET. I guess it also does it for POST when the\n",
    "        data are sent as a dict with the urlencoded header. \n",
    "        See SPARQL 1.1 protocol notes at https://www.w3.org/TR/sparql11-protocol/#query-operation        \n",
    "        \"\"\"\n",
    "        query_form = form\n",
    "        if 'mediatype' in kwargs:\n",
    "            media_type = kwargs['mediatype']\n",
    "        else:\n",
    "            if query_form == 'construct' or query_form == 'describe':\n",
    "            #if query_form == 'construct':\n",
    "                media_type = 'text/turtle'\n",
    "            else:\n",
    "                media_type = 'application/sparql-results+json' # default for SELECT and ASK query forms\n",
    "        self.requestheader['Accept'] = media_type\n",
    "            \n",
    "        # Build the payload dictionary (query and graph data) to be sent to the endpoint\n",
    "        payload = {'query' : query_string}\n",
    "        if 'default' in kwargs:\n",
    "            payload['default-graph-uri'] = kwargs['default']\n",
    "        \n",
    "        if 'named' in kwargs:\n",
    "            payload['named-graph-uri'] = kwargs['named']\n",
    "\n",
    "        if verbose:\n",
    "            print('querying SPARQL endpoint')\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        if self.http_method == 'post':\n",
    "            response = requests.post(self.endpoint, data=payload, headers=self.requestheader)\n",
    "        else:\n",
    "            response = requests.get(self.endpoint, params=payload, headers=self.requestheader)\n",
    "        #print('from cache:', response.from_cache) # uncomment if you want to see if cached data are used\n",
    "        elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "        self.response = response.text\n",
    "        sleep(self.sleep) # Throttle as a courtesy to avoid hitting the endpoint too fast.\n",
    "\n",
    "        if verbose:\n",
    "            print('done retrieving data in', int(elapsed_time), 's')\n",
    "\n",
    "        if query_form == 'construct' or query_form == 'describe':\n",
    "            return response.text\n",
    "        else:\n",
    "            if media_type != 'application/sparql-results+json':\n",
    "                return response.text\n",
    "            else:\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                except:\n",
    "                    return None # Returns no value if an error. \n",
    "\n",
    "                if query_form == 'select':\n",
    "                    # Extract the values from the response JSON\n",
    "                    results = data['results']['bindings']\n",
    "                else:\n",
    "                    results = data['boolean'] # True or False result from ASK query \n",
    "                return results           \n",
    "\n",
    "# ------------------------\n",
    "# mapping functions\n",
    "# ------------------------\n",
    "\n",
    "def identity(value):\n",
    "    \"\"\"Return the value argument with any leading and trailing whitespace removed.\"\"\"\n",
    "    return value.strip()\n",
    "\n",
    "def set_instance_of(string):\n",
    "    \"\"\"Match the type string with possible types for the data source and return the type Q ID.\"\"\"\n",
    "    if string == '':\n",
    "        return ''\n",
    "\n",
    "    for work_type in work_types:\n",
    "        if string == work_type['type_string']:\n",
    "            return work_type['qid']\n",
    "\n",
    "    print('Did not find datatype for type:', string)\n",
    "    return ''\n",
    "\n",
    "def detect_language(string):\n",
    "    \"\"\"Detect the language of the label.\"\"\"\n",
    "    if string == '':\n",
    "        return ''\n",
    "    try:\n",
    "        lang_list = detect_langs(string)\n",
    "        lang_string = str(lang_list[0])\n",
    "        confidence = float(lang_string[3:])\n",
    "        lang = lang_string[:2]\n",
    "    except: #exceptions occur when no info to decide, e.g. numbers\n",
    "        lang = 'zxx'\n",
    "        confidence = float(0)\n",
    "    if confidence < precision_cutoff:\n",
    "        print('Warning: language confidence for', lang, 'below', precision_cutoff, ':', confidence)\n",
    "    if lang in language_qid:\n",
    "        return language_qid[lang]\n",
    "    else:\n",
    "        print('Warning: detected language', lang, 'not in list of known languages.')\n",
    "        return ''\n",
    "\n",
    "def title_en(string):\n",
    "    \"\"\"Detect the language of the label.\"\"\"\n",
    "    if string == '':\n",
    "        return ''\n",
    "    try:\n",
    "        lang_list = detect_langs(string)\n",
    "        lang_string = str(lang_list[0])\n",
    "        confidence = float(lang_string[3:])\n",
    "        lang = lang_string[:2]\n",
    "    except: #exceptions occur when no info to decide, e.g. numbers\n",
    "        lang = 'zxx'\n",
    "        confidence = float(0)\n",
    "    if lang == 'en':\n",
    "        return string\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "    \n",
    "def calculate_pages(range):\n",
    "    \"\"\"Calculate the number of pages from the page range.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    Supports properly formatted Roman numerals and doesn't care about whitespace.\"\"\"\n",
    "    if range == '':\n",
    "        return ''\n",
    "    numbers = range.split('-')\n",
    "    \n",
    "    # If there is only a single number or an empty cell, return the empty string.\n",
    "    if len(numbers) < 2:\n",
    "        return ''\n",
    "    # Edge case where it isn't a well-formed range and has multiple hyphens\n",
    "    if len(numbers) > 2:\n",
    "        return ''\n",
    "    \n",
    "    # Step through the two numbers to try to convert them from Roman numerals if not integers.\n",
    "    for index, number in enumerate(numbers):\n",
    "        number = number.strip()\n",
    "        if not number.isnumeric():\n",
    "            numbers[index] = roman_to_decimal(number)\n",
    "            \n",
    "            # Will return -1 error if it contains characters not valid for Roman numerals \n",
    "            if numbers[index] < 0:\n",
    "                return ''\n",
    "    \n",
    "    number_pages = int(numbers[1]) - int(numbers[0]) + 1 # Need to add one since first page in range counts\n",
    "    if number_pages < 1:\n",
    "        return ''\n",
    "    return str(number_pages)\n",
    "    \n",
    "    return value\n",
    "\n",
    "def clean_doi(value):\n",
    "    \"\"\"Turn DOI into uppercase and remove leading and trailing whitespace.\"\"\"\n",
    "    cleaned_value = value.upper().strip()\n",
    "    return cleaned_value\n",
    "\n",
    "def disambiguate_published_in(value):\n",
    "    \"\"\"Use the value in the ISSN column to try to find the containing work.\n",
    "    \n",
    "    Note:\n",
    "    -----\n",
    "    For journal articles, this performs a legitimate WQS search for the journal title using the ISSN.\n",
    "    For book chapters, the ISSN column may contain the Q ID of the containing book, inserted there during\n",
    "    a pre-processing step (a hack, but typically books would not have an ISSN and this column would be empty).\"\"\"\n",
    "    if value == '':\n",
    "        return value\n",
    "    \n",
    "    # The value is a Q ID and was determined during a pre-processing step (i.e. for book chapters)\n",
    "    if value[0] == 'Q':\n",
    "        return value\n",
    "\n",
    "    # Look up the ISSN from CrossRef in Wikidata\n",
    "    # Build query string\n",
    "    query_string = '''select distinct ?container ?containerLabel where {\n",
    "      ?container wdt:P236 \"''' + value + '''\".\n",
    "      optional {\n",
    "      ?container rdfs:label ?containerLabel.\n",
    "      filter(lang(?containerLabel)=\"''' + default_language + '''\")\n",
    "      }\n",
    "    }'''\n",
    "    #print(query_string)\n",
    "\n",
    "    wdqs = Sparqler(useragent=user_agent)\n",
    "    query_results = wdqs.query(query_string)\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    if len(query_results) == 0:\n",
    "        return ''\n",
    "\n",
    "    # !!!!!!!!!!!!!!!!!! Enable this code when the error log is set up\n",
    "    \"\"\"\n",
    "    if len(query_results) > 1:\n",
    "        print('Warning! More than one container in Wikidata matched the ISSN ', file=log_object)\n",
    "        print(query_results, '\\n', file=log_object)\n",
    "    \"\"\"\n",
    "    # Extract Q ID from SPARQL query results. If there is more than one result, the last one will be used for the Q ID\n",
    "    for result in query_results:\n",
    "        container_qid = extract_local_name(result['container']['value'])\n",
    "        # Skipping this since container name isn't passed into the function.\n",
    "        \"\"\"\n",
    "        journal_name = result['containerLabel']['value']\n",
    "        if journal_name != crossref_results['journal_title']:\n",
    "            # NOTE: did empirical testing to see which kind of fuzzy matching worked best\n",
    "            #ratio = fuzz.ratio(journal_name, crossref_results['journal_title'])\n",
    "            #partial_ratio = fuzz.partial_ratio(journal_name, crossref_results['journal_title'])\n",
    "            #sort_ratio = fuzz.token_sort_ratio(journal_name, crossref_results['journal_title'])\n",
    "            #set_ratio = fuzz.token_set_ratio(journal_name, crossref_results['journal_title'])\n",
    "            w_ratio = fuzz.WRatio(journal_name, crossref_results['journal_title'])\n",
    "            #print('name similarity ratio', ratio)\n",
    "            #print('partial ratio', partial_ratio)\n",
    "            #print('sort_ratio', sort_ratio)\n",
    "            #print('set_ratio', set_ratio)\n",
    "            if w_ratio < 99:\n",
    "                print('article:', crossref_results['label_' + default_language], 'w_ratio:', w_ratio, 'Warning: Wikidata journal: \"' + journal_name + '\"', journal_qid, 'does not match CrossRef journal title: \"' + crossref_results['journal_title'] + '\"\\n', file=log_object)\n",
    "        #print('article:', crossref_results['label_' + default_language], 'journal:', journal_qid, journal_name)\n",
    "        \"\"\"\n",
    "    return container_qid\n",
    "\n",
    "def isbn10(string):\n",
    "    \"\"\"Check whether the ISBN value has 10 characters or not.\"\"\"\n",
    "    test = string.replace('-', '')\n",
    "    if len(test) == 10:\n",
    "        return string\n",
    "    return ''\n",
    "\n",
    "def isbn13(string):\n",
    "    \"\"\"Check whether the ISBN value has 13 characters or not.\"\"\"\n",
    "    test = string.replace('-', '')\n",
    "    if len(test) == 13:\n",
    "        return string\n",
    "    return ''\n",
    "\n",
    "def disambiguate_publisher(name_string):\n",
    "    \"\"\"Look up the publisher Q ID from a list derived from a SPARQL query https://w.wiki/4pbi\"\"\"\n",
    "    # Set publisher Q ID to empty string if there's no publisher string\n",
    "    if name_string == '':\n",
    "        return ''\n",
    "    \n",
    "    best_match_score = 0\n",
    "    best_match = ''\n",
    "    best_match_label = ''\n",
    "    for qid, publisher in publishers.iterrows():  # The publishers DataFrame is a global variable\n",
    "        w_ratio = fuzz.WRatio(name_string, publisher['label'])\n",
    "        if w_ratio > best_match_score:\n",
    "            best_match = qid\n",
    "            best_match_label = publisher['label']\n",
    "            best_match_score = w_ratio\n",
    "            \n",
    "    if best_match_score < 98:\n",
    "        print('w_ratio:', best_match_score, 'Warning: poor match of: \"' + best_match_label + '\"', best_match, 'to stated publisher: \"' + name_string + '\"\\n')\n",
    "        #print('w_ratio:', best_match_score, 'Warning: poor match of: \"' + best_match_label + '\"', best_match, 'to stated publisher: \"' + name_string + '\"\\n', file=log_object)\n",
    "    return best_match\n",
    "\n",
    "def disambiguate_place_of_publication(value):\n",
    "    \"\"\"Look up place of publication Q ID from a list derived from query https://w.wiki/63Ap\n",
    "    If there is a single match, the Q ID is returned.\n",
    "    If there are no matches, the string is returned unprocessed.\n",
    "    If there are multiple matches, a dict with possible values is returned.\"\"\"\n",
    "    if value == '':\n",
    "        return ''\n",
    "    \n",
    "    if 'New York' in value:\n",
    "        return 'Q60'\n",
    "    \n",
    "    if 'New Brunswick' in value:\n",
    "        return 'Q138338'\n",
    "    \n",
    "    if 'California' in value:\n",
    "        value = value.replace('California', 'CA')\n",
    "    \n",
    "    if 'Calif' in value:\n",
    "        value = value.replace('Calif', 'CA')\n",
    "        \n",
    "    if 'Massachusetts' in value:\n",
    "        value = value.replace('Massachusetts', 'MA')\n",
    "        \n",
    "    if 'Cambridge' in value:\n",
    "        if 'Cambridge, M' in value:\n",
    "            return 'Q49111'\n",
    "        else:\n",
    "            return 'Q350'\n",
    "    \n",
    "    location_list = []\n",
    "    for qid, location in publisher_locations.iterrows():  # The publisher_locations DataFrame is a global variable\n",
    "        if location['label'] in value:\n",
    "            location_list.append({'qid': qid, 'label': location['label']})\n",
    "    if len(location_list) == 0:\n",
    "        return value\n",
    "    elif len(location_list) == 1:\n",
    "        return location_list[0]['qid']\n",
    "    else:\n",
    "        return location_list\n",
    "    \n",
    "    return value\n",
    "\n",
    "def today():\n",
    "    \"\"\"Generate the current UTC xsd:date\"\"\"\n",
    "    whole_time_string_z = datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "def set_reference(input_url):\n",
    "    \"\"\"Set any URL that is present in the field as the reference URL value.\"\"\"\n",
    "    url = include_reference_url(input_url) # Screen for suitable URLs\n",
    "    if url != '':\n",
    "        return url\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def set_stated_in(input_url):\n",
    "    \"\"\"If no URL is present, set a fixed value to be used as the stated_in value.\"\"\"\n",
    "    url = include_reference_url(input_url) # Screen for suitable URLs\n",
    "    if url == '':\n",
    "        return 'Q114403967' # Vanderbilt Divinity publications database\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# ---------------------------\n",
    "# Major processes functions\n",
    "# ---------------------------\n",
    "\n",
    "def build_function(function_name_string, passed_value_string):\n",
    "    if len(passed_value_string) == 0:\n",
    "        expression = function_name_string + \"('')\"\n",
    "    else:\n",
    "        # Hack for cases where the data string is enclosed in single quotes\n",
    "        if passed_value_string[0] == \"'\" and passed_value_string[-1] == \"'\":\n",
    "            expression = function_name_string + '(\"\"\"' + passed_value_string + '\"\"\")'\n",
    "        else:\n",
    "            expression = function_name_string + \"('''\" + passed_value_string + \"''')\"\n",
    "    output_value = eval(expression)\n",
    "    return output_value\n",
    "\n",
    "def evaluate_function(prop, work_data):\n",
    "    # The mapping function may not require an argument. In that case, there's no source column.\n",
    "    if 'source' in prop:\n",
    "        # If the source data CSV doesn't have any column named according to mappings, the output for that\n",
    "        # variable is an empty string.\n",
    "        if prop['source'] in work_data:\n",
    "            output_value = build_function(prop['value'], work_data[prop['source']])\n",
    "            if output_value == '':\n",
    "                no_value = True\n",
    "            else:\n",
    "                no_value = False\n",
    "        else:\n",
    "            output_value = ''\n",
    "            no_value = True\n",
    "    # Case where there's no argument passed to mapping function\n",
    "    else:\n",
    "        expression = prop['value'] + '()'\n",
    "        output_value = eval(expression)\n",
    "        if output_value == '':\n",
    "            no_value = True\n",
    "        else:\n",
    "            no_value = False\n",
    "    return no_value, output_value\n",
    "\n",
    "def extract_metadata(mapping, work_data):\n",
    "    \"\"\"Steps through fields described in the config file, maps them to columns in the source data, and\n",
    "    uses processing functions to transform the input data to forms required in the output table.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mapping : complex structure\n",
    "        Maps column headers (\"variable\") in the destination table to column headers (\"source\") in the source table.\n",
    "        The \"value\" key indicates the function used to determine the value to be used in the destination table.\n",
    "    work_data : dict\n",
    "        A row of data from the source data table with column headers as the keys.\n",
    "    \"\"\"\n",
    "    out_dict = {'qid': '', 'unique_identifier': work_data[mapping['constants']['unique_identifier_column']]}\n",
    "    out_dict['label_' + default_language] = work_data[mapping['constants']['label_column']]\n",
    "    out_dict['description_' + default_language] = set_description(work_data[mapping['constants']['description_code_column']])\n",
    "\n",
    "    for out_property in config['outfiles'][0]['prop_list']:\n",
    "        \n",
    "        # Find the mapping variable that matches the config property\n",
    "        for prop in mapping['properties']:\n",
    "            if prop['variable'] == out_property['variable']:\n",
    "                break\n",
    "    \n",
    "        out_field = out_property['variable']\n",
    "        out_dict[out_field + '_uuid'] = ''\n",
    "        \n",
    "        no_value, output_value = evaluate_function(prop, work_data)\n",
    "        \n",
    "        '''\n",
    "        # The mapping function may not require an argument. In that case, there's no source column.\n",
    "        if 'source' in prop:\n",
    "            # If the source data CSV doesn't have any column named according to mappings, the output for that\n",
    "            # variable is an empty string.\n",
    "            if prop['source'] in work_data:\n",
    "                output_value = build_function(prop['value'], work_data[prop['source']])\n",
    "                if output_value == '':\n",
    "                    no_value = True\n",
    "                else:\n",
    "                    no_value = False\n",
    "            else:\n",
    "                output_value = ''\n",
    "                no_value = True\n",
    "        # Case where there's no argument passed to mapping function\n",
    "        else:\n",
    "            expression = prop['value'] + '()'\n",
    "            output_value = eval(expression)\n",
    "            if output_value == '':\n",
    "                no_value = True\n",
    "            else:\n",
    "                no_value = False\n",
    "        '''\n",
    "\n",
    "        # Populate the values-related columns\n",
    "        if out_property['value_type'] == 'date':\n",
    "            out_dict[out_field + '_nodeId'] = ''\n",
    "            out_dict[out_field + '_val'] = output_value\n",
    "            out_dict[out_field + '_prec'] = ''\n",
    "\n",
    "        elif out_property['value_type'] == 'quantity':\n",
    "            out_dict[out_field + '_nodeId'] = ''\n",
    "            out_dict[out_field + '_val'] = output_value\n",
    "            if no_value:\n",
    "                out_dict[out_field + '_unit'] = ''\n",
    "            else:\n",
    "                out_dict[out_field + '_unit'] = prop['quantity_unit']\n",
    "\n",
    "        # This is not actually implemented and will generate an error if used\n",
    "        elif out_property['value_type'] == 'globecoordinate':\n",
    "            out_dict[out_field + '_nodeId'] = ''\n",
    "            out_dict[out_field + '_val'] = output_value\n",
    "            if no_value:\n",
    "                out_dict[out_field + '_long'] = ''\n",
    "                out_dict[out_field + '_prec'] = ''\n",
    "            else:\n",
    "                out_dict[out_field + '_long'] = work_data[out_field + '_long']\n",
    "                out_dict[out_field + '_prec'] = work_data[out_field + '_prec']\n",
    "\n",
    "        else:\n",
    "            out_dict[out_field] = output_value\n",
    "\n",
    "        # Populate the qualifier columns\n",
    "        for qualifier in out_property['qual']:\n",
    "            if no_value:\n",
    "                no_qual_value = True\n",
    "            else:\n",
    "                # Find the mapping variable that matches the config property\n",
    "                for qual in prop['qual']:\n",
    "                    if qual['variable'] == qualifier['variable']:\n",
    "                        break\n",
    "\n",
    "                # Skip reading a value from a source column if the function doesn't need input.\n",
    "                if 'source' in qual:\n",
    "                    expression = qual['value'] + \"('''\" + work_data[qual['source']] + \"''')\"\n",
    "                    qual_output_value = eval(expression)\n",
    "                    if qual_output_value == '':\n",
    "                        no_qual_value = True\n",
    "                    else:\n",
    "                        no_qual_value = False\n",
    "                else:\n",
    "                    no_qual_value = False\n",
    "                    expression = qual['value'] + '()'\n",
    "                    qual_output_value = eval(expression) # If this evalutes as empty string, result is same as no_ref_value = True\n",
    "\n",
    "\n",
    "            qual_field = out_field + '_' + qualifier['variable']\n",
    "            # To my knowledge, dates are the only complex types used as qualifiers (no quantities or globecoordinates).\n",
    "            if qualifier['value_type'] == 'date':\n",
    "                out_dict[qual_field + '_nodeId'] = ''\n",
    "                if no_qual_value:\n",
    "                    out_dict[qual_field + '_val'] = ''\n",
    "                else:\n",
    "                    out_dict[qual_field + '_val'] = qual_output_value\n",
    "                out_dict[qual_field + '_prec'] = ''\n",
    "            else:\n",
    "                if no_qual_value:\n",
    "                    out_dict[qual_field] = ''\n",
    "                else:\n",
    "                    out_dict[qual_field] = qual_output_value\n",
    "                \n",
    "        # Populate the reference columns\n",
    "        # There's only a hash ID column if there's at least one reference.\n",
    "        if len(out_property['ref']) > 0:\n",
    "            out_dict[out_field + '_ref1_hash'] = ''\n",
    "            \n",
    "        for reference in out_property['ref']:\n",
    "            if no_value:\n",
    "                no_ref_value = True\n",
    "            else:\n",
    "                # Find the mapping variable that matches the config property\n",
    "                for ref in prop['ref']:\n",
    "                    if ref['variable'] == reference['variable']:\n",
    "                        break\n",
    "\n",
    "                # Some functions like today() don't need input from the source table, and therefore \n",
    "                # skip reading a value from a source column.\n",
    "                if 'source' in ref:\n",
    "                    expression = ref['value'] + \"('''\" + work_data[ref['source']] + \"''')\"\n",
    "                    ref_output_value = eval(expression)\n",
    "                    if ref_output_value == '':\n",
    "                        no_ref_value = True\n",
    "                    else:\n",
    "                        no_ref_value = False\n",
    "                else:\n",
    "                    no_ref_value = False\n",
    "                    expression = ref['value'] + '()'\n",
    "                    ref_output_value = eval(expression) # If this evalutes as empty string, result is same as no_ref_value = True\n",
    "\n",
    "            ref_field = out_field + '_ref1_' + reference['variable']\n",
    "            # To my knowledge, dates are the only complex types used as qualifiers (no quantities or globecoordinates).\n",
    "            if reference['value_type'] == 'date':\n",
    "                out_dict[ref_field + '_nodeId'] = ''\n",
    "                if no_ref_value:\n",
    "                    out_dict[ref_field + '_val'] = ''\n",
    "                else:\n",
    "                    out_dict[ref_field + '_val'] = ref_output_value\n",
    "                out_dict[ref_field + '_prec'] = ''\n",
    "            else:\n",
    "                if no_ref_value:\n",
    "                    out_dict[ref_field] = ''\n",
    "                else:\n",
    "                    out_dict[ref_field] = ref_output_value\n",
    "                    \n",
    "    #print(out_dict)\n",
    "    return out_dict\n",
    "\n",
    "def disambiguate_agents(authors, pmid, coauthors):\n",
    "\n",
    "    max_pmids_to_check = 10\n",
    "    # If there is a PubMed ID for the article, retrieve the author info\n",
    "    if pmid != '':\n",
    "        pubmed_author_info = retrieve_pubmed_data(pmid)\n",
    "        print('retrieved data from PubMed ID', pmid)\n",
    "        for author_index in range(len(pubmed_author_info)):\n",
    "            pubmed_author_info[author_index]['name'] = pubmed_author_info[author_index]['forename'] + ' ' + pubmed_author_info[author_index]['surname']\n",
    "    else:\n",
    "        print('no PubMed data')\n",
    "\n",
    "    # Augment CrossRef data with PubMed data. Typically the PubMed data is more likely to have the affiliations\n",
    "    # Names are generally very similar, but vary with added or missing periods on initials and suffixes\n",
    "    if pmid != '':\n",
    "        for author_index in range(len(authors)):\n",
    "            found = False\n",
    "            crossref_name = authors[author_index]['givenName'] + ' ' + authors[author_index]['familyName']\n",
    "            #print(crossref_name)\n",
    "            for pubmed_author in pubmed_author_info:\n",
    "                ratio = fuzz.ratio(pubmed_author['name'], crossref_name)\n",
    "                #print(ratio, pubmed_author['name'])\n",
    "                if ratio > 87: # had to drop down to this level because some people with missing \"Jr\" weren't matching\n",
    "                    found = True\n",
    "                    result_string = 'fuzzy label match: ' + str(ratio) + pubmed_author['name'] + ' / ' + crossref_name\n",
    "                    #print(result_string)\n",
    "                    break\n",
    "            if not found:\n",
    "                print('Did not find a match in the PubMed data for', crossref_name)\n",
    "            else:\n",
    "                #print(pubmed_author)\n",
    "                #print(authors[author_index])\n",
    "\n",
    "                # If there is a PubMed affiliation and no affiliation in the CrossRef data, add the PubMed affiliation\n",
    "                if pubmed_author['affiliation'] != '':\n",
    "                    if len(authors[author_index]['affiliation']) == 0:\n",
    "                        authors[author_index]['affiliation'].append(pubmed_author['affiliation'])\n",
    "\n",
    "                # If there is an ORCID in PubMed and no ORCID in the CrossRef data, add the ORCID to CrossRef data\n",
    "                # Not sure how often this happens since I think maybe usually of one has it, the other does, too.\n",
    "                if pubmed_author['orcid'] != '':\n",
    "                    if authors[author_index]['orcid'] == '':\n",
    "                        authors[author_index]['orcid'] = pubmed_author['orcid']\n",
    "\n",
    "                #print(authors[author_index])\n",
    "\n",
    "            #print()\n",
    "    #print(json.dumps(pubmed_author_info, indent=2))\n",
    "\n",
    "    # Perform screening operations on authors to try to determine their Q IDs\n",
    "    found_qid_values = []\n",
    "    not_found_author_list = []\n",
    "    author_count = 1\n",
    "    for author in authors:\n",
    "        print(author_count)\n",
    "        found = False\n",
    "        \n",
    "        # First eliminate the case where all of the name pieces are empty\n",
    "        if (author['givenName'] + ' ' + author['familyName']).strip() == '':\n",
    "            break\n",
    "            \n",
    "        # Record stated_as\n",
    "        stated_as = (author['givenName'] + ' ' + author['familyName']).strip()\n",
    "            \n",
    "        # Fix case where names are stupidly in all caps\n",
    "        name_pieces = author['givenName'].strip().split(' ')\n",
    "        author['givenName'] = ' '.join(fix_all_caps(name_pieces))\n",
    "        name_pieces = author['familyName'].strip().split(' ')\n",
    "        author['familyName'] = ' '.join(fix_all_caps(name_pieces))\n",
    "        \n",
    "        # Screen for exact match to Wikidata labels\n",
    "        for index, researcher in researchers.iterrows():\n",
    "            if researcher['label_en'] == author['givenName'] + ' ' + author['familyName']:\n",
    "                found = True\n",
    "                result_string = 'researcher exact label match: ' + researcher['qid'] + ' ' + researcher['label_en']\n",
    "                name = researcher['label_en']\n",
    "                qid = researcher['qid']\n",
    "                break\n",
    "        if not found:\n",
    "            # screen for exact match to alternate names\n",
    "            for index, altname in altnames.iterrows():\n",
    "                if altname['altLabel'] == author['givenName'] + ' ' + author['familyName']:\n",
    "                    found = True\n",
    "                    result_string = 'researcher altname match: ' + altname['qid'] + ' ' + altname['altLabel']\n",
    "                    name = altname['altLabel']\n",
    "                    qid = altname['qid']\n",
    "                    break\n",
    "            if not found:\n",
    "                # If the researcher has an ORCID, see if it's at Wikidata\n",
    "                if author['orcid'] != '':\n",
    "                    hit = searchWikidataForQIdByOrcid(author['orcid'])\n",
    "                    if hit != {}:\n",
    "                        found = True\n",
    "                        result_string = 'Wikidata ORCID search: ' + hit['qid'] + ' ' + hit['label'] + ' / ' + hit['description']\n",
    "                        name = hit['label']\n",
    "                        qid = hit['qid']\n",
    "\n",
    "                if not found:\n",
    "                    # screen for fuzzy match to Wikidata-derived labels\n",
    "                    for index, researcher in researchers.iterrows():\n",
    "                        # Require the surname to match the label surname exactly\n",
    "                        split_names = find_surname_givens(researcher['label_en']) # returns False if no family name\n",
    "                        if split_names: # skip names that don't have 2 parts !!! also misses non-English labels!\n",
    "                            if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                                w_ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], researcher['label_en'])\n",
    "                                #w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], researcher['label_en'])\n",
    "                                if w_ratio > 90:\n",
    "                                    found = True\n",
    "                                    result_string = 'fuzzy label match: ' + str(w_ratio) + ' ' + researcher['qid'] + ' ' + researcher['label_en'] + ' / ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                    name = researcher['label_en']\n",
    "                                    qid = researcher['qid']\n",
    "                                    break\n",
    "                    if not found:\n",
    "                        # screen for fuzzy match to alternate names\n",
    "                        for index, altname in altnames.iterrows():\n",
    "                            split_names = find_surname_givens(altname['altLabel'])\n",
    "                            if split_names: # skip names that don't have 2 parts\n",
    "                                if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                                    w_ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], altname['altLabel'])\n",
    "                                    #w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], altname['altLabel'])\n",
    "                                    if w_ratio > 90:\n",
    "                                        found = True\n",
    "                                        result_string = 'researcher altname fuzzy match: ' + str(w_ratio) + ' ' + altname['qid'] + ' ' + altname['altLabel'] + ' / ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                        name = altname['altLabel']\n",
    "                                        qid = altname['qid']\n",
    "                                        break\n",
    "                        if not found:\n",
    "                            name = author['givenName'] + ' ' + author['familyName']\n",
    "                            print('Searching Wikidata for', name)\n",
    "                            print('researcher known affiliations: ', author['affiliation'])\n",
    "                            print()\n",
    "                            hits = search_name_at_wikidata(name)\n",
    "                            #print(hits)\n",
    "\n",
    "                            qids = []\n",
    "                            for hit in hits:\n",
    "                                qids.append(hit['qid'])\n",
    "                            return_list = screen_qids(qids, screens) # screens is a global variable loaded at the start\n",
    "                            #print(return_list)\n",
    "\n",
    "                            for hit in return_list:\n",
    "                                # Check each possible name match to the list of known co-authors/co-editors\n",
    "                                # If there is a match, then use that Q ID and quit trying to match.\n",
    "                                if hit['qid'] in list(coauthors.index):\n",
    "                                    found = True\n",
    "                                    qid = hit['qid']\n",
    "                                    result_string = 'Match with known coauthor'\n",
    "                                    \n",
    "                            if not found:\n",
    "                                # Save discovered data to return if not matched\n",
    "                                discovered_data = []\n",
    "                                for hit in return_list:                                \n",
    "                                    hit_data = hit\n",
    "                                    split_names = find_surname_givens(hit['label'])\n",
    "\n",
    "                                    # Require the surname to match the Wikidata label surname exactly\n",
    "                                    # This prevents a high fraction of fuzzy matches where the last names are similar but not the same\n",
    "                                    if split_names: # skip names that don't have 2 parts\n",
    "                                        if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                                            #print(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print(hit)\n",
    "                                            w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('w_ratio:', w_ratio)\n",
    "                                            #ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('ratio:', ratio)\n",
    "                                            #partial_ratio = fuzz.partial_ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('partial_ratio:', partial_ratio)\n",
    "                                            #token_sort_ratio = fuzz.token_sort_ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('token_sort_ratio:', token_sort_ratio)\n",
    "                                            #token_set_ratio = fuzz.token_set_ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                            #print('token_set_ratio:', token_set_ratio)\n",
    "\n",
    "                                            # This screen requires a high degree of similarity between the overall ORCID names and Wikidata labels\n",
    "                                            if w_ratio > 80:\n",
    "                                                print('Wikidata search fuzzy match:', w_ratio, author['givenName'] + ' ' + author['familyName'], ' / ', 'https://www.wikidata.org/wiki/'+ hit['qid'], hit['label'])\n",
    "                                                print('Wikidata description: ', hit['description'])\n",
    "\n",
    "                                                # Here we need to check Wikidata employer and affiliation and fuzzy match against known affiliations\n",
    "                                                occupations, employers, affiliations = search_wikidata_occ_emp_aff(hit['qid'])\n",
    "                                                print('occupations:', occupations)\n",
    "                                                hit_data['occupations'] = occupations\n",
    "                                                print('employers:', employers)\n",
    "                                                hit_data['employers'] = employers\n",
    "                                                print('affiliations', affiliations)\n",
    "                                                hit_data['affiliations'] = affiliations\n",
    "                                                print()\n",
    "\n",
    "                                                # Perform a check of the employer to make sure we didn't miss somebody in the earlier\n",
    "                                                # string matching\n",
    "                                                for employer in employers:\n",
    "                                                    if 'Vanderbilt University' in employer: # catch university and med center\n",
    "                                                        found = True\n",
    "                                                        result_string = 'Match Vanderbilt employer in Wikidata: ' + hit['qid'] + ' ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                                        qid = hit['qid']\n",
    "\n",
    "                                                # If the author doesn't have any known affiliations, there is no point in checking PubMed\n",
    "                                                if author['affiliation'] != []:\n",
    "                                                    # Search Wikidata for articles written by this match\n",
    "                                                    articles_in_wikidata = search_wikidata_article(hit['qid'])\n",
    "                                                    #print(articles_in_wikidata)\n",
    "\n",
    "                                                    # Step through articles with PubMed IDs found in Wikidata and see if the author affiliation or ORCID matches any of the articles\n",
    "                                                    check = 0\n",
    "                                                    for article_in_wikidata in articles_in_wikidata:\n",
    "                                                        if article_in_wikidata['pmid'] != '':\n",
    "                                                            check += 1\n",
    "                                                            if check > max_pmids_to_check:\n",
    "                                                                print('More articles, but stopping after checking', max_pmids_to_check)\n",
    "                                                                break # break out of article-checking loop\n",
    "                                                            print('Checking article, PMID:', article_in_wikidata['pmid'], article_in_wikidata['title'])\n",
    "                                                            pubmed_match = identified_in_pubmed(article_in_wikidata['pmid'], author['givenName'] + ' ' + author['familyName'], author['affiliation'], author['orcid'])\n",
    "                                                            if not pubmed_match:\n",
    "                                                                #print('no match')\n",
    "                                                                print()\n",
    "                                                            else:\n",
    "                                                                found = True\n",
    "                                                                result_string = 'PubMed affilation match: ' + hit['qid'] + ' ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                                                qid = hit['qid']\n",
    "                                                                break # break out of article-checking loop\n",
    "\n",
    "                                                if found:\n",
    "                                                    break # break out of hit list loop\n",
    "                                                print()\n",
    "                                                # If none of the matching criteria are met, save the data for future use\n",
    "                                                discovered_data.append(hit_data)\n",
    "\n",
    "        if not found:\n",
    "            not_found_author_list.append({'name_string': author['givenName'] + ' ' + author['familyName'], 'series_ordinal': author_count, 'possible_matches': discovered_data})\n",
    "            print('not found:', author['givenName'] + ' ' + author['familyName'])\n",
    "\n",
    "        else:\n",
    "            found_qid_values.append({'qid': qid, 'stated_as': stated_as, 'series_ordinal': author_count})\n",
    "            print(result_string)\n",
    "            for index, department in departments.iterrows():\n",
    "                if qid == department['qid']:\n",
    "                    for lindex, department_label in department_labels.iterrows():\n",
    "                        if department_label['qid'] == department['affiliation']:\n",
    "                            print(department_label['label_en'])\n",
    "                            break\n",
    "        print()\n",
    "        author_count += 1\n",
    "\n",
    "    print()\n",
    "    return found_qid_values, not_found_author_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Idiosyncratic steps that need to be done between the Zotero output and running the \"standardized\" script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step involves re-setting the Url column to use the screened URLs if the Zotero output title matches\n",
    "# the title in the screened full work CSV.\n",
    "\n",
    "source_data = file_path + 'output_examples_div_pubs.csv'\n",
    "#source_data = 'output-example_baldwinbookschaptersarticles.csv'\n",
    "works = pd.read_csv(source_data, na_filter=False, dtype = str)\n",
    "#works = works.iloc[180:225] # test for full text URL substitutions\n",
    "#works = works.iloc[125:183] # test for pub location\n",
    "#works = works.iloc[2033:2052] # test of ref screening\n",
    "#works = works.iloc[1970:1988] # good rows to test for languages\n",
    "works = works.iloc[100:120]\n",
    "\n",
    "for label, work_series in works.iterrows():\n",
    "    try:\n",
    "        # Find the row(s) in the full_works DataFrame that matches the series. There should be only one.\n",
    "        # Create a series of URL values for those rows. Since there should be only one, get the 0th value.\n",
    "        new_url = full_works.loc[full_works.index==work_series['Title'], 'Url'][0]\n",
    "        # Set a new value for the Url column in the works DataFrame using the looked-up URL.\n",
    "        works.loc[label, 'Url'] = new_url\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "works.to_csv(file_path + 'preprocessed.csv', index = False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main routine\n",
    "\n",
    "NOTE: Before continuing on after this step, you need to correct any of the publication locations that weren't determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('loading data from files')\n",
    "publishers = pd.read_csv(file_path + 'publishers.csv', na_filter=False, dtype = str)\n",
    "publishers = publishers.set_index('qid')\n",
    "\n",
    "publisher_locations = pd.read_csv(file_path + 'publisher_locations.csv', na_filter=False, dtype = str)\n",
    "publisher_locations = publisher_locations.set_index('qid')\n",
    "\n",
    "researchers = pd.read_csv(reference_file_path + 'researchers.csv', na_filter=False, dtype = str)\n",
    "altnames = pd.read_csv(reference_file_path + 'vanderbilt_wikidata_altlabels.csv', na_filter=False, dtype = str)\n",
    "departments = pd.read_csv(reference_file_path + 'departments.csv', na_filter=False, dtype = str)\n",
    "department_labels = pd.read_csv(reference_file_path + 'department_labels.csv', na_filter=False, dtype = str)\n",
    "\n",
    "works = pd.read_csv(file_path + 'preprocessed.csv', na_filter=False, dtype = str)\n",
    "\n",
    "with open(file_path + 'config.yaml', 'r') as file_object:\n",
    "    config = yaml.safe_load(file_object)\n",
    "\n",
    "with open(file_path + 'mapping.yaml', 'r') as file_object:\n",
    "    mapping = yaml.safe_load(file_object)\n",
    "\n",
    "with open(file_path + 'mapping_agents.yaml', 'r') as file_object:\n",
    "    mapping_agents = yaml.safe_load(file_object)\n",
    "    \n",
    "# screens.yaml is a configuration file that defines the kinds of screens to be performed on potential agent Q ID matches from Wikidata\n",
    "with open(file_path + 'screens.yaml', 'r') as file_object:\n",
    "    screens = yaml.safe_load(file_object)\n",
    "\n",
    "print('retrieving existing works from Wikidata')\n",
    "query_string = '''select distinct ?work ?workLabel ?doi ?pmid where {\n",
    "  {?author wdt:P1416 wd:Q7914452.} # Div school\n",
    "  union\n",
    "  {?author wdt:P1416 wd:Q114065689.} # graduate department of religion\n",
    "\n",
    "  {?work wdt:P50 ?author.} # author\n",
    "  union\n",
    "  {?work wdt:P98 ?author.} # editor\n",
    "\n",
    "  optional {\n",
    "    ?work rdfs:label ?workLabel.\n",
    "    filter(lang(?workLabel)=\"''' + default_language + '''\")\n",
    "    }\n",
    "\n",
    "  optional {?work wdt:P356 ?doi.}\n",
    "  optional {?work wdt:P698 ?pmid.}  \n",
    "  }\n",
    "'''\n",
    "\n",
    "wdqs = Sparqler(useragent=user_agent)\n",
    "query_results = wdqs.query(query_string)\n",
    "sleep(sparql_sleep)\n",
    "\n",
    "found_works = []\n",
    "for result in query_results:\n",
    "    work_dict = {}\n",
    "    #work_dict['qid'] = extract_local_name(result['work']['value'])\n",
    "    work_dict['qid'] = result['work']['value']\n",
    "    if 'workLabel' in result:\n",
    "        work_dict['label'] = result['workLabel']['value']\n",
    "    else:\n",
    "        work_dict['label'] = ''\n",
    "    if 'doi' in result:\n",
    "        work_dict['doi'] = result['doi']['value'].upper() # valid DOIs are all upper case, but could be some bad ones\n",
    "    else:\n",
    "        work_dict['doi'] = ''\n",
    "    if 'pmid' in result:\n",
    "        work_dict['pmid'] = result['pmid']['value']\n",
    "    else:\n",
    "        work_dict['pmid'] = ''\n",
    "    found_works.append(work_dict)\n",
    "existing_works_df = pd.DataFrame(found_works) # Note: qids are full IRIs\n",
    "existing_works_df.to_csv(file_path + 'existing_works_in_wikidata.csv', index = False)\n",
    "\n",
    "print('retrieving author/editor data from Wikidata')\n",
    "existing_works_qids_list = list(existing_works_df.loc[:, 'qid']) # Generate a list of work Q IDs from the qid column\n",
    "existing_works_qids_string = '>\\n<'.join(existing_works_qids_list) # Join the list into a string with one Q ID per line\n",
    "existing_works_qids_string = '<' + existing_works_qids_string + '>'\n",
    "\n",
    "query_string = '''\n",
    "select distinct ?agent ?label ?orcid where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + existing_works_qids_string + '''}\n",
    "  \n",
    "{?value wdt:P50 ?agent.}\n",
    "union\n",
    "{?value wdt:P98 ?agent.}\n",
    "\n",
    "?agent rdfs:label ?label.\n",
    "FILTER(lang(?label)=\"''' + default_language + '''\")\n",
    "\n",
    "optional {?agent wdt:P496 ?orcid.}\n",
    "\n",
    "MINUS # remove Vanderbilt Div people\n",
    "{\n",
    "  {?agent wdt:P1416 wd:Q7914452.} # Div school\n",
    "  union\n",
    "  {?agent wdt:P1416 wd:Q114065689.} # graduate department of religion\n",
    "}\n",
    "  }\n",
    "'''\n",
    "#print(query_string)\n",
    "\n",
    "wdqs = Sparqler(useragent=user_agent)\n",
    "query_results = wdqs.query(query_string)\n",
    "sleep(sparql_sleep)\n",
    "\n",
    "coauthors = []\n",
    "for result in query_results:\n",
    "    author_dict = {}\n",
    "    author_dict['qid'] = extract_local_name(result['agent']['value'])\n",
    "    if 'label' in result:\n",
    "        author_dict['label'] = result['label']['value']\n",
    "    else:\n",
    "        author_dict['label'] = ''\n",
    "    if 'orcid' in result:\n",
    "        author_dict['orcid'] = result['orcid']['value']\n",
    "    else:\n",
    "        author_dict['orcid'] = ''\n",
    "    coauthors.append(author_dict)\n",
    "coauthors = pd.DataFrame(coauthors) # NOTE: Q IDs don't include Wikidata namespace\n",
    "coauthors.to_csv(file_path + 'coauthors_from_wikidata.csv', index = False)\n",
    "\n",
    "print('done retrieving author/editor data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "works_list = []\n",
    "agents_list = []\n",
    "for index, work_data in works.iterrows():\n",
    "    print(work_data[mapping['constants']['label_column']])\n",
    "    \n",
    "    # Use the mappings to extract and process the main metadata from the source columns\n",
    "    row = extract_metadata(mapping, work_data)\n",
    "    \n",
    "    # Check whether the work is already in Wikidata\n",
    "    if work_not_found_in_wikidata(row['label_en'], row['doi'], row['pmid'], existing_works_df, verbose=True):\n",
    "        \n",
    "        agents_dict = {'unique_identifier': work_data[mapping['constants']['unique_identifier_column']]}\n",
    "\n",
    "        # For each agent type (author, editor, etc.) extract the name information\n",
    "        for agent_type in mapping_agents['sources']:\n",
    "            source_column = agent_type['source']\n",
    "            agent_structured_data = build_function(agent_type['value'], work_data[source_column])\n",
    "            agents_dict[agent_type['variable']] = json.dumps(agent_structured_data)\n",
    "\n",
    "        # Get the reference values for that work\n",
    "        has_values = False\n",
    "        for reference_type in mapping_agents['ref']:\n",
    "            no_value, output_value = evaluate_function(reference_type, work_data)\n",
    "            if no_value:\n",
    "                agents_dict[reference_type['variable']] = ''\n",
    "            else:\n",
    "                has_values = True\n",
    "                agents_dict[reference_type['variable']] = output_value\n",
    "\n",
    "        # Do not add the work to the list if there is no author or editor information\n",
    "        if has_values:\n",
    "            works_list.append(row)\n",
    "            agents_list.append(agents_dict)\n",
    "        else:\n",
    "            print('Warning! No agents associated with this work. Not added to output files.')\n",
    "    print()\n",
    "out_frame = pd.DataFrame(works_list)\n",
    "agents_frame = pd.DataFrame(agents_list)\n",
    "\n",
    "out_frame.to_csv(file_path + 'articles.csv', index = False)\n",
    "agents_frame.to_csv(file_path + 'stored_retrieved_agents.csv', index = False)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interlude\n",
    "\n",
    "After running the code above, the VanderBot script must be run on the `articles.csv` file.\n",
    "\n",
    "The following code must be run, and then run the VanderBot script again to add the author and author string data.\n",
    "\n",
    "NOTE: The coauthor screen is really effective at decreasing the amount of searching that needs to be done in Wikidata. So after a first pass at running this code, one can examine the `unidentified_people.json` file to find the obvious matches (people listed as theologians, people with unusualy names that match exactly, etc.), then add them to the `coauthors_from_wikidata.csv` file. Even though they aren't actually coauthors yet, they will become coauthors as soon as the agents data generated here are uploaded to Wikidata, and if this code cell is then re-run, those matched people will automatically get put correctly into the `author.csv` or `editor.csv` file. This is much simpler than trying to manually move them from `author_strings.csv` or to manually enter them in `editor.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "todays_date = today()\n",
    "\n",
    "# Create empty data structure to hold output data for each agent type as it is generated\n",
    "agents_dict = {}\n",
    "for agent_type in mapping_agents['sources']:\n",
    "    agents_dict[agent_type['variable']] = []\n",
    "\n",
    "# Load existing data if any (primarily if script crashes and has to be rerun)\n",
    "#agents_dict['author'] = read_dicts_from_csv(file_path + 'authors.csv')\n",
    "#agents_dict['editor'] = read_dicts_from_csv(file_path + 'editors.csv')\n",
    "author_strings_list = []\n",
    "#author_strings_list = read_dicts_from_csv(file_path + 'author_strings.csv')\n",
    "unidentified = []\n",
    "\n",
    "# Open the file containing known co-authors/co-editors\n",
    "coauthors = pd.read_csv(file_path + 'coauthors_from_wikidata.csv', na_filter=False, dtype = str)\n",
    "coauthors = coauthors.set_index('qid')\n",
    "\n",
    "# Open the file containing the stored data about authors and editors retrieved from the data source\n",
    "stored_retrieved_agents = pd.read_csv(file_path + 'stored_retrieved_agents.csv', na_filter=False, dtype = str)\n",
    "stored_retrieved_agents = stored_retrieved_agents.set_index('unique_identifier')\n",
    "\n",
    "# Open the article items file after upload in order to get the Q IDs for the newly written articles\n",
    "articles = pd.read_csv(file_path + 'articles.csv', na_filter=False, dtype = str)\n",
    "articles = articles.set_index('unique_identifier')\n",
    "\n",
    "\n",
    "#for article in articles:\n",
    "for article_unique_identifier, article in articles.iterrows():\n",
    "    qid = article['qid']\n",
    "    doi = article['doi']\n",
    "    pmid = article['pmid']\n",
    "    print(qid, article_unique_identifier)\n",
    "    unidentified_for_article = {'qid': 'https://wikidata.org/entity/' + qid, 'unique_identifier': article_unique_identifier}\n",
    "    \n",
    "    # NOTE: in order for this lookup to work, the unique_identifier for the work must actually be unique in the\n",
    "    # articles table.\n",
    "    article_agents = stored_retrieved_agents.loc[article_unique_identifier] # result is a Series if unique\n",
    "    \n",
    "    unidentifieds_exist = False\n",
    "    for agent_type in mapping_agents['sources']:\n",
    "        agent_type_name = agent_type['variable']\n",
    "\n",
    "        # Disambiguate agents against existing Wikidata people items\n",
    "        found_agent_qids, author_name_strings = disambiguate_agents(json.loads(article_agents[agent_type_name]), pmid, coauthors)\n",
    "\n",
    "        # Add data about unidentified people with possible Q ID matches to the list for further work.\n",
    "        if len(author_name_strings) != 0:\n",
    "            unidentifieds_exist = True\n",
    "        unidentified_for_article[agent_type_name] = author_name_strings\n",
    "\n",
    "        # Create a list of dictionaries\n",
    "        for agent in found_agent_qids:\n",
    "            out_dict = {}\n",
    "            out_dict['qid'] = qid\n",
    "            out_dict['label_' + default_language] = article['label_' + default_language]\n",
    "            out_dict[agent_type_name + '_uuid'] = ''\n",
    "            out_dict[agent_type_name] = agent['qid']\n",
    "            \n",
    "            out_dict[agent_type_name + '_stated_as'] = agent['stated_as']\n",
    "\n",
    "            if mapping_agents['constants']['suppress_series_ordinal']:\n",
    "                out_dict[agent_type_name + '_series_ordinal'] = ''\n",
    "            else:\n",
    "                out_dict[agent_type_name + '_series_ordinal'] = agent['series_ordinal']\n",
    "                        \n",
    "            # Loop through all of the reference types specified in the agents mapping file\n",
    "            out_dict[agent_type_name + '_ref1_hash'] = ''\n",
    "            for reference_type in mapping_agents['ref']:\n",
    "                out_dict[agent_type_name + '_ref1_' + reference_type['variable']] = article_agents[reference_type['variable']]\n",
    "            out_dict[agent_type_name + '_ref1_retrieved_nodeId'] = ''\n",
    "            out_dict[agent_type_name + '_ref1_retrieved_val'] = todays_date\n",
    "            out_dict[agent_type_name + '_ref1_retrieved_prec'] = ''\n",
    "            agents_dict[agent_type_name].append(out_dict)\n",
    "\n",
    "        if len(agents_dict[agent_type_name]) > 0:\n",
    "            # Convert list of dicts to DataFrame and save\n",
    "            output_dataframe = pd.DataFrame(agents_dict[agent_type_name])\n",
    "            output_dataframe.to_csv(file_path + file_path + agent_type_name + '.csv', index = False)\n",
    "\n",
    "        # Special treatment for authors since only authors have a \"name string property\"\n",
    "        if agent_type_name == 'author':\n",
    "            for author in author_name_strings:\n",
    "                out_dict = {}\n",
    "                out_dict['qid'] = qid\n",
    "                out_dict['label_' + default_language] = article['label_' + default_language]\n",
    "                out_dict['author_string_uuid'] = ''\n",
    "                out_dict['author_string'] = author['name_string']\n",
    "            \n",
    "                if mapping_agents['constants']['suppress_series_ordinal']:\n",
    "                    out_dict['author_string_series_ordinal'] = ''\n",
    "                else:\n",
    "                    out_dict['author_string_series_ordinal'] = author['series_ordinal']\n",
    "                    \n",
    "                out_dict['author_string_ref1_hash'] = ''\n",
    "                for reference_type in mapping_agents['ref']:\n",
    "                    out_dict['author_string_ref1_' + reference_type['variable']] = article_agents[reference_type['variable']]\n",
    "                out_dict['author_string_ref1_retrieved_nodeId'] = ''\n",
    "                out_dict['author_string_ref1_retrieved_val'] = todays_date\n",
    "                out_dict['author_string_ref1_retrieved_prec'] = ''\n",
    "                author_strings_list.append(out_dict)\n",
    "\n",
    "            #print(author_strings_list)\n",
    "            if len(author_strings_list) > 0:\n",
    "                # Convert list of dicts to DataFrame and save\n",
    "                output_dataframe = pd.DataFrame(author_strings_list)\n",
    "                output_dataframe.to_csv(file_path + 'author_strings.csv', index = False)\n",
    "\n",
    "    #if not(unidentified_for_article['author'] == [] and unidentified_for_article['editor'] == []):\n",
    "    if unidentifieds_exist:\n",
    "        unidentified.append(unidentified_for_article)\n",
    "        \n",
    "    # Save the potential author and editor matches in a file\n",
    "    # Save after each article in case of crash; maybe later just write at end\n",
    "    with open(file_path + 'unidentified_people.json', 'wt', encoding='utf-8') as file_object:\n",
    "        file_object.write(json.dumps(unidentified, indent=2))\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
