{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# publoader.ipynb, a Python script for uploading files and data to Wikimedia Commons using the API.\n",
    "\n",
    "# (c) 2022 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "# ----------------\n",
    "# Global variables\n",
    "# ----------------\n",
    "\n",
    "script_version = '0.0.1'\n",
    "default_language = 'en'\n",
    "precision_cutoff = 0.95\n",
    "phrase_length_cutoff = 2\n",
    "sparql_sleep = 0.1 # minimal delay between SPARQL queries\n",
    "names_separator = ';'\n",
    "name_part_separator = ',' # Set to empty string if names aren't reversed\n",
    "\n",
    "# The user_agent string identifies this application to Wikimedia APIs.\n",
    "# If you modify this script, you need to change the user-agent string to something else!\n",
    "user_agent = 'PubLoader/' + script_version + ' (mailto:steve.baskauf@vanderbilt.edu)'\n",
    "\n",
    "# !!! Need to set up an error log!\n",
    "\n",
    "# ----------------\n",
    "# Module imports\n",
    "# ----------------\n",
    "\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from langdetect import detect_langs\n",
    "import re # regex\n",
    "\n",
    "full_works = pd.read_csv('full_work_div_pub.csv', na_filter=False, dtype = str)\n",
    "full_works = full_works.set_index('Title')\n",
    "\n",
    "language_qid = {\n",
    "    'en': 'Q1860',\n",
    "    'de': 'Q188',\n",
    "    'fr': 'Q150',\n",
    "    'es': 'Q1321',\n",
    "    'it': 'Q652',\n",
    "    'nl': 'Q7411',\n",
    "    'zh': 'Q7850',\n",
    "    'no': 'Q9043',\n",
    "    'ar': 'Q13955',\n",
    "    'he': 'Q9288',\n",
    "    'pt': 'Q5146'\n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "# List of known work types used by CrossRef\n",
    "work_types = [\n",
    "    {\n",
    "    'type_string': 'journal-article',\n",
    "    'qid': 'Q18918145', # academic journal article, alternatively Q13442814 scholarly article\n",
    "    'description': 'journal article'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'book',\n",
    "    'qid': 'Q3331189', # \"version, edition, or translation\"\n",
    "    'description': 'book'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'book-chapter',\n",
    "    'qid': 'Q21481766', # \"academic chapter\"\n",
    "    'description': 'academic book chapter'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'monograph',\n",
    "    'qid': 'Q193495', # monograph\n",
    "    'description': 'monograph'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'reference-book',\n",
    "    'qid': 'Q5292', # encyclopedia\n",
    "    'description': 'encyclopedia'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'reference-entry',\n",
    "    'qid': 'Q13433827', # encyclopedia article, some are handbook articles\n",
    "    'description': 'encyclopedia article'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'dataset',\n",
    "    'qid': 'Q13433827', # also used for encyclopedia articles\n",
    "    'description': 'encyclopedia article'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'other',\n",
    "    'qid': 'Q55915575', # \n",
    "    'description': 'scholarly work'\n",
    "    }\n",
    "]\n",
    "\"\"\"\n",
    "\n",
    "# List of known work types used by Zotero\n",
    "work_types = [\n",
    "    {\n",
    "    'type_string': 'journalArticle',\n",
    "    'qid': 'Q18918145', # academic journal article, alternatively Q13442814 scholarly article\n",
    "    'description': 'journal article'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'book',\n",
    "    'qid': 'Q3331189', # \"version, edition, or translation\"\n",
    "    'description': 'book'\n",
    "    },\n",
    "    {\n",
    "    'type_string': 'bookSection',\n",
    "    'qid': 'Q21481766', # \"academic chapter\"\n",
    "    'description': 'academic book chapter'\n",
    "    }\n",
    "]\n",
    "\n",
    "url_exclusion_strings = [\n",
    "    'login',\n",
    "    'proxy',\n",
    "    #'search.proquest.com',\n",
    "    'worldcat',\n",
    "    'wp-content',\n",
    "    'site.ebrary.com',\n",
    "    'cro3.org/',\n",
    "    'worldbookonline.com/pl/infofinder'\n",
    "]\n",
    "\n",
    "url_inclusion_strings = [\n",
    "    'doi',\n",
    "    'jstor',\n",
    "    #'oxfordjournals.org/content',\n",
    "    'article',\n",
    "    'academia.edu',\n",
    "    'content',\n",
    "    'proquest.com/docview',\n",
    "    'handle'\n",
    "]\n",
    "\n",
    "# ------------------------\n",
    "# Utility functions\n",
    "# ------------------------\n",
    "\n",
    "def extract_local_name(iri):\n",
    "    \"\"\"Extract the local name part of an IRI, e.g. a Q ID from a Wikidata IRI\"\"\"\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[-1]\n",
    "\n",
    "url_pattern = \"^https?:\\\\/\\\\/(?:www\\\\.)?[-a-zA-Z0-9@:%._\\\\+~#=]{1,256}\\\\.[a-zA-Z0-9()]{1,6}\\\\b(?:[-a-zA-Z0-9()@:%_\\\\+.~#?&\\\\/=]*)$\"\n",
    "\n",
    "def include_reference_url(url):\n",
    "    \"\"\"Returned strings are suitable to use for references. Currently it's assumed that the criteria are the\n",
    "    same for full work available.\"\"\"\n",
    "    url = url.lower() # convert to all lowercase\n",
    "    \n",
    "    # Exclude invalid URLs\n",
    "    if re.match(url_pattern, url) is None:\n",
    "        return ''\n",
    "\n",
    "    # If the URL matches one of the pre-screened URLs, use it\n",
    "    matched_series = full_works.loc[full_works['Url']==url, 'Url']\n",
    "    # matched_series will be a Series composed of all values in the Url column that match. There should be 1 or 0.\n",
    "    if len(matched_series) == 1:\n",
    "        return url\n",
    "    \n",
    "    # Exclude any URLs containing strings that indicate a login is required\n",
    "    for screening_string in url_exclusion_strings:\n",
    "        if screening_string in url:\n",
    "            return ''\n",
    "        \n",
    "    # Must contain one of the strings that indicate metadata and possible acces\n",
    "    for screening_string in url_inclusion_strings:\n",
    "        if screening_string in url:\n",
    "            return url\n",
    "        \n",
    "    return ''\n",
    "\n",
    "def roman_integer_value(r):\n",
    "    \"\"\"Return value of Roman numeral symbol.\n",
    "    \n",
    "    Note:\n",
    "    -----\n",
    "    Code from https://www.geeksforgeeks.org/python-program-for-converting-roman-numerals-to-decimal-lying-between-1-to-3999/\"\"\"    \n",
    "    if (r == 'I'):\n",
    "        return 1\n",
    "    if (r == 'V'):\n",
    "        return 5\n",
    "    if (r == 'X'):\n",
    "        return 10\n",
    "    if (r == 'L'):\n",
    "        return 50\n",
    "    if (r == 'C'):\n",
    "        return 100\n",
    "    if (r == 'D'):\n",
    "        return 500\n",
    "    if (r == 'M'):\n",
    "        return 1000\n",
    "    return -1\n",
    "\n",
    "def roman_to_decimal(numeral):\n",
    "    \"\"\"Convert Roman numerals to integers.\n",
    "    \n",
    "    Note:\n",
    "    -----\n",
    "    Code from https://www.geeksforgeeks.org/python-program-for-converting-roman-numerals-to-decimal-lying-between-1-to-3999/\"\"\"\n",
    "    str = numeral.upper()\n",
    "    res = 0\n",
    "    i = 0\n",
    "\n",
    "    while (i < len(str)):\n",
    "\n",
    "        # Getting value of symbol s[i]\n",
    "        s1 = roman_integer_value(str[i])\n",
    "        \n",
    "        # Return a negative number if error.\n",
    "        if s1 < 0:\n",
    "            return -1\n",
    "\n",
    "        if (i + 1 < len(str)):\n",
    "\n",
    "            # Getting value of symbol s[i + 1]\n",
    "            s2 = roman_integer_value(str[i + 1])\n",
    "            \n",
    "            # Return a negative number if error.\n",
    "            if s2 < 0:\n",
    "                return -1\n",
    "\n",
    "            # Comparing both values\n",
    "            if (s1 >= s2):\n",
    "\n",
    "                # Value of current symbol is greater\n",
    "                # or equal to the next symbol\n",
    "                res = res + s1\n",
    "                i = i + 1\n",
    "            else:\n",
    "\n",
    "                # Value of current symbol is greater\n",
    "                # or equal to the next symbol\n",
    "                res = res + s2 - s1\n",
    "                i = i + 2\n",
    "        else:\n",
    "            res = res + s1\n",
    "            i = i + 1\n",
    "\n",
    "    return res\n",
    "\n",
    "def title_if_no_lowercase(string):\n",
    "    \"\"\"Changes to titlecase only if there are no lowercase letters in the string.\"\"\"\n",
    "    lower = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    is_lower = False\n",
    "    for letter in string:\n",
    "        if letter in lower:\n",
    "            is_lower = True\n",
    "    if is_lower:\n",
    "        return string\n",
    "    else:\n",
    "        return string.title()\n",
    "\n",
    "def fix_all_caps(name_pieces):\n",
    "    \"\"\"Input is a list of name strings from name split by spaces\"\"\"\n",
    "    clean_pieces = []\n",
    "    for piece in name_pieces:\n",
    "        # Special handing for names starting with apostrophe-based prefixes\n",
    "        apostrophe_list = [\"van't\", \"'t\", \"O'\", \"D'\", \"d'\", \"N'\"]\n",
    "        apostrophe_prefix = ''\n",
    "        for possible_apostrophe_prefix in apostrophe_list:\n",
    "            if possible_apostrophe_prefix in piece:\n",
    "                # Remove prefix\n",
    "                piece = piece.replace(possible_apostrophe_prefix, '')\n",
    "                apostrophe_prefix = possible_apostrophe_prefix\n",
    "        \n",
    "        # Special handling for name parts that are lowercase\n",
    "        lower_case_list = ['von', 'de', 'van', 'la', 'der']\n",
    "        if piece.lower() in lower_case_list:\n",
    "            piece = piece.lower()\n",
    "        else:\n",
    "            # Special handling for hyphenated names; doesn't work for an edge case with more than 2 hyphens\n",
    "            if '-' in piece:\n",
    "                halves = piece.split('-')\n",
    "                piece = title_if_no_lowercase(halves[0]) + '-' + title_if_no_lowercase(halves[1])\n",
    "            else:\n",
    "                piece = title_if_no_lowercase(piece)\n",
    "        \n",
    "        # put any apostrophe prefix back on the front\n",
    "        if apostrophe_prefix:\n",
    "            piece = apostrophe_prefix + piece\n",
    "        \n",
    "        clean_pieces.append(piece)\n",
    "    return clean_pieces\n",
    "  \n",
    "def extract_name_pieces(name):\n",
    "    \"\"\"add description here\"\"\"\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods, sometimes periods are close up with no spaces\n",
    "    name = name.replace('.', ' ')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "        \n",
    "    # Fix stupid situation where name is written in ALL CAPS\n",
    "    pieces = fix_all_caps(pieces)\n",
    "    return pieces, suffix\n",
    "    \n",
    "def extract_names_from_list(names_string):\n",
    "    \"\"\"Extract multiple authors from a character-separated list in a single string.\"\"\"\n",
    "    if names_string == '':\n",
    "        return []\n",
    "    \n",
    "    names_list = names_string.split(names_separator)\n",
    "    \n",
    "    output_list = []\n",
    "    # If names are last name first\n",
    "    if name_part_separator:\n",
    "        for name in names_list:\n",
    "            pieces = name.split(name_part_separator)\n",
    "            if len(pieces) == 1: # an error, name wasn't reversed\n",
    "                print('Name error:', names_string)\n",
    "            elif len(pieces) == 2: # no Jr.\n",
    "                surname_pieces, suffix = extract_name_pieces(pieces[0].strip())\n",
    "                given_pieces, dummy = extract_name_pieces(pieces[1].strip())\n",
    "            elif len(pieces) == 3: # has Jr.\n",
    "                # Note Jr. is handled inconsistently, sometimes placed after entire name, sometimes after surname\n",
    "                if 'Jr' in pieces[2]:\n",
    "                    surname_pieces, suffix = extract_name_pieces(pieces[0].strip() + ', ' + pieces[2].strip())\n",
    "                    given_pieces, dummy = extract_name_pieces(pieces[1].strip())\n",
    "                else:\n",
    "                    surname_pieces, suffix = extract_name_pieces(pieces[0].strip() + ', ' + pieces[1].strip())\n",
    "                    given_pieces, dummy = extract_name_pieces(pieces[2].strip())                    \n",
    "            else:\n",
    "                print('Name error:', names_string)\n",
    "                \n",
    "            surname = ' '.join(surname_pieces)\n",
    "            given = ' '.join(given_pieces)\n",
    "            output_list.append({'orcid': '', 'givenName': given, 'familyName': surname, 'suffix': suffix, 'affiliation': []})\n",
    "    else:\n",
    "        pass # need to write code for case where they aren't reversed\n",
    "        \n",
    "    \n",
    "    return output_list\n",
    "\n",
    "# ------------------------\n",
    "# SPARQL query class\n",
    "# ------------------------\n",
    "\n",
    "# This is a condensed version of the more full-featured script at \n",
    "# https://github.com/HeardLibrary/digital-scholarship/blob/master/code/wikidata/sparqler.py\n",
    "# It includes only the method for the query form.\n",
    "\n",
    "class Sparqler:\n",
    "    \"\"\"Build SPARQL queries of various sorts\n",
    "\n",
    "    Parameters\n",
    "    -----------\n",
    "    useragent : str\n",
    "        Required if using the Wikidata Query Service, otherwise optional.\n",
    "        Use the form: appname/v.v (URL; mailto:email@domain.com)\n",
    "        See https://meta.wikimedia.org/wiki/User-Agent_policy\n",
    "    endpoint: URL\n",
    "        Defaults to Wikidata Query Service if not provided.\n",
    "    method: str\n",
    "        Possible values are \"post\" (default) or \"get\". Use \"get\" if read-only query endpoint.\n",
    "        Must be \"post\" for update endpoint.\n",
    "    sleep: float\n",
    "        Number of seconds to wait between queries. Defaults to 0.1\n",
    "        \n",
    "    Required modules:\n",
    "    -------------\n",
    "    requests, datetime, time\n",
    "    \"\"\"\n",
    "    def __init__(self, method='post', endpoint='https://query.wikidata.org/sparql', useragent=None, sleep=0.1):\n",
    "        # attributes for all methods\n",
    "        self.http_method = method\n",
    "        self.endpoint = endpoint\n",
    "        if useragent is None:\n",
    "            if self.endpoint == 'https://query.wikidata.org/sparql':\n",
    "                print('You must provide a value for the useragent argument when using the Wikidata Query Service.')\n",
    "                print()\n",
    "                raise KeyboardInterrupt # Use keyboard interrupt instead of sys.exit() because it works in Jupyter notebooks\n",
    "        self.sleep = sleep\n",
    "\n",
    "        self.requestheader = {}\n",
    "        if useragent:\n",
    "            self.requestheader['User-Agent'] = useragent\n",
    "        \n",
    "        if self.http_method == 'post':\n",
    "            self.requestheader['Content-Type'] = 'application/x-www-form-urlencoded'\n",
    "\n",
    "    def query(self, query_string, form='select', verbose=False, **kwargs):\n",
    "        \"\"\"Send a SPARQL query to the endpoint.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        form : str\n",
    "            The SPARQL query form.\n",
    "            Possible values are: \"select\" (default), \"ask\", \"construct\", and \"describe\".\n",
    "        mediatype: str\n",
    "            The response media type (MIME type) of the query results.\n",
    "            Some possible values for \"select\" and \"ask\" are: \"application/sparql-results+json\" (default) and \"application/sparql-results+xml\".\n",
    "            Some possible values for \"construct\" and \"describe\" are: \"text/turtle\" (default) and \"application/rdf+xml\".\n",
    "            See https://docs.aws.amazon.com/neptune/latest/userguide/sparql-media-type-support.html#sparql-serialization-formats-neptune-output\n",
    "            for response serializations supported by Neptune.\n",
    "        verbose: bool\n",
    "            Prints status when True. Defaults to False.\n",
    "        default: list of str\n",
    "            The graphs to be merged to form the default graph. List items must be URIs in string form.\n",
    "            If omitted, no graphs will be specified and default graph composition will be controlled by FROM clauses\n",
    "            in the query itself. \n",
    "            See https://www.w3.org/TR/sparql11-query/#namedGraphs and https://www.w3.org/TR/sparql11-protocol/#dataset\n",
    "            for details.\n",
    "        named: list of str\n",
    "            Graphs that may be specified by IRI in a query. List items must be URIs in string form.\n",
    "            If omitted, named graphs will be specified by FROM NAMED clauses in the query itself.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        If the form is \"select\" and mediatype is \"application/json\", a list of dictionaries containing the data.\n",
    "        If the form is \"ask\" and mediatype is \"application/json\", a boolean is returned.\n",
    "        If the mediatype is \"application/json\" and an error occurs, None is returned.\n",
    "        For other forms and mediatypes, the raw output is returned.\n",
    "\n",
    "        Notes\n",
    "        -----\n",
    "        To get UTF-8 text in the SPARQL queries to work properly, send URL-encoded text rather than raw text.\n",
    "        That is done automatically by the requests module for GET. I guess it also does it for POST when the\n",
    "        data are sent as a dict with the urlencoded header. \n",
    "        See SPARQL 1.1 protocol notes at https://www.w3.org/TR/sparql11-protocol/#query-operation        \n",
    "        \"\"\"\n",
    "        query_form = form\n",
    "        if 'mediatype' in kwargs:\n",
    "            media_type = kwargs['mediatype']\n",
    "        else:\n",
    "            if query_form == 'construct' or query_form == 'describe':\n",
    "            #if query_form == 'construct':\n",
    "                media_type = 'text/turtle'\n",
    "            else:\n",
    "                media_type = 'application/sparql-results+json' # default for SELECT and ASK query forms\n",
    "        self.requestheader['Accept'] = media_type\n",
    "            \n",
    "        # Build the payload dictionary (query and graph data) to be sent to the endpoint\n",
    "        payload = {'query' : query_string}\n",
    "        if 'default' in kwargs:\n",
    "            payload['default-graph-uri'] = kwargs['default']\n",
    "        \n",
    "        if 'named' in kwargs:\n",
    "            payload['named-graph-uri'] = kwargs['named']\n",
    "\n",
    "        if verbose:\n",
    "            print('querying SPARQL endpoint')\n",
    "\n",
    "        start_time = datetime.now()\n",
    "        if self.http_method == 'post':\n",
    "            response = requests.post(self.endpoint, data=payload, headers=self.requestheader)\n",
    "        else:\n",
    "            response = requests.get(self.endpoint, params=payload, headers=self.requestheader)\n",
    "        elapsed_time = (datetime.now() - start_time).total_seconds()\n",
    "        self.response = response.text\n",
    "        sleep(self.sleep) # Throttle as a courtesy to avoid hitting the endpoint too fast.\n",
    "\n",
    "        if verbose:\n",
    "            print('done retrieving data in', int(elapsed_time), 's')\n",
    "\n",
    "        if query_form == 'construct' or query_form == 'describe':\n",
    "            return response.text\n",
    "        else:\n",
    "            if media_type != 'application/sparql-results+json':\n",
    "                return response.text\n",
    "            else:\n",
    "                try:\n",
    "                    data = response.json()\n",
    "                except:\n",
    "                    return None # Returns no value if an error. \n",
    "\n",
    "                if query_form == 'select':\n",
    "                    # Extract the values from the response JSON\n",
    "                    results = data['results']['bindings']\n",
    "                else:\n",
    "                    results = data['boolean'] # True or False result from ASK query \n",
    "                return results           \n",
    "\n",
    "# ------------------------\n",
    "# mapping functions\n",
    "# ------------------------\n",
    "\n",
    "def identity(value):\n",
    "    \"\"\"Return the value argument with any leading and trailing whitespace removed.\"\"\"\n",
    "    return value.strip()\n",
    "\n",
    "def set_instance_of(string):\n",
    "    \"\"\"Match the type string with possible types for the data source and return the type Q ID.\"\"\"\n",
    "    if string == '':\n",
    "        return ''\n",
    "\n",
    "    for work_type in work_types:\n",
    "        if string == work_type['type_string']:\n",
    "            return work_type['qid']\n",
    "\n",
    "    print('Did not find datatype for type:', string)\n",
    "    return ''\n",
    "\n",
    "def set_description(string):\n",
    "    \"\"\"Match the type string with possible types for the data source and return the description.\"\"\"\n",
    "    if string == '':\n",
    "        return ''\n",
    "    \n",
    "    for work_type in work_types:\n",
    "        if string == work_type['type_string']:\n",
    "            return work_type['description']\n",
    "\n",
    "    print('Did not find datatype for type:', string)\n",
    "    return ''\n",
    "\n",
    "def detect_language(string):\n",
    "    \"\"\"Detect the language of the label.\"\"\"\n",
    "    if string == '':\n",
    "        return ''\n",
    "    try:\n",
    "        lang_list = detect_langs(string)\n",
    "        lang_string = str(lang_list[0])\n",
    "        confidence = float(lang_string[3:])\n",
    "        lang = lang_string[:2]\n",
    "    except: #exceptions occur when no info to decide, e.g. numbers\n",
    "        lang = 'zxx'\n",
    "        confidence = float(0)\n",
    "    if confidence < precision_cutoff:\n",
    "        print('Warning: language confidence for', lang, 'below', precision_cutoff, ':', confidence)\n",
    "    if lang in language_qid:\n",
    "        return language_qid[lang]\n",
    "    else:\n",
    "        print('Warning: detected language', lang, 'not in list of known languages.')\n",
    "        return ''\n",
    "\n",
    "def title_en(string):\n",
    "    \"\"\"Detect the language of the label.\"\"\"\n",
    "    if string == '':\n",
    "        return ''\n",
    "    try:\n",
    "        lang_list = detect_langs(string)\n",
    "        lang_string = str(lang_list[0])\n",
    "        confidence = float(lang_string[3:])\n",
    "        lang = lang_string[:2]\n",
    "    except: #exceptions occur when no info to decide, e.g. numbers\n",
    "        lang = 'zxx'\n",
    "        confidence = float(0)\n",
    "    if lang == 'en':\n",
    "        return string\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "    \n",
    "def calculate_pages(range):\n",
    "    \"\"\"Calculate the number of pages from the page range.\n",
    "    \n",
    "    Note\n",
    "    ----\n",
    "    Supports properly formatted Roman numerals and doesn't care about whitespace.\"\"\"\n",
    "    if range == '':\n",
    "        return ''\n",
    "    numbers = range.split('-')\n",
    "    \n",
    "    # If there is only a single number or an empty cell, return the empty string.\n",
    "    if len(numbers) < 2:\n",
    "        return ''\n",
    "    # Edge case where it isn't a well-formed range and has multiple hyphens\n",
    "    if len(numbers) > 2:\n",
    "        return ''\n",
    "    \n",
    "    # Step through the two numbers to try to convert them from Roman numerals if not integers.\n",
    "    for index, number in enumerate(numbers):\n",
    "        number = number.strip()\n",
    "        if not number.isnumeric():\n",
    "            numbers[index] = roman_to_decimal(number)\n",
    "            \n",
    "            # Will return -1 error if it contains characters not valid for Roman numerals \n",
    "            if numbers[index] < 0:\n",
    "                return ''\n",
    "    \n",
    "    number_pages = int(numbers[1]) - int(numbers[0]) + 1 # Need to add one since first page in range counts\n",
    "    if number_pages < 1:\n",
    "        return ''\n",
    "    return str(number_pages)\n",
    "    \n",
    "    return value\n",
    "\n",
    "def clean_doi(value):\n",
    "    \"\"\"Turn DOI into uppercase and remove leading and trailing whitespace.\"\"\"\n",
    "    cleaned_value = value.upper().strip()\n",
    "    return cleaned_value\n",
    "\n",
    "def disambiguate_published_in(value):\n",
    "    \"\"\"Use the value in the ISSN column to try to find the containing work.\n",
    "    \n",
    "    Note:\n",
    "    -----\n",
    "    For journal articles, this performs a legitimate WQS search for the journal title using the ISSN.\n",
    "    For book chapters, the ISSN column may contain the Q ID of the containing book, inserted there during\n",
    "    a pre-processing step (a hack, but typically books would not have an ISSN and this column would be empty).\"\"\"\n",
    "    if value == '':\n",
    "        return value\n",
    "    \n",
    "    # The value is a Q ID and was determined during a pre-processing step (i.e. for book chapters)\n",
    "    if value[0] == 'Q':\n",
    "        return value\n",
    "\n",
    "    # Look up the ISSN from CrossRef in Wikidata\n",
    "    # Build query string\n",
    "    query_string = '''select distinct ?container ?containerLabel where {\n",
    "      ?container wdt:P236 \"''' + value + '''\".\n",
    "      optional {\n",
    "      ?container rdfs:label ?containerLabel.\n",
    "      filter(lang(?containerLabel)=\"''' + default_language + '''\")\n",
    "      }\n",
    "    }'''\n",
    "    #print(query_string)\n",
    "\n",
    "    wdqs = Sparqler(useragent=user_agent)\n",
    "    query_results = wdqs.query(query_string)\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    if len(query_results) == 0:\n",
    "        return ''\n",
    "\n",
    "    # !!!!!!!!!!!!!!!!!! Enable this code when the error log is set up\n",
    "    \"\"\"\n",
    "    if len(query_results) > 1:\n",
    "        print('Warning! More than one container in Wikidata matched the ISSN ', file=log_object)\n",
    "        print(query_results, '\\n', file=log_object)\n",
    "    \"\"\"\n",
    "    # Extract Q ID from SPARQL query results. If there is more than one result, the last one will be used for the Q ID\n",
    "    for result in query_results:\n",
    "        container_qid = extract_local_name(result['container']['value'])\n",
    "        # Skipping this since container name isn't passed into the function.\n",
    "        \"\"\"\n",
    "        journal_name = result['containerLabel']['value']\n",
    "        if journal_name != crossref_results['journal_title']:\n",
    "            # NOTE: did empirical testing to see which kind of fuzzy matching worked best\n",
    "            #ratio = fuzz.ratio(journal_name, crossref_results['journal_title'])\n",
    "            #partial_ratio = fuzz.partial_ratio(journal_name, crossref_results['journal_title'])\n",
    "            #sort_ratio = fuzz.token_sort_ratio(journal_name, crossref_results['journal_title'])\n",
    "            #set_ratio = fuzz.token_set_ratio(journal_name, crossref_results['journal_title'])\n",
    "            w_ratio = fuzz.WRatio(journal_name, crossref_results['journal_title'])\n",
    "            #print('name similarity ratio', ratio)\n",
    "            #print('partial ratio', partial_ratio)\n",
    "            #print('sort_ratio', sort_ratio)\n",
    "            #print('set_ratio', set_ratio)\n",
    "            if w_ratio < 99:\n",
    "                print('article:', crossref_results['label_' + default_language], 'w_ratio:', w_ratio, 'Warning: Wikidata journal: \"' + journal_name + '\"', journal_qid, 'does not match CrossRef journal title: \"' + crossref_results['journal_title'] + '\"\\n', file=log_object)\n",
    "        #print('article:', crossref_results['label_' + default_language], 'journal:', journal_qid, journal_name)\n",
    "        \"\"\"\n",
    "    return container_qid\n",
    "\n",
    "def isbn10(string):\n",
    "    \"\"\"Check whether the ISBN value has 10 characters or not.\"\"\"\n",
    "    test = string.replace('-', '')\n",
    "    if len(test) == 10:\n",
    "        return string\n",
    "    return ''\n",
    "\n",
    "def isbn13(string):\n",
    "    \"\"\"Check whether the ISBN value has 13 characters or not.\"\"\"\n",
    "    test = string.replace('-', '')\n",
    "    if len(test) == 13:\n",
    "        return string\n",
    "    return ''\n",
    "\n",
    "def disambiguate_publisher(name_string):\n",
    "    \"\"\"Look up the publisher Q ID from a list derived from a SPARQL query https://w.wiki/4pbi\"\"\"\n",
    "    # Set publisher Q ID to empty string if there's no publisher string\n",
    "    if name_string == '':\n",
    "        return ''\n",
    "    \n",
    "    best_match_score = 0\n",
    "    best_match = ''\n",
    "    best_match_label = ''\n",
    "    for qid, publisher in publishers.iterrows():  # The publishers DataFrame is a global variable\n",
    "        w_ratio = fuzz.WRatio(name_string, publisher['label'])\n",
    "        if w_ratio > best_match_score:\n",
    "            best_match = qid\n",
    "            best_match_label = publisher['label']\n",
    "            best_match_score = w_ratio\n",
    "            \n",
    "    if best_match_score < 98:\n",
    "        print('w_ratio:', best_match_score, 'Warning: poor match of: \"' + best_match_label + '\"', best_match, 'to stated publisher: \"' + name_string + '\"\\n')\n",
    "        #print('w_ratio:', best_match_score, 'Warning: poor match of: \"' + best_match_label + '\"', best_match, 'to stated publisher: \"' + name_string + '\"\\n', file=log_object)\n",
    "    return best_match\n",
    "\n",
    "def disambiguate_place_of_publication(value):\n",
    "    \"\"\"Look up place of publication Q ID from a list derived from query https://w.wiki/63Ap\n",
    "    If there is a single match, the Q ID is returned.\n",
    "    If there are no matches, the string is returned unprocessed.\n",
    "    If there are multiple matches, a dict with possible values is returned.\"\"\"\n",
    "    if value == '':\n",
    "        return ''\n",
    "    \n",
    "    if 'New York' in value:\n",
    "        return 'Q60'\n",
    "    \n",
    "    if 'New Brunswick' in value:\n",
    "        return 'Q138338'\n",
    "    \n",
    "    if 'California' in value:\n",
    "        value = value.replace('California', 'CA')\n",
    "    \n",
    "    if 'Calif' in value:\n",
    "        value = value.replace('Calif', 'CA')\n",
    "        \n",
    "    if 'Massachusetts' in value:\n",
    "        value = value.replace('Massachusetts', 'MA')\n",
    "        \n",
    "    if 'Cambridge' in value:\n",
    "        if 'Cambridge, M' in value:\n",
    "            return 'Q49111'\n",
    "        else:\n",
    "            return 'Q350'\n",
    "    \n",
    "    location_list = []\n",
    "    for qid, location in publisher_locations.iterrows():  # The publisher_locations DataFrame is a global variable\n",
    "        if location['label'] in value:\n",
    "            location_list.append({'qid': qid, 'label': location['label']})\n",
    "    if len(location_list) == 0:\n",
    "        return value\n",
    "    elif len(location_list) == 1:\n",
    "        return location_list[0]['qid']\n",
    "    else:\n",
    "        return location_list\n",
    "    \n",
    "    return value\n",
    "\n",
    "def today():\n",
    "    \"\"\"Generate the current UTC xsd:date\"\"\"\n",
    "    whole_time_string_z = datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "def set_reference(input_url):\n",
    "    \"\"\"Set any URL that is present in the field as the reference URL value.\"\"\"\n",
    "    url = include_reference_url(input_url) # Screen for suitable URLs\n",
    "    if url != '':\n",
    "        return url\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def set_stated_in(input_url):\n",
    "    \"\"\"If no URL is present, set a fixed value to be used as the stated_in value.\"\"\"\n",
    "    url = include_reference_url(input_url) # Screen for suitable URLs\n",
    "    if url == '':\n",
    "        return 'Q114403967' # Vanderbilt Divinity publications database\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "# ---------------------------\n",
    "# Major processes functions\n",
    "# ---------------------------\n",
    "\n",
    "def build_function(function_name_string, passed_value_string):\n",
    "    if len(passed_value_string) == 0:\n",
    "        expression = function_name_string + \"('')\"\n",
    "    else:\n",
    "        # Hack for cases where the data string is enclosed in single quotes\n",
    "        if passed_value_string[0] == \"'\" and passed_value_string[-1] == \"'\":\n",
    "            expression = function_name_string + '(\"\"\"' + passed_value_string + '\"\"\")'\n",
    "        else:\n",
    "            expression = function_name_string + \"('''\" + passed_value_string + \"''')\"\n",
    "    output_value = eval(expression)\n",
    "    return output_value\n",
    "\n",
    "def extract_metadata(mapping, work_data):\n",
    "    \"\"\"Steps through fields described in the config file, maps them to columns in the source data, and\n",
    "    uses processing functions to transform the input data to forms required in the output table.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    mapping : complex structure\n",
    "        Maps column headers (\"variable\") in the destination table to column headers (\"source\") in the source table.\n",
    "        The \"value\" key indicates the function used to determine the value to be used in the destination table.\n",
    "    work_data : dict\n",
    "        A row of data from the source data table with column headers as the keys.\n",
    "    \"\"\"\n",
    "    out_dict = {'qid': ''}\n",
    "\n",
    "    for out_property in config['outfiles'][0]['prop_list']:\n",
    "        \n",
    "        # Find the mapping variable that matches the config property\n",
    "        for prop in mapping['properties']:\n",
    "            if prop['variable'] == out_property['variable']:\n",
    "                break\n",
    "    \n",
    "        out_field = out_property['variable']\n",
    "        out_dict[out_field + '_uuid'] = ''\n",
    "        \n",
    "        # The mapping function may not require an argument. In that case, there's no source column.\n",
    "        if 'source' in prop:\n",
    "            # If the source data CSV doesn't have any column named according to mappings, the output for that\n",
    "            # variable is an empty string.\n",
    "            if prop['source'] in work_data:\n",
    "                output_value = build_function(prop['value'], work_data[prop['source']])\n",
    "                if output_value == '':\n",
    "                    no_value = True\n",
    "                else:\n",
    "                    no_value = False\n",
    "            else:\n",
    "                output_value = ''\n",
    "                no_value = True\n",
    "        # Case where there's no argument passed to mapping function\n",
    "        else:\n",
    "            expression = prop['value'] + '()'\n",
    "            output_value = eval(expression)\n",
    "            if output_value == '':\n",
    "                no_value = True\n",
    "            else:\n",
    "                no_value = False\n",
    "\n",
    "        # Populate the values-related columns\n",
    "        if out_property['value_type'] == 'date':\n",
    "            out_dict[out_field + '_nodeId'] = ''\n",
    "            out_dict[out_field + '_val'] = output_value\n",
    "            out_dict[out_field + '_prec'] = ''\n",
    "\n",
    "        elif out_property['value_type'] == 'quantity':\n",
    "            out_dict[out_field + '_nodeId'] = ''\n",
    "            out_dict[out_field + '_val'] = output_value\n",
    "            if no_value:\n",
    "                out_dict[out_field + '_unit'] = ''\n",
    "            else:\n",
    "                out_dict[out_field + '_unit'] = prop['quantity_unit']\n",
    "\n",
    "        # This is not actually implemented and will generate an error if used\n",
    "        elif out_property['value_type'] == 'globecoordinate':\n",
    "            out_dict[out_field + '_nodeId'] = ''\n",
    "            out_dict[out_field + '_val'] = output_value\n",
    "            if no_value:\n",
    "                out_dict[out_field + '_long'] = ''\n",
    "                out_dict[out_field + '_prec'] = ''\n",
    "            else:\n",
    "                out_dict[out_field + '_long'] = work_data[out_field + '_long']\n",
    "                out_dict[out_field + '_prec'] = work_data[out_field + '_prec']\n",
    "\n",
    "        else:\n",
    "            out_dict[out_field] = output_value\n",
    "\n",
    "        # Populate the qualifier columns\n",
    "        for qualifier in out_property['qual']:\n",
    "            if no_value:\n",
    "                no_qual_value = True\n",
    "            else:\n",
    "                # Find the mapping variable that matches the config property\n",
    "                for qual in prop['qual']:\n",
    "                    if qual['variable'] == qualifier['variable']:\n",
    "                        break\n",
    "\n",
    "                # Skip reading a value from a source column if the function doesn't need input.\n",
    "                if 'source' in qual:\n",
    "                    expression = qual['value'] + \"('''\" + work_data[qual['source']] + \"''')\"\n",
    "                    qual_output_value = eval(expression)\n",
    "                    if qual_output_value == '':\n",
    "                        no_qual_value = True\n",
    "                    else:\n",
    "                        no_qual_value = False\n",
    "                else:\n",
    "                    no_qual_value = False\n",
    "                    expression = qual['value'] + '()'\n",
    "                    qual_output_value = eval(expression) # If this evalutes as empty string, result is same as no_ref_value = True\n",
    "\n",
    "\n",
    "            qual_field = out_field + '_' + qualifier['variable']\n",
    "            # To my knowledge, dates are the only complex types used as qualifiers (no quantities or globecoordinates).\n",
    "            if qualifier['value_type'] == 'date':\n",
    "                out_dict[qual_field + '_nodeId'] = ''\n",
    "                if no_qual_value:\n",
    "                    out_dict[qual_field + '_val'] = ''\n",
    "                else:\n",
    "                    out_dict[qual_field + '_val'] = qual_output_value\n",
    "                out_dict[qual_field + '_prec'] = ''\n",
    "            else:\n",
    "                if no_qual_value:\n",
    "                    out_dict[qual_field] = ''\n",
    "                else:\n",
    "                    out_dict[qual_field] = qual_output_value\n",
    "                \n",
    "        # Populate the reference columns\n",
    "        # There's only a hash ID column if there's at least one reference.\n",
    "        if len(out_property['ref']) > 0:\n",
    "            out_dict[out_field + '_ref1_hash'] = ''\n",
    "            \n",
    "        for reference in out_property['ref']:\n",
    "            if no_value:\n",
    "                no_ref_value = True\n",
    "            else:\n",
    "                # Find the mapping variable that matches the config property\n",
    "                for ref in prop['ref']:\n",
    "                    if ref['variable'] == reference['variable']:\n",
    "                        break\n",
    "\n",
    "                # Some functions like today() don't need input from the source table, and therefore \n",
    "                # skip reading a value from a source column.\n",
    "                if 'source' in ref:\n",
    "                    expression = ref['value'] + \"('''\" + work_data[ref['source']] + \"''')\"\n",
    "                    ref_output_value = eval(expression)\n",
    "                    if ref_output_value == '':\n",
    "                        no_ref_value = True\n",
    "                    else:\n",
    "                        no_ref_value = False\n",
    "                else:\n",
    "                    no_ref_value = False\n",
    "                    expression = ref['value'] + '()'\n",
    "                    ref_output_value = eval(expression) # If this evalutes as empty string, result is same as no_ref_value = True\n",
    "\n",
    "            ref_field = out_field + '_ref1_' + reference['variable']\n",
    "            # To my knowledge, dates are the only complex types used as qualifiers (no quantities or globecoordinates).\n",
    "            if reference['value_type'] == 'date':\n",
    "                out_dict[ref_field + '_nodeId'] = ''\n",
    "                if no_ref_value:\n",
    "                    out_dict[ref_field + '_val'] = ''\n",
    "                else:\n",
    "                    out_dict[ref_field + '_val'] = ref_output_value\n",
    "                out_dict[ref_field + '_prec'] = ''\n",
    "            else:\n",
    "                if no_ref_value:\n",
    "                    out_dict[ref_field] = ''\n",
    "                else:\n",
    "                    out_dict[ref_field] = ref_output_value\n",
    "                    \n",
    "    #print(out_dict)\n",
    "    return out_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "Idiosyncratic steps that need to be done between the Zotero output and running the \"standardized\" script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This step involves re-setting the Url column to use the screened URLs if the Zotero output title matches\n",
    "# the title in the screened full work CSV.\n",
    "\n",
    "source_data = 'output_examples_div_pubs.csv'\n",
    "#source_data = 'output-example_baldwinbookschaptersarticles.csv'\n",
    "works = pd.read_csv(source_data, na_filter=False, dtype = str)\n",
    "#works = works.iloc[180:225] # test for full text URL substitutions\n",
    "#works = works.iloc[159:164]\n",
    "#works = works.iloc[2033:2052] # test of ref screening\n",
    "#works = works.iloc[1970:1988] # good rows to test for languages\n",
    "works = works.iloc[:5]\n",
    "\n",
    "for label, work_series in works.iterrows():\n",
    "    try:\n",
    "        # Find the row(s) in the full_works DataFrame that matches the series. There should be only one.\n",
    "        # Create a series of URL values for those rows. Since there should be only one, get the 0th value.\n",
    "        new_url = full_works.loc[full_works.index==work_series['Title'], 'Url'][0]\n",
    "        # Set a new value for the Url column in the works DataFrame using the looked-up URL.\n",
    "        works.loc[label, 'Url'] = new_url\n",
    "        \n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "works.to_csv('preprocessed.csv', index = False)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main routine\n",
    "\n",
    "NOTE: Before continuing on after this step, you need to correct any of the publication locations that weren't determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "publishers = pd.read_csv('publishers.csv', na_filter=False, dtype = str)\n",
    "publishers.set_index('qid', inplace=True)\n",
    "\n",
    "publisher_locations = pd.read_csv('publisher_locations.csv', na_filter=False, dtype = str)\n",
    "publisher_locations.set_index('qid', inplace=True)\n",
    "\n",
    "source_data = 'preprocessed.csv'\n",
    "works = pd.read_csv(source_data, na_filter=False, dtype = str)\n",
    "\n",
    "with open('config.yaml', 'r') as file_object:\n",
    "    config = yaml.safe_load(file_object)\n",
    "\n",
    "with open('mapping.yaml', 'r') as file_object:\n",
    "    mapping = yaml.safe_load(file_object)\n",
    "\n",
    "with open('mapping_agents.yaml', 'r') as file_object:\n",
    "    mapping_agents = yaml.safe_load(file_object)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "works_list = []\n",
    "agents_list = []\n",
    "for index, work_data in works.iterrows():\n",
    "    print(work_data['Title'])\n",
    "    locally_unique_id = work_data['Key']\n",
    "    \n",
    "    # Use the mappings to extract and process the main metadata from the source columns\n",
    "    row = extract_metadata(mapping, work_data)\n",
    "    works_list.append(row)\n",
    "    \n",
    "    agents_dict = {'id': locally_unique_id}\n",
    "    # For each agent type (author, editor, etc.) extract the name information\n",
    "    for agent_type in mapping_agents['sources']:\n",
    "        source_column = agent_type['source']\n",
    "        agent_structured_data = build_function(agent_type['value'], work_data[source_column])\n",
    "        agents_dict[agent_type['variable']] = json.dumps(agent_structured_data)\n",
    "    agents_list.append(agents_dict)\n",
    "\n",
    "out_frame = pd.DataFrame(works_list)\n",
    "agents_frame = pd.DataFrame(agents_list)\n",
    "\n",
    "out_frame.to_csv('test_works_to_write.csv', index = False)\n",
    "agents_frame.to_csv('stored_retrieved_authors.csv', index = False)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors_string = 'Floyd-Thomas, Stacey M.; De La Torre, Miguel A.'\n",
    "#authors_string = 'Armour, Ellen; Garland-Thomson, Rosemarie'\n",
    "#authors_string = 'Darity, William A., Jr.'\n",
    "#authors_string = 'Resner, Jr., Andr√©'\n",
    "#authors_string = 'Williams IV, Joe'\n",
    "authors_string = ''\n",
    "extract_names_from_list(authors_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for index, work_data in works.iterrows():\n",
    "    url = work_data['Place']\n",
    "    if url != '':\n",
    "        print(disambiguate_place_of_publication(url))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
