{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from time import sleep\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import sys # Read CLI arguments\n",
    "\n",
    "# ----------------\n",
    "# Configuration settings\n",
    "# ----------------\n",
    "\n",
    "if len(sys.argv) == 2: # if exactly one argument passed (i.e. the configuration file path)\n",
    "    file_path = sys.argv[1] # sys.argv[0] is the script name\n",
    "else:\n",
    "    file_path = 'act.csv'\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "# ----------------\n",
    "# Utility functions\n",
    "# ----------------\n",
    "\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderBot/1.6.1 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# extracts the UUID and qId from a statement IRI\n",
    "def extract_statement_uuid(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/statement/Q7552806-8B88E0CA-BCC8-49D5-9AC2-F1755464F1A2\n",
    "    pieces = iri.split('/')\n",
    "    statement_id = pieces[5]\n",
    "    pieces = statement_id.split('-')\n",
    "    return pieces[1] + '-' + pieces[2] + '-' + pieces[3] + '-' + pieces[4] + '-' + pieces[5], pieces[0]\n",
    "\n",
    "# function to use in sort\n",
    "def sort_funct(row):\n",
    "    return row['filename']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_dict(file_path)\n",
    "input_list = []\n",
    "iri_values = ''  # VALUES list for query\n",
    "for record in data:\n",
    "    record_dict = {'act_id': record['RecordNumber']}\n",
    "    # some records have spaces with other junk after them\n",
    "    strings = record['filename'].split(' ')\n",
    "    # use only the first string in the list (item 0)\n",
    "    record_dict['filename'] = strings[0]\n",
    "    # to generate the IRIs, the underscores need to be replaced with escaped spaces (%20)\n",
    "    filename = strings[0].replace('_','%20')\n",
    "    url = 'http://commons.wikimedia.org/wiki/Special:FilePath/' + filename\n",
    "    record_dict['url'] = url\n",
    "    input_list.append(record_dict)\n",
    "    iri_values += '<' + url + '>\\n'\n",
    "\n",
    "# remove trailing newline\n",
    "iri_values = iri_values[:len(iri_values)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(output_list, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iri_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "select distinct ?qid ?iri\n",
    "where {'''\n",
    "query += '''\n",
    "      VALUES ?iri\n",
    "    {\n",
    "    ''' + iri_values + '''\n",
    "    }\n",
    "?qid wdt:P18 ?iri.\n",
    "}'''\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# send request to Wikidata Query Service\n",
    "# ----------------\n",
    "\n",
    "print('querying SPARQL endpoint to acquire item metadata')\n",
    "response = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "#print(response.text)\n",
    "data = response.json()\n",
    "\n",
    "# extract the values from the response JSON\n",
    "results = data['results']['bindings']\n",
    "\n",
    "print('done retrieving data')\n",
    "print(json.dumps(results, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# extract Q IDs from the results and match them with the ACT IDs\n",
    "# ----------------\n",
    "\n",
    "output_list = []\n",
    "for record in input_list:\n",
    "    found = False\n",
    "    for result in results:\n",
    "        if record['url'] == result['iri']['value']:\n",
    "            found = True\n",
    "            qid = extract_qnumber(result['qid']['value'])\n",
    "            record['qid'] = qid\n",
    "            break\n",
    "    if not found:\n",
    "        record['qid'] = ''\n",
    "    output_list.append(record)\n",
    "print(json.dumps(output_list, indent = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list.sort(key = sort_funct) # sort by the filename field\n",
    "fieldnames = ['act_id', 'qid', 'filename', 'url']\n",
    "write_dicts_to_csv(output_list, 'output.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
