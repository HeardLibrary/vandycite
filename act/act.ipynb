{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from time import sleep\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import sys # Read CLI arguments\n",
    "import urllib.parse\n",
    "\n",
    "# ----------------\n",
    "# Configuration settings\n",
    "# ----------------\n",
    "\n",
    "if len(sys.argv) == 2: # if exactly one argument passed (i.e. the configuration file path)\n",
    "    file_path = sys.argv[1] # sys.argv[0] is the script name\n",
    "else:\n",
    "    file_path = 'act.csv'\n",
    "\n",
    "commons_urls = 'act_CopyrightStatement-wikimedia_202108181034.csv'\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "commons_prefix = 'http://commons.wikimedia.org/wiki/Special:FilePath/'\n",
    "commons_page_prefix = 'https://commons.wikimedia.org/wiki/File:'\n",
    "extensions_list = ['.jpg', '.JPG', '.png', '.PNG', '.tif']\n",
    "\n",
    "# ----------------\n",
    "# Utility functions\n",
    "# ----------------\n",
    "\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderBot/1.6.1 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# extracts the UUID and qId from a statement IRI\n",
    "def extract_statement_uuid(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/statement/Q7552806-8B88E0CA-BCC8-49D5-9AC2-F1755464F1A2\n",
    "    pieces = iri.split('/')\n",
    "    statement_id = pieces[5]\n",
    "    pieces = statement_id.split('-')\n",
    "    return pieces[1] + '-' + pieces[2] + '-' + pieces[3] + '-' + pieces[4] + '-' + pieces[5], pieces[0]\n",
    "\n",
    "# function to use in sort\n",
    "def sort_funct(row):\n",
    "    return row['filename']\n",
    "\n",
    "def commons_url_to_filename(url):\n",
    "    # form of URL is: http://commons.wikimedia.org/wiki/Special:FilePath/Castle%20De%20Haar%20%281892-1913%29%20-%20360%C2%B0%20Panorama%20of%20Castle%20%26%20Castle%20Grounds.jpg\n",
    "    string = url.split(commons_prefix)[1] # get local name file part of URL\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_url(filename):\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_prefix + encoded_filename\n",
    "    return url\n",
    "\n",
    "def commons_page_url_to_filename(url):\n",
    "    # form of URL is: https://commons.wikimedia.org/wiki/File:Castle_De_Haar_(1892-1913)_-_360%C2%B0_Panorama_of_Castle_%26_Castle_Grounds.jpg\n",
    "    string = url.split(commons_page_prefix)[1] # get local name file part of URL\n",
    "    string = string.replace('_', ' ')\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_page_url(filename):\n",
    "    filename = filename.replace(' ', '_')\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_page_prefix + encoded_filename\n",
    "    url = url.replace('%28', '(').replace('%29', ')').replace('%2C', ',')\n",
    "    return url\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# original script (spring 2021?)\n",
    "\n",
    "Was used to clean up IDs and query to find items already in Wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_dict(file_path)\n",
    "input_list = []\n",
    "iri_values = ''  # VALUES list for query\n",
    "for record in data:\n",
    "    record_dict = {'act_id': record['RecordNumber']}\n",
    "    # some records have spaces with other junk after them\n",
    "    strings = record['filename'].split(' ')\n",
    "    # use only the first string in the list (item 0)\n",
    "    record_dict['filename'] = strings[0]\n",
    "    # to generate the IRIs, the underscores need to be replaced with escaped spaces (%20)\n",
    "    filename = strings[0].replace('_','%20')\n",
    "    url = 'http://commons.wikimedia.org/wiki/Special:FilePath/' + filename\n",
    "    record_dict['url'] = url\n",
    "    input_list.append(record_dict)\n",
    "    iri_values += '<' + url + '>\\n'\n",
    "\n",
    "# remove trailing newline\n",
    "iri_values = iri_values[:len(iri_values)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(output_list, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iri_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''\n",
    "select distinct ?qid ?iri\n",
    "where {'''\n",
    "query += '''\n",
    "      VALUES ?iri\n",
    "    {\n",
    "    ''' + iri_values + '''\n",
    "    }\n",
    "?qid wdt:P18 ?iri.\n",
    "}'''\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# send request to Wikidata Query Service\n",
    "# ----------------\n",
    "\n",
    "print('querying SPARQL endpoint to acquire item metadata')\n",
    "response = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "#print(response.text)\n",
    "data = response.json()\n",
    "\n",
    "# extract the values from the response JSON\n",
    "results = data['results']['bindings']\n",
    "\n",
    "print('done retrieving data')\n",
    "print(json.dumps(results, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# extract Q IDs from the results and match them with the ACT IDs\n",
    "# ----------------\n",
    "\n",
    "output_list = []\n",
    "for record in input_list:\n",
    "    found = False\n",
    "    for result in results:\n",
    "        if record['url'] == result['iri']['value']:\n",
    "            found = True\n",
    "            qid = extract_qnumber(result['qid']['value'])\n",
    "            record['qid'] = qid\n",
    "            break\n",
    "    if not found:\n",
    "        record['qid'] = ''\n",
    "    output_list.append(record)\n",
    "print(json.dumps(output_list, indent = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list.sort(key = sort_funct) # sort by the filename field\n",
    "fieldnames = ['act_id', 'qid', 'filename', 'url']\n",
    "write_dicts_to_csv(output_list, 'output.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script for cleaned up IRIs (fall 2021)\n",
    "\n",
    "Uses new functions designed for converting between various forms of Commons IRIs. See [the development script](https://github.com/HeardLibrary/vandycite/blob/master/commons_test/commons_identifier_conversion.ipynb) for details.\n",
    "\n",
    "configureation section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import json\n",
    "\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "commons_prefix = 'http://commons.wikimedia.org/wiki/Special:FilePath/'\n",
    "commons_page_prefix = 'https://commons.wikimedia.org/wiki/File:'\n",
    "\n",
    "def commons_url_to_filename(url):\n",
    "    # form of URL is: http://commons.wikimedia.org/wiki/Special:FilePath/Castle%20De%20Haar%20%281892-1913%29%20-%20360%C2%B0%20Panorama%20of%20Castle%20%26%20Castle%20Grounds.jpg\n",
    "    string = url.split(commons_prefix)[1] # get local name file part of URL\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_url(filename):\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_prefix + encoded_filename\n",
    "    return url\n",
    "\n",
    "def commons_page_url_to_filename(url):\n",
    "    # form of URL is: https://commons.wikimedia.org/wiki/File:Castle_De_Haar_(1892-1913)_-_360%C2%B0_Panorama_of_Castle_%26_Castle_Grounds.jpg\n",
    "    string = url.split(commons_page_prefix)[1] # get local name file part of URL\n",
    "    string = string.replace('_', ' ')\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_page_url(filename):\n",
    "    filename = filename.replace(' ', '_')\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_page_prefix + encoded_filename\n",
    "    url = url.replace('%28', '(').replace('%29', ')').replace('%2C', ',')\n",
    "    return url\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderBot/1.8 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# function to use in sort\n",
    "def sort_funct(row):\n",
    "    return row['filename']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script was actually used to do the cleanup.\n",
    "\n",
    "RUN ONE TIME ONLY!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "commons_items = read_dict(commons_urls)\n",
    "output_table = []\n",
    "for item_number in range(len(commons_items)):\n",
    "    metadata = {}\n",
    "    item = commons_items[item_number]\n",
    "    commons_url = item['CopyrightStatement'].strip()\n",
    "    commons_url = commons_url.replace('http:', 'https:')\n",
    "    commons_url = commons_url.replace('/Image', '/File')\n",
    "    commons_url = commons_url.replace('https://commons.m.wikimedia.org/wiki/File:', commons_page_prefix)\n",
    "    try:\n",
    "        filename = commons_page_url_to_filename(commons_url)\n",
    "        # Some of the strings have file owner names included after the file extension, or have fragment identifiers that need to be removed\n",
    "        for extension in extensions_list:\n",
    "            if extension in filename:\n",
    "                # extract what's before the extension, then put the extension back on\n",
    "                filename = filename.split(extension)[0] + extension\n",
    "            # if the extension isn't on the list, nothing will happen\n",
    "        transformed_url = filename_to_commons_page_url(filename)\n",
    "        metadata['RecordNumber'] = item['RecordNumber']\n",
    "        metadata['commons_page_url'] = commons_url\n",
    "        metadata['filename'] = filename\n",
    "        metadata['commons_uri'] = filename_to_commons_url(filename)\n",
    "        output_table.append(metadata)\n",
    "    except:\n",
    "        print(item_number, 'error:', commons_url)\n",
    "write_dicts_to_csv(output_table, 'clean_ids.csv', ['RecordNumber', 'filename', 'commons_page_url', 'commons_uri'])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start of script for check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# Load data and construct query using list of IRIs\n",
    "# ----------------\n",
    "\n",
    "file_path = 'clean_ids.csv'\n",
    "\n",
    "data = read_dict(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# ----------------\n",
    "# Construct query using list of IRIs\n",
    "# ----------------\n",
    "\n",
    "iri_values = ''  # VALUES list for query\n",
    "#input_list = []\n",
    "\n",
    "for record_number in range(len(data)):\n",
    "    #record_dict = {'act_id': record['RecordNumber']}\n",
    "    #url = record['commons_uri']\n",
    "    iri_values += '<' + data[record_number]['commons_uri'] + '>\\n'\n",
    "    #input_list.append(record_dict)\n",
    "\n",
    "# Remove trailing newline\n",
    "iri_values = iri_values[:len(iri_values)-1]\n",
    "\n",
    "# Create query to find items having the IRI as their image\n",
    "query = '''\n",
    "select distinct ?qid ?iri\n",
    "where {'''\n",
    "query += '''\n",
    "      VALUES ?iri\n",
    "    {\n",
    "    ''' + iri_values + '''\n",
    "    }\n",
    "?qid wdt:P18 ?iri.\n",
    "}'''\n",
    "#print(query)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querying SPARQL endpoint to acquire item metadata\n",
      "done retrieving data\n"
     ]
    }
   ],
   "source": [
    "# ----------------\n",
    "# send request to Wikidata Query Service\n",
    "# ----------------\n",
    "\n",
    "print('querying SPARQL endpoint to acquire item metadata')\n",
    "response = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "#print(response.text)\n",
    "respons_json = response.json()\n",
    "\n",
    "# extract the values from the response JSON\n",
    "results = respons_json['results']['bindings']\n",
    "\n",
    "print('done retrieving data')\n",
    "#print(json.dumps(results, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# ----------------\n",
    "# extract Q IDs from the results and match them with the ACT IDs\n",
    "# ----------------\n",
    "\n",
    "#output_list = []\n",
    "for record_number in range(len(data)):\n",
    "    found = False\n",
    "    for result in results:\n",
    "        if data[record_number]['commons_uri'] == result['iri']['value']:\n",
    "            found = True\n",
    "            qid = extract_qnumber(result['qid']['value'])\n",
    "            data[record_number]['qid'] = qid\n",
    "            break\n",
    "    if not found:\n",
    "        data[record_number]['qid'] = ''\n",
    "    #output_list.append(record)\n",
    "#print(json.dumps(data, indent = 2))\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "data.sort(key = sort_funct) # sort by the filename field\n",
    "fieldnames = ['RecordNumber', 'qid', 'filename', 'commons_uri', 'commons_page_url']\n",
    "write_dicts_to_csv(data, 'output.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
