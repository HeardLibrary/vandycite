{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download images from the ACT website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import requests\n",
    "import math\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "\n",
    "from PIL import Image\n",
    "from PIL.ExifTags import TAGS\n",
    "import os\n",
    "import datetime\n",
    "import csv\n",
    "import json\n",
    "import exifread # https://github.com/ianare/exif-py\n",
    "\n",
    "# Global variables\n",
    "IMAGE_DOWNLOAD_DIRECTORY = '/Users/baskausj/Downloads/act_images/'\n",
    "CLEAN_DATA_DIRECTORY = '/Users/baskausj/github/vandycite/act/processed_lists/'\n",
    "ACT_BASE_URL = 'https://diglib.library.vanderbilt.edu/act-imagelink.pl?RC='\n",
    "YEARS_SINCE_INCEPTION_SCREENING_AGE = 150\n",
    "\n",
    "# Functions\n",
    "def csv_read(path: str, **kwargs) -> pd.DataFrame:\n",
    "    \"\"\"Loads a CSV table into a Pandas DataFrame with all cells as strings and blank cells as empty strings\n",
    "    \n",
    "    Keyword argument:\n",
    "    rows -- the number of rows of the table to return when used for testing. When omitted, all rows are returned.\n",
    "    \"\"\"\n",
    "    dataframe = pd.read_csv(path, na_filter=False, dtype = str)\n",
    "    if 'rows' in kwargs:\n",
    "        return dataframe.head(kwargs['rows']).copy(deep=True)\n",
    "    else:\n",
    "        return dataframe\n",
    "\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "today = generate_utc_date()\n",
    "\n",
    "# From https://github.com/HeardLibrary/vandycite/blob/master/act/create_items/create_act_items.ipynb\n",
    "def pad_zeros_left(date_string):\n",
    "    length = len(date_string)\n",
    "    pad = 4-length\n",
    "    return '0' * pad + date_string\n",
    "\n",
    "def generate_date_string(date, bce):\n",
    "    if bce:\n",
    "        date_string = '-'\n",
    "    else:\n",
    "        date_string = ''\n",
    "    date_string += pad_zeros_left(str(date)) + '-01-01T00:00:00Z'\n",
    "    return date_string\n",
    " \n",
    "# Parse the ACT date string into structured components\n",
    "# From https://github.com/HeardLibrary/vandycite/blob/master/act/create_items/create_act_items.ipynb\n",
    "def process_act_date(act_date):\n",
    "    act_circa = False\n",
    "    act_range = False\n",
    "    act_century = False\n",
    "    non_numeric = False\n",
    "    date = 0\n",
    "    start_date = 0\n",
    "    end_date = 0\n",
    "    \n",
    "    # If there is no date from ACT, kill the function and return False\n",
    "    if act_date == '':\n",
    "        return False, date, act_range, start_date, end_date, act_century, act_circa\n",
    "    \n",
    "    # Determine circa status of ACT date\n",
    "    if 'ca.' in act_date:\n",
    "        act_circa = True\n",
    "        # Remove the \"ca.\" from the beginning and clean whitespace\n",
    "        act_date = act_date.split('ca.')[1].strip()\n",
    "    \n",
    "    # Test whether the ACT date is a number\n",
    "    try:\n",
    "        date = int(act_date)\n",
    "        #print('numeric date:', date)\n",
    "    except:\n",
    "        non_numeric = True\n",
    "        #print('non-numeric string:', act_date)\n",
    "        \n",
    "    if non_numeric:\n",
    "        # Determine century status of ACT date\n",
    "        if 'century' in act_date: # single century date\n",
    "            act_century = True\n",
    "            # Remove the \"century\" and \"th\", \"rd\", \"st\", etc. from the end\n",
    "            act_date = act_date[:-10]\n",
    "            non_numeric = False\n",
    "            try:\n",
    "                date = int(act_date) * 100 - 50 # set the date at mid-century\n",
    "            except:\n",
    "                print('numeric conversion error on', act_date)\n",
    "        elif 'centuries' in act_date:\n",
    "            act_century = True\n",
    "            act_range = True\n",
    "            # Remove the \"centuries\" and \"th\", \"rd\", \"st\", etc. from the end\n",
    "            act_date = act_date[:-10].strip()\n",
    "            try:\n",
    "                pieces = act_date.split('-')\n",
    "                start_date = int(pieces[0][:-2]) * 100 - 50 # set the date at mid-century\n",
    "                end_date = int(pieces[1][:-2]) * 100 - 50 # set the date at mid-century\n",
    "            except:\n",
    "                print('error in processing century range')\n",
    "    # Process date ranges (non-numeric because they include \"-\")\n",
    "    if non_numeric and not act_century:\n",
    "        #print(act_date)\n",
    "        try:\n",
    "            pieces = act_date.split('-')\n",
    "            start_date = int(pieces[0])\n",
    "            end_date = int(pieces[1])\n",
    "            act_range = True\n",
    "        except:\n",
    "            print('error in processing date range')\n",
    "\n",
    "        \n",
    "    # if there is a range of dates, set the single date as the midpoint\n",
    "    if start_date != 0 or end_date != 0:\n",
    "        date = math.floor((start_date + end_date)/2)\n",
    "            \n",
    "    return True, date, act_range, start_date, end_date, act_century, act_circa\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in clean metadata CSV as text into dataframe, with NA values as empty strings\n",
    "metadata_path = '../processed_lists/clean_metadata_2022-09-29.csv'\n",
    "act_metadata_df = pd.read_csv(metadata_path, dtype=str, na_filter=False)\n",
    "\n",
    "# Find all rows whose lower case \"CopyrightStatement\" column value equals \"image donated by Jim Womack and Anne Richardson\" or variant\n",
    "richardson_df1 = act_metadata_df.loc[act_metadata_df['CopyrightStatement'] == 'image donated by Jim Womack and Anne Richardson']\n",
    "richardson_df2 = act_metadata_df.loc[act_metadata_df['CopyrightStatement'] == 'Image donated by Jim Womack and Anne Richardson']\n",
    "richardson_df3 = act_metadata_df.loc[act_metadata_df['CopyrightStatement'] == 'Image donated by Anne Richardson']\n",
    "# Combine the two dataframes\n",
    "richardson_df = pd.concat([richardson_df1, richardson_df2, richardson_df3])\n",
    "\n",
    "# Set the RecordNumber column as the index\n",
    "richardson_df.set_index('RecordNumber', inplace=True)\n",
    "\n",
    "# Save the slice as a CSV\n",
    "#richardson_df.to_csv(IMAGE_DOWNLOAD_DIRECTORY + 'richardson.csv')\n",
    "\n",
    "# Slice the values from the ImageLink column and create a new dataframe with them, using the same index as the original dataframe\n",
    "filenames_df = richardson_df.loc[:, 'ImageLink']\n",
    "filenames_df = pd.DataFrame(filenames_df)\n",
    "\n",
    "# Add a column named file_url\n",
    "filenames_df['file_url'] = ''\n",
    "\n",
    "# Save the slice as a CSV\n",
    "filenames_df.to_csv(IMAGE_DOWNLOAD_DIRECTORY + 'filenames.csv')\n",
    "\n",
    "filenames_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case a script crash requires restarting, reload the CSV file\n",
    "filenames_df = pd.read_csv(IMAGE_DOWNLOAD_DIRECTORY + 'filenames.csv', dtype=str, na_filter=False)\n",
    "\n",
    "# Set the index to the RecordNumber column\n",
    "filenames_df = filenames_df.set_index('RecordNumber')\n",
    "\n",
    "# Loop through the rows and download the images from the URL\n",
    "for index, row in filenames_df.iterrows():\n",
    "    # If the file_url column is not empty, skip this row\n",
    "    if row['file_url'] != '':\n",
    "        continue\n",
    "\n",
    "    # Convert the filename to a string\n",
    "    filename_string = row['ImageLink']\n",
    "\n",
    "    # If the filename contains only numeric characters, padd it on the left with 0s until it has 8 characters\n",
    "    if filename_string.isnumeric():\n",
    "        filename_string = filename_string.zfill(8)\n",
    "    url = 'https://diglib.library.vanderbilt.edu/cdri/fulljpeg/' + filename_string + '.jpg'\n",
    "    print(url)\n",
    "    \n",
    "    # Add the file URL to the dataframe\n",
    "    filenames_df.at[index, 'file_url'] = url\n",
    "\n",
    "    # Download the image\n",
    "    filename = filename_string + '.jpg'\n",
    "    r = requests.get(url)\n",
    "    with open(IMAGE_DOWNLOAD_DIRECTORY + filename, 'wb') as outfile:\n",
    "        outfile.write(r.content)\n",
    "\n",
    "    # Wait 0.5 second before downloading the next image\n",
    "    time.sleep(0.5)\n",
    "\n",
    "    # Write the dataframe to a CSV file after each download\n",
    "    # to keep track of which ones were done in case the script crashes.\n",
    "    filenames_df.to_csv(IMAGE_DOWNLOAD_DIRECTORY + 'filenames.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the images.csv file needed for input into CommonsTool\n",
    "\n",
    "In some cases, the file download failed due to a 404. These files have dimensions of 0,0 and kilobyte values of 1.\n",
    "\n",
    "After completing this step, I had to manually indicate whether it was 3D or 2D when creating the artwork_metadata.csv file. It can be minimally created by copying and pasting columns from the images.csv file. In the case of Anne's images, most were 3D, so I just needed to mark the 2D ones and fill in 3D for the rest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe to build the images.csv table.       \n",
    "images_df = csv_read(IMAGE_DOWNLOAD_DIRECTORY + 'filenames.csv') # add rows keyword to limit number of rows read\n",
    "#images_df = csv_read(IMAGE_DOWNLOAD_DIRECTORY + 'filenames.csv', rows=2) # add rows keyword to limit number of rows read\n",
    "\n",
    "# Rename the RecordNumber to local_identifier\n",
    "images_df = images_df.rename(columns={'RecordNumber': 'local_identifier'})\n",
    "\n",
    "# Set the index to the RecordNumber column\n",
    "images_df = images_df.set_index('local_identifier')\n",
    "\n",
    "# Extract the file name from the file_url column.\n",
    "images_df['local_filename'] = images_df['file_url'].apply(lambda x: x.split('/')[-1])\n",
    "\n",
    "# Remove the ImageLink and file_url columns.\n",
    "images_df = images_df.drop(['ImageLink', 'file_url'], axis=1)\n",
    "\n",
    "# Code hacked from https://github.com/HeardLibrary/linked-data/blob/master/commonsbot/extract_image_metadata.ipynb\n",
    "\n",
    "for index,image_row in images_df.iterrows():\n",
    "    image_name = image_row['local_filename']\n",
    "    if image_name[0] == '.': # skip hidden files\n",
    "        continue\n",
    "\n",
    "    extension = image_name.split('.')[-1] # separate into pieces by full stops and take the last piece\n",
    "    image_path = IMAGE_DOWNLOAD_DIRECTORY + image_name\n",
    "    # trap errors when the file isn't an image\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            width, height = img.size\n",
    "    except:\n",
    "        width = 0\n",
    "        height = 0\n",
    "        \n",
    "    try:\n",
    "        # First try to get the actual image creation date from the EXIF\n",
    "        # Code from https://stackoverflow.com/questions/23064549/get-date-and-time-when-photo-was-taken-from-exif-data-using-pil\n",
    "        with open(image_path, 'rb') as fh:\n",
    "            tags = exifread.process_file(fh, stop_tag='EXIF DateTimeOriginal')\n",
    "            date_taken = tags['EXIF DateTimeOriginal']\n",
    "            create_date_string = str(date_taken)[:10].replace(':', '-')\n",
    "            #print('EXIF DateTimeOriginal', create_date_string)\n",
    "            if create_date_string == '0000-00-00':\n",
    "                raise Exception('Bad date')\n",
    "            #print('image date')\n",
    "    except:\n",
    "        # If that's unavailable, then use the file creation date.\n",
    "        # Note: this code is Mac/Linux-specific and would need to be modified if run on Windows.\n",
    "        timestamp = os.stat(image_path).st_birthtime\n",
    "        time_object = datetime.datetime.fromtimestamp(timestamp)\n",
    "        create_date_string = time_object.strftime(\"%Y-%m-%d\")\n",
    "        #print('file date', create_date_string)\n",
    "        \n",
    "\n",
    "    if create_date_string == '1969-12-31':\n",
    "        timestamp = os.stat(image_path).st_mtime \n",
    "        time_object = datetime.datetime.fromtimestamp(timestamp)\n",
    "        create_date_string = time_object.strftime(\"%Y-%m-%d\")\n",
    "        #print('file modified', create_date_string)\n",
    "\n",
    "    #print(image_path, create_date_string)\n",
    "\n",
    "    #print(height, width)\n",
    "    #print()\n",
    "\n",
    "    # Set the values in the current row of the dataframe\n",
    "    images_df.loc[index, 'qid'] = ''\n",
    "    images_df.loc[index, 'rank'] = 'primary'\n",
    "    images_df.loc[index, 'label'] = '' \n",
    "    images_df.loc[index, 'notes'] = '' \n",
    "    images_df.loc[index, 'kilobytes'] = round(os.path.getsize(image_path)/1024)\n",
    "    images_df.loc[index, 'height'] = height\n",
    "    images_df.loc[index, 'width'] = width\n",
    "    images_df.loc[index, 'photo_inception'] = create_date_string\n",
    "    images_df.loc[index, 'subdir'] = ''\n",
    "\n",
    "# Unset the index\n",
    "images_df = images_df.reset_index()\n",
    "\n",
    "# Move qid to the first column in the dataframe\n",
    "qid_column = images_df.pop('qid')\n",
    "images_df.insert(0, 'qid', qid_column)\n",
    "\n",
    "# Move local_identifier to the third column in the dataframe\n",
    "local_identifier_column = images_df.pop('local_identifier')\n",
    "images_df.insert(2, 'local_identifier', local_identifier_column)\n",
    "\n",
    "# Save the dataframe to a CSV file in the same directory as this notebook\n",
    "images_df.to_csv('images.csv', index=False)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Wikidata input file\n",
    "\n",
    "In the test, this was called `act_artworks.csv` and it's column headers were mapped in the `config.csv` file. The fields need to be filled from the ACT download file, filtered for Anne's images (`richardson.csv`).\n",
    "\n",
    "Note: before this step, I added a column to the images.csv file called `dimension` to record whether it was a 3D or 2D work. This value needs to be in the `richardson.csv` file in order to do the Commons upload, but since I needed to sort through the images to see which ones were black and white or otherwise unusable anyway, this was the easiest way to to get the value and I can transfer it over to the `richardson.csv` source data file from the `images.csv` file since there is a one-to-one correspondence between the images and the artwork items anyway. Both spreadsheets will have `dimension` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to transfer dimension column from images.csv to richardson.csv by matching the RecordNumber column.\n",
    "\n",
    "# Read in the images.csv file\n",
    "images_df = csv_read('images.csv')\n",
    "\n",
    "# Set the index to the local_identifier column\n",
    "images_df = images_df.set_index('local_identifier')\n",
    "\n",
    "# Read in the richardson.csv file\n",
    "richardson_df = csv_read('richardson.csv')\n",
    "\n",
    "# Set the index to the RecordNumber column\n",
    "richardson_df = richardson_df.set_index('RecordNumber')\n",
    "\n",
    "# Loop through the rows of the richardson.csv file\n",
    "for index, row in richardson_df.iterrows():\n",
    "    # If the RecordNumber column is empty, skip this row\n",
    "    if index == '':\n",
    "        continue\n",
    "\n",
    "    # If the RecordNumber column is not empty, get the value from the images.csv file\n",
    "    try:\n",
    "        dimension = images_df.loc[index, 'dimension']\n",
    "    except:\n",
    "        dimension = ''\n",
    "    \n",
    "    # Set the value in the dimension column\n",
    "    richardson_df.at[index, 'dimension'] = dimension\n",
    "\n",
    "# Unset the index\n",
    "richardson_df = richardson_df.reset_index()\n",
    "\n",
    "# Move the RecordNumber column to the first column in the dataframe\n",
    "record_number_column = richardson_df.pop('RecordNumber')\n",
    "richardson_df.insert(0, 'RecordNumber', record_number_column)\n",
    "\n",
    "# Save the dataframe to a CSV file in the same directory as this notebook\n",
    "richardson_df.to_csv('richardson.csv', index=False)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Have to modify the original data to remove the duplicates Charlotte identified and to replace rows with the cleaned data. \n",
    "\n",
    "Note: At this stage, I manually added the `part_of` column to the richardson.csv file since we had added it to the duplicate file. But none of the works in the original file have values for part_of, so we should probably go back later and add it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw data from the ACT database export. Modify it using cleaned data.     \n",
    "raw_df = csv_read('richardson.csv') # add rows keyword to limit number of rows read\n",
    "# Set the RecordNumber (ACT ID) column as the index\n",
    "raw_df = raw_df.set_index('RecordNumber')\n",
    "\n",
    "# Create a list of the works to skip\n",
    "redundant_works_to_skip = csv_read('redundant_items.csv')\n",
    "redundant_works_list = redundant_works_to_skip['RecordNumber'].tolist()\n",
    "\n",
    "# These are the records that need to be replaced in the raw_df\n",
    "duplicate_works_metadata = csv_read('richardson_duplicate_titles_prep.csv')\n",
    "# Set the RecordNumber (ACT ID) column as the index\n",
    "duplicate_works_metadata = duplicate_works_metadata.set_index('RecordNumber')\n",
    "\n",
    "# Loop through the rows of the raw_df\n",
    "for index, row in raw_df.iterrows():\n",
    "    # If the index is in the redundant_works_list, remove it from the dataframe\n",
    "    if index in redundant_works_list:\n",
    "        raw_df.drop(index, inplace=True)\n",
    "        continue\n",
    "\n",
    "# Loop through the rows of the duplicate_works_metadata. \n",
    "# Replace the corresponding row in the raw_df with the row from the duplicate_works_metadata.\n",
    "for index, row in duplicate_works_metadata.iterrows():\n",
    "    raw_df.loc[index] = duplicate_works_metadata.loc[index]\n",
    "\n",
    "# Save the raw_df as a CSV file\n",
    "raw_df.to_csv('richardson.csv', index=True)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now actually create the output file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the raw data from the ACT database export. Modify it using cleaned data.     \n",
    "raw_df = csv_read('richardson.csv') # add rows keyword to limit number of rows read\n",
    "#raw_df = csv_read('richardson.csv', rows=2) # add rows keyword to limit number of rows read\n",
    "\n",
    "# Open the images.csv file and read it into a dataframe.\n",
    "images_df = csv_read('images.csv')\n",
    "\n",
    "country_mappings = pd.read_csv('country_mappings.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# Create a list of the column names for the output CSV file.\n",
    "column_list = [\n",
    "    'qid',\n",
    "    'label_en',\n",
    "    'description_en',\n",
    "    'act_uuid',\n",
    "    'act',\n",
    "    'act_ref1_hash',\n",
    "    'act_ref1_retrieved_nodeId',\n",
    "    'act_ref1_retrieved_val',\n",
    "    'act_ref1_retrieved_prec',\n",
    "    'inventory_number_uuid',\n",
    "    'inventory_number',\n",
    "    'inventory_number_collection',\n",
    "    'inventory_number_ref1_hash',\n",
    "    'inventory_number_ref1_referenceUrl',\n",
    "    'inventory_number_ref1_retrieved_nodeId',\n",
    "    'inventory_number_ref1_retrieved_val',\n",
    "    'inventory_number_ref1_retrieved_prec',\n",
    "    'title_uuid',\n",
    "    'title',\n",
    "    'title_ref1_hash',\n",
    "    'title_ref1_referenceUrl',\n",
    "    'title_ref1_retrieved_nodeId',\n",
    "    'title_ref1_retrieved_val',\n",
    "    'title_ref1_retrieved_prec',\n",
    "    'creator_uuid',\n",
    "    'creator',\n",
    "    'creator_object_has_role',\n",
    "    'creator_ref1_hash',\n",
    "    'creator_ref1_referenceUrl',\n",
    "    'creator_ref1_retrieved_nodeId',\n",
    "    'creator_ref1_retrieved_val',\n",
    "    'creator_ref1_retrieved_prec',\n",
    "    'instance_of_uuid',\n",
    "    'instance_of',\n",
    "    'instance_of_ref1_hash',\n",
    "    'instance_of_ref1_referenceUrl',\n",
    "    'instance_of_ref1_retrieved_nodeId',\n",
    "    'instance_of_ref1_retrieved_val',\n",
    "    'instance_of_ref1_retrieved_prec',\n",
    "    'inception_uuid',\n",
    "    'inception',\n",
    "    'inception_nodeId',\n",
    "    'inception_val',\n",
    "    'inception_prec',\n",
    "    'inception_earliest_date_nodeId',\n",
    "    'inception_earliest_date_val',\n",
    "    'inception_earliest_date_prec',\n",
    "    'inception_latest_date_nodeId',\n",
    "    'inception_latest_date_val',\n",
    "    'inception_latest_date_prec',\n",
    "    'inception_sourcing_circumstances',\n",
    "    'inception_ref1_hash',\n",
    "    'inception_ref1_referenceUrl',\n",
    "    'inception_ref1_retrieved_nodeId',\n",
    "    'inception_ref1_retrieved_val',\n",
    "    'inception_ref1_retrieved_prec',\n",
    "    'country_of_origin_uuid',\n",
    "    'country_of_origin',\n",
    "    'country_of_origin_ref1_hash',\n",
    "    'country_of_origin_ref1_referenceUrl',\n",
    "    'country_of_origin_ref1_retrieved_nodeId',\n",
    "    'country_of_origin_ref1_retrieved_val',\n",
    "    'country_of_origin_ref1_retrieved_prec',\n",
    "    'copyright_status_uuid',\n",
    "    'copyright_status',\n",
    "    'copyright_status_applies_to_jurisdiction',\n",
    "    'copyright_status_determination_method',\n",
    "    'copyright_status_ref1_hash',\n",
    "    'copyright_status_ref1_referenceUrl',\n",
    "    'copyright_status_ref1_retrieved_nodeId',\n",
    "    'copyright_status_ref1_retrieved_val',\n",
    "    'copyright_status_ref1_retrieved_prec',\n",
    "    'image_uuid',\n",
    "    'image',\n",
    "    'image_ref1_hash',\n",
    "    'image_ref1_referenceUrl',\n",
    "    'image_ref1_retrieved_nodeId',\n",
    "    'image_ref1_retrieved_val',\n",
    "    'image_ref1_retrieved_prec',\n",
    "    'collection_uuid',\n",
    "    'collection',\n",
    "    'collection_ref1_hash',\n",
    "    'collection_ref1_referenceUrl',\n",
    "    'collection_ref1_retrieved_nodeId',\n",
    "    'collection_ref1_retrieved_val',\n",
    "    'collection_ref1_retrieved_prec',\n",
    "    'iiif_manifest_uuid',\n",
    "    'iiif_manifest',\n",
    "    'iiif_manifest_ref1_hash',\n",
    "    'iiif_manifest_ref1_referenceUrl',\n",
    "    'iiif_manifest_ref1_retrieved_nodeId',\n",
    "    'iiif_manifest_ref1_retrieved_val',\n",
    "    'iiif_manifest_ref1_retrieved_prec',\n",
    "    'part_of_uuid',\n",
    "    'part_of',\n",
    "    'part_of_ref1_hash',\n",
    "    'part_of_ref1_referenceUrl',\n",
    "    'part_of_ref1_retrieved_nodeId',\n",
    "    'part_of_ref1_retrieved_val',\n",
    "    'part_of_ref1_retrieved_prec'\n",
    "]\n",
    "\n",
    "property_list = [\n",
    "    'act',\n",
    "    'inventory_number',\n",
    "    'title',\n",
    "    'creator',\n",
    "    'instance_of',\n",
    "    'inception',\n",
    "    'country_of_origin',\n",
    "    'copyright_status',\n",
    "    'image',\n",
    "    'collection',\n",
    "    'iiif_manifest',\n",
    "    'part_of'\n",
    "]\n",
    "\n",
    "\n",
    "# Create a new dataframe with the columns in the order specified in column_list.\n",
    "act_artworks_df = pd.DataFrame(columns=column_list)\n",
    "\n",
    "# Step through the rows in the raw dataframe.\n",
    "for index, row in raw_df.iterrows():\n",
    "    # Create a dict using the column names as keys with empty strings as values.\n",
    "    act_artworks_dict = dict.fromkeys(column_list, '')\n",
    "\n",
    "    # Add the values from the raw dataframe to the dict.\n",
    "\n",
    "    # Check if the length of the label or description is greater than 250 characters.\n",
    "    # If so, truncate the string. This is a hard limit imposed by Wikidata.\n",
    "    if len(row['Title']) > 250:\n",
    "        act_artworks_dict['label_en'] = row['Title'][:250]\n",
    "        print(row['RecordNumber'], 'Warning: label truncated to 250 characters')\n",
    "        print(row['Title'][:250])\n",
    "        print()\n",
    "    else:\n",
    "        act_artworks_dict['label_en'] = row['Title']\n",
    "    if row['ObjectFunction'] != '':\n",
    "        act_artworks_dict['description_en'] = row['ObjectFunction'].lower() + ' ACT ID: ' + row['RecordNumber']\n",
    "    else:\n",
    "        act_artworks_dict['description_en'] = 'artwork ACT ID: ' + row['RecordNumber']\n",
    "    \n",
    "    # Screen for works without a DateCreation value\n",
    "    if row['DateCreation'] == '':\n",
    "        print(row['RecordNumber'], 'Warning: no DateCreation value')\n",
    "        print()\n",
    "        continue\n",
    "\n",
    "    # Skip works whose image download failed.\n",
    "    # These are detectable because their kilobytes value in the image_df is 1.\n",
    "    image_match = images_df.loc[images_df['local_identifier'] == row['RecordNumber']]\n",
    "    if len(image_match) == 0:\n",
    "        print(row['RecordNumber'], 'Warning: skipping, no image found')\n",
    "        print()\n",
    "        continue\n",
    "\n",
    "    if image_match['kilobytes'].values[0] == '1':\n",
    "        print(row['RecordNumber'], 'Warning: skipping, image download failed')\n",
    "        print()\n",
    "        continue\n",
    "\n",
    "    act_artworks_dict['act'] = row['RecordNumber']\n",
    "    act_artworks_dict['title'] = row['Title']\n",
    "    act_artworks_dict['creator'] = '_:' # Assume anonymous creator and change manually later if necessary.\n",
    "    act_artworks_dict['creator_object_has_role'] = 'Q4233718'\n",
    "    act_artworks_dict['instance_of'] = 'Q838948' # Assume \"work of art\" and change manually later if necessary.\n",
    "\n",
    "    # Process dates.\n",
    "    act_bce = False\n",
    "    act_found, act_date, act_range, act_start_date, act_end_date, act_century, act_circa = process_act_date(row['DateCreation'])\n",
    "\n",
    "    # Screen for works whose date is less than 200 years old (2023-200 = 1823)\n",
    "    \n",
    "    if act_date > 1823:\n",
    "        print(row['RecordNumber'], 'Warning: date is less than 200 years old')\n",
    "        print()\n",
    "        continue\n",
    "\n",
    "    if not act_found:\n",
    "        act_artworks_dict['inception_val'] = ''\n",
    "        act_artworks_dict['inception_sourcing_circumstances'] = ''\n",
    "        act_artworks_dict['inception_prec'] = ''\n",
    "        act_artworks_dict['inception_earliest_date_val'] = ''\n",
    "        act_artworks_dict['inception_earliest_date_prec'] = ''\n",
    "        act_artworks_dict['inception_latest_date_val'] = ''\n",
    "        act_artworks_dict['inception_latest_date_prec'] = ''\n",
    "        \n",
    "    if act_found:\n",
    "        act_artworks_dict['inception_val'] = generate_date_string(act_date, act_bce)\n",
    "        if act_circa:\n",
    "            act_artworks_dict['inception_sourcing_circumstances'] = 'Q5727902'\n",
    "        else:\n",
    "            act_artworks_dict['inception_sourcing_circumstances'] = ''\n",
    "        if act_century:\n",
    "            act_artworks_dict['inception_prec'] = '7'\n",
    "        else:\n",
    "            act_artworks_dict['inception_prec'] = '9'\n",
    "        if act_range:\n",
    "            act_artworks_dict['inception_earliest_date_val'] = generate_date_string(act_start_date, act_bce)\n",
    "            if act_century:\n",
    "                act_artworks_dict['inception_earliest_date_prec'] = '7'\n",
    "            else:\n",
    "                act_artworks_dict['inception_earliest_date_prec'] = '9'\n",
    "            act_artworks_dict['inception_latest_date_val'] = generate_date_string(act_end_date, act_bce)\n",
    "            if act_century:\n",
    "                act_artworks_dict['inception_latest_date_prec'] = '7'\n",
    "            else:\n",
    "                act_artworks_dict['inception_latest_date_prec'] = '9'\n",
    "        else:\n",
    "            act_artworks_dict['inception_earliest_date_val'] = ''\n",
    "            act_artworks_dict['inception_earliest_date_prec'] = ''\n",
    "            act_artworks_dict['inception_latest_date_val'] = ''\n",
    "            act_artworks_dict['inception_latest_date_prec'] = ''\n",
    "\n",
    "    # Process country of origin\n",
    "    country_string = row['LocationCountry'].strip()\n",
    "    if row['OriginalLocation'] != '':\n",
    "        country_string = row['OriginalLocation'].strip() # override if there is an original location (rare)\n",
    "    if country_string != '':\n",
    "        country_qid_series = country_mappings.loc[country_mappings.string == country_string, 'qid']\n",
    "        if len(country_qid_series) == 1: # must be at least one match\n",
    "            act_artworks_dict['country_of_origin'] = country_qid_series.values[0]\n",
    "        else:\n",
    "            act_artworks_dict['country_of_origin'] = ''\n",
    "\n",
    "    # *** Find out if the work is Public Domain ***\n",
    "    \n",
    "    act_copyright_string = row['CopyrightStatus'].strip()\n",
    "    \n",
    "    # Start off with no values and overwrite as discovered\n",
    "    act_artworks_dict['copyright_status'] = ''\n",
    "    act_artworks_dict['copyright_status_applies_to_jurisdiction'] = ''\n",
    "    act_artworks_dict['copyright_status_determination_method'] = ''\n",
    "\n",
    "    # Determine years since inception date, if known\n",
    "    if act_artworks_dict['inception_val'] != '':\n",
    "        try:\n",
    "            years_since_inception = int(today[:4]) - int(act_artworks_dict['inception_val'][:4])\n",
    "        except:\n",
    "            years_since_inception = 0\n",
    "        if years_since_inception > YEARS_SINCE_INCEPTION_SCREENING_AGE:\n",
    "            act_artworks_dict['copyright_status'] = 'Q19652' # Public Domain\n",
    "            act_artworks_dict['copyright_status_applies_to_jurisdiction'] = 'Q60332278' # countries with 100 years pma or shorter\n",
    "            act_artworks_dict['copyright_status_determination_method'] = 'Q29940705' # 100 years or more after author's death\n",
    "        \n",
    "    if 'public domain' in row['CopyrightStatus'].lower():\n",
    "        # Determination method will be left blank since we don't know how the sources decided this\n",
    "        act_artworks_dict['copyright_status'] = 'Q19652' # OK to write over value if already determined from dates\n",
    "\n",
    "    # Add part_of data\n",
    "    act_artworks_dict['part_of'] = row['part_of']\n",
    "\n",
    "    # Add references for properties that have values.\n",
    "    for property in property_list:\n",
    "        if act_artworks_dict[property] != '':\n",
    "            act_artworks_dict[property + '_ref1_referenceUrl'] = ACT_BASE_URL + act_artworks_dict['act']\n",
    "            act_artworks_dict[property + '_ref1_retrieved_val'] = today\n",
    "\n",
    "    # Remove the act_ref1_referenceUrl column since the ACT ID will be linked to a URL\n",
    "    act_artworks_dict.pop('act_ref1_referenceUrl')\n",
    "\n",
    "    # Add the dict to the dataframe.\n",
    "    act_artworks_df = act_artworks_df.append(act_artworks_dict, ignore_index=True)\n",
    "\n",
    "# Write the new dataframe to a CSV file.\n",
    "act_artworks_df.to_csv('act_artworks.csv', index=False)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing titles\n",
    "\n",
    "This is a one-time manipulation to sort out unique title/description combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read in the richardson.xlsx file\n",
    "richardson_df = pd.read_excel('richardson.xlsx', dtype=str, na_filter=False)\n",
    "richardson_df.head()\n",
    "\n",
    "# Create a dataframe to hold the rows with unique values in the Title column.\n",
    "# Start by creating a blank dataframe with the same column headers as the richardson_df dataframe.\n",
    "unique_titles_df = pd.DataFrame(columns=richardson_df.columns)\n",
    "\n",
    "# Create another dataframe for the rows with duplicate values in the Title column.\n",
    "# Start by creating a blank dataframe with the same column headers as the richardson_df dataframe.\n",
    "duplicate_titles_df = pd.DataFrame(columns=richardson_df.columns)\n",
    "\n",
    "# Create a list to hold the titles from the Title column.\n",
    "title_list = richardson_df['Title'].tolist()\n",
    "\n",
    "# Loop through the rows in the richardson_df dataframe.\n",
    "for index, row in richardson_df.iterrows():\n",
    "    # Get the title from the Title column.\n",
    "    title = row['Title']\n",
    "\n",
    "    # Determine if the title is in the title_list more than once.\n",
    "    if title_list.count(title) > 1:\n",
    "        # If the title is in the title_list more than once, add the row to the duplicate_titles_df dataframe.\n",
    "        duplicate_titles_df = duplicate_titles_df.append(row, ignore_index=True)\n",
    "    else:\n",
    "        # If the title is in the title_list only once, add the row to the unique_titles_df dataframe.\n",
    "        unique_titles_df = unique_titles_df.append(row, ignore_index=True)\n",
    "\n",
    "# Write the unique_titles_df dataframe to a Excel file.\n",
    "unique_titles_df.to_excel('richardson_unique_titles.xlsx', index=False)\n",
    "\n",
    "# Write the duplicate_titles_df dataframe to a Excel file.\n",
    "duplicate_titles_df.to_excel('richardson_duplicate_titles.xlsx', index=False)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step to generate the artwork_metadata.csv file needed as input for the commonstool.py script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Open the cleaned richardson.csv file (final cleaned up metadata dump) and read it into a dataframe.\n",
    "richardson_df = csv_read('richardson.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# Open the act_artworks.csv file (VanderBot input/output file) and read it into a dataframe. (Needed to get the Q IDs that correspond to the ACT IDs.)\n",
    "act_artworks_df = csv_read('act_artworks.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# Create a list of the column names for the output CSV file.\n",
    "output_column_list = [\n",
    "    'qid',\n",
    "    'label_en',\n",
    "    'act_id',\n",
    "    'dimension',\n",
    "    'status',\n",
    "    'inception_val'\n",
    "]\n",
    "\n",
    "# Create a new dataframe with the columns in the order specified in column_list.\n",
    "artwork_metadata_df = pd.DataFrame(columns=output_column_list)\n",
    "\n",
    "# Copy the qid, label_en, inception_val, and act columns from the act_artworks_df dataframe to the artwork_metadata_df dataframe.\n",
    "artwork_metadata_df['qid'] = act_artworks_df['qid']\n",
    "artwork_metadata_df['label_en'] = act_artworks_df['label_en']\n",
    "artwork_metadata_df['act_id'] = act_artworks_df['act']\n",
    "artwork_metadata_df['inception_val'] = act_artworks_df['inception_val']\n",
    "\n",
    "# Loop through each row in the artwork_metadata_df dataframe and add the dimension for the row whose RecordNumber value matches the act_id value in the artwork_metadata_df row.\n",
    "for index, row in artwork_metadata_df.iterrows():\n",
    "    # Get the act_id value from the artwork_metadata_df row.\n",
    "    act_id = row['act_id']\n",
    "\n",
    "    # Get the dimension value from the richardson_df row whose RecordNumber value matches the act_id value.\n",
    "    dimension = richardson_df.loc[richardson_df['RecordNumber'] == act_id, 'dimension'].values[0]\n",
    "\n",
    "    # Add the dimension value to the artwork_metadata_df row.\n",
    "    artwork_metadata_df.at[index, 'dimension'] = dimension\n",
    "\n",
    "# Write the artwork_metadata_df dataframe to a CSV file.\n",
    "artwork_metadata_df.to_csv('artwork_metadata.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add data to images.csv file. After running and checking the output, I saved a copy of the original images.csv file as images_unscreened.csv and renamed images_out.csv to images.csv ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Open dataframe containing the local_filename/local_identifier data and read it into a dataframe.\n",
    "images_df = csv_read('images.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# Open the artwork_metadata.csv file and read it into a dataframe.\n",
    "artwork_metadata_df = csv_read('artwork_metadata.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# Populate the qid and label column of the images_df dataframe with the Q IDs from the qid column and label_en column of the artwork_metadata_df by matching the local_identifier column of the images_df with the act_id column of the artwork_metadata_df.\n",
    "for index, row in artwork_metadata_df.iterrows():\n",
    "    # Get the act_id value from the artwork_metadata_df row.\n",
    "    act_id = row['act_id']\n",
    "\n",
    "    # Find the row in the images_df dataframe whose local_identifier value matches the act_id value and get the index of that row.\n",
    "    images_df_index = images_df.index[images_df['local_identifier'] == act_id].tolist()[0]\n",
    "\n",
    "    # Set the qid value in the images_df row to the qid value from the artwork_metadata_df row.\n",
    "    images_df.at[images_df_index, 'qid'] = row['qid']\n",
    "\n",
    "    # Set the label value in the images_df row to the label_en value from the artwork_metadata_df row.\n",
    "    images_df.at[images_df_index, 'label'] = row['label_en']\n",
    "\n",
    "# Remove all of the rows that do not have a qid value from the dataframe.\n",
    "images_df = images_df[images_df['qid'] != '']\n",
    "\n",
    "# Save the images_df dataframe to a CSV file.\n",
    "images_df.to_csv('images_out.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fix Macklin Bible artists\n",
    "\n",
    "The artists were originally put in as anonymous, but the ACT website actually has all of their names. So they all need to be changed to the actual artist names.\n",
    "\n",
    "The first step is to delete all of the existing anonymous creator statements. The output file is used with vanderdeletebot.py to delete the statements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the macklin.csv file of artist names and read it into a dataframe.\n",
    "macklin_df = csv_read('macklin.csv', na_filter=False, dtype = str)\n",
    "# Set the qid column as the index\n",
    "macklin_df = macklin_df.set_index('qid')\n",
    "\n",
    "# Open the act_artworks.csv file (VanderBot input/output file) and read it into a dataframe.\n",
    "act_artworks_df = csv_read('act_artworks.csv', na_filter=False, dtype = str)\n",
    "# Set the qid column as the index\n",
    "act_artworks_df = act_artworks_df.set_index('qid')\n",
    "\n",
    "# Create an empty dataframe to hold the rows whose artist needs to be deleted.\n",
    "delete_artist_df = pd.DataFrame(columns=['qid', 'creator_uuid'])\n",
    "\n",
    "# Loop through each row in the Macklin dataframe to get the qid. \n",
    "# Find the row in the act_artworks_df dataframe that has the same qid. \n",
    "# Add the qid and the creator_uuid as a new row to the delete_artist_df dataframe.\n",
    "for index, row in macklin_df.iterrows():\n",
    "    # Get the qid from the Macklin dataframe row index.\n",
    "    qid = index\n",
    "\n",
    "    # Get the creator_uuid from the act_artworks_df dataframe row whose qid matches the qid from the Macklin dataframe row.\n",
    "    creator_uuid = act_artworks_df.loc[qid, 'creator_uuid']\n",
    "\n",
    "    # Add the qid and the creator_uuid as a new row to the delete_artist_df dataframe.\n",
    "    delete_artist_df = delete_artist_df.append({'qid': qid, 'creator_uuid': creator_uuid}, ignore_index=True)\n",
    "\n",
    "# Save the delete_artist_df dataframe to a CSV file.\n",
    "delete_artist_df.to_csv('deletions.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete any rows in the spreadsheet where the artists weren't found.\n",
    "\n",
    "The next step is to add change the creator UUID identifiers from the act_artworks.csv file to empty strings, and replace the creator value with the actual Q ID for the first artist of each work. NOTE: the creator_object_has_role column, creator ref1_hash column, and creator_ref1_retrieved_nodeId column also need to be cleared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# Open the macklin.csv file of artist names and read it into a dataframe.\n",
    "macklin_df = csv_read('macklin.csv', na_filter=False, dtype = str)\n",
    "# Set the qid column as the index\n",
    "macklin_df = macklin_df.set_index('qid')\n",
    "\n",
    "# Open the act_artworks.csv file (VanderBot input/output file) and read it into a dataframe.\n",
    "act_artworks_df = csv_read('act_artworks.csv', na_filter=False, dtype = str)\n",
    "# Set the qid column as the index\n",
    "act_artworks_df = act_artworks_df.set_index('qid')\n",
    "\n",
    "# Step through each row of the main dataframe.\n",
    "for index, row in macklin_df.iterrows():\n",
    "    # Find the row in the act_artworks_df dataframe that has the same index as the macklin_df row index.\n",
    "    qid = index\n",
    "\n",
    "    # Get the creator value from the macklin_df row.\n",
    "    creator = row['artist1_qid']\n",
    "\n",
    "    # Set the value of the creator_uuid, creator_object_has_role, creator_ref1_hash, \n",
    "    # and creator_ref1_retrieved_nodeId columns in the act_artworks_df row to empty string\n",
    "    act_artworks_df.at[qid, 'creator_uuid'] = ''\n",
    "    act_artworks_df.at[qid, 'creator_object_has_role'] = ''\n",
    "    act_artworks_df.at[qid, 'creator_ref1_hash'] = ''\n",
    "    act_artworks_df.at[qid, 'creator_ref1_retrieved_nodeId'] = ''\n",
    "\n",
    "    # Set the value of the creator column in the act_artworks_df row to the creator value from the macklin_df row.\n",
    "    act_artworks_df.at[qid, 'creator'] = creator\n",
    "\n",
    "# Save the modified act_artworks_df dataframe to a CSV file.\n",
    "act_artworks_df.to_csv('act_artworks.csv')\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Vanderbot to update the first artist added to the creator field.\n",
    "\n",
    "Manually used the creator_additional.csv file and csv-metadata.json files from the gallery to add the second artists (sort and manual copy).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
