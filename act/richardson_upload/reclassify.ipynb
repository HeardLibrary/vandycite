{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, need to get the statement UUIDs from the Wikidata upload record and put them in a separate CSV file that VanderDeleteBot can use to remove the P31 statements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Wikidata upload file data\n",
    "wikidata_data = pd.read_csv('act_artworks.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# Load the Richardson data dump data\n",
    "richardson_data = pd.read_csv('richardson.csv', na_filter=False, dtype = str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the qid and instance_of_uuid values for each row in the wikidata_data dataframe that has a value \n",
    "# in the classification column in the richardson_data dataframe and whose Wikidata P31 value is still Q838948 (work of art). \n",
    "# Use the act column in the wikidata_data\n",
    "# dataframe and the RecordNumber column in the richardson_data dataframe to match rows.\n",
    "# Create a new dataframe with the qid and instance_of_uuid values.\n",
    "\n",
    "deletions_df = pd.DataFrame(columns=['qid', 'instance_of_uuid'])\n",
    "for index, row in richardson_data.iterrows():\n",
    "    if row['classification'] != '':\n",
    "        try:\n",
    "            qid = wikidata_data.loc[wikidata_data['act'] == row['RecordNumber'], 'qid'].iloc[0]\n",
    "            p31_value = wikidata_data.loc[wikidata_data['act'] == row['RecordNumber'], 'instance_of'].iloc[0]\n",
    "            if p31_value == 'Q838948':\n",
    "                instance_of_uuid = wikidata_data.loc[wikidata_data['act'] == row['RecordNumber'], 'instance_of_uuid'].iloc[0]\n",
    "                deletions_df = deletions_df.append({'qid': qid, 'instance_of_uuid': instance_of_uuid}, ignore_index=True)\n",
    "        except:\n",
    "            print('No match found for ' + row['RecordNumber'])\n",
    "\n",
    "# Write the deletions_df dataframe to a csv file\n",
    "deletions_df.to_csv('deletions.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to change the existing P31 statements to improved ones using the classification label/qid mappings from the category_breakdown.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_mappings = pd.read_csv('category_breakdown.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# Step through each row in the richardson_data dataframe and replace the value in the instance_of column \n",
    "# of the wikidata_data dataframe based on the classification mappings and the corresponding value in the \n",
    "# classification column of the richardson_data dataframe.\n",
    "for index, row in richardson_data.iterrows():\n",
    "    if row['classification'] != '':\n",
    "        try:\n",
    "            p31_value = wikidata_data.loc[wikidata_data['act'] == row['RecordNumber'], 'instance_of'].iloc[0]\n",
    "            if p31_value == 'Q838948':\n",
    "                # Replace the value in the instance_of column of the wikidata_data dataframe with the mapped value for the classification value.\n",
    "                classification = row['classification']\n",
    "                instance_of = classification_mappings.loc[classification_mappings['classification_label'] == classification, 'qid'].iloc[0]\n",
    "                wikidata_data.loc[wikidata_data['act'] == row['RecordNumber'], 'instance_of'] = instance_of\n",
    "\n",
    "                # Set the values of the instance_of_uuid, instance_of_ref1_hash, and instance_of_ref1_retrieved_nodeId columns in the row to empty strings.\n",
    "                wikidata_data.loc[wikidata_data['act'] == row['RecordNumber'], 'instance_of_uuid'] = ''\n",
    "                wikidata_data.loc[wikidata_data['act'] == row['RecordNumber'], 'instance_of_ref1_hash'] = ''\n",
    "                wikidata_data.loc[wikidata_data['act'] == row['RecordNumber'], 'instance_of_ref1_retrieved_nodeId'] = ''\n",
    "        except:\n",
    "            #print('No match found for ' + row['RecordNumber'])\n",
    "            pass\n",
    "\n",
    "# Sort the wikidata_data dataframe by the qid column\n",
    "wikidata_data = wikidata_data.sort_values(by=['qid'])\n",
    "\n",
    "# Write the wikidata_data dataframe to a csv file\n",
    "wikidata_data.to_csv('act_artworks_modified.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P31 reclassifications\n",
    "\n",
    "On 2024-02-27, Charlotte provided some improved classifications that should be used as replacements for the current values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "# Load the Wikidata upload file data\n",
    "wikidata_data = pd.read_csv('act_artworks.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# Load the Richardson data dump data\n",
    "richardson_data = pd.read_csv('richardson.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# Open the reclassifications.csv file and read the data into a dataframe\n",
    "reclassifications_df = pd.read_csv('reclassifications.csv', na_filter=False, dtype = str)\n",
    "reclassifications_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the qid and instance_of_uuid values for each row in the reclassifications_df dataframe \n",
    "# Use the qid column in the reclassifications_df dataframe and the qid column in the wikidata_data dataframe to match rows.\n",
    "\n",
    "# Create a new dataframe with the qid and instance_of_uuid values.\n",
    "\n",
    "deletions_df = pd.DataFrame(columns=['qid', 'instance_of_uuid'])\n",
    "for index, row in reclassifications_df.iterrows():\n",
    "    try:\n",
    "        qid = row['qid']\n",
    "        p31_value = wikidata_data.loc[wikidata_data['qid'] == row['qid'], 'instance_of'].iloc[0]\n",
    "        instance_of_uuid = wikidata_data.loc[wikidata_data['qid'] == row['qid'], 'instance_of_uuid'].iloc[0]\n",
    "        deletions_df = deletions_df.append({'qid': qid, 'instance_of_uuid': instance_of_uuid}, ignore_index=True)\n",
    "    except:\n",
    "        print('No match found for ' + row['RecordNumber'])\n",
    "\n",
    "# Write the deletions_df dataframe to a csv file\n",
    "deletions_df.to_csv('deletions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step through each row in the reclassifications_df dataframe and replace the value in the instance_of column \n",
    "# of the wikidata_data dataframe based on the value in the p31 column of the reclassifications_df.\n",
    "for index, row in reclassifications_df.iterrows():\n",
    "    try:\n",
    "        p31_value = row['p31']\n",
    "        wikidata_data.loc[wikidata_data['qid'] == row['qid'], 'instance_of'] = p31_value\n",
    "\n",
    "        # Set the values of the instance_of_uuid, instance_of_ref1_hash, and instance_of_ref1_retrieved_nodeId columns in the row to empty strings.\n",
    "        wikidata_data.loc[wikidata_data['qid'] == row['qid'], 'instance_of_uuid'] = ''\n",
    "        wikidata_data.loc[wikidata_data['qid'] == row['qid'], 'instance_of_ref1_hash'] = ''\n",
    "        wikidata_data.loc[wikidata_data['qid'] == row['qid'], 'instance_of_ref1_retrieved_nodeId'] = ''\n",
    "    except:\n",
    "        #print('No match found for ' + row['RecordNumber'])\n",
    "        pass\n",
    "\n",
    "# Sort the wikidata_data dataframe by the qid column\n",
    "wikidata_data = wikidata_data.sort_values(by=['qid'])\n",
    "\n",
    "# Write the wikidata_data dataframe to a csv file\n",
    "wikidata_data.to_csv('act_artworks_modified.csv', index=False)\n",
    "print('done')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
