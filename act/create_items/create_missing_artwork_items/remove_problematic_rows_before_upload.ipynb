{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is the last step before completing Phase 2 of the ACT project by uploading with VanderBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup # web-scraping library, use PIP to install beautifulsoup4 (included in Anaconda)\n",
    "import requests\n",
    "import json\n",
    "import re # regex\n",
    "from time import sleep\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "get_server_sleep = 0.1 # number of seconds to wait before get calls to webserver\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "date_problems_frame = pd.read_csv('issues_with_inception_dates.csv', na_filter=False, dtype = str)\n",
    "date_problems_list = list(date_problems_frame['act'])\n",
    "creator_mismatches_frame = pd.read_csv('creator_name_mismatches.csv', na_filter=False, dtype = str)\n",
    "creator_mismatches_list = list(creator_mismatches_frame['act'])\n",
    "works_frame = pd.read_csv('abstract_artworks_charlotte_edits.csv', na_filter=False, dtype = str)\n",
    "works_frame = works_frame.head(7).copy()\n",
    "works_frame.head()\n",
    "\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderBot/1.6 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "def check_commons_page_for_wikidata_image_link(image_filename):\n",
    "    # image_filename = 'Christ sur la mer de Galilée (Delacroix) Walters Art Museum 37.186.jpg'\n",
    "    # image_filename = 'Christ_and_SocratesSAAM_1974.28.341A_B_1.jpg'\n",
    "    page_url = 'https://commons.wikimedia.org/wiki/File:' + image_filename\n",
    "    response = requests.get(page_url)\n",
    "    \n",
    "    # Create a soup object and find the file info table\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    \n",
    "    image_tags = soup.findAll('img', src= 'https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/20px-Wikidata-logo.svg.png')\n",
    "    if len(image_tags) > 0:\n",
    "        anchor = image_tags[0].parent\n",
    "        link = anchor['href']\n",
    "        qid = extract_localname(link)\n",
    "    else:\n",
    "        qid = ''\n",
    "    sleep(get_server_sleep) # Don't hit the API too fast\n",
    "    return qid\n",
    "\n",
    "def check_commons_page_for_wikidata_link(image_filename):\n",
    "    # image_filename = 'Christ sur la mer de Galilée (Delacroix) Walters Art Museum 37.186.jpg'\n",
    "    # image_filename = 'Christ_and_SocratesSAAM_1974.28.341A_B_1.jpg'\n",
    "    page_url = 'https://commons.wikimedia.org/wiki/File:' + image_filename\n",
    "    response = requests.get(page_url)\n",
    "    \n",
    "    # Create a soup object and find the file info table\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    tables = soup.findAll('table', class_= re.compile('fileinfotpl'))\n",
    "    if len(tables) > 0:\n",
    "        # Have to check for this span because there are other subtables with a tags besides the one at the top\n",
    "        span = tables[0].findAll('span', id = 'artwork')\n",
    "        if len(span) > 0:\n",
    "            # The link to the Wikidata item will be in an href\n",
    "            # Need to go up to parent, since the anchor is sometimes a sibling tag and not a child tag\n",
    "            anchors = span[0].parent.findAll('a', href = re.compile('https://www.wikidata.org/wiki/'))\n",
    "            if len(anchors) > 0:\n",
    "                try:\n",
    "                    link = anchors[0]['href']\n",
    "                    qid = extract_localname(link)\n",
    "                except:\n",
    "                    qid = ''\n",
    "            else:\n",
    "                qid = ''\n",
    "        else:\n",
    "            qid = ''\n",
    "    else:\n",
    "        qid = ''\n",
    "    sleep(get_server_sleep) # Don't hit the API too fast\n",
    "    return qid\n",
    "\n",
    "# function to get local name from an IRI\n",
    "def extract_localname(iri):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[len(pieces)-1] # return the last piece\n",
    "\n",
    "def get_wikidata_item_label(qid):\n",
    "    query = '''\n",
    "    select distinct ?label\n",
    "    where {\n",
    "    wd:''' + qid + ''' rdfs:label ?label.\n",
    "    filter(lang(?label)=\"en\")\n",
    "    }'''\n",
    "    #print(query)\n",
    "\n",
    "    #print('querying SPARQL endpoint to acquire item metadata')\n",
    "    response = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    #print(response.text)\n",
    "    data = response.json()\n",
    "\n",
    "    # extract the values from the response JSON\n",
    "    results = data['results']['bindings']\n",
    "\n",
    "    #print('done retrieving data')\n",
    "    #print(json.dumps(results, indent=2))\n",
    "    if len(results) > 0:\n",
    "        label = results[0]['label']['value']\n",
    "    else:\n",
    "        label = ''\n",
    "    sleep(sparql_sleep)\n",
    "    return label\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull out rows with Commons page having link to Wikidata\n",
    "\n",
    "There are a bunch of rows, mostly with details, where the link to Wikidata wasn't found. This checks using the new function that looks for the tiny Wikidata flag image to get the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wikidata_link_found_output_list = []\n",
    "work_qid_list = []\n",
    "work_label_list = []\n",
    "remaing_works_list = []\n",
    "for work_index, work_row in works_frame.iterrows():\n",
    "    print(work_row['act'])\n",
    "    \n",
    "    qid = check_commons_page_for_wikidata_link(work_row['image'])\n",
    "    #print(qid)\n",
    "\n",
    "    if qid != '':\n",
    "        wikidata_link_found_output_list.append(work_row)\n",
    "        work_qid_list.append(qid)\n",
    "        label = get_wikidata_item_label(qid)\n",
    "        work_label_list.append(label)\n",
    "        #print(label)\n",
    "    else:\n",
    "        remaing_works_list.append(work_row)\n",
    "    #print()\n",
    "\n",
    "remaing_works_frame = pd.DataFrame(remaing_works_list)\n",
    "\n",
    "wikidata_link_found_output_frame = pd.DataFrame(wikidata_link_found_output_list)\n",
    "\n",
    "# Stick the lists of found work Q IDs and labels onto the end of the DataFrame before saving\n",
    "wikidata_link_found_output_frame['work_qid'] = work_qid_list\n",
    "wikidata_link_found_output_frame['work_label'] = work_label_list\n",
    "\n",
    "wikidata_link_found_output_frame.to_csv('wikidata_link_found.csv')\n",
    "wikidata_link_found_output_frame.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull out rows with date problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_problems_output_list = []\n",
    "remaing_works_list = []\n",
    "for work_index, work_row in remaing_works_frame.iterrows():\n",
    "    #print(work_row['act'])\n",
    "    \n",
    "    if work_row['act'] in date_problems_list: # only try to match if it's on the problem list\n",
    "        for date_index, date_row in date_problems_frame.iterrows():\n",
    "            if date_row['act'] == work_row['act']:\n",
    "                date_problems_output_list.append(work_row)\n",
    "    else:\n",
    "        remaing_works_list.append(work_row)\n",
    "\n",
    "remaing_works_frame = pd.DataFrame(remaing_works_list)\n",
    "\n",
    "date_problems_output_frame = pd.DataFrame(date_problems_output_list)\n",
    "date_problems_output_frame.to_csv('date_problems.csv')\n",
    "date_problems_output_frame.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull out rows with missing creator values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_creators_output_list = []\n",
    "remaing_works_list = []\n",
    "for work_index, work_row in remaing_works_frame.iterrows():\n",
    "    #print(work_row['act'])\n",
    "    \n",
    "    if work_row['creator'] == '':\n",
    "        missing_creators_output_list.append(work_row)\n",
    "    else:\n",
    "        remaing_works_list.append(work_row)\n",
    "\n",
    "remaing_works_frame = pd.DataFrame(remaing_works_list)\n",
    "\n",
    "missing_creators_output_frame = pd.DataFrame(missing_creators_output_list)\n",
    "missing_creators_output_frame.to_csv('missing_creators.csv')\n",
    "missing_creators_output_frame.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull out rows with creator name mismatches "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator_mismatch_output_list = []\n",
    "remaing_works_list = []\n",
    "for work_index, work_row in remaing_works_frame.iterrows():\n",
    "    #print(work_row['act'])\n",
    "    \n",
    "    if work_row['act'] in creator_mismatches_list: # only try to match if it's on the problem list\n",
    "        for date_index, date_row in creator_mismatches_frame.iterrows():\n",
    "            if date_row['act'] == work_row['act']:\n",
    "                creator_mismatch_output_list.append(work_row)\n",
    "    else:\n",
    "        remaing_works_list.append(work_row)\n",
    "\n",
    "remaing_works_frame = pd.DataFrame(remaing_works_list)\n",
    "\n",
    "creator_mismatch_output_frame = pd.DataFrame(creator_mismatch_output_list)\n",
    "creator_mismatch_output_frame.to_csv('creator_mismatch_problems.csv')\n",
    "creator_mismatch_output_frame.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the works that remain after screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remaing_works_frame.to_csv('works_to_write.csv')\n",
    "remaing_works_frame.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
