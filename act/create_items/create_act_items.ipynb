{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script to create Wikidata items for ACT artwork\n",
    "\n",
    "Initially, this will be for works that are already in Commons, but for which the links to Commons are from non-artwork Wikidata items. Eventually, we can modify this for any work that's in Commons and doesn't have a Wikidata item.\n",
    "\n",
    "## Properties whose values need to be cleaned/generated\n",
    "\n",
    "Taken from [here](https://github.com/HeardLibrary/vandycite/blob/master/act/processed_lists/candidate_properties_to_write.csv).\n",
    "\n",
    "P571 (inception): get from ACT \"DateCreation\" and Commons \"date\" fields\n",
    "\n",
    "## Configuration section\n",
    "\n",
    "Run once at the start\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "import json\n",
    "import csv\n",
    "import math\n",
    "import datetime\n",
    "# Pandas for data frame management\n",
    "import pandas as pd\n",
    "\n",
    "# Load data from the two sources (ACT database dump and Commons Mediawiki table scrape)\n",
    "act_data = pd.read_csv('act_data_fix.csv', na_filter=False, dtype = str)\n",
    "commons_data = pd.read_csv('commons_data_fix.csv', na_filter=False, dtype = str)\n",
    "ids = pd.read_csv('clean_ids.csv', na_filter=False, dtype = str)\n",
    "\n",
    "# For testing purposes, just use the first few rows\n",
    "#act_data = act_data.head(100).copy()\n",
    "#commons_data = commons_data.head(100).copy()\n",
    "\n",
    "# --------------------\n",
    "# Low-level functions\n",
    "# --------------------\n",
    "\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "today = generate_utc_date()\n",
    "\n",
    "# Read from a CSV file on disk into a list of dictionaries (representing a table)\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        table = []\n",
    "        for row in dict_object:\n",
    "            table.append(row)\n",
    "    return table\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def pad_zeros_left(date_string):\n",
    "    length = len(date_string)\n",
    "    pad = 4-length\n",
    "    return '0' * pad + date_string\n",
    "\n",
    "def generate_date_string(date, bce):\n",
    "    if bce:\n",
    "        date_string = '-'\n",
    "    else:\n",
    "        date_string = ''\n",
    "    date_string += pad_zeros_left(str(date)) + '-01-01T00:00:00Z'\n",
    "    return date_string\n",
    "\n",
    "# ----------------------------\n",
    "# Intermediate-level functions\n",
    "# ----------------------------\n",
    "\n",
    "# Parse the ACT date string into structured components\n",
    "def process_act_date(act_date):\n",
    "    act_circa = False\n",
    "    act_range = False\n",
    "    act_century = False\n",
    "    non_numeric = False\n",
    "    date = 0\n",
    "    start_date = 0\n",
    "    end_date = 0\n",
    "    \n",
    "    # If there is no date from ACT, kill the function and return False\n",
    "    if act_date == '':\n",
    "        return False, date, act_range, start_date, end_date, act_century, act_circa\n",
    "    \n",
    "    # Determine circa status of ACT date\n",
    "    if 'ca.' in act_date:\n",
    "        act_circa = True\n",
    "        # Remove the \"ca.\" from the beginning and clean whitespace\n",
    "        act_date = act_date.split('ca.')[1].strip()\n",
    "    \n",
    "    # Test whether the ACT date is a number\n",
    "    try:\n",
    "        date = int(act_date)\n",
    "        #print('numeric date:', date)\n",
    "    except:\n",
    "        non_numeric = True\n",
    "        #print('non-numeric string:', act_date)\n",
    "        \n",
    "    if non_numeric:\n",
    "        # Determine century status of ACT date\n",
    "        if 'century' in act_date: # single century date\n",
    "            act_century = True\n",
    "            # Remove the \"century\" and \"th\", \"rd\", \"st\", etc. from the end\n",
    "            act_date = act_date[:-10]\n",
    "            non_numeric = False\n",
    "            try:\n",
    "                date = int(act_date) * 100 - 50 # set the date at mid-century\n",
    "            except:\n",
    "                print('numeric conversion error on', act_date)\n",
    "        elif 'centuries' in act_date:\n",
    "            act_century = True\n",
    "            act_range = True\n",
    "            # Remove the \"centuries\" and \"th\", \"rd\", \"st\", etc. from the end\n",
    "            act_date = act_date[:-10].strip()\n",
    "            try:\n",
    "                pieces = act_date.split('-')\n",
    "                start_date = int(pieces[0][:-2]) * 100 - 50 # set the date at mid-century\n",
    "                end_date = int(pieces[1][:-2]) * 100 - 50 # set the date at mid-century\n",
    "            except:\n",
    "                print('error in processing century range')\n",
    "    # Process date ranges (non-numeric because they include \"-\")\n",
    "    if non_numeric and not act_century:\n",
    "        #print(act_date)\n",
    "        try:\n",
    "            pieces = act_date.split('-')\n",
    "            start_date = int(pieces[0])\n",
    "            end_date = int(pieces[1])\n",
    "            act_range = True\n",
    "        except:\n",
    "            print('error in processing date range')\n",
    "\n",
    "        \n",
    "    # if there is a range of dates, set the single date as the midpoint\n",
    "    if start_date != 0 or end_date != 0:\n",
    "        date = math.floor((start_date + end_date)/2)\n",
    "            \n",
    "    return True, date, act_range, start_date, end_date, act_century, act_circa\n",
    "    \n",
    "# Disassemble Wikibase-style dateTime strings into year, precision, and BCE components\n",
    "def extract_from_iso_date_string(string):\n",
    "    pieces = string.split('/')\n",
    "    # precision comes after the slash in the Wikibase format\n",
    "    precision = pieces[1]\n",
    "    # check for negative sign for BCE dates\n",
    "    if pieces[0][0] == '-':\n",
    "        bce = True\n",
    "    else:\n",
    "        bce = False\n",
    "    no_sign_dateTime = pieces[0][1:] # skip sign\n",
    "    pieces = no_sign_dateTime.split('-')\n",
    "    year = pieces[0]\n",
    "    return int(year), precision, bce\n",
    "\n",
    "# Parse any structured date data that was scraped from the Commons Mediawiki table\n",
    "def process_commons_date(commons_date_string):\n",
    "    # Set all values to defaults to return something even if they aren't determined from the data\n",
    "    commons_circa = False\n",
    "    commons_range = False\n",
    "    date = 0\n",
    "    precision = '9'\n",
    "    bce = False\n",
    "    start_date = 0\n",
    "    start_precision = '9'\n",
    "    start_bce = False\n",
    "    end_date = 0\n",
    "    end_precision = '9'\n",
    "    end_bce = False\n",
    "\n",
    "    commons_date_list = json.loads(commons_date_string)\n",
    "    found = False\n",
    "    for string in commons_date_list:\n",
    "        # Find the part of the extracted metadata that includes the structured data\n",
    "        if 'date QS' in string:\n",
    "            found = True\n",
    "            pieces = string.split(',') # split into fields by comma\n",
    "            pieces = pieces[1:] # get rid of initial \"inception field\"\n",
    "            date, precision, bce = extract_from_iso_date_string(pieces[0])\n",
    "            # Remove the initial date from the list\n",
    "            pieces = pieces[1:]\n",
    "            \n",
    "            # Check for circa\n",
    "            if len(pieces) >= 2:\n",
    "                # Check if last piece is \"circa\"\n",
    "                if pieces[len(pieces)-1] == 'Q5727902':\n",
    "                    commons_circa = True\n",
    "                    # Remove the last two items from the list\n",
    "                    pieces = pieces[:-2]\n",
    "            #print(commons_circa, pieces)\n",
    "            \n",
    "            # Extract start date (if any)\n",
    "            if len(pieces) > 0 and (pieces[0] == 'P1319' or pieces[0] == 'P580'): # check for earliest date or start time\n",
    "                commons_range = True\n",
    "                start_date, start_precision, start_bce = extract_from_iso_date_string(pieces[1]) # start date follows the P ID\n",
    "                # Remove the first two pieces\n",
    "                if len(pieces) > 0:\n",
    "                    pieces = pieces[2:]\n",
    "            \n",
    "            if len(pieces) > 0 and (pieces[0] == 'P1326' or pieces[0] == 'P582'): # check for latest date or end time\n",
    "                commons_range = True\n",
    "                end_date, end_precision, end_bce = extract_from_iso_date_string(pieces[1]) # start date follows the P ID\n",
    "    return found, date, precision, bce, commons_range, start_date, start_precision, start_bce, end_date, end_precision, end_bce, commons_circa\n",
    "    \n",
    "# -------------------\n",
    "# Top level functions\n",
    "# -------------------\n",
    "    \n",
    "def process_dates(act_id, output_dict, issue_log, act_url, commons_url, act_date_string, commons_date_string):\n",
    "    # Hard code ACT BCE to False, at least until it's determined whether any ACT dates\n",
    "    # are designated as BCE or have negative signs.\n",
    "    act_bce = False\n",
    "    \n",
    "    act_found, act_date, act_range, act_start_date, act_end_date, act_century, act_circa  = process_act_date(act_date_string)\n",
    "    #print(act_date, act_start_date, act_end_date)\n",
    "    \n",
    "    commons_found, commons_date, commons_precision, commons_bce, commons_range, commons_start_date, commons_start_precision, commons_start_bce, commons_end_date, commons_end_precision, commons_end_bce, commons_circa = process_commons_date(commons_date_string)\n",
    "    #print(commons_date, commons_start_date, commons_end_date)\n",
    "    \n",
    "    # Perform quality control and determine output values\n",
    "    if not act_found and not commons_found:\n",
    "        issue_log += act_id + ' | ' + filename + ' | no dates retrieved. Commons data: ' + commons_date_string + '\\n'\n",
    "        output_dict['inception_ref1_referenceUrl'] = ''\n",
    "        output_dict['inception_ref1_retrieved_val'] = ''\n",
    "        output_dict['inception_val'] = ''\n",
    "        output_dict['inception_sourcing_circumstances'] = ''\n",
    "        output_dict['inception_prec'] = ''\n",
    "        output_dict['inception_earliest_date_val'] = ''\n",
    "        output_dict['inception_earliest_date_prec'] = ''\n",
    "        output_dict['inception_latest_date_val'] = ''\n",
    "        output_dict['inception_latest_date_prec'] = ''\n",
    "        \n",
    "    if act_found and not commons_found:\n",
    "        output_dict['inception_ref1_referenceUrl'] = act_url\n",
    "        output_dict['inception_ref1_retrieved_val'] = today\n",
    "        output_dict['inception_val'] = generate_date_string(act_date, act_bce)\n",
    "        if act_circa:\n",
    "            output_dict['inception_sourcing_circumstances'] = 'Q5727902'\n",
    "        else:\n",
    "            output_dict['inception_sourcing_circumstances'] = ''\n",
    "        if act_century:\n",
    "            output_dict['inception_prec'] = '7'\n",
    "        else:\n",
    "            output_dict['inception_prec'] = '9'\n",
    "        if act_range:\n",
    "            output_dict['inception_earliest_date_val'] = generate_date_string(act_start_date, act_bce)\n",
    "            if act_century:\n",
    "                output_dict['inception_earliest_date_prec'] = '7'\n",
    "            else:\n",
    "                output_dict['inception_earliest_date_prec'] = '9'\n",
    "            output_dict['inception_latest_date_val'] = generate_date_string(act_end_date, act_bce)\n",
    "            if act_century:\n",
    "                output_dict['inception_latest_date_prec'] = '7'\n",
    "            else:\n",
    "                output_dict['inception_latest_date_prec'] = '9'\n",
    "        else:\n",
    "            output_dict['inception_earliest_date_val'] = ''\n",
    "            output_dict['inception_earliest_date_prec'] = ''\n",
    "            output_dict['inception_latest_date_val'] = ''\n",
    "            output_dict['inception_latest_date_prec'] = ''\n",
    "            \n",
    "    if not act_found and commons_found:\n",
    "        output_dict['inception_ref1_referenceUrl'] = commons_url\n",
    "        output_dict['inception_ref1_retrieved_val'] = today\n",
    "        output_dict['inception_val'] = generate_date_string(commons_date, commons_bce)\n",
    "        if commons_circa:\n",
    "            output_dict['inception_sourcing_circumstances'] = 'Q5727902'\n",
    "        else:\n",
    "            output_dict['inception_sourcing_circumstances'] = ''\n",
    "        output_dict['inception_prec'] = commons_precision\n",
    "        if act_range:\n",
    "            output_dict['inception_earliest_date_val'] = generate_date_string(commons_start_date, commons_start_bce)\n",
    "            output_dict['inception_earliest_date_prec'] = commons_start_precision\n",
    "            output_dict['inception_latest_date_val'] = generate_date_string(commons_end_date, act_bce)\n",
    "            output_dict['inception_latest_date_prec'] = commons_end_precision\n",
    "        else:\n",
    "            output_dict['inception_earliest_date_val'] = ''\n",
    "            output_dict['inception_earliest_date_prec'] = ''\n",
    "            output_dict['inception_latest_date_val'] = ''\n",
    "            output_dict['inception_latest_date_prec'] = ''\n",
    "        \n",
    "    # In the event that dates are available from both sources, use the ACT date data, but flag\n",
    "    # as a potential error if any of the dates disagree.\n",
    "    if act_found and commons_found:\n",
    "        output_dict['inception_ref1_referenceUrl'] = act_url\n",
    "        output_dict['inception_ref1_retrieved_val'] = today\n",
    "        # Inconsistency check for CE/BCE\n",
    "        if act_bce != commons_bce:\n",
    "            issue_log += act_id + ' | ' + filename + ' | Disagreement between ACT and Commons on CE/BCE.\\n'\n",
    "        \n",
    "        # Check for mismatch between the primary inception date of ACT and Commons\n",
    "        if act_date != commons_date:\n",
    "            issue_log += act_id + ' | ' + filename + ' | ACT inception: ' + str(act_date) + ', Commons inception: ' + str(commons_date) + '\\n'\n",
    "        output_dict['inception_val'] = generate_date_string(act_date, act_bce)\n",
    "        \n",
    "        # Inconsistency check for circa\n",
    "        if act_circa != commons_circa:\n",
    "            issue_log += act_id + ' | ' + filename + ' | Disagreement between ACT and Commons on circa.\\n'\n",
    "        if act_circa:\n",
    "            output_dict['inception_sourcing_circumstances'] = 'Q5727902'\n",
    "        else:\n",
    "            output_dict['inception_sourcing_circumstances'] = ''\n",
    "            \n",
    "        if act_century:\n",
    "            act_precision = '7'\n",
    "        else:\n",
    "            act_precision = '9'\n",
    "            \n",
    "        # Perform a precision consistency check. \n",
    "        if act_precision != commons_precision:\n",
    "            issue_log += act_id + ' | ' + filename + ' | ACT precision: ' + act_precision + ', Commons precision: ' + commons_precision + '\\n'\n",
    "        output_dict['inception_prec'] = act_precision\n",
    "        \n",
    "        if act_range:\n",
    "            # Perform date range consistency check\n",
    "            if not(commons_start_date == 0 and commons_end_date == 0): # Skip check if no Commons range\n",
    "                if not(act_start_date == commons_start_date and act_end_date == commons_end_date):\n",
    "                    issue_log += act_id + ' | ' + filename + ' | ACT date range: ' + str(act_start_date) + '-' + str(act_end_date) + ', Commons date range: ' + str(commons_start_date) + '-' + str(commons_end_date) + '\\n'\n",
    "            output_dict['inception_earliest_date_val'] = generate_date_string(act_start_date, act_bce)\n",
    "            output_dict['inception_earliest_date_prec'] = act_precision\n",
    "            output_dict['inception_latest_date_val'] = generate_date_string(act_end_date, act_bce)\n",
    "            output_dict['inception_latest_date_prec'] = act_precision\n",
    "        else:\n",
    "            output_dict['inception_earliest_date_val'] = ''\n",
    "            output_dict['inception_earliest_date_prec'] = ''\n",
    "            output_dict['inception_latest_date_val'] = ''\n",
    "            output_dict['inception_latest_date_prec'] = ''\n",
    "            \n",
    "\n",
    "    # Check dates for reasonableness\n",
    "    if output_dict['inception_val'] > today:\n",
    "        issue_log += act_id + ' | ' + filename + ' | Inception date occurs in the future. Date: ' + output_dict['inception_val'] + '\\n'\n",
    "    if output_dict['inception_earliest_date_val'] > output_dict['inception_latest_date_val']:\n",
    "        issue_log += act_id + ' | ' + filename + ' | Final date in range before initial date: ' + output_dict['inception_earliest_date_val'] + ', ' + output_dict['inception_latest_date_val'] + '\\n'\n",
    "\n",
    "    return output_dict, issue_log\n",
    "\n",
    "def process_labels(filename, commons_data_type, commons_data):\n",
    "    languages = [\n",
    "        {'string':'English', 'code': 'en'},\n",
    "        {'string':'Русский', 'code': 'ru'},\n",
    "        {'string':'Deutsch', 'code': 'de'},\n",
    "        {'string':'Français', 'code': 'fr'},\n",
    "        {'string':'Ελληνικά', 'code': 'el'},\n",
    "        {'string':'Norsk bokmål', 'code': 'nb'},\n",
    "        {'string':'Português', 'code': 'pt'},\n",
    "        {'string':'Italiano', 'code': 'it'},\n",
    "        {'string':'Español', 'code': 'es'},\n",
    "        {'string':'עברית', 'code': 'he'},\n",
    "        {'string':'Català', 'code': 'ca'},\n",
    "        {'string':'Slovenščina', 'code': 'sl'},\n",
    "        {'string':'Hrvatski', 'code': 'hr'},\n",
    "        {'string':'Svenska', 'code': 'sv'},\n",
    "        {'string':'Српски / srpski', 'code': 'sr'},\n",
    "        {'string': '中文', 'code': 'zh-Hans'}\n",
    "         ]\n",
    "\n",
    "    # Look up the title in the Commons data. For information template, it's usually description; for artwork: title\n",
    "    commons_title_list = []\n",
    "    if commons_data_type == 'none':\n",
    "        commons_title = ''\n",
    "    else:\n",
    "        if commons_data_type == 'artwork':\n",
    "            title_field = 'title'\n",
    "        else:\n",
    "            title_field = 'description'\n",
    "        commons_title_list = json.loads(commons_data.loc[commons_data.filename == filename, title_field].values[0])\n",
    "    #print(act_title, commons_title_list)\n",
    "    \n",
    "    work_language_strings = []\n",
    "    build_string = ''\n",
    "    language_code = 'en' # assume starting with English if no explicit tag\n",
    "    # Examine all of the blobs of the Commons language list\n",
    "    for blob in commons_title_list:\n",
    "        found = False\n",
    "        for language in languages:\n",
    "            if language['string'] in blob:\n",
    "                found = True\n",
    "                found_code = language['code']\n",
    "                break\n",
    "        if found:\n",
    "            # close off previous language string\n",
    "            work_language_strings.append({'lang': language_code, 'title': build_string.strip()})\n",
    "            # start new build string\n",
    "            language_code = found_code\n",
    "            build_string = ''\n",
    "        else:\n",
    "            if language_code == 'zh-Hans': # don't put spaces between Chinese characters\n",
    "                build_string += blob\n",
    "            else:\n",
    "                if blob[0] == ',': # don't put a space before a comma\n",
    "                    build_string += blob\n",
    "                else:\n",
    "                    build_string += ' ' + blob # otherwise, put spaces between the concatenated blobs\n",
    "    # When through the entire list, finish off the last string\n",
    "    work_language_strings.append({'lang': language_code, 'title': build_string.strip()})\n",
    "    # If the very first blob was a language tag and nothing was added to the initial string, then delete it\n",
    "    if work_language_strings[0]['title'] == '':\n",
    "        work_language_strings = work_language_strings[1:]\n",
    "            \n",
    "    return work_language_strings\n",
    "\n",
    "def process_type(act_id, act_type_string, commons_type_string, issue_log):\n",
    "    types = [\n",
    "        {'string': 'drawing', 'qid': 'Q93184'},\n",
    "        {'string': 'painting', 'qid': 'Q3305213'},\n",
    "        {'string': 'manuscript', 'qid': 'Q87167'},\n",
    "        {'string': 'sculpture', 'qid': 'Q860861'},\n",
    "        {'string': 'carving', 'qid': 'Q97570030'},\n",
    "        {'string': 'textile', 'qid': 'Q28823'},\n",
    "        {'string': 'garment', 'qid': 'Q11460'},\n",
    "        {'string': 'photograph', 'qid': 'Q125191'},\n",
    "        {'string': 'architecture', 'qid': 'Q811979'},\n",
    "        {'string': 'mosaic', 'qid': 'Q133067'},\n",
    "        {'string': 'fresco', 'qid': 'Q22669139'},\n",
    "        {'string': 'print', 'qid': 'Q11060274'},\n",
    "        {'string': 'mural', 'qid': 'Q219423'},\n",
    "        {'string': 'watercolor', 'qid': 'Q3305213'}\n",
    "         ]\n",
    "\n",
    "    #print(act_type_string, commons_type_string)\n",
    "    commons_type_list = json.loads(commons_type_string)\n",
    "    qid = ''\n",
    "    type_string = ''\n",
    "    data_source = ''\n",
    "    # Look for type in ACT data\n",
    "    found = False\n",
    "    for kind in types:\n",
    "        if kind['string'] in act_type_string.lower():\n",
    "            found = True\n",
    "            type_string = kind['string']\n",
    "            qid = kind['qid']\n",
    "            data_source = 'act'\n",
    "            break\n",
    "    if not found:\n",
    "        found = False\n",
    "        for kind in types:\n",
    "            for blob in commons_type_list:\n",
    "                if kind['string'] in blob.lower():\n",
    "                    found = True\n",
    "                    type_string = kind['string']\n",
    "                    qid = kind['qid']\n",
    "                    data_source = 'commons'\n",
    "                    break\n",
    "    if not found:\n",
    "        issue_log += act_id + ' | Could not identify type (instance of)\\n'\n",
    "        \n",
    "    if type_string == 'architecture':\n",
    "        type_string = 'architectural structure'\n",
    "    \n",
    "    return type_string, qid, issue_log, data_source\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract/clean inception date\n",
    "\n",
    "The dates given in ACT and Commons can have several characteristics:\n",
    "\n",
    "- beginning and ending ranges\n",
    "- circa. Designated very consistently as \"ca.\" in ACT. \n",
    "- century designation (essentially setting the precision to the century level)\n",
    "\n",
    "The Commons data also sometimes is structured using Wikidata date Q IDs, qualifiers, and standard xsd:dateTime format:\n",
    "- Starts with `QS:P571` (inception)\n",
    "- Sometimes has `P1319` (earliest date) and `P1326` (latest date)\n",
    "- Sometimes had `P580` (start time) and `P582` (end time)\n",
    "- Sometimes has qualifier `P1480` (sourcing circumstances) with value `Q5727902` (circa)\n",
    "- Date precisions can be 7 (century), 8 (decade), or 9 (year)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entire_issue_log = ''\n",
    "\n",
    "# If there are any existing data in VanderBot format, load it (or just the file headers)\n",
    "output_list = read_dicts_from_csv('abstract_artworks.csv')\n",
    "\n",
    "labels_data = []\n",
    "\n",
    "for index, work in act_data.iterrows():\n",
    "    issue_log = ''\n",
    "    output_dict = {}\n",
    "    act_id = work['RecordNumber']\n",
    "    act_url = 'http://diglib.library.vanderbilt.edu/act-imagelink.pl?RC=' + act_id\n",
    "    \n",
    "    # Look up the Commons page URL\n",
    "    commons_url = ids.loc[ids.RecordNumber == act_id, 'commons_page_url'].values[0]\n",
    "    \n",
    "    # Look up the Commons filename in the IDs file using the ACT ID\n",
    "    # This seems inefficient to have to look up the row twice, but I suppose Pandas is\n",
    "    # efficient enough with a small dataset like this that it doesn't matter.\n",
    "    filename = ids.loc[ids.RecordNumber == act_id, 'filename'].values[0]\n",
    "\n",
    "    # Determine whether the information or artwork template was used, or if Commons data not available\n",
    "    commons_date_type_series = commons_data.loc[commons_data.filename == filename, 'template_type']    \n",
    "    if len(commons_date_type_series) == 1:\n",
    "        commons_data_type = commons_date_type_series.values[0]\n",
    "    else:\n",
    "        commons_data_type = 'none'\n",
    "    print('commons data type:', commons_data_type)\n",
    "    output_dict['commons_template'] = commons_data_type\n",
    "    \n",
    "    # *** Process title information to generate English label ***\n",
    "    \n",
    "    act_title = work['Title']\n",
    "    \n",
    "    # Extract information about titles/labels from Commons data\n",
    "    work_language_strings = process_labels(filename, commons_data_type, commons_data)\n",
    "    \n",
    "    # Attach the extracted language data to the list for later saving\n",
    "    labels_data.append({'act_id': act_id, 'act_title': act_title, 'commons_data_type': commons_data_type, 'commons_language_strings': work_language_strings})\n",
    "    \n",
    "    # Add discovered label data to the output\n",
    "    output_dict['label_en'] = act_title # Use the ACT title as the default\n",
    "    \n",
    "    commons_label = ''\n",
    "    for work_language in work_language_strings:\n",
    "        if work_language['lang'] == 'en':\n",
    "            commons_label = work_language['title']\n",
    "    output_dict['label_commons'] = commons_label\n",
    "    \n",
    "    # *** Other fields ***\n",
    "    \n",
    "    # Create ACT ID output\n",
    "    output_dict['act'] = act_id\n",
    "    output_dict['act_ref1_retrieved_val'] = today\n",
    "    \n",
    "    # Create Commons image output\n",
    "    # VanderBot will convert the raw, unencoded file name into the appropriate IRI for Wikidata\n",
    "    output_dict['image'] = filename\n",
    "    output_dict['image_ref1_referenceUrl'] = act_url\n",
    "    output_dict['image_ref1_retrieved_val'] = today\n",
    "    \n",
    "    # *** Process inception dates ***\n",
    "    \n",
    "    # Get the ACT date string value\n",
    "    act_date_string = work['DateCreation']\n",
    "\n",
    "    # Look up the date value in the Commons data\n",
    "    if commons_data_type != 'none':\n",
    "        commons_date_string = commons_data.loc[commons_data.filename == filename, 'date'].values[0]\n",
    "    else:\n",
    "        commons_date_string = '[]' # case where no commons data exists for this work\n",
    "\n",
    "    #print(act_date_string, commons_date_string)\n",
    "    \n",
    "    output_dict, issue_log = process_dates(act_id, output_dict, issue_log, act_url, commons_url, act_date_string, commons_date_string)\n",
    "    \n",
    "    # *** Determine type (instance of)\n",
    "    \n",
    "    act_type_string = work['ObjectFunction']\n",
    "    \n",
    "    # Look up the object type value in the Commons data\n",
    "    if commons_data_type != 'none':\n",
    "        commons_type_string = commons_data.loc[commons_data.filename == filename, 'object type'].values[0]\n",
    "    else:\n",
    "        commons_type_string = '[]' # case where no commons data exists for this work\n",
    "    type_string, instance_of_qid, issue_log, data_source = process_type(act_id, act_type_string, commons_type_string, issue_log)\n",
    "    #print(type_string, instance_of_qid)\n",
    "    \n",
    "    output_dict['instance_of'] = instance_of_qid\n",
    "    if data_source == 'act':\n",
    "        output_dict['instance_of_ref1_referenceUrl'] = act_url\n",
    "    elif data_source == 'commons':\n",
    "        output_dict['instance_of_ref1_referenceUrl'] = commons_url\n",
    "    else:\n",
    "        output_dict['instance_of_ref1_referenceUrl'] = ''\n",
    "        \n",
    "    if data_source != '':\n",
    "        output_dict['instance_of_ref1_retrieved_val'] = today\n",
    "    else:\n",
    "        output_dict['instance_of_ref1_retrieved_val'] = ''\n",
    "    \n",
    "    # *** next ***\n",
    "    \n",
    "    if issue_log != '': # include extra blank line only if some issues were added for this work\n",
    "        issue_log += '\\n'\n",
    "    entire_issue_log += issue_log\n",
    "    print()\n",
    "    output_list.append(output_dict)\n",
    "    \n",
    "#print(entire_issue_log)\n",
    "#print(json.dumps(labels_data, indent =2, ensure_ascii=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(entire_issue_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(output_list[1:], indent =2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('language_labels.json', 'wt', encoding='utf8') as file_object:\n",
    "    json.dump(labels_data, file_object, indent = 2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eventually this should maybe be hard-coded. Currently, it's jury-rigged by having a dummy\n",
    "# value in the first data row of the table to force the column headers to get picked up.\n",
    "fieldnames = list(output_list[0].keys())\n",
    "\n",
    "write_dicts_to_csv(output_list, 'abstract_artworks_out.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
