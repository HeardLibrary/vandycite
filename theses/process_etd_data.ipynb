{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "import json\n",
    "import datetime\n",
    "import requests\n",
    "from time import sleep\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from langdetect import detect_langs\n",
    "import re\n",
    "import xml.etree.ElementTree as et # library to traverse XML tree\n",
    "\n",
    "sparql_sleep = 0.1\n",
    "default_language = 'en'\n",
    "accept_media_type = 'application/json'\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "user_agent_header = 'write_edt/0.1 (https://github.com/HeardLibrary/linked-data/; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "\n",
    "# PubMed required info\n",
    "email_address = 'steve.baskauf@vanderbilt.edu' # put your email address here\n",
    "tool_name = 'retrieve_doi_data0.1' # give your application a name here, no spaces\n",
    "\n",
    "\n",
    "# ----------------\n",
    "# Define functions\n",
    "# ----------------\n",
    "\n",
    "def generate_sparql_header_dictionary(accept_media_type,user_agent_header):\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "#        'Content-Type': 'application/sparql-query',\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "# The following function requires the request header generated above\n",
    "sparql_request_header = generate_sparql_header_dictionary(accept_media_type, user_agent_header)\n",
    "\n",
    "# Read from a CSV file into a list of dictionaries\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# Write list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def detect_language(string):\n",
    "    try:\n",
    "        lang_list = detect_langs(string)\n",
    "        lang_string = str(lang_list[0])\n",
    "        confidence = float(lang_string[3:])\n",
    "        lang = lang_string[:2]\n",
    "    except: #exceptions occur when no info to decide, e.g. numbers\n",
    "        lang = 'zxx'\n",
    "        confidence = float(0)\n",
    "    return lang, confidence\n",
    "\n",
    "# Use language detection on the title of the thesis\n",
    "def determine_language_from_title(language_list, title):\n",
    "    lang, confidence = detect_language(title)\n",
    "    if lang == 'en' and confidence > 0.99:\n",
    "        language_list.append('Q1860')\n",
    "    elif lang == 'es':\n",
    "        language_list.append('Q1321')\n",
    "    elif lang == 'fr':\n",
    "        language_list.append('Q150')\n",
    "    else:\n",
    "        language_list.append('')\n",
    "\n",
    "    return language_list\n",
    "\n",
    "\n",
    "# Rule-based function for deciding whether to combine columns\n",
    "def merge_column_values(header_list, etd_frame):\n",
    "    \"\"\"Uses some rules to combine all columns in a single column. Returns a list.\"\"\"\n",
    "    column_list = []\n",
    "    for identifier, data in etd_frame.iterrows():\n",
    "        for extra_header in header_list[1:len(header_list) + 1]:\n",
    "            #print(data[header_list[0]], data[extra_header])\n",
    "            # The basic column doesn't have a value, use the other column\n",
    "            if data[header_list[0]].strip() == '':\n",
    "                if data[extra_header].strip() != '':\n",
    "                    data[header_list[0]] = data[extra_header].strip()\n",
    "            # Check whether basic column value is different from the other column\n",
    "            else:\n",
    "                if data[extra_header] != '':\n",
    "                    # Special handling for Handle\n",
    "                    if header_list[0] == 'dc.identifier.uri':\n",
    "                        # If the second column has the Handle in it, use it, otherwise leave the first value\n",
    "                        if 'http://hdl.handle.net/' in data[extra_header]:\n",
    "                            data[header_list[0]] = data[extra_header]\n",
    "                    else:\n",
    "                        if data[header_list[0]].strip() != data[extra_header].strip():\n",
    "                            data[header_list[0]] += ' *merged* ' + data[extra_header].strip()\n",
    "        column_list.append(data[header_list[0]])\n",
    "    return column_list\n",
    "\n",
    "def extract_author_values(dataframe):\n",
    "    \"\"\"Uses some rules to combine author names from two columns. Returns a list of names with proper order.\"\"\"\n",
    "    author_list = []\n",
    "    for identifier, data in dataframe.iterrows():\n",
    "        \n",
    "        last_first_string = ''\n",
    "        \n",
    "        # The dc.contrubutor.author column doesn't have a value\n",
    "        if data['dc.contributor.author'] == '':\n",
    "            if data['dc.creator'] != '': # Use dc.creator\n",
    "                last_first_string = data['dc.creator']\n",
    "        # The dc.contributor.author column has a value\n",
    "        else:\n",
    "            # Use the dc.contributor.author value since there's no dc.creator\n",
    "            if data['dc.creator'] == '':\n",
    "                last_first_string = data['dc.contributor.author']\n",
    "            # Both dc.contributor.author and dc.creator have values\n",
    "            else:\n",
    "                # If they are both the same, just use one of them\n",
    "                if data['dc.contributor.author'] == data['dc.creator']:\n",
    "                    last_first_string = data['dc.creator']\n",
    "                # If they are different, then there's an error and concatinate both.\n",
    "                else:\n",
    "                    last_first_string = data['dc.contributor.author'] + ' *merged* ' + data['dc.creator']\n",
    "\n",
    "        author_list.append(invert_author_names(last_first_string))\n",
    "        \n",
    "    return author_list\n",
    "\n",
    "def invert_author_names(name):\n",
    "    \"\"\"Input inverted author names, handle suffixes, and invert. Returns a tuple of name and suffix.\"\"\"\n",
    "    whitespace_pieces = name.split(',')\n",
    "    \n",
    "    # Remove whitespace from all pieces of the name\n",
    "    pieces = []\n",
    "    for piece in whitespace_pieces:\n",
    "        pieces.append(piece.strip())\n",
    "        \n",
    "    name_suffix = ''\n",
    "    # Look for suffixes. Note: must check longer versions before shorter (i.e. III before II, Jr. before Jr)\n",
    "    suffix_list = [\n",
    "        {'string': 'Jr.', 'value': 'Jr.'} , \n",
    "        {'string': 'Jr', 'value': 'Jr.'} , \n",
    "        {'string': 'JR.', 'value': 'Jr.'} , \n",
    "        {'string': 'JR', 'value': 'Jr.'} , \n",
    "        {'string': 'III', 'value': 'III'} , \n",
    "        {'string': 'II', 'value': 'II'} , \n",
    "        {'string': 'IV', 'value': 'IV'} , \n",
    "        {'string': 'VI', 'value': 'VI'} , \n",
    "        {'string': 'V', 'value': 'V'}\n",
    "    ]\n",
    "    \n",
    "    # Don't know how to interpret, put pieces back together and flag as error.\n",
    "    if len(pieces) > 3:\n",
    "        name = ' '.join(pieces) + ' *error*'\n",
    "        \n",
    "    # If three pieces search for a suffix\n",
    "    elif len(pieces) == 3:\n",
    "        found = False\n",
    "        for piece in pieces:\n",
    "            for suffix in suffix_list:\n",
    "                if piece == suffix['string']:\n",
    "                    name_suffix = suffix['value']\n",
    "                    found = True\n",
    "                    break # Stop checking\n",
    "            if found:\n",
    "                pieces.remove(suffix['string']) # Remove the suffix piece from the list\n",
    "                name = pieces[1] + ' ' + pieces[0] # Invert and join the remaining two pieces\n",
    "        \n",
    "    # If two pieces, look for trailing suffixes\n",
    "    elif len(pieces) == 2:\n",
    "        for piece_number in range(2):\n",
    "            for suffix in suffix_list:\n",
    "                # If the name piece ends with the suffix string, remove it and set the value of name_suffix\n",
    "                if len(pieces[piece_number]) > len(suffix['string']) and pieces[piece_number][-len(suffix['string']):] == suffix['string']:\n",
    "                    pieces[piece_number] = pieces[piece_number][:-len(suffix['string'])].strip()\n",
    "                    name_suffix = suffix['value']\n",
    "                    break\n",
    "        \n",
    "        name = pieces[1] + ' ' + pieces[0]\n",
    "        \n",
    "    # One piece, just look for suffix.\n",
    "    else:\n",
    "        for suffix in suffix_list:\n",
    "            # If the name ends with the suffix string, remove it and set the value of name_suffix\n",
    "            if len(pieces[0]) > len(suffix['string']) and pieces[0][-len(suffix['string']):] == suffix['string']:\n",
    "                pieces[0] = pieces[0][:-len(suffix['string'])].strip()\n",
    "                name_suffix = suffix['value']\n",
    "                break\n",
    "\n",
    "        name = pieces[0]\n",
    "    \n",
    "    return ' '.join(fix_all_caps(name.split(' '))), name_suffix\n",
    "\n",
    "def extract_advisor_values(dataframe):\n",
    "    \"\"\"Uses some rules to combine advisor names from two columns. Returns a list of lists of names with proper order.\"\"\"\n",
    "    advisor_list = []\n",
    "    for identifier, data in dataframe.iterrows():\n",
    "        \n",
    "        last_first_string = ''\n",
    "        error = False\n",
    "        \n",
    "        # The dc.contributor.advisor column doesn't have a value\n",
    "        if data['dc.contributor.advisor'] == '':\n",
    "            if data['dc.contributor.committeeChair'] != '': # Use dc.contributor.committeeChair\n",
    "                last_first_string = data['dc.contributor.committeeChair']\n",
    "        # The dc.contributor.advisor column has a value\n",
    "        else:\n",
    "            # Use the dc.contributor.advisor value since there's no dc.contributor.committeeChair\n",
    "            if data['dc.contributor.committeeChair'] == '':\n",
    "                last_first_string = data['dc.contributor.advisor']\n",
    "            # Both dc.contributor.advisor and dc.contributor.committeeChair have values\n",
    "            else:\n",
    "                # If they are both the same, just use one of them\n",
    "                if data['dc.contributor.advisor'] == data['dc.contributor.committeeChair']:\n",
    "                    last_first_string = data['dc.contributor.committeeChair']\n",
    "                # If they are different, then there's an error and concatinate both.\n",
    "                else:\n",
    "                    last_first_string = data['dc.contributor.advisor'] + ' *merged* ' + data['dc.contributor.committeeChair']\n",
    "                    error = True\n",
    "                    \n",
    "        if error:\n",
    "            pass\n",
    "        else:\n",
    "            # Split the multiple advisors if more than one\n",
    "            last_first_advisors = last_first_string.split('||')\n",
    "            coadvisors = []\n",
    "            for advisor in last_first_advisors:\n",
    "                coadvisors.append(invert_author_names(remove_honorifics(advisor)))\n",
    "\n",
    "        advisor_list.append(coadvisors)\n",
    "\n",
    "    return advisor_list\n",
    "\n",
    "def remove_honorifics(name):\n",
    "    \"\"\"Removes prefixes like Dr. and Prof. and suffixes like Ph.D and M.D. Returns a name string.\"\"\"\n",
    "\n",
    "    # List Ph.D. first since it usuall comes after M.D.\n",
    "    suffix_list = [\n",
    "        {'string': 'Ph.D.', 'value': 'Ph.D.'}, \n",
    "        {'string': 'PhD', 'value': 'Ph.D.'},\n",
    "        {'string': 'M.D.', 'value': 'M.D.'}, \n",
    "        {'string': 'MD', 'value': 'M.D.'}\n",
    "    ]\n",
    "    \n",
    "    # List Ph.D. first since it usuall comes after M.D.\n",
    "    prefix_list = [\n",
    "        {'string': 'Dr.', 'value': 'Dr.'}, \n",
    "        {'string': 'Dr', 'value': 'Dr.'},\n",
    "        {'string': 'Professor', 'value': 'Prof.'}, \n",
    "        {'string': 'Prof.', 'value': 'Prof.'}, \n",
    "        {'string': 'Prof', 'value': 'Prof.'}\n",
    "    ]\n",
    "    # Remove suffix honorifics\n",
    "    for suffix in suffix_list:\n",
    "        # If the name ends with the suffix string, remove it.\n",
    "        if len(name) > len(suffix['string']) and name[-len(suffix['string']):] == suffix['string']:\n",
    "            name = name[:-len(suffix['string'])].strip()\n",
    "            # remove any trailing commas\n",
    "            if name[-1] == ',':\n",
    "                name = name[:-1].strip()\n",
    "\n",
    "    # Remove prefix honorifics\n",
    "    for prefix in prefix_list:\n",
    "        # If the name begins with the prefix string, remove it.\n",
    "        if len(name) > len(prefix['string']) and name[:len(prefix['string'])] == prefix['string']:\n",
    "            name = name[len(prefix['string']):].strip()\n",
    "        \n",
    "    return name\n",
    "        \n",
    "# Function copied from https://github.com/HeardLibrary/linked-data/blob/master/publications/crossref/retrieve_doi_data.ipynb\n",
    "def title_if_no_lowercase(string):\n",
    "    \"\"\"Changes to titlecase only if there are no lowercase letters in the string.\"\"\"\n",
    "    lower = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    is_lower = False\n",
    "    for letter in string:\n",
    "        if letter in lower:\n",
    "            is_lower = True\n",
    "    if is_lower:\n",
    "        return string\n",
    "    else:\n",
    "        return string.title()\n",
    "\n",
    "# Function copied from https://github.com/HeardLibrary/linked-data/blob/master/publications/crossref/retrieve_doi_data.ipynb\n",
    "def fix_all_caps(name_pieces):\n",
    "    \"\"\"Input is a list of name strings from name split by spaces\"\"\"\n",
    "    clean_pieces = []\n",
    "    for piece in name_pieces:\n",
    "        # Special handing for names starting with apostrophe-based prefixes\n",
    "        apostrophe_list = [\"van't\", \"'t\", \"O'\", \"D'\", \"d'\", \"N'\"]\n",
    "        apostrophe_prefix = ''\n",
    "        for possible_apostrophe_prefix in apostrophe_list:\n",
    "            if possible_apostrophe_prefix in piece:\n",
    "                # Remove prefix\n",
    "                piece = piece.replace(possible_apostrophe_prefix, '')\n",
    "                apostrophe_prefix = possible_apostrophe_prefix\n",
    "        \n",
    "        # Special handling for name parts that are lowercase\n",
    "        lower_case_list = ['von', 'de', 'van', 'der']\n",
    "        if piece.lower() in lower_case_list:\n",
    "            piece = piece.lower()\n",
    "        else:\n",
    "            # Special handling for hyphenated names; doesn't work for an edge case with more than 2 hyphens\n",
    "            if '-' in piece:\n",
    "                halves = piece.split('-')\n",
    "                piece = title_if_no_lowercase(halves[0]) + '-' + title_if_no_lowercase(halves[1])\n",
    "            else:\n",
    "                piece = title_if_no_lowercase(piece)\n",
    "        \n",
    "        # put any apostrophe prefix back on the front\n",
    "        if apostrophe_prefix:\n",
    "            piece = apostrophe_prefix + piece\n",
    "        \n",
    "        clean_pieces.append(piece)\n",
    "    return clean_pieces\n",
    "# ---------------\n",
    "# The following functions were modified from https://github.com/HeardLibrary/linked-data/blob/master/publications/crossref/retrieve_doi_data.ipynb\n",
    "# ---------------\n",
    "\n",
    "def find_surname_givens(name):\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', ' ')\n",
    "    name = name.replace(',', ' ')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Get rid of empty pieces formed from extra spaces\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "        \n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "\n",
    "def disambiguate_authors(authors):\n",
    "    filename = 'researchers.csv'\n",
    "    researchers = read_dicts_from_csv(file_path + filename)\n",
    "\n",
    "    filename = 'vanderbilt_wikidata_altlabels.csv'\n",
    "    altnames = read_dicts_from_csv(file_path + filename)\n",
    "\n",
    "    filename = 'departments.csv'\n",
    "    departments = read_dicts_from_csv(file_path + filename)\n",
    "\n",
    "    filename = 'department_labels.csv'\n",
    "    department_labels = read_dicts_from_csv(file_path + filename)\n",
    "\n",
    "    max_pmids_to_check = 5\n",
    "\n",
    "    # screens.json is a configuration file that defines the kinds of screens to be performed on potential Q ID matches from Wikidata\n",
    "    screens = load_json_into_data_struct('screens.json')\n",
    "\n",
    "    # Perform screening operations on authors to try to determine their Q IDs\n",
    "    found_qid_values = []\n",
    "    not_found_author_list = []\n",
    "    author_count = 1\n",
    "    for author in authors:\n",
    "        print(author_count)\n",
    "        found = False\n",
    "        \n",
    "        # First eliminate the case where all of the name pieces are empty\n",
    "        if (author['givenName'] + ' ' + author['familyName']).strip() == '':\n",
    "            break\n",
    "            \n",
    "        # Record stated_as\n",
    "        stated_as = (author['givenName'] + ' ' + author['familyName']).strip()\n",
    "            \n",
    "        # Fix case where names are stupidly in all caps\n",
    "        name_pieces = author['givenName'].strip().split(' ')\n",
    "        author['givenName'] = ' '.join(fix_all_caps(name_pieces))\n",
    "        name_pieces = author['familyName'].strip().split(' ')\n",
    "        author['familyName'] = ' '.join(fix_all_caps(name_pieces))\n",
    "        \n",
    "        # Screen for exact match to Wikidata labels\n",
    "        for researcher in researchers:\n",
    "            if researcher['label_en'] == author['givenName'] + ' ' + author['familyName']:\n",
    "                found = True\n",
    "                result_string = 'researcher exact label match: ' + researcher['qid'] + ' ' + researcher['label_en']\n",
    "                name = researcher['label_en']\n",
    "                qid = researcher['qid']\n",
    "                break\n",
    "        if not found:\n",
    "            # screen for exact match to alternate names\n",
    "            for altname in altnames:\n",
    "                if altname['altLabel'] == author['givenName'] + ' ' + author['familyName']:\n",
    "                    found = True\n",
    "                    result_string = 'researcher altname match: ' + altname['qid'] + ' ' + altname['altLabel']\n",
    "                    name = altname['altLabel']\n",
    "                    qid = altname['qid']\n",
    "                    break\n",
    "            if not found:\n",
    "                # If the researcher has an ORCID, see if it's at Wikidata\n",
    "                if author['orcid'] != '':\n",
    "                    hit = searchWikidataForQIdByOrcid(author['orcid'])\n",
    "                    if hit != {}:\n",
    "                        found = True\n",
    "                        result_string = 'Wikidata ORCID search: ' + hit['qid'] + ' ' + hit['label'] + ' / ' + hit['description']\n",
    "                        name = hit['label']\n",
    "                        qid = hit['qid']\n",
    "\n",
    "                if not found:\n",
    "                    # screen for fuzzy match to Wikidata-derived labels\n",
    "                    for researcher in researchers:\n",
    "                        # Require the surname to match the label surname exactly\n",
    "                        split_names = find_surname_givens(researcher['label_en']) # returns False if no family name\n",
    "                        if split_names: # skip names that don't have 2 parts !!! also misses non-English labels!\n",
    "                            if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                                w_ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], researcher['label_en'])\n",
    "                                #w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], researcher['label_en'])\n",
    "                                if w_ratio > 90:\n",
    "                                    found = True\n",
    "                                    result_string = 'fuzzy label match: ' + str(w_ratio) + ' ' + researcher['qid'] + ' ' + researcher['label_en'] + ' / ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                    name = researcher['label_en']\n",
    "                                    qid = researcher['qid']\n",
    "                                    break\n",
    "                    if not found:\n",
    "                        # screen for fuzzy match to alternate names\n",
    "                        for altname in altnames:\n",
    "                            split_names = find_surname_givens(altname['altLabel'])\n",
    "                            if split_names: # skip names that don't have 2 parts\n",
    "                                if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                                    w_ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], altname['altLabel'])\n",
    "                                    #w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], altname['altLabel'])\n",
    "                                    if w_ratio > 90:\n",
    "                                        found = True\n",
    "                                        result_string = 'researcher altname fuzzy match: ' + str(w_ratio) + ' ' + altname['qid'] + ' ' + altname['altLabel'] + ' / ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                        name = altname['altLabel']\n",
    "                                        qid = altname['qid']\n",
    "                                        break\n",
    "                        if not found:\n",
    "                            name = author['givenName'] + ' ' + author['familyName']\n",
    "                            print('Searching Wikidata for', name)\n",
    "                            print('researcher known affiliations: ', author['affiliation'])\n",
    "                            print()\n",
    "                            hits = search_name_at_wikidata(name)\n",
    "                            #print(hits)\n",
    "\n",
    "                            qids = []\n",
    "                            for hit in hits:\n",
    "                                qids.append(hit['qid'])\n",
    "                            return_list = screen_qids(qids, screens)\n",
    "                            #print(return_list)\n",
    "\n",
    "                            # Save discovered data to return if not matched\n",
    "                            discovered_data = []\n",
    "                            for hit in return_list:\n",
    "                                hit_data = hit\n",
    "                                split_names = find_surname_givens(hit['label'])\n",
    "\n",
    "                                # Require the surname to match the Wikidata label surname exactly\n",
    "                                # This prevents a high fraction of fuzzy matches where the last names are similar but not the same\n",
    "                                if split_names: # skip names that don't have 2 parts\n",
    "                                    if split_names['family'] == author['familyName']: # require exact match to family name\n",
    "                                        #print(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                        #print(hit)\n",
    "                                        w_ratio = fuzz.WRatio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                        #print('w_ratio:', w_ratio)\n",
    "                                        #ratio = fuzz.ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                        #print('ratio:', ratio)\n",
    "                                        #partial_ratio = fuzz.partial_ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                        #print('partial_ratio:', partial_ratio)\n",
    "                                        #token_sort_ratio = fuzz.token_sort_ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                        #print('token_sort_ratio:', token_sort_ratio)\n",
    "                                        #token_set_ratio = fuzz.token_set_ratio(author['givenName'] + ' ' + author['familyName'], hit['label'])\n",
    "                                        #print('token_set_ratio:', token_set_ratio)\n",
    "\n",
    "                                        # This screen requires a high degree of similarity between the overall ORCID names and Wikidata labels\n",
    "                                        if w_ratio > 80:\n",
    "                                            print('Wikidata search fuzzy match:', w_ratio, author['givenName'] + ' ' + author['familyName'], ' / ', 'https://www.wikidata.org/wiki/'+ hit['qid'], hit['label'])\n",
    "                                            print('Wikidata description: ', hit['description'])\n",
    "\n",
    "                                            # Here we need to check Wikidata employer and affiliation and fuzzy match against known affiliations\n",
    "                                            occupations, employers, affiliations = search_wikidata_occ_emp_aff(hit['qid'])\n",
    "                                            print('occupations:', occupations)\n",
    "                                            hit_data['occupations'] = occupations\n",
    "                                            print('employers:', employers)\n",
    "                                            hit_data['employers'] = employers\n",
    "                                            print('affiliations', affiliations)\n",
    "                                            hit_data['affiliations'] = affiliations\n",
    "                                            print()\n",
    "\n",
    "                                            # Perform a check of the employer to make sure we didn't miss somebody in the earlier\n",
    "                                            # string matching\n",
    "                                            for employer in employers:\n",
    "                                                if 'Vanderbilt University' in employer: # catch university and med center\n",
    "                                                    found = True\n",
    "                                                    result_string = 'Match Vanderbilt employer in Wikidata: ' + hit['qid'] + ' ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                                    qid = hit['qid']\n",
    "\n",
    "                                            # If the author doesn't have any known affiliations, there is no point in checking PubMed\n",
    "                                            if author['affiliation'] != []:\n",
    "                                                # Search Wikidata for articles written by this match\n",
    "                                                articles_in_wikidata = search_wikidata_article(hit['qid'])\n",
    "                                                #print(articles_in_wikidata)\n",
    "\n",
    "                                                # Step through articles with PubMed IDs found in Wikidata and see if the author affiliation or ORCID matches any of the articles\n",
    "                                                check = 0\n",
    "                                                for article_in_wikidata in articles_in_wikidata:\n",
    "                                                    if article_in_wikidata['pmid'] != '':\n",
    "                                                        check += 1\n",
    "                                                        if check > max_pmids_to_check:\n",
    "                                                            print('More articles, but stopping after checking', max_pmids_to_check)\n",
    "                                                            break # break out of article-checking loop\n",
    "                                                        print('Checking article, PMID:', article_in_wikidata['pmid'], article_in_wikidata['title'])\n",
    "                                                        pubmed_match = identified_in_pubmed(article_in_wikidata['pmid'], author['givenName'] + ' ' + author['familyName'], author['affiliation'], author['orcid'])\n",
    "                                                        if not pubmed_match:\n",
    "                                                            #print('no match')\n",
    "                                                            print()\n",
    "                                                        else:\n",
    "                                                            found = True\n",
    "                                                            result_string = 'PubMed affilation match: ' + hit['qid'] + ' ' + author['givenName'] + ' ' + author['familyName']\n",
    "                                                            qid = hit['qid']\n",
    "                                                            break # break out of article-checking loop\n",
    "\n",
    "                                            if found:\n",
    "                                                break # break out of hit list loop\n",
    "                                            print()\n",
    "                                            # If none of the matching criteria are met, save the data for future use\n",
    "                                            discovered_data.append(hit_data)\n",
    "\n",
    "        if not found:\n",
    "            not_found_author_list.append({'name_string': author['givenName'] + ' ' + author['familyName'], 'series_ordinal': author_count, 'possible_matches': discovered_data})\n",
    "            print('not found:', author['givenName'] + ' ' + author['familyName'])\n",
    "\n",
    "        else:\n",
    "            found_qid_values.append({'qid': qid, 'stated_as': stated_as, 'series_ordinal': author_count})\n",
    "            print(result_string)\n",
    "            for department in departments:\n",
    "                if qid == department['qid']:\n",
    "                    for department_label in department_labels:\n",
    "                        if department_label['qid'] == department['affiliation']:\n",
    "                            print(department_label['label_en'])\n",
    "                            break\n",
    "        print()\n",
    "        author_count += 1\n",
    "\n",
    "    print()\n",
    "    return found_qid_values, not_found_author_list\n",
    "\n",
    "def searchWikidataForQIdByOrcid(orcid):\n",
    "    query_string = '''\n",
    "select distinct ?qid ?label ?description where {\n",
    "    ?qid wdt:P496 \"''' + orcid + '''\".\n",
    "    ?qid rdfs:label ?label.\n",
    "    FILTER(lang(?label)=\"'''+ default_language + '''\")\n",
    "    OPTIONAL {\n",
    "    ?qid schema:description ?description.\n",
    "    FILTER(lang(?description)=\"'''+ default_language + '''\")\n",
    "    }\n",
    "      }\n",
    "    '''\n",
    "    #print(query_string)\n",
    "\n",
    "    results = send_sparql_query(query_string)\n",
    "    #print(json.dumps(results, indent=2))\n",
    "\n",
    "    if len(results) > 1:\n",
    "        print('Warning!!! Multiple items with same ORCID!')\n",
    "        print(results)\n",
    "    if len(results) == 0:\n",
    "        out_dict = {}        \n",
    "    else:\n",
    "        out_dict = {\n",
    "            'qid': extract_local_name(results[0]['qid']['value']),\n",
    "            'label': results[0]['label']['value']\n",
    "            }\n",
    "        if 'description' in results[0]:\n",
    "            out_dict['description'] = results[0]['description']['value']\n",
    "        else:\n",
    "            out_dict['description'] = ''           \n",
    "    return out_dict\n",
    "\n",
    "# Generate the current UTC xsd:date\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "# Extracts the local name part of an IRI, e.g. a qNumber from a Wikidata IRI\n",
    "def extract_local_name(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    last_piece = len(pieces)\n",
    "    return pieces[last_piece - 1]\n",
    "\n",
    "# Load JSON file data from local drive into a Python data structure\n",
    "def load_json_into_data_struct(path):\n",
    "    with open(path, 'rt', encoding='utf-8') as file_object:\n",
    "        file_text = file_object.read()\n",
    "    structure = json.loads(file_text)\n",
    "    # uncomment the following line to view the data\n",
    "    # print(json.loads(structure, indent = 2))\n",
    "    return(structure)\n",
    "\n",
    "# Sends a query to the query service endpoint. \n",
    "# NOTE: request_header and endpoint are global variables defined earlier in the script\n",
    "def send_sparql_query(query_string):\n",
    "    # You can delete the two print statements if the queries are short. However, for large/long queries,\n",
    "    # it's good to let the user know what's going on.\n",
    "    #print('querying SPARQL endpoint to acquire item metadata')\n",
    "    #response = requests.post(endpoint, data=query_string.encode('utf-8'), headers=sparql_request_header)\n",
    "    failed = True\n",
    "    while failed:\n",
    "        try:\n",
    "            failed = False\n",
    "            response = requests.post(endpoint, data=dict(query=query_string), headers=sparql_request_header)\n",
    "        except:\n",
    "            failed = True\n",
    "            print()\n",
    "            print('Query service error, waiting one minute')\n",
    "            sleep(60)\n",
    "            \n",
    "    #print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "    try:\n",
    "        data = response.json()\n",
    "\n",
    "        # Extract the values from the response JSON\n",
    "        results = data['results']['bindings']\n",
    "    except:\n",
    "        results = response.text\n",
    "    \n",
    "    #print('done retrieving data')\n",
    "    # print(json.dumps(results, indent=2))\n",
    "    \n",
    "    sleep(sparql_sleep) # delay to avoid hitting the Query Service too fast\n",
    "    return results\n",
    "\n",
    "def generate_name_alternatives(name):\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods, sometimes periods are close up with no spaces\n",
    "    name = name.replace('.', ' ')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "        \n",
    "    # Fix stupid situation where name is written in ALL CAPS\n",
    "    pieces = fix_all_caps(pieces)        \n",
    "\n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        # make sure first character is alphabetic\n",
    "        # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "        # typical cases are like (Kit) or \"Kit\"\n",
    "        if not piece[0:1].isalpha():\n",
    "            piece = piece[1:len(piece)] # remove the first non-alphabetic character\n",
    "        if len(piece) > 0:\n",
    "            initials.append(piece[0:1])\n",
    "        \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += pieces[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # full name with suffix\n",
    "    if suffix != '':\n",
    "        name_version = ''\n",
    "        for piece_number in range(0, len(pieces)-1):\n",
    "            name_version += pieces[piece_number] + ' '\n",
    "        name_version += pieces[len(pieces)-1] + suffix\n",
    "        alternatives.append(name_version)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    name_version = pieces[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    name_version = pieces[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first and last name only\n",
    "    name_version = pieces[0] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial and last name only\n",
    "    name_version = initials[0] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    name_version = initials[0] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials with last name\n",
    "    name_version = initials[0] + ' '\n",
    "    for piece_number in range(1, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + ' '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += initials[piece_number] + '. '\n",
    "    name_version += pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    name_version = ''\n",
    "    for piece_number in range(0, len(pieces)-1):\n",
    "        name_version += initials[piece_number]\n",
    "    name_version += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(name_version)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def search_name_at_wikidata(name):\n",
    "    # carry out search for most languages that use Latin characters, plus some other commonly used languages\n",
    "    # See https://doi.org/10.1145/3233391.3233965\n",
    "    language_codes = [\n",
    "        'en',\n",
    "        'es',\n",
    "        'pt',\n",
    "        'fr',\n",
    "        'it',\n",
    "        'nl',\n",
    "        'de',\n",
    "        'da',\n",
    "        'et',\n",
    "        'hu',\n",
    "        'ga',\n",
    "        'ro',\n",
    "        'sk',\n",
    "        'sl',\n",
    "        'zu',\n",
    "        'tr',\n",
    "        'sv',\n",
    "        'zh',\n",
    "        'ru',\n",
    "        'ja',\n",
    "        'ar',\n",
    "        'pl',\n",
    "        'uk',\n",
    "        'ca',\n",
    "        'cs',\n",
    "        'la',\n",
    "        'nb',\n",
    "        'hu',\n",
    "        'he',\n",
    "        'eo',\n",
    "        'fi',\n",
    "        'ko'\n",
    "      ]\n",
    "    name_list = generate_name_alternatives(name)\n",
    "    alternatives = ''\n",
    "    for language_code in language_codes:\n",
    "        for alternative in name_list:\n",
    "            # get rid of quotes, which will break the query\n",
    "            alternative = alternative.replace('\"', '')\n",
    "            alternative = alternative.replace(\"'\", '')\n",
    "            alternatives += '\"' + alternative + '\"@' + language_code + '\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    results = []\n",
    "#    r = requests.post(endpoint, data=query.encode('utf-8'), headers=sparql_request_header)\n",
    "\n",
    "    statements = send_sparql_query(query)\n",
    "    \n",
    "    try:\n",
    "        for statement in statements:\n",
    "            wikidata_iri = statement['item']['value']\n",
    "            if 'label' in statement:\n",
    "                name = statement['label']['value']\n",
    "            else:\n",
    "                name = ''\n",
    "            qnumber = extract_local_name(wikidata_iri)\n",
    "            results.append({'qid': qnumber, 'name': name})\n",
    "    except:\n",
    "        results = [{'error': statements}]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return results\n",
    "\n",
    "def screen_qids(qids, screens):\n",
    "    qid_values =''\n",
    "    for qid in qids:\n",
    "        qid_values += 'wd:' + qid + '\\n'\n",
    "\n",
    "    graph_pattern = ''\n",
    "    first_screen = True\n",
    "    for screen in screens:\n",
    "        # Each requirement in a screen has an AND relationship (all must be satisfied)\n",
    "        subgraph_pattern = ''\n",
    "        for requirement in screen:\n",
    "\n",
    "            # Set the value if required or use a dummy variable if any value is allowed\n",
    "            if requirement['entity'] == '':\n",
    "                value = '?var' + requirement['property'] # add the property string to the variable to guarantee uniqueness\n",
    "            elif re.match(r'Q\\d+', requirement['entity']): # regex to match Q IDs\n",
    "                value = 'wd:' + requirement['entity']\n",
    "            else: # if not nothing or a Q ID, assume it's a string literal\n",
    "                if requirement['lang'] == '':\n",
    "                    value = '\"' + requirement['entity'] + '\"'\n",
    "                else:\n",
    "                    value = '\"' + requirement['entity'] + '\"@' + requirement['lang']\n",
    "\n",
    "            # Set the property (label, description, or P value)\n",
    "            if requirement['property'] == 'label':\n",
    "                property = 'rdfs:label'\n",
    "            elif requirement['property'] == 'description':\n",
    "                property = 'schema:description'\n",
    "            else:\n",
    "                property = 'wdt:' + requirement['property']\n",
    "\n",
    "            # Place the value in either the subject or object position in the triple\n",
    "            if requirement['position'] == 'object':\n",
    "                triple_pattern = '?qid ' + property + ' ' + value + '.'\n",
    "            else:\n",
    "                triple_pattern = value + ' ' + property + ' ?qid.'\n",
    "\n",
    "            # Add filters if needed\n",
    "            if requirement['filter_type'] == '<' or requirement['filter_type'] == '>': \n",
    "                # note: string comparison only e.g. for datetimes, needs modification for actual numbers\n",
    "                triple_pattern += '\\nFILTER (STR(?var' + requirement['property'] + ') ' + requirement['filter_type'] + ' \"' + requirement['filter_string'] + '\")'\n",
    "\n",
    "            if requirement['filter_type'] == 'in': \n",
    "                # note: string comparison only\n",
    "                triple_pattern += '\\nFILTER (CONTAINS(?var' + requirement['property'] + ', \"' + requirement['filter_string'] + '\"))'\n",
    "\n",
    "            # Use MINUS if you want to exclude items that fit the pattern.\n",
    "            if requirement['require'] == 'exclude':\n",
    "                triple_pattern = 'minus {' + triple_pattern + '}'\n",
    "\n",
    "            triple_pattern += '\\n'\n",
    "            #print(triple_pattern)\n",
    "            subgraph_pattern += triple_pattern\n",
    "\n",
    "        # Now attach the subgraph pattern to any previous subgraph patterns using UNION to great an OR relationship\n",
    "        subgraph_pattern = '{\\n' + subgraph_pattern + '}\\n' # create a subgraph pattern so that several can be UNIONed\n",
    "        if first_screen: # The first subgraph pattern doesn't need the UNION inserted\n",
    "            first_screen = False\n",
    "        else:\n",
    "            graph_pattern = graph_pattern + 'UNION\\n'\n",
    "        graph_pattern += subgraph_pattern \n",
    "\n",
    "    query_string = '''\n",
    "    select distinct ?qid ?label ?description where {\n",
    "      VALUES ?qid\n",
    "      {\n",
    "      ''' + qid_values + '''}\n",
    "    ''' + graph_pattern + '''\n",
    "    \n",
    "    ?qid rdfs:label ?label.\n",
    "    FILTER(lang(?label)=\"'''+ default_language + '''\")\n",
    "    \n",
    "    OPTIONAL {\n",
    "    ?qid schema:description ?description.\n",
    "    FILTER(lang(?description)=\"'''+ default_language + '''\")\n",
    "    }\n",
    "      }\n",
    "    '''\n",
    "    #print(query_string)\n",
    "\n",
    "    results = send_sparql_query(query_string)\n",
    "    #print(json.dumps(results, indent=2))\n",
    "\n",
    "    return_list = []\n",
    "    for result in results:\n",
    "        out_dict = {\n",
    "            'qid': extract_local_name(result['qid']['value']),\n",
    "            'label': result['label']['value']\n",
    "            }\n",
    "        if 'description' in result:\n",
    "            out_dict['description'] = result['description']['value']\n",
    "        else:\n",
    "            out_dict['description'] = ''           \n",
    "        return_list.append(out_dict)\n",
    "    return return_list\n",
    "\n",
    "# returns lists of occupations, employers, and affiliations for a person with Wikidata ID qid\n",
    "def search_wikidata_occ_emp_aff(qid):\n",
    "    results_list = []\n",
    "\n",
    "    query_string = '''select distinct ?occupation ?employer ?affiliation where {\n",
    "        optional {\n",
    "            wd:'''+ qid + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = \"'''+ default_language + '''\")            \n",
    "            }\n",
    "\n",
    "        optional {\n",
    "            wd:'''+ qid + ''' wdt:P108 ?employerId.\n",
    "            ?employerId rdfs:label ?employer.\n",
    "            FILTER(lang(?employer) = \"'''+ default_language + '''\")            \n",
    "            }\n",
    "\n",
    "        optional {\n",
    "            wd:'''+ qid + ''' wdt:P1416 ?affiliationId.\n",
    "            ?affiliationId rdfs:label ?affiliation.\n",
    "            FILTER(lang(?affiliation) = \"'''+ default_language + '''\")            \n",
    "            }\n",
    "        }'''\n",
    "    \n",
    "    #print(query_string)\n",
    "    statements = send_sparql_query(query_string)\n",
    "    #print(statements)\n",
    "    \n",
    "    # pull all possible occupations\n",
    "    occupationList = []\n",
    "    employerList = []\n",
    "    affiliationList = []\n",
    "    for statement in statements:\n",
    "        if 'occupation' in statement:\n",
    "            occupationList.append(statement['occupation']['value'])\n",
    "        if 'employer' in statement:\n",
    "            employerList.append(statement['employer']['value'])\n",
    "        if 'affiliation' in statement:\n",
    "            affiliationList.append(statement['affiliation']['value'])\n",
    "    occupationList = list(set(occupationList))\n",
    "    employerList = list(set(employerList))\n",
    "    affiliationList = list(set(affiliationList))\n",
    "    #print(occupationList)\n",
    "    #print(employerList)\n",
    "    #print(affiliationList)\n",
    "    \n",
    "    # delay to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return occupationList, employerList, affiliationList \n",
    "\n",
    "# returns a list of results of articles by person with Wikidata ID qid\n",
    "def search_wikidata_article(qid):\n",
    "    results_list = []\n",
    "    # P50 is \"author\"; P698 is the PubMed ID of the article\n",
    "    query = '''select distinct ?title ?pmid where {\n",
    "      ?article wdt:P50 wd:''' + qid + '''.\n",
    "      optional {\n",
    "          ?article rdfs:label ?title.\n",
    "          FILTER(lang(?title) = \"'''+ default_language + '''\")\n",
    "          }\n",
    "      optional {?article wdt:P698 ?pmid.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    statements = send_sparql_query(query)\n",
    "\n",
    "    try:\n",
    "        for statement in statements:\n",
    "            if 'title' in statement:\n",
    "                title = statement['title']['value']\n",
    "                #print('title=',title)\n",
    "            else:\n",
    "                title = ''\n",
    "            if 'pmid' in statement:\n",
    "                pmid = statement['pmid']['value']\n",
    "            else:\n",
    "                pmid = ''\n",
    "            results_list.append({'title': title, 'pmid': pmid})\n",
    "    except:\n",
    "        results_list = [statements]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return results_list\n",
    "\n",
    "# NOTE: The affiliation is a list of strings. All other arguments are strings\n",
    "def identified_in_pubmed(pmid, name, affiliations, orcid):\n",
    "    department_test_ratio = 70 # ratio required when a generic name similarity is crosschecked with dept name\n",
    "    test_ratio = 90 # similarity required for a potential match of a generic wikidata match\n",
    "    screen = False\n",
    "    potentialOrcid = ''\n",
    "\n",
    "    #print('Checking authors in PubMed article: ', pmid)\n",
    "    pubmed_authors = retrieve_pubmed_data(pmid)\n",
    "    if pubmed_authors == []:\n",
    "        print('PubMed ID does not seem to be valid.')\n",
    "    #print(pubmed_authors)\n",
    "    for pubmed_author in pubmed_authors:\n",
    "        # Perform a check based on pubmed_author surnames and departments. \n",
    "        # Note: only SURNAME is checked, so coauthor problems are possible as above.\n",
    "        # More complex checking could be done by looking up the name in ORCID, if available.\n",
    "        # Always report, but only match when person and department names are similar.\n",
    "        name_test_ratio = fuzz.token_set_ratio(pubmed_author['surname'], name)\n",
    "        #print(nameTestRatio, pubmed_author['surname'])\n",
    "        if name_test_ratio >= test_ratio:\n",
    "            if pubmed_author['orcid'] != '' and orcid != '':\n",
    "                # both employee and pubmed_author must have ORCIDs to do this check\n",
    "                if orcid != extract_local_name(pubmed_author['orcid']):\n",
    "                    # Reject the article if the matched surname has an inconsistent ORCID\n",
    "                    print('*** ' + pubmed_author['forename'] + ' ' + pubmed_author['surname'] + ' is NOT the same person; ORCID ' + pubmed_author['orcid'] + ' does not match.')\n",
    "                    return screen\n",
    "                # If the PubMed metadata gives an ORCID for the matched person, record it\n",
    "                else:\n",
    "                    print(pubmed_author['forename'] + ' ' + pubmed_author['surname'] + ' has matching ORCID ' + pubmed_author['orcid'])\n",
    "                    screen = True\n",
    "                    return screen # don't continue the loop since ORCIDs match\n",
    "\n",
    "            # If there is an affiliation, display it. \n",
    "            # If the department name matches the affiliation, call it a match\n",
    "            if pubmed_author['affiliation'] != '': \n",
    "                for affiliation in affiliations:\n",
    "                    set_ratio = fuzz.token_set_ratio(affiliation, pubmed_author['affiliation'])\n",
    "                    print('Affiliation test: ', set_ratio, pubmed_author['affiliation'])\n",
    "                    if set_ratio >= department_test_ratio:\n",
    "                        print('*** pubmed_author/affiliation match!')\n",
    "                        screen = True\n",
    "                        return screen # don't continue the loop (look up pubmed_author) since it's an affiliation match\n",
    "\n",
    "    return screen\n",
    "\n",
    "def retrieve_pubmed_data(pmid):\n",
    "    fetch_url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "    param_dict = {\n",
    "        'tool': tool_name, \n",
    "        'email': email_address,\n",
    "        'db': 'pubmed', \n",
    "         #'retmode': 'xml', \n",
    "        'rettype': 'abstract', \n",
    "        'id': pmid\n",
    "    }\n",
    "    \n",
    "    failed = True\n",
    "    while failed:\n",
    "        try:\n",
    "            failed = False\n",
    "            response = requests.get(fetch_url, params=param_dict)\n",
    "        except:\n",
    "            failed = True\n",
    "            print()\n",
    "            print('PubMed API error, waiting one minute')\n",
    "            sleep(60)\n",
    "            \n",
    "    #print(response.url)\n",
    "    if response.status_code == 404:\n",
    "        affiliations = [] # return an empty list if the constructed URL won't dereference\n",
    "    else:\n",
    "        pubData = response.text  # the response text is XML\n",
    "        #print(pubData)  # uncomment this line to see the XML\n",
    "\n",
    "        # process the returned XML, see https://docs.python.org/2/library/xml.etree.elementtree.html\n",
    "        root = et.fromstring(pubData)\n",
    "        try:\n",
    "            title = root.findall('.//ArticleTitle')[0].text\n",
    "        except:\n",
    "            title = ''\n",
    "        names = root.findall('.//Author')\n",
    "        affiliations = []\n",
    "        for name in names:\n",
    "            try:\n",
    "                affiliation = name.find('./AffiliationInfo/Affiliation').text\n",
    "            except:\n",
    "                affiliation = ''\n",
    "            try:\n",
    "                surname = name.find('./LastName').text\n",
    "            except:\n",
    "                surname = ''\n",
    "            try:\n",
    "                forename = name.find('./ForeName').text\n",
    "            except:\n",
    "                forename = ''\n",
    "            try:\n",
    "                id_field = name.find('./Identifier')\n",
    "                if id_field.get('Source') == 'ORCID':\n",
    "                    orcid = id_field.text\n",
    "                else:\n",
    "                    orcid = ''\n",
    "            except:\n",
    "                orcid = ''\n",
    "\n",
    "            #print(lastName)\n",
    "            #print(affiliation)\n",
    "            affiliations.append({'affiliation': affiliation, 'surname': surname, 'forename': forename, 'orcid': orcid})\n",
    "        #print()\n",
    "\n",
    "    # See https://www.ncbi.nlm.nih.gov/books/NBK25497/ for usage guidelines. \n",
    "    # An API key is required for more than 3 requests per second.\n",
    "    sleep(0.35) # wait before hitting the API again to avoid getting blocked\n",
    "    #print(affiliations)\n",
    "    return affiliations\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "etd_frame = pd.read_csv('etd_metadata.csv', na_filter=False, dtype = str)\n",
    "#etd_frame = etd_frame.iloc[3759:3765].copy() # uncomment to test on a subset\n",
    "#etd_frame = etd_frame.iloc[:5].copy() # uncomment to test on a subset\n",
    "etd_frame.set_index('id', inplace=True)\n",
    "\n",
    "# Get a list of the column headers\n",
    "headers_list = list(etd_frame.columns)\n",
    "#print(headers_list)\n",
    "\n",
    "#etd_frame = etd_frame.head(1) # Uncomment to test on first line of table only\n",
    "etd_frame.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process data for output\n",
    "\n",
    "- collapse duplicate columns having same name and dates\n",
    "- generate output or fixed values\n",
    "- rename columns if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dict = {}\n",
    "\n",
    "# Combine columns with the same beginning of column header\n",
    "print('collapsing duplicate columns')\n",
    "for header in headers_list:\n",
    "    if '[' not in header:\n",
    "        header_list = []\n",
    "        for multi_header in headers_list:\n",
    "            if header in multi_header and header.count('.') == multi_header.count('.'):\n",
    "                header_list.append(multi_header)\n",
    "        #print(header_list)\n",
    "        # Add the column to the output dict with the column header as the key\n",
    "        output_dict[header_list[0]] = merge_column_values(header_list, etd_frame)\n",
    "\n",
    "# Combine date columns into one \"created\" column (using the earliest date)\n",
    "print('merging published dates')\n",
    "created_date_list = []\n",
    "for identifier, data in etd_frame.iterrows():\n",
    "    date = [data['dc.date.accessioned'][:4], data['dc.date.created'][:4], data['dc.date.issued'][:4], data['dc.date.submitted'][-4:]]\n",
    "    while '' in date:\n",
    "        date.remove('') # remove missing data\n",
    "    created_date_list.append(min(date)) # Dates are strings, but min will return first in alphabetical order, so earliest\n",
    "output_dict['published'] = created_date_list\n",
    "\n",
    "# Get rid of the source date columns\n",
    "del output_dict['dc.date.accessioned']\n",
    "del output_dict['dc.date.created']\n",
    "del output_dict['dc.date.issued']\n",
    "del output_dict['dc.date.submitted']\n",
    "\n",
    "# Rename columns by changing the keys for the columns in the output_dict\n",
    "print('renaming columns')\n",
    "output_dict['label_en'] = output_dict.pop('dc.title')\n",
    "output_dict['handle'] = output_dict.pop('dc.identifier.uri')\n",
    "output_dict['full_text_available'] = output_dict['handle'].copy()\n",
    "\n",
    "# Make a list with the same number of items as all other columns\n",
    "# Use the Q ID for VU\n",
    "output_dict['dissert_submit_to'] = ['Q29052'] * len(created_date_list)\n",
    "\n",
    "# Determine the language of the work\n",
    "print('determining the work language')\n",
    "language_list = []\n",
    "for identifier, data in etd_frame.iterrows():\n",
    "    if data['dc.language.iso'] != '':\n",
    "        if data['dc.language.iso'][:2].lower() == 'en':\n",
    "            language_list.append('Q1860')\n",
    "        elif data['dc.language.iso'][:2].lower() == 'es':\n",
    "            language_list.append('Q1321')\n",
    "        elif data['dc.language.iso'][:2].lower() == 'fr':\n",
    "            language_list.append('Q150')\n",
    "        else:\n",
    "            language_list = determine_language_from_title(language_list, data['dc.title'])\n",
    "    else:\n",
    "        language_list = determine_language_from_title(language_list, data['dc.title'])\n",
    "output_dict['language'] = language_list\n",
    "\n",
    "# Remove non-English titles from the title column (they'll have to be added manually after item creation)\n",
    "print('removing non-English titles')\n",
    "scrub = []\n",
    "for title in output_dict['label_en']:\n",
    "    lang, confidence = detect_language(title)\n",
    "    if lang == 'en' and confidence > 0.99:\n",
    "        scrub.append(title)\n",
    "    else:\n",
    "        scrub.append('')\n",
    "output_dict['title_en'] = scrub\n",
    "\n",
    "# Determine whether it's a masters or doctoral thesis\n",
    "print('determining thesis type')\n",
    "description = []\n",
    "instance_of = []\n",
    "for identifier, data in etd_frame.iterrows():\n",
    "    print(data['dc.title'])\n",
    "    if data['thesis.degree.name'] != '' and data['thesis.degree.name'][0].lower() == 'm': # check first letter since can be MS or MA; others are PhD\n",
    "        description.append(\"master's thesis\")\n",
    "        instance_of.append('Q1907875')\n",
    "    elif data['thesis.degree.name'] != '' and data['thesis.degree.name'][0].lower() == 'p':\n",
    "        description.append('doctoral dissertation')\n",
    "        instance_of.append('Q187685')\n",
    "    else:\n",
    "        description.append('thesis')\n",
    "        instance_of.append('Q1266946')\n",
    "output_dict['description_' + default_language] = description\n",
    "output_dict['instance_of'] = instance_of\n",
    "\n",
    "print('outputing data')\n",
    "collapsed_columns_frame = pd.DataFrame(output_dict, index = etd_frame.index)\n",
    "collapsed_columns_frame.to_csv('collapsed.csv') # Uncomment to save data at this point\n",
    "\n",
    "#print(collapsed_columns_frame.head())\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output CSV label/description fields to be populated without references\n",
    "out_fields_labels = ['label_' + default_language, 'description_' + default_language]\n",
    "\n",
    "# Output CSV property fields to be populated without references\n",
    "out_fields_noref = ['instance_of']\n",
    "\n",
    "# Output CSV fields that include reference fields with only retrieved date\n",
    "out_fields_no_url = ['handle', 'full_text_available']\n",
    "\n",
    "# Output CSV fields that include reference fields with both reference URL and retrieved date\n",
    "out_fields_ref = ['published', 'title_' + default_language, 'language', 'dissert_submit_to']\n",
    "\n",
    "# Function hacked from extract_doi_metadata() function of linked-data/publications/crossref/retrieve_doi_data.ipynb\n",
    "def extract_metadata(crossref_results, handle, today):   \n",
    "    out_dict = {'qid': ''}\n",
    "    for field in out_fields_labels:   \n",
    "        #print(field, crossref_results[field])\n",
    "        out_dict[field] = crossref_results[field]\n",
    "    #print()\n",
    "    for field in out_fields_noref:   \n",
    "        #print(field, crossref_results[field])\n",
    "        out_dict[field + '_uuid'] = ''\n",
    "        out_dict[field] = crossref_results[field]\n",
    "    #print()\n",
    "    \n",
    "    # Fields with a retrieved date, but reference URL not needed\n",
    "    for field in out_fields_no_url:\n",
    "        #print(field, crossref_results[field])\n",
    "        out_dict[field + '_uuid'] = ''\n",
    "        if field == 'published':\n",
    "            out_dict[field + '_nodeId'] = ''\n",
    "            out_dict[field + '_val'] = crossref_results[field]\n",
    "            out_dict[field + '_prec'] = ''\n",
    "        else:\n",
    "            out_dict[field] = crossref_results[field]\n",
    "        # Only add a reference if there is a value for that field\n",
    "        if crossref_results[field] == '':\n",
    "            out_dict[field + '_ref1_hash'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_nodeId'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_val'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_prec'] = ''\n",
    "        else:\n",
    "            out_dict[field + '_ref1_hash'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_nodeId'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_val'] = today\n",
    "            out_dict[field + '_ref1_retrieved_prec'] = ''\n",
    "    #print()\n",
    "    \n",
    "    # Fields with both reference URLs and retrieved dates\n",
    "    for field in out_fields_ref:\n",
    "        #print(field, crossref_results[field])\n",
    "        out_dict[field + '_uuid'] = ''\n",
    "        if field == 'published':\n",
    "            out_dict[field + '_nodeId'] = ''\n",
    "            out_dict[field + '_val'] = crossref_results[field]\n",
    "            out_dict[field + '_prec'] = ''\n",
    "        else:\n",
    "            out_dict[field] = crossref_results[field]\n",
    "        # Only add a reference if there is a value for that field\n",
    "        if crossref_results[field] == '':\n",
    "            out_dict[field + '_ref1_hash'] = ''\n",
    "            out_dict[field + '_ref1_referenceUrl'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_nodeId'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_val'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_prec'] = ''\n",
    "        else:\n",
    "            out_dict[field + '_ref1_hash'] = ''\n",
    "            out_dict[field + '_ref1_referenceUrl'] = handle\n",
    "            out_dict[field + '_ref1_retrieved_nodeId'] = ''\n",
    "            out_dict[field + '_ref1_retrieved_val'] = today\n",
    "            out_dict[field + '_ref1_retrieved_prec'] = ''\n",
    "    return(out_dict)\n",
    "\n",
    "today = generate_utc_date()\n",
    "articles_list = []\n",
    "\n",
    "for identifier, data in collapsed_columns_frame.iterrows():\n",
    "\n",
    "    handle = data['handle']\n",
    "    print(handle)\n",
    "\n",
    "    primary_metadata = extract_metadata(data, handle, today)\n",
    "    articles_list.append(primary_metadata)\n",
    "#print(json.dumps(articles_list, indent = 2))\n",
    "\n",
    "# Writ the data to the file after every lookup in case the script crashes\n",
    "fieldnames = list(articles_list[0].keys()) # get field names from first dict in list\n",
    "write_dicts_to_csv(articles_list, 'articles.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract author and advisor information\n",
    "\n",
    "Note: the output of this is designed to mimic what is produced after the first stage of https://github.com/HeardLibrary/linked-data/blob/master/publications/crossref/retrieve_doi_data.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "authors = extract_author_values(collapsed_columns_frame)\n",
    "advisors = extract_advisor_values(collapsed_columns_frame)\n",
    "column_headers = ['doi', 'authors', 'editors']\n",
    "output_list = []\n",
    "for edt_number in range(len(authors)):\n",
    "    author_name_parts = find_surname_givens(authors[edt_number][0])\n",
    "    print(edt_number, author_name_parts)\n",
    "    output_dict = {'doi': collapsed_columns_frame.iloc[edt_number][['handle']][0], \n",
    "                   'authors': json.dumps([\n",
    "                       {'orcid': collapsed_columns_frame.iloc[edt_number]['dc.creator.orcid'], \n",
    "                        'sequence': 'first', \n",
    "                        'givenName': author_name_parts['given'], \n",
    "                        'familyName': author_name_parts['family'], \n",
    "                        'affiliation': ''}\n",
    "                   ])\n",
    "                  }\n",
    "    print(advisors[edt_number])\n",
    "    advisor_list = []\n",
    "    for advisor in advisors[edt_number]:\n",
    "        # If advisor is missing or not a full name, just omit.\n",
    "        if advisor[0] != '':\n",
    "            author_name_parts = find_surname_givens(advisor[0])\n",
    "            if author_name_parts:\n",
    "                advisor_list.append({'orcid': '', \n",
    "                                'sequence': '', \n",
    "                                'givenName': author_name_parts['given'], \n",
    "                                'familyName': author_name_parts['family'], \n",
    "                                'affiliation': ''})\n",
    "            else:\n",
    "                advisor_list.append({'orcid': '', \n",
    "                                'sequence': '', \n",
    "                                'givenName': '', \n",
    "                                'familyName': '', \n",
    "                                'affiliation': ''})                \n",
    "        else:\n",
    "            advisor_list.append({'orcid': '', \n",
    "                            'sequence': '', \n",
    "                            'givenName': '', \n",
    "                            'familyName': '', \n",
    "                            'affiliation': ''})\n",
    "            \n",
    "    output_dict['editors'] = json.dumps(advisor_list)\n",
    "    output_list.append(output_dict)\n",
    "    \n",
    "#print(json.dumps(output_list, indent = 2))\n",
    "write_dicts_to_csv(output_list, 'stored_retrieved_authors.csv', column_headers)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the author and advisor (\"editor\") CSVs in form required to write with VanderBot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "today = generate_utc_date()\n",
    "file_path = ''\n",
    "alt_reference ='' # not used\n",
    "\n",
    "people_dict = {'author':[], 'editor': []}\n",
    "unidentified = {'author':[], 'editor': []}\n",
    "author_strings_list = []\n",
    "# Load existing data if any (primarily if script crashes and has to be rerun)\n",
    "people_dict['author'] = read_dicts_from_csv(file_path + 'authors.csv')\n",
    "people_dict['editor'] = read_dicts_from_csv(file_path + 'editors.csv')\n",
    "author_strings_list = read_dicts_from_csv(file_path + 'author_strings.csv')\n",
    "\n",
    "# Open the file containing the stored data about authors and editors retrieved from CrossRef\n",
    "stored_retrieved_authors = read_dicts_from_csv(file_path + 'stored_retrieved_authors.csv')\n",
    "\n",
    "# Open the article items file after upload in order to get the Q IDs for the newly written articles\n",
    "articles = read_dicts_from_csv(file_path + 'articles.csv')\n",
    "unidentified = []\n",
    "\n",
    "for article in articles:\n",
    "    qid = article['qid']\n",
    "    doi = article['handle']\n",
    "    print(qid, doi)\n",
    "    #pmid = article['pmid']\n",
    "    unidentified_for_article = {'qid': 'https://wikidata.org/entity/' + qid, 'handle': doi}\n",
    "    \n",
    "    found = False\n",
    "    for article_authors in stored_retrieved_authors:\n",
    "        if article['handle'] == article_authors['doi']:\n",
    "            found = True\n",
    "            authors = json.loads(article_authors['authors'])\n",
    "            editors = json.loads(article_authors['editors'])\n",
    "            break\n",
    "    if found:\n",
    "        for persontype in ['author', 'editor']:\n",
    "        \n",
    "            # Disambiguate authors against existing Wikidata people items\n",
    "            found_author_qids, author_name_strings = disambiguate_authors(json.loads(article_authors[persontype + 's']))\n",
    "            \n",
    "            # Add data about unidentified people with possible Q ID matches to the list for further work.\n",
    "            unidentified_for_article[persontype] = author_name_strings\n",
    "\n",
    "            for author in found_author_qids:\n",
    "                out_dict = {}\n",
    "                out_dict['qid'] = qid\n",
    "                out_dict['label_en'] = article['label_en']\n",
    "                out_dict[persontype + '_uuid'] = ''\n",
    "                out_dict[persontype] = author['qid']\n",
    "                out_dict[persontype + '_series_ordinal'] = author['series_ordinal']\n",
    "                out_dict[persontype + '_stated_as'] = author['stated_as']\n",
    "                out_dict[persontype + '_ref1_hash'] = ''\n",
    "                if alt_reference == '':\n",
    "                    out_dict[persontype + '_ref1_referenceUrl'] = doi\n",
    "                else:\n",
    "                    out_dict[persontype + '_ref1_referenceUrl'] = alt_reference\n",
    "                out_dict[persontype + '_ref1_retrieved_nodeId'] = ''\n",
    "                out_dict[persontype + '_ref1_retrieved_val'] = today\n",
    "                out_dict[persontype + '_ref1_retrieved_prec'] = ''\n",
    "                people_dict[persontype].append(out_dict)\n",
    "            #print(authors_list)\n",
    "\n",
    "            if len(people_dict[persontype]) > 0:\n",
    "                fieldnames = list(people_dict[persontype][0].keys()) \n",
    "                write_dicts_to_csv(people_dict[persontype], file_path + persontype + 's.csv', fieldnames)\n",
    "\n",
    "            if persontype == 'author':\n",
    "                for author in author_name_strings:\n",
    "                    out_dict = {}\n",
    "                    out_dict['qid'] = qid\n",
    "                    out_dict['label_en'] = article['label_en']\n",
    "                    out_dict['author_string_uuid'] = ''\n",
    "                    out_dict['author_string'] = author['name_string']\n",
    "                    out_dict['author_string_series_ordinal'] = author['series_ordinal']\n",
    "                    out_dict['author_string_ref1_hash'] = ''\n",
    "                    if alt_reference == '':\n",
    "                        out_dict['author_string_ref1_referenceUrl'] = doi\n",
    "                    else:\n",
    "                        out_dict['author_string_ref1_referenceUrl'] = alt_reference\n",
    "                    out_dict['author_string_ref1_retrieved_nodeId'] = ''\n",
    "                    out_dict['author_string_ref1_retrieved_val'] = today\n",
    "                    out_dict['author_string_ref1_retrieved_prec'] = ''\n",
    "                    author_strings_list.append(out_dict)\n",
    "\n",
    "                #print(author_strings_list)\n",
    "                if len(author_strings_list) > 0:\n",
    "                    fieldnames = list(author_strings_list[0].keys()) \n",
    "                    write_dicts_to_csv(author_strings_list, file_path + 'author_strings.csv', fieldnames)\n",
    "                    \n",
    "        if not(unidentified_for_article['author'] == [] and unidentified_for_article['editor'] == []):\n",
    "            unidentified.append(unidentified_for_article)\n",
    "        \n",
    "    # Save the potential author and editor matches in a file\n",
    "    # Save after each article in case of crash; maybe later just write at end\n",
    "    with open(file_path + 'unidentified_people.json', 'wt', encoding='utf-8') as file_object:\n",
    "        file_object.write(json.dumps(unidentified, indent=2))\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
