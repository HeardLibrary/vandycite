{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Code\n",
    "\n",
    "This code block includes import statements, function definitions, and declarations of variables that are common to the rest of the script. It needs to be run once before the other code blocks.\n",
    "\n",
    "**Note: some code in this block is found in the stand-alone file vb_common_code.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "from bs4 import BeautifulSoup # web-scraping library\n",
    "import json\n",
    "from time import sleep\n",
    "import csv\n",
    "import math\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from fuzzywuzzy import process\n",
    "import xml.etree.ElementTree as et # library to traverse XML tree\n",
    "import urllib\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "# For a particular processing round, set a short name for the department here.\n",
    "# This name is used to generate a set of unique processing files for that department.\n",
    "testEmployer = 'Vanderbilt University' # to test against Wikidata employer property\n",
    "employerQId = 'Q29052' # Vanderbilt University\n",
    "deathDateLimit = '2000' # any death dates before this date will be assumed to not be a match\n",
    "birthDateLimit = '1920' # any birth dates before this date will be assumed to not be a match\n",
    "wikibase_instance_namespace = 'http://www.wikidata.org/entity/'\n",
    "\n",
    "# ---------------------\n",
    "# function definitions\n",
    "# ---------------------\n",
    "\n",
    "wikidata_endpoint_url = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "user_agent_header = 'VanderBot/1.7 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "\n",
    "# NCBI identification requirements:\n",
    "# tool name and email address should be sent with all requests\n",
    "# see https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\n",
    "emailAddress = 'steve.baskauf@vanderbilt.edu' # put your email address here\n",
    "toolName = 'VanderBot' # give your application a name here\n",
    "\n",
    "# Extracts the local name part of an IRI, e.g. a qNumber from a Wikidata IRI\n",
    "def extract_local_name(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    last_piece = len(pieces)\n",
    "    return pieces[last_piece - 1]\n",
    "\n",
    "# Generate the current UTC xsd:date\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "today = generate_utc_date()\n",
    "\n",
    "# generates a dictionary to be passed in a requests GET method to generate the request header\n",
    "def generate_header_dictionary_non_sparql(acceptMediaType):\n",
    "    userAgentHeader = 'VanderBot/1.3 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : acceptMediaType,\n",
    "        'User-Agent': userAgentHeader\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "# write a list of lists to a CSV file\n",
    "def write_lists_to_csv(fileName, array):\n",
    "    with open(fileName, 'w', newline='', encoding='utf-8') as fileObject:\n",
    "        writerObject = csv.writer(fileObject)\n",
    "        for row in array:\n",
    "            writerObject.writerow(row)\n",
    "\n",
    "# Read from a CSV file into a list of dictionaries\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# Write list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def lookup_department(researcher_qid, orcid_department, departments, department_labels):\n",
    "    for department in departments:\n",
    "        if researcher_qid == department['qid']:\n",
    "            result = ''\n",
    "            #print(department['affiliation'])\n",
    "            for label in department_labels:\n",
    "                if department['affiliation'] == label['qid']:\n",
    "                    partial_ratio = fuzz.partial_ratio(orcid_department, label['label_en'])\n",
    "                    #set_ratio = fuzz.token_set_ratio(orcid_department, label['label_en'])\n",
    "                    if partial_ratio > 80:\n",
    "                        #print('partial ratio', partial_ratio)\n",
    "                        #print('set_ratio', set_ratio)\n",
    "                        result = str(partial_ratio) + ' ' + orcid_department + '/' + label['label_en']\n",
    "            return result\n",
    "\n",
    "def find_surname_givens(name):\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', ' ')\n",
    "    name = name.replace(',', ' ')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Get rid of empty pieces formed from extra spaces\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "    \n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "def generate_header_dictionary(accept_media_type,user_agent_header):\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "#        'Content-Type': 'application/sparql-query',\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "# The following function requires the request header generated above\n",
    "sparql_request_header = generate_header_dictionary(accept_media_type,user_agent_header)\n",
    "# The query is a valid SPARQL query string\n",
    "\n",
    "# Sends a query to the query service endpoint. \n",
    "# NOTE: request_header and endpoint are global variables defined earlier in the script\n",
    "def send_sparql_query(query_string):\n",
    "    # You can delete the two print statements if the queries are short. However, for large/long queries,\n",
    "    # it's good to let the user know what's going on.\n",
    "    print('querying SPARQL endpoint to acquire item metadata')\n",
    "    #response = requests.post(endpoint, data=query_string.encode('utf-8'), headers=sparql_request_header)\n",
    "    response = requests.post(wikidata_endpoint_url, data=dict(query=query_string), headers=sparql_request_header) # use URL-encoded method\n",
    "    #print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the values from the response JSON\n",
    "    results = data['results']['bindings']\n",
    "    \n",
    "    print('done retrieving data')\n",
    "    # print(json.dumps(results, indent=2))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage altLabels\n",
    "\n",
    "This process starts with altLabels downloaded from Wikidata and adds alternate names from ORCID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Vanderbilt people's altLabels from Wikidata\n",
    "\n",
    "Developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/wikidata/download-vanderbilt-people-altlabels.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''select distinct  ?person ?altLabel where {\n",
    "  ?person p:P108 ?statement.\n",
    "  {?statement ps:P108 wd:Q29052.}\n",
    "  union\n",
    "  {?statement ps:P108 wd:Q7914455.}\n",
    "  ?person skos:altLabel ?altLabel.\n",
    "  FILTER(lang(?altLabel)=\"en\")\n",
    "}'''\n",
    "\n",
    "# The endpoint defaults to returning XML, so the Accept: header is required\n",
    "r = requests.get(wikidata_endpoint_url, params={'query' : query}, headers={'Accept' : 'application/json'})\n",
    "\n",
    "data = r.json()\n",
    "#print(json.dumps(data,indent = 2))\n",
    "\n",
    "table = [['qid', 'altLabel', 'source']]\n",
    "items = data['results']['bindings']\n",
    "for item in items:\n",
    "    wikidata_iri = extract_local_name(item['person']['value'])\n",
    "    alt_label = ''\n",
    "    if 'altLabel' in item:\n",
    "        alt_label = item['altLabel']['value']\n",
    "    table.append([wikidata_iri, alt_label, 'wikidata'])\n",
    "    \n",
    "filename = 'vanderbilt_wikidata_altlabels.csv'\n",
    "write_lists_to_csv(filename, table)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'vanderbilt_wikidata_altlabels.csv'\n",
    "researcher_altlabels = read_dicts_from_csv(filename)\n",
    "fieldnames = researcher_altlabels[0].keys() # get the field names from the existing file\n",
    "\n",
    "filename = 'orcid_other_names.csv'\n",
    "orcid_other = read_dicts_from_csv(filename)\n",
    "\n",
    "filename = 'researchers.csv'\n",
    "researcher_items = read_dicts_from_csv(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orcid_other[0])\n",
    "print()\n",
    "print(researcher_items[0])\n",
    "print()\n",
    "print(researcher_altlabels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in orcid_other[:100]:\n",
    "    print(name['altName'], name['orcid'])\n",
    "    for researcher in researcher_items:\n",
    "        if name['orcid'] == researcher['orcid']:\n",
    "            print(researcher['label_en'], researcher['qid'])\n",
    "            break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query ORCID for Vanderbilt University people\n",
    "\n",
    "Script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/orcid/orcid-get-json.ipynb\n",
    "\n",
    "Retrieves results 100 at a time, then processes them by extracting desired information.  **NOTE: takes hours to run.**\n",
    "\n",
    "Saves results in a file and the alternative names in a second file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [['orcid', 'given_names', 'family_name', 'start_date', 'end_date', 'department', 'organization']]\n",
    "other_name_list = [['orcid', 'altName']]\n",
    "\n",
    "# use the API to search for people associated with Vanderbilt University\n",
    "# First search is for only one record, just to get the number of hits found\n",
    "search_uri = 'https://pub.orcid.org/v2.0/search/?q=affiliation-org-name:\"Vanderbilt+University\"&start=1&rows=1'\n",
    "accept_media_type = 'application/json'\n",
    "response = requests.get(search_uri, headers = generate_header_dictionary_non_sparql(accept_media_type))\n",
    "data = response.json()\n",
    "#print(data)\n",
    "number_results = data[\"num-found\"]\n",
    "print(data[\"num-found\"])\n",
    "number_pages = math.floor(number_results/100)\n",
    "#print(number_pages)\n",
    "remainder = number_results - 100*number_pages\n",
    "#print(remainder)\n",
    "\n",
    "for page_count in range(0, number_pages + 1):  # the remainder will be caught when page_count = number_pages\n",
    "    print('page: ', page_count)\n",
    "    search_uri = 'https://pub.orcid.org/v2.0/search/?q=affiliation-org-name:\"Vanderbilt+University\"&start='+str(page_count*100+1)\n",
    "    response = requests.get(search_uri, headers={'Accept' : 'application/json'})\n",
    "    print(response.url)\n",
    "    data = response.json()\n",
    "    orcids_dicts_list = data['result']\n",
    "\n",
    "    # extract the identifier strings from the data structure\n",
    "    orcids = []\n",
    "    for orcid_dict in orcids_dicts_list:\n",
    "        dictionary = {'id': orcid_dict['orcid-identifier']['path'], 'iri': orcid_dict['orcid-identifier']['uri']}\n",
    "        orcids.append(dictionary)\n",
    "\n",
    "    for orcid_index in range(0, len(orcids)):\n",
    "        response = requests.get(orcids[orcid_index]['iri'], headers={'Accept' : 'application/json'})\n",
    "        data = response.json()\n",
    "        #print(json.dumps(data, indent = 2))\n",
    "        orcid_id = data['orcid-identifier']['path']\n",
    "        #print(orcid_id)\n",
    "        # if there isn't a name, then go on to the next ORCID\n",
    "        if not data['person']['name']:\n",
    "            continue\n",
    "        if data['person']['name']['given-names']:  \n",
    "            given_names = data['person']['name']['given-names']['value']\n",
    "        else:\n",
    "            continue\n",
    "        if data['person']['name']['family-name']:\n",
    "            family_name = data['person']['name']['family-name']['value']\n",
    "        # This has been a big pain when people don't have surnames.\n",
    "        # It causes matches with everyone who has the same first name!\n",
    "        else:\n",
    "            continue\n",
    "        #print(given_names, ' ', family_name)\n",
    "        other_names = data['person']['other-names']['other-name']\n",
    "        for other_name in other_names:\n",
    "            #print(other_name['content'])\n",
    "            other_name_list.append([orcid_id, other_name['content']])\n",
    "\n",
    "        affiliations = data['activities-summary']['employments']['affiliation-group']\n",
    "        #print(json.dumps(affiliations, indent = 2))\n",
    "        for affiliation in affiliations:\n",
    "            summaries = affiliation['summaries']\n",
    "            #print(summaries)\n",
    "            #print()\n",
    "            for summary in summaries:\n",
    "                employment = summary['employment-summary']\n",
    "                #print(json.dumps(employment, indent = 2))\n",
    "                start_date = ''\n",
    "                if employment['start-date']:\n",
    "                    if employment['start-date']['year']:\n",
    "                        start_date += employment['start-date']['year']['value']\n",
    "                        start_month = employment['start-date']['month']\n",
    "                        if start_month:\n",
    "                            start_date += '-' + start_month['value']\n",
    "                            start_day = employment['start-date']['day']\n",
    "                            if start_day:\n",
    "                                start_date += '-' + start_day['value']\n",
    "                #print('start date: ', start_date)\n",
    "                end_date = ''\n",
    "                if employment['end-date']:\n",
    "                    if employment['end-date']['year']:\n",
    "                        end_date += employment['end-date']['year']['value']\n",
    "                        end_month = employment['end-date']['month']\n",
    "                        if end_month:\n",
    "                            end_date += '-' + end_month['value']\n",
    "                            end_day = employment['end-date']['day']\n",
    "                            if end_day:\n",
    "                                end_date += '-' + end_day['value']\n",
    "                #print('end date: ', end_date)\n",
    "                department = employment['department-name']\n",
    "                # if there is no value for department, set it to empty string\n",
    "                if not department:\n",
    "                    department = ''\n",
    "                #print(department)\n",
    "                if employment['organization']:\n",
    "                    organization = employment['organization']['name']\n",
    "                #print(organization)\n",
    "                if 'Vanderbilt University' in organization:\n",
    "                    print(orcid_id, given_names, family_name, start_date, end_date, department, organization)\n",
    "                    table.append([orcid_id, given_names, family_name, start_date, end_date, department, organization])\n",
    "                #print(table)\n",
    "        sleep(.25)\n",
    "\n",
    "print()\n",
    "print('Done')\n",
    "file_name = 'orcid_data.csv'\n",
    "write_lists_to_csv(file_name, table)\n",
    "file_name = 'orcid_other_names.csv'\n",
    "write_lists_to_csv(file_name, other_name_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data for upcoming cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'wikidata/researchers.csv'\n",
    "researcher_items = read_dicts_from_csv(filename)\n",
    "fieldnames = researcher_items[0].keys() # get the field names from the existing file\n",
    "\n",
    "filename = 'vanderbilt_wikidata_altlabels.csv'\n",
    "researcher_altlabels = read_dicts_from_csv(filename)\n",
    "\n",
    "filename = 'orcid_other_names.csv'\n",
    "orcid_other = read_dicts_from_csv(filename)\n",
    "\n",
    "filename = 'orcid_data.csv'\n",
    "orcid_people = read_dicts_from_csv(filename)\n",
    "\n",
    "filename = 'wikidata/departments.csv'\n",
    "departments = read_dicts_from_csv(filename)\n",
    "\n",
    "filename = 'wikidata/department_labels.csv'\n",
    "department_labels = read_dicts_from_csv(filename)\n",
    "\n",
    "filename = 'wikidata/qid_not_equal_orcid.csv'\n",
    "not_equal = read_dicts_from_csv(filename)\n",
    "\n",
    "print(fieldnames)\n",
    "print()\n",
    "print(researcher_items[0])\n",
    "print()\n",
    "print(orcid_people[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Wikidata people for new ORCIDs\n",
    "\n",
    "Check the downloaded records for Wikidata against the ORCID download to see if any people in Wikidata whose employer is Vanderbilt have new ORCIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 1\n",
    "#for researcher_item_index in range(500):\n",
    "for researcher_item_index in range(len(researcher_items)):\n",
    "    count +=1\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "    # First screen is to only check items if their ORCID is unknown\n",
    "    if researcher_items[researcher_item_index]['orcid'] == '':\n",
    "        researcher_names = find_surname_givens(researcher_items[researcher_item_index]['label_en'])\n",
    "        for orcid_person in orcid_people:\n",
    "            bad_match = False\n",
    "            # Second screen requires the surname of the checked ORCID to match the Wikidata label surname exactly\n",
    "            # This prevents a high fraction of fuzzy matches where the last names are similar but not the same\n",
    "            if researcher_names['family'] == orcid_person['familyName']: # require exact match to family name\n",
    "                w_ratio = fuzz.WRatio(orcid_person['givenNames'] + ' ' + orcid_person['familyName'], researcher_items[researcher_item_index]['label_en'])\n",
    "\n",
    "                # Third screen requires a high degree of similarity between the overall ORCID names and Wikidata labels\n",
    "                if w_ratio > 90:\n",
    "                    # The fourth screen eliminates near matches that have previously been determined by a human to be wrong\n",
    "                    for mismatch in not_equal:\n",
    "                        if mismatch['qid'] == researcher_items[researcher_item_index]['qid'] and mismatch['orcid'] == orcid_person['orcid']:\n",
    "                            bad_match = True\n",
    "                            # quit the mismatch list checking\n",
    "                            break\n",
    "                    if not bad_match:\n",
    "                        #ratio = fuzz.ratio(orcid_person['givenNames'] + ' ' + orcid_person['familyName'], researcher_item['label_en'])\n",
    "                        #partial_ratio = fuzz.partial_ratio(orcid_person['givenNames'] + ' ' + orcid_person['familyName'], researcher_item['label_en'])\n",
    "                        #sort_ratio = fuzz.token_sort_ratio(orcid_person['givenNames'] + ' ' + orcid_person['familyName'], researcher_item['label_en'])\n",
    "                        #set_ratio = fuzz.token_set_ratio(orcid_person['givenNames'] + ' ' + orcid_person['familyName'], researcher_item['label_en'])\n",
    "                        print(w_ratio, researcher_items[researcher_item_index]['label_en'], wikibase_instance_namespace + researcher_items[researcher_item_index]['qid'])\n",
    "                        dept_match = lookup_department(researcher_items[researcher_item_index]['qid'], orcid_person['department'], departments, department_labels)\n",
    "                        print('   ', orcid_person['givenNames'] + ' ' + orcid_person['familyName'], 'https://orcid.org/' + orcid_person['orcid'])\n",
    "                        print(orcid_person['department'])\n",
    "                        if dept_match != '':\n",
    "                            print('Dept. match:', dept_match)\n",
    "                        print()\n",
    "                        #print('name similarity ratio', ratio)\n",
    "                        #print('partial ratio', partial_ratio)\n",
    "                        #print('sort_ratio', sort_ratio)\n",
    "                        #print('set_ratio', set_ratio)\n",
    "                        #print('w_ratio', w_ratio)\n",
    "                        researcher_items[researcher_item_index]['orcid'] = orcid_person['orcid']\n",
    "                        researcher_items[researcher_item_index]['orcid_ref1_retrieved_val'] = today\n",
    "                        write_dicts_to_csv(researcher_items, 'wikidata/researchers.csv', fieldnames)\n",
    "                        break\n",
    "                        \n",
    "                    else:\n",
    "                        # when a bad match was detected, go on to the next ORCID person\n",
    "                        #print('detected:', researcher_items[researcher_item_index]['qid'], orcid_person['orcid'])\n",
    "                        pass\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check new orcid download for people not yet associated with VU in Wikidata\n",
    "\n",
    "This part of the script looks for people who ORCID said worked at Vanderbilt, but who don't have any employer property in Wikidata saying that they work(ed) at VU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(orcid_people))\n",
    "orcid_list = ''\n",
    "for person in orcid_people:\n",
    "    orcid_list += '\"' + person['orcid'] + '\"\\n'\n",
    "#print(orcid_list)\n",
    "\n",
    "query_string = '''select distinct ?person ?name ?orcid where {\n",
    "  values ?orcid\n",
    "  {\n",
    "''' + orcid_list +'''  }\n",
    "  ?person wdt:P496 ?orcid.\n",
    "  ?person rdfs:label ?name.\n",
    "  filter(lang(?name) = \"en\")\n",
    "  minus {\n",
    "    {?person wdt:P108 wd:Q29052.}\n",
    "    union\n",
    "    {?person wdt:P108 wd:Q7914455.}\n",
    "  }\n",
    "  }'''\n",
    "\n",
    "#print(query_string)\n",
    "\n",
    "data = send_sparql_query(query_string)\n",
    "#print(json.dumps(data, indent = 2))\n",
    "\n",
    "missing_persons_list = []\n",
    "#for person in data[:2]:\n",
    "for person in data:\n",
    "    person_dict = {}\n",
    "    person_dict['name'] = person['name']['value']\n",
    "    person_dict['orcid_url'] = 'https://orcid.org/' + person['orcid']['value']\n",
    "    person_dict['orcid'] = person['orcid']['value']\n",
    "    person_dict['qid'] = extract_local_name(person['person']['value'])\n",
    "    person_dict['item_url'] = person['person']['value']\n",
    "    missing_persons_list.append(person_dict)\n",
    "#print(missing_persons_list)\n",
    "\n",
    "fieldnames = ['name', 'orcid_url', 'orcid', 'item_url', 'qid']\n",
    "filename = 'missing_persons.csv'\n",
    "write_dicts_to_csv(missing_persons_list, filename, fieldnames)\n",
    "\n",
    "# Issues to deal with:\n",
    "# new people (this is good)\n",
    "# LargeDatasetBot creates records in Wikidata with only initial when ORCID has full names.\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check for duplicate Wikidata items with the same ORCID\n",
    "\n",
    "\n",
    "Also periodically run\n",
    "\n",
    "```\n",
    "select distinct ?person1 ?person2 ?orcid where {\n",
    "  {?person1 wdt:P108 wd:Q29052.}\n",
    "  union\n",
    "  {?person1 wdt:P108 wd:Q7914455.}\n",
    "  ?person1 wdt:P496 ?orcid.\n",
    "  ?person2 wdt:P496 ?orcid.\n",
    "  filter(?person1 != ?person2)\n",
    "  }\n",
    "order by ?orcid\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcid_list = ''\n",
    "for person in missing_persons_list:\n",
    "    orcid_list += '\"' + person['orcid'] + '\"\\n'\n",
    "#print(orcid_list)\n",
    "\n",
    "query_string = '''select distinct ?person1 ?person2 ?orcid where {\n",
    "  values ?orcid\n",
    "  {\n",
    "''' + orcid_list +'''  }\n",
    "  ?person1 wdt:P496 ?orcid.\n",
    "  ?person2 wdt:P496 ?orcid.\n",
    "  filter(?person1 != ?person2)\n",
    "  }\n",
    "order by ?orcid'''\n",
    "\n",
    "print(query_string)\n",
    "# This query can by copied and pasted into the Query Service\n",
    "# Use those links to merge the duplicates.\n",
    "# Then go back and rerun the code in the cell above.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run acquire_wikidata_metadata.py\n",
    "\n",
    "Use the Q IDs saved in the `missing_persons.csv` file to create the spreadsheet for the new people to be added. Used a modified `config.json` (`config_minimal.json`) file to retrieve without the employer field, then add the employer column headers to create `researchers.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'wikidata/missing_persons/researchers.csv'\n",
    "missing_persons = read_dicts_from_csv(filename)\n",
    "fieldnames = list(missing_persons[0].keys()) # get the field names from the existing file\n",
    "print(missing_persons[0])\n",
    "\n",
    "print()\n",
    "\n",
    "filename = 'orcid_data.csv'\n",
    "orcid_people = read_dicts_from_csv(filename)\n",
    "print(orcid_people[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for person_index in range(50):\n",
    "for person_index in range(len(missing_persons)):\n",
    "    found = False\n",
    "    for orcid_person in orcid_people:\n",
    "        if missing_persons[person_index]['orcid'] == orcid_person['orcid']:\n",
    "            found = True\n",
    "            break\n",
    "        found = False\n",
    "    if not found:\n",
    "        print('did not find')\n",
    "    else:\n",
    "        # Note: these values seem to be controlled, so fuzzy matching isn't required\n",
    "        if 'Vanderbilt University Medical Center' == orcid_person['organization']:\n",
    "            missing_persons[person_index]['employer'] = 'Q7914455'\n",
    "        # Checked manually and all others are VU or some unit of VU\n",
    "        else:\n",
    "            missing_persons[person_index]['employer'] = 'Q29052'\n",
    "        if orcid_person['startDate'] != '':\n",
    "            missing_persons[person_index]['employer_start_time_val'] = orcid_person['startDate']\n",
    "        if orcid_person['endDate'] != '':\n",
    "            missing_persons[person_index]['employer_end_time_val'] = orcid_person['endDate']\n",
    "        missing_persons[person_index]['employer_ref1_referenceUrl'] = 'https://orcid.org/' + orcid_person['orcid']\n",
    "        # Manually add the date when the ORCID download was made\n",
    "        missing_persons[person_index]['employer_ref1_retrieved_val'] = '2021-03-27'\n",
    "        department = orcid_person['department']\n",
    "        if 'Department of ' in department:\n",
    "            department = department.split('Department of ')[1]\n",
    "        missing_persons[person_index]['department'] = department\n",
    "        missing_persons[person_index]['organization'] = orcid_person['organization']\n",
    "\n",
    "filename = 'wikidata/missing_persons/researchers.csv'\n",
    "write_dicts_to_csv(missing_persons, filename, fieldnames + ['department', 'organization'])\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this cell, I went through and googled people to determine their gender manually.\n",
    "\n",
    "Modified the `config.json` to include department within same CSV, then generated the `csv-metadata.json` and CSV header.\n",
    "\n",
    "Sorted by department, then pasted in the appropriate department Q ID into the cell.\n",
    "\n",
    "After the upload, copied the new data into the `researchers.csv` and `departments.csv` files in the parent directory. I then reran the download script to clean up the data and remove the duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
