{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Code\n",
    "\n",
    "This code block includes import statements, function definitions, and declarations of variables that are common to the rest of the script. It needs to be run once before the other code blocks.\n",
    "\n",
    "**Note: some code in this block is found in the stand-alone file vb_common_code.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "from bs4 import BeautifulSoup # web-scraping library\n",
    "import json\n",
    "from time import sleep\n",
    "import csv\n",
    "import math\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from fuzzywuzzy import process\n",
    "import xml.etree.ElementTree as et # library to traverse XML tree\n",
    "import urllib\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "# For a particular processing round, set a short name for the department here.\n",
    "# This name is used to generate a set of unique processing files for that department.\n",
    "testEmployer = 'Vanderbilt University' # to test against Wikidata employer property\n",
    "employerQId = 'Q29052' # Vanderbilt University\n",
    "deathDateLimit = '2000' # any death dates before this date will be assumed to not be a match\n",
    "birthDateLimit = '1920' # any birth dates before this date will be assumed to not be a match\n",
    "wikibase_instance_namespace = 'http://www.wikidata.org/entity/'\n",
    "\n",
    "# ---------------------\n",
    "# function definitions\n",
    "# ---------------------\n",
    "\n",
    "wikidata_endpoint_url = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "user_agent_header = 'VanderBot/1.7 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "\n",
    "# NCBI identification requirements:\n",
    "# tool name and email address should be sent with all requests\n",
    "# see https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\n",
    "emailAddress = 'steve.baskauf@vanderbilt.edu' # put your email address here\n",
    "toolName = 'VanderBot' # give your application a name here\n",
    "\n",
    "# Extracts the local name part of an IRI, e.g. a qNumber from a Wikidata IRI\n",
    "def extract_local_name(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    last_piece = len(pieces)\n",
    "    return pieces[last_piece - 1]\n",
    "\n",
    "# Generate the current UTC xsd:date\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "today = generate_utc_date()\n",
    "\n",
    "# generates a dictionary to be passed in a requests GET method to generate the request header\n",
    "def generate_header_dictionary_non_sparql(acceptMediaType):\n",
    "    userAgentHeader = 'VanderBot/1.3 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : acceptMediaType,\n",
    "        'User-Agent': userAgentHeader\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "# write a list of lists to a CSV file\n",
    "def write_lists_to_csv(fileName, array):\n",
    "    with open(fileName, 'w', newline='', encoding='utf-8') as fileObject:\n",
    "        writerObject = csv.writer(fileObject)\n",
    "        for row in array:\n",
    "            writerObject.writerow(row)\n",
    "\n",
    "# Read from a CSV file into a list of dictionaries\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# Write list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def lookup_department(researcher_qid, orcid_department, departments, department_labels):\n",
    "    for department in departments:\n",
    "        if researcher_qid == department['qid']:\n",
    "            result = ''\n",
    "            #print(department['affiliation'])\n",
    "            for label in department_labels:\n",
    "                if department['affiliation'] == label['qid']:\n",
    "                    partial_ratio = fuzz.partial_ratio(orcid_department, label['label_en'])\n",
    "                    #set_ratio = fuzz.token_set_ratio(orcid_department, label['label_en'])\n",
    "                    if partial_ratio > 80:\n",
    "                        #print('partial ratio', partial_ratio)\n",
    "                        #print('set_ratio', set_ratio)\n",
    "                        result = str(partial_ratio) + ' ' + orcid_department + '/' + label['label_en']\n",
    "            return result\n",
    "\n",
    "def find_surname_givens(name):\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', ' ')\n",
    "    name = name.replace(',', ' ')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Get rid of empty pieces formed from extra spaces\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "    \n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "def generate_header_dictionary(accept_media_type,user_agent_header):\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "#        'Content-Type': 'application/sparql-query',\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "# The following function requires the request header generated above\n",
    "sparql_request_header = generate_header_dictionary(accept_media_type,user_agent_header)\n",
    "# The query is a valid SPARQL query string\n",
    "\n",
    "# Sends a query to the query service endpoint. \n",
    "# NOTE: request_header and endpoint are global variables defined earlier in the script\n",
    "def send_sparql_query(query_string):\n",
    "    # You can delete the two print statements if the queries are short. However, for large/long queries,\n",
    "    # it's good to let the user know what's going on.\n",
    "    print('querying SPARQL endpoint to acquire item metadata')\n",
    "    #response = requests.post(endpoint, data=query_string.encode('utf-8'), headers=sparql_request_header)\n",
    "    response = requests.post(wikidata_endpoint_url, data=dict(query=query_string), headers=sparql_request_header) # use URL-encoded method\n",
    "    #print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the values from the response JSON\n",
    "    results = data['results']['bindings']\n",
    "    \n",
    "    print('done retrieving data')\n",
    "    # print(json.dumps(results, indent=2))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manage altLabels\n",
    "\n",
    "This process starts with altLabels downloaded from Wikidata and adds alternate names from ORCID"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Vanderbilt people's altLabels from Wikidata\n",
    "\n",
    "Developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/wikidata/download-vanderbilt-people-altlabels.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''select distinct  ?person ?altLabel where {\n",
    "  ?person p:P108 ?statement.\n",
    "  {?statement ps:P108 wd:Q29052.}\n",
    "  union\n",
    "  {?statement ps:P108 wd:Q7914455.}\n",
    "  ?person skos:altLabel ?altLabel.\n",
    "  FILTER(lang(?altLabel)=\"en\")\n",
    "}'''\n",
    "\n",
    "# The endpoint defaults to returning XML, so the Accept: header is required\n",
    "r = requests.get(wikidata_endpoint_url, params={'query' : query}, headers={'Accept' : 'application/json'})\n",
    "\n",
    "data = r.json()\n",
    "#print(json.dumps(data,indent = 2))\n",
    "\n",
    "table = [['qid', 'altLabel', 'source']]\n",
    "items = data['results']['bindings']\n",
    "for item in items:\n",
    "    wikidata_iri = extract_local_name(item['person']['value'])\n",
    "    alt_label = ''\n",
    "    if 'altLabel' in item:\n",
    "        alt_label = item['altLabel']['value']\n",
    "    table.append([wikidata_iri, alt_label, 'wikidata'])\n",
    "    \n",
    "filename = 'vanderbilt_wikidata_altlabels.csv'\n",
    "write_lists_to_csv(filename, table)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'vanderbilt_wikidata_altlabels.csv'\n",
    "researcher_altlabels = read_dicts_from_csv(filename)\n",
    "fieldnames = researcher_altlabels[0].keys() # get the field names from the existing file\n",
    "\n",
    "filename = 'orcid_other_names.csv'\n",
    "orcid_other = read_dicts_from_csv(filename)\n",
    "\n",
    "filename = 'researchers.csv'\n",
    "researcher_items = read_dicts_from_csv(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(orcid_other[0])\n",
    "print()\n",
    "print(researcher_items[0])\n",
    "print()\n",
    "print(researcher_altlabels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in orcid_other:\n",
    "    # Note: the alternate names list contains all VU affiliated people, including students. If they didn't also\n",
    "    # indicate that VU was their employer, they got screened out of the main ORCID list. So there will be a lot\n",
    "    # of non-matches here.\n",
    "    for researcher in researcher_items:\n",
    "        if name['orcid'] == researcher['orcid']:\n",
    "            researcher_dict = {'qid': researcher['qid'], 'altLabel': name['altName'], 'source': 'orcid'}\n",
    "            researcher_altlabels.append(researcher_dict)\n",
    "            break\n",
    "\n",
    "filename = 'vanderbilt_wikidata_altlabels.csv'\n",
    "write_dicts_to_csv(researcher_altlabels, filename, fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
