{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Code\n",
    "\n",
    "This code block includes import statements, function definitions, and declarations of variables that are common to the rest of the script. It needs to be run once before the other code blocks.\n",
    "\n",
    "**Note: the code in this block is found in the stand-alone file vb_common_code.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "from bs4 import BeautifulSoup # web-scraping library\n",
    "import json\n",
    "from time import sleep\n",
    "import csv\n",
    "import math\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from fuzzywuzzy import process\n",
    "import xml.etree.ElementTree as et # library to traverse XML tree\n",
    "import urllib\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "# For a particular processing round, set a short name for the department here.\n",
    "# This name is used to generate a set of unique processing files for that department.\n",
    "testEmployer = 'Vanderbilt University' # to test against Wikidata employer property\n",
    "employerQId = 'Q29052' # Vanderbilt University\n",
    "deathDateLimit = '2000' # any death dates before this date will be assumed to not be a match\n",
    "birthDateLimit = '1920' # any birth dates before this date will be assumed to not be a match\n",
    "wikibase_instance_namespace = 'http://www.wikidata.org/entity/'\n",
    "\n",
    "# ---------------------\n",
    "# utility functions used across blocks\n",
    "# ---------------------\n",
    "\n",
    "wikidataEndpointUrl = 'https://query.wikidata.org/sparql'\n",
    "\n",
    "# NCBI identification requirements:\n",
    "# tool name and email address should be sent with all requests\n",
    "# see https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\n",
    "emailAddress = 'steve.baskauf@vanderbilt.edu' # put your email address here\n",
    "toolName = 'VanderBot' # give your application a name here\n",
    "\n",
    "# generates a dictionary to be passed in a requests GET method to generate the request header\n",
    "def generate_header_dictionary_non_sparql(acceptMediaType):\n",
    "    userAgentHeader = 'VanderBot/1.3 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : acceptMediaType,\n",
    "        'User-Agent': userAgentHeader\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "# write a list of lists to a CSV file\n",
    "def write_lists_to_csv(fileName, array):\n",
    "    with open(fileName, 'w', newline='', encoding='utf-8') as fileObject:\n",
    "        writerObject = csv.writer(fileObject)\n",
    "        for row in array:\n",
    "            writerObject.writerow(row)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query ORCID for Vanderbilt University people\n",
    "\n",
    "Script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/orcid/orcid-get-json.ipynb\n",
    "\n",
    "Retrieves results 100 at a time, then processes them by extracting desired information.  **NOTE: takes hours to run.**\n",
    "\n",
    "Saves results in a file and the alternative names in a second file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [['orcid', 'given_names', 'family_name', 'start_date', 'end_date', 'department', 'organization']]\n",
    "other_name_list = [['orcid', 'altName']]\n",
    "\n",
    "# use the API to search for people associated with Vanderbilt University\n",
    "# First search is for only one record, just to get the number of hits found\n",
    "search_uri = 'https://pub.orcid.org/v2.0/search/?q=affiliation-org-name:\"Vanderbilt+University\"&start=1&rows=1'\n",
    "accept_media_type = 'application/json'\n",
    "response = requests.get(search_uri, headers = generate_header_dictionary_non_sparql(accept_media_type))\n",
    "data = response.json()\n",
    "#print(data)\n",
    "number_results = data[\"num-found\"]\n",
    "print(data[\"num-found\"])\n",
    "number_pages = math.floor(number_results/100)\n",
    "#print(number_pages)\n",
    "remainder = number_results - 100*number_pages\n",
    "#print(remainder)\n",
    "\n",
    "for page_count in range(0, number_pages + 1):  # the remainder will be caught when page_count = number_pages\n",
    "    print('page: ', page_count)\n",
    "    search_uri = 'https://pub.orcid.org/v2.0/search/?q=affiliation-org-name:\"Vanderbilt+University\"&start='+str(page_count*100+1)\n",
    "    response = requests.get(search_uri, headers={'Accept' : 'application/json'})\n",
    "    print(response.url)\n",
    "    data = response.json()\n",
    "    orcids_dicts_list = data['result']\n",
    "\n",
    "    # extract the identifier strings from the data structure\n",
    "    orcids = []\n",
    "    for orcid_dict in orcids_dicts_list:\n",
    "        dictionary = {'id': orcid_dict['orcid-identifier']['path'], 'iri': orcid_dict['orcid-identifier']['uri']}\n",
    "        orcids.append(dictionary)\n",
    "\n",
    "    for orcid_index in range(0, len(orcids)):\n",
    "        response = requests.get(orcids[orcid_index]['iri'], headers={'Accept' : 'application/json'})\n",
    "        data = response.json()\n",
    "        #print(json.dumps(data, indent = 2))\n",
    "        orcid_id = data['orcid-identifier']['path']\n",
    "        #print(orcid_id)\n",
    "        # if there isn't a name, then go on to the next ORCID\n",
    "        if not data['person']['name']:\n",
    "            continue\n",
    "        if data['person']['name']['given-names']:  \n",
    "            given_names = data['person']['name']['given-names']['value']\n",
    "        else:\n",
    "            continue\n",
    "        if data['person']['name']['family-name']:\n",
    "            family_name = data['person']['name']['family-name']['value']\n",
    "        # This has been a big pain when people don't have surnames.\n",
    "        # It causes matches with everyone who has the same first name!\n",
    "        else:\n",
    "            continue\n",
    "        #print(given_names, ' ', family_name)\n",
    "        other_names = data['person']['other-names']['other-name']\n",
    "        for other_name in other_names:\n",
    "            #print(other_name['content'])\n",
    "            other_name_list.append([orcid_id, other_name['content']])\n",
    "\n",
    "        affiliations = data['activities-summary']['employments']['affiliation-group']\n",
    "        #print(json.dumps(affiliations, indent = 2))\n",
    "        for affiliation in affiliations:\n",
    "            summaries = affiliation['summaries']\n",
    "            #print(summaries)\n",
    "            #print()\n",
    "            for summary in summaries:\n",
    "                employment = summary['employment-summary']\n",
    "                #print(json.dumps(employment, indent = 2))\n",
    "                start_date = ''\n",
    "                if employment['start-date']:\n",
    "                    if employment['start-date']['year']:\n",
    "                        start_date += employment['start-date']['year']['value']\n",
    "                        start_month = employment['start-date']['month']\n",
    "                        if start_month:\n",
    "                            start_date += '-' + start_month['value']\n",
    "                            start_day = employment['start-date']['day']\n",
    "                            if start_day:\n",
    "                                start_date += '-' + start_day['value']\n",
    "                #print('start date: ', start_date)\n",
    "                end_date = ''\n",
    "                if employment['end-date']:\n",
    "                    if employment['end-date']['year']:\n",
    "                        end_date += employment['end-date']['year']['value']\n",
    "                        end_month = employment['end-date']['month']\n",
    "                        if end_month:\n",
    "                            end_date += '-' + end_month['value']\n",
    "                            end_day = employment['end-date']['day']\n",
    "                            if end_day:\n",
    "                                end_date += '-' + end_day['value']\n",
    "                #print('end date: ', end_date)\n",
    "                department = employment['department-name']\n",
    "                # if there is no value for department, set it to empty string\n",
    "                if not department:\n",
    "                    department = ''\n",
    "                #print(department)\n",
    "                if employment['organization']:\n",
    "                    organization = employment['organization']['name']\n",
    "                #print(organization)\n",
    "                if 'Vanderbilt University' in organization:\n",
    "                    print(orcid_id, given_names, family_name, start_date, end_date, department, organization)\n",
    "                    table.append([orcid_id, given_names, family_name, start_date, end_date, department, organization])\n",
    "                #print(table)\n",
    "        sleep(.25)\n",
    "\n",
    "print()\n",
    "print('Done')\n",
    "file_name = 'orcid_data.csv'\n",
    "write_lists_to_csv(file_name, table)\n",
    "file_name = 'orcid_other_names.csv'\n",
    "write_lists_to_csv(file_name, other_name_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Vanderbilt people's altLabels from Wikidata\n",
    "\n",
    "Developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/wikidata/download-vanderbilt-people-altlabels.py\n",
    "\n",
    "These values aren't used for anything currently (2020-03-19), so running this is optional. But it will be useful in the future when we want to start collecting aliases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''select distinct  ?person ?altLabel where {\n",
    "  ?person p:P108 ?statement.\n",
    "  ?statement ps:P108  wd:Q29052.\n",
    "  ?person skos:altLabel ?altLabel.\n",
    "  FILTER(lang(?altLabel)=\"en\")\n",
    "}'''\n",
    "\n",
    "# The endpoint defaults to returning XML, so the Accept: header is required\n",
    "r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers={'Accept' : 'application/json'})\n",
    "\n",
    "data = r.json()\n",
    "#print(json.dumps(data,indent = 2))\n",
    "\n",
    "table = [['wikidataIri', 'altLabel']]\n",
    "items = data['results']['bindings']\n",
    "for item in items:\n",
    "    wikidata_iri = item['person']['value']\n",
    "    alt_label = ''\n",
    "    if 'altLabel' in item:\n",
    "        alt_label = item['altLabel']['value']\n",
    "    table.append([wikidata_iri, alt_label])\n",
    "    \n",
    "filename = 'vanderbilt_wikidata_altlabels.csv'\n",
    "write_lists_to_csv(filename, table)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
