{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Code\n",
    "\n",
    "This code block includes import statements, function definitions, and declarations of variables that are common to the rest of the script. It needs to be run once before the other code blocks.\n",
    "\n",
    "**Note: some code in this block is found in the stand-alone file vb_common_code.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "from bs4 import BeautifulSoup # web-scraping library\n",
    "import json\n",
    "from time import sleep\n",
    "import csv\n",
    "import math\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from fuzzywuzzy import process\n",
    "import xml.etree.ElementTree as et # library to traverse XML tree\n",
    "import urllib\n",
    "import datetime\n",
    "import string\n",
    "\n",
    "# For a particular processing round, set a short name for the department here.\n",
    "# This name is used to generate a set of unique processing files for that department.\n",
    "testEmployer = 'Vanderbilt University' # to test against Wikidata employer property\n",
    "employerQId = 'Q29052' # Vanderbilt University\n",
    "deathDateLimit = '2000' # any death dates before this date will be assumed to not be a match\n",
    "birthDateLimit = '1920' # any birth dates before this date will be assumed to not be a match\n",
    "wikibase_instance_namespace = 'http://www.wikidata.org/entity/'\n",
    "\n",
    "# ---------------------\n",
    "# function definitions\n",
    "# ---------------------\n",
    "\n",
    "wikidata_endpoint_url = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "user_agent_header = 'VanderBot/1.7 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "\n",
    "# NCBI identification requirements:\n",
    "# tool name and email address should be sent with all requests\n",
    "# see https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\n",
    "emailAddress = 'steve.baskauf@vanderbilt.edu' # put your email address here\n",
    "toolName = 'VanderBot' # give your application a name here\n",
    "\n",
    "# Generate the current UTC xsd:date\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "today = generate_utc_date()\n",
    "\n",
    "# generates a dictionary to be passed in a requests GET method to generate the request header\n",
    "def generate_header_dictionary_non_sparql(acceptMediaType):\n",
    "    userAgentHeader = 'VanderBot/1.3 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : acceptMediaType,\n",
    "        'User-Agent': userAgentHeader\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "# write a list of lists to a CSV file\n",
    "def write_lists_to_csv(fileName, array):\n",
    "    with open(fileName, 'w', newline='', encoding='utf-8') as fileObject:\n",
    "        writerObject = csv.writer(fileObject)\n",
    "        for row in array:\n",
    "            writerObject.writerow(row)\n",
    "\n",
    "# Read from a CSV file into a list of dictionaries\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# Write list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def lookup_department(researcher_qid, orcid_department, departments, department_labels):\n",
    "    for department in departments:\n",
    "        if researcher_qid == department['qid']:\n",
    "            result = ''\n",
    "            #print(department['affiliation'])\n",
    "            for label in department_labels:\n",
    "                if department['affiliation'] == label['qid']:\n",
    "                    partial_ratio = fuzz.partial_ratio(orcid_department, label['label_en'])\n",
    "                    #set_ratio = fuzz.token_set_ratio(orcid_department, label['label_en'])\n",
    "                    if partial_ratio > 80:\n",
    "                        #print('partial ratio', partial_ratio)\n",
    "                        #print('set_ratio', set_ratio)\n",
    "                        result = str(partial_ratio) + ' ' + orcid_department + '/' + label['label_en']\n",
    "            return result\n",
    "\n",
    "def find_surname_givens(name):\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', ' ')\n",
    "    name = name.replace(',', ' ')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Get rid of empty pieces formed from extra spaces\n",
    "    while '' in pieces:\n",
    "        pieces.remove('')\n",
    "    \n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "def generate_header_dictionary(accept_media_type,user_agent_header):\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "#        'Content-Type': 'application/sparql-query',\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "# The following function requires the request header generated above\n",
    "sparql_request_header = generate_header_dictionary(accept_media_type,user_agent_header)\n",
    "# The query is a valid SPARQL query string\n",
    "\n",
    "# Sends a query to the query service endpoint. \n",
    "# NOTE: request_header and endpoint are global variables defined earlier in the script\n",
    "def send_sparql_query(query_string):\n",
    "    # You can delete the two print statements if the queries are short. However, for large/long queries,\n",
    "    # it's good to let the user know what's going on.\n",
    "    print('querying SPARQL endpoint to acquire item metadata')\n",
    "    #response = requests.post(endpoint, data=query_string.encode('utf-8'), headers=sparql_request_header)\n",
    "    response = requests.post(wikidata_endpoint_url, data=dict(query=query_string), headers=sparql_request_header) # use URL-encoded method\n",
    "    #print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the values from the response JSON\n",
    "    results = data['results']['bindings']\n",
    "    \n",
    "    print('done retrieving data')\n",
    "    # print(json.dumps(results, indent=2))\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query ORCID for Vanderbilt University people\n",
    "\n",
    "Script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/orcid/orcid-get-json.ipynb\n",
    "\n",
    "Retrieves results 100 at a time, then processes them by extracting desired information.  **NOTE: takes hours to run.**\n",
    "\n",
    "Saves results in a file and the alternative names in a second file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = [['orcid', 'given_names', 'family_name', 'start_date', 'end_date', 'department', 'organization']]\n",
    "other_name_list = [['orcid', 'altName']]\n",
    "\n",
    "# use the API to search for people associated with Vanderbilt University\n",
    "# First search is for only one record, just to get the number of hits found\n",
    "search_uri = 'https://pub.orcid.org/v2.0/search/?q=affiliation-org-name:\"Vanderbilt+University\"&start=1&rows=1'\n",
    "accept_media_type = 'application/json'\n",
    "response = requests.get(search_uri, headers = generate_header_dictionary_non_sparql(accept_media_type))\n",
    "data = response.json()\n",
    "#print(data)\n",
    "number_results = data[\"num-found\"]\n",
    "print(data[\"num-found\"])\n",
    "number_pages = math.floor(number_results/100)\n",
    "#print(number_pages)\n",
    "remainder = number_results - 100*number_pages\n",
    "#print(remainder)\n",
    "\n",
    "for page_count in range(0, number_pages + 1):  # the remainder will be caught when page_count = number_pages\n",
    "    print('page: ', page_count)\n",
    "    search_uri = 'https://pub.orcid.org/v2.0/search/?q=affiliation-org-name:\"Vanderbilt+University\"&start='+str(page_count*100+1)\n",
    "    response = requests.get(search_uri, headers={'Accept' : 'application/json'})\n",
    "    print(response.url)\n",
    "    data = response.json()\n",
    "    orcids_dicts_list = data['result']\n",
    "\n",
    "    # extract the identifier strings from the data structure\n",
    "    orcids = []\n",
    "    for orcid_dict in orcids_dicts_list:\n",
    "        dictionary = {'id': orcid_dict['orcid-identifier']['path'], 'iri': orcid_dict['orcid-identifier']['uri']}\n",
    "        orcids.append(dictionary)\n",
    "\n",
    "    for orcid_index in range(0, len(orcids)):\n",
    "        response = requests.get(orcids[orcid_index]['iri'], headers={'Accept' : 'application/json'})\n",
    "        data = response.json()\n",
    "        #print(json.dumps(data, indent = 2))\n",
    "        orcid_id = data['orcid-identifier']['path']\n",
    "        #print(orcid_id)\n",
    "        # if there isn't a name, then go on to the next ORCID\n",
    "        if not data['person']['name']:\n",
    "            continue\n",
    "        if data['person']['name']['given-names']:  \n",
    "            given_names = data['person']['name']['given-names']['value']\n",
    "        else:\n",
    "            continue\n",
    "        if data['person']['name']['family-name']:\n",
    "            family_name = data['person']['name']['family-name']['value']\n",
    "        # This has been a big pain when people don't have surnames.\n",
    "        # It causes matches with everyone who has the same first name!\n",
    "        else:\n",
    "            continue\n",
    "        #print(given_names, ' ', family_name)\n",
    "        other_names = data['person']['other-names']['other-name']\n",
    "        for other_name in other_names:\n",
    "            #print(other_name['content'])\n",
    "            other_name_list.append([orcid_id, other_name['content']])\n",
    "\n",
    "        affiliations = data['activities-summary']['employments']['affiliation-group']\n",
    "        #print(json.dumps(affiliations, indent = 2))\n",
    "        for affiliation in affiliations:\n",
    "            summaries = affiliation['summaries']\n",
    "            #print(summaries)\n",
    "            #print()\n",
    "            for summary in summaries:\n",
    "                employment = summary['employment-summary']\n",
    "                #print(json.dumps(employment, indent = 2))\n",
    "                start_date = ''\n",
    "                if employment['start-date']:\n",
    "                    if employment['start-date']['year']:\n",
    "                        start_date += employment['start-date']['year']['value']\n",
    "                        start_month = employment['start-date']['month']\n",
    "                        if start_month:\n",
    "                            start_date += '-' + start_month['value']\n",
    "                            start_day = employment['start-date']['day']\n",
    "                            if start_day:\n",
    "                                start_date += '-' + start_day['value']\n",
    "                #print('start date: ', start_date)\n",
    "                end_date = ''\n",
    "                if employment['end-date']:\n",
    "                    if employment['end-date']['year']:\n",
    "                        end_date += employment['end-date']['year']['value']\n",
    "                        end_month = employment['end-date']['month']\n",
    "                        if end_month:\n",
    "                            end_date += '-' + end_month['value']\n",
    "                            end_day = employment['end-date']['day']\n",
    "                            if end_day:\n",
    "                                end_date += '-' + end_day['value']\n",
    "                #print('end date: ', end_date)\n",
    "                department = employment['department-name']\n",
    "                # if there is no value for department, set it to empty string\n",
    "                if not department:\n",
    "                    department = ''\n",
    "                #print(department)\n",
    "                if employment['organization']:\n",
    "                    organization = employment['organization']['name']\n",
    "                #print(organization)\n",
    "                if 'Vanderbilt University' in organization:\n",
    "                    print(orcid_id, given_names, family_name, start_date, end_date, department, organization)\n",
    "                    table.append([orcid_id, given_names, family_name, start_date, end_date, department, organization])\n",
    "                #print(table)\n",
    "        sleep(.25)\n",
    "\n",
    "print()\n",
    "print('Done')\n",
    "file_name = 'orcid_data.csv'\n",
    "write_lists_to_csv(file_name, table)\n",
    "file_name = 'orcid_other_names.csv'\n",
    "write_lists_to_csv(file_name, other_name_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download Vanderbilt people's altLabels from Wikidata\n",
    "\n",
    "Developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/wikidata/download-vanderbilt-people-altlabels.py\n",
    "\n",
    "These values aren't used for anything currently (2020-03-19), so running this is optional. But it will be useful in the future when we want to start collecting aliases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '''select distinct  ?person ?altLabel where {\n",
    "  ?person p:P108 ?statement.\n",
    "  ?statement ps:P108  wd:Q29052.\n",
    "  ?person skos:altLabel ?altLabel.\n",
    "  FILTER(lang(?altLabel)=\"en\")\n",
    "}'''\n",
    "\n",
    "# The endpoint defaults to returning XML, so the Accept: header is required\n",
    "r = requests.get(wikidata_endpoint_url, params={'query' : query}, headers={'Accept' : 'application/json'})\n",
    "\n",
    "data = r.json()\n",
    "#print(json.dumps(data,indent = 2))\n",
    "\n",
    "table = [['wikidataIri', 'altLabel']]\n",
    "items = data['results']['bindings']\n",
    "for item in items:\n",
    "    wikidata_iri = item['person']['value']\n",
    "    alt_label = ''\n",
    "    if 'altLabel' in item:\n",
    "        alt_label = item['altLabel']['value']\n",
    "    table.append([wikidata_iri, alt_label])\n",
    "    \n",
    "filename = 'vanderbilt_wikidata_altlabels.csv'\n",
    "write_lists_to_csv(filename, table)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Wikidata people for new ORCIDs\n",
    "\n",
    "Check the downloaded records for Wikidata against the ORCID download to see if any people in Wikidata whose employer is Vanderbilt have new ORCIDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'wikidata/researchers.csv'\n",
    "researcher_items = read_dicts_from_csv(filename)\n",
    "fieldnames = researcher_items[0].keys() # get the field names from the existing file\n",
    "\n",
    "filename = 'vanderbilt_wikidata_altlabels.csv'\n",
    "researcher_altlabels = read_dicts_from_csv(filename)\n",
    "\n",
    "filename = 'orcid_other_names.csv'\n",
    "orcid_other = read_dicts_from_csv(filename)\n",
    "\n",
    "filename = 'orcid_data.csv'\n",
    "orcid_people = read_dicts_from_csv(filename)\n",
    "\n",
    "filename = 'wikidata/departments.csv'\n",
    "departments = read_dicts_from_csv(filename)\n",
    "\n",
    "filename = 'wikidata/department_labels.csv'\n",
    "department_labels = read_dicts_from_csv(filename)\n",
    "\n",
    "print(researcher_items[0])\n",
    "print()\n",
    "print(orcid_people[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# NOTES for future work.\n",
    "# Also, if this is run repeatedly, there needs to be a list of Q IDs and ORCIDs known to not be the same person so\n",
    "# that they don't need to be checked again each time.\n",
    "# That file is done: qid_not_equal_orcid.csv\n",
    "\n",
    "count = 1\n",
    "#for researcher_item_index in range(500):\n",
    "for researcher_item_index in range(len(researcher_items)):\n",
    "    count +=1\n",
    "    if count%100 == 0:\n",
    "        print(count)\n",
    "    if researcher_items[researcher_item_index]['orcid'] == '':\n",
    "        researcher_names = find_surname_givens(researcher_items[researcher_item_index]['label_en'])\n",
    "        for orcid_person in orcid_people:\n",
    "            if researcher_names['family'] == orcid_person['familyName']: # require exact match to family name\n",
    "                w_ratio = fuzz.WRatio(orcid_person['givenNames'] + ' ' + orcid_person['familyName'], researcher_items[researcher_item_index]['label_en'])\n",
    "\n",
    "                if w_ratio > 90:\n",
    "                    #ratio = fuzz.ratio(orcid_person['givenNames'] + ' ' + orcid_person['familyName'], researcher_item['label_en'])\n",
    "                    #partial_ratio = fuzz.partial_ratio(orcid_person['givenNames'] + ' ' + orcid_person['familyName'], researcher_item['label_en'])\n",
    "                    #sort_ratio = fuzz.token_sort_ratio(orcid_person['givenNames'] + ' ' + orcid_person['familyName'], researcher_item['label_en'])\n",
    "                    #set_ratio = fuzz.token_set_ratio(orcid_person['givenNames'] + ' ' + orcid_person['familyName'], researcher_item['label_en'])\n",
    "                    print(w_ratio, researcher_items[researcher_item_index]['label_en'], wikibase_instance_namespace + researcher_items[researcher_item_index]['qid'])\n",
    "                    dept_match = lookup_department(researcher_items[researcher_item_index]['qid'], orcid_person['department'], departments, department_labels)\n",
    "                    print('   ', orcid_person['givenNames'] + ' ' + orcid_person['familyName'], 'https://orcid.org/' + orcid_person['orcid'])\n",
    "                    print(orcid_person['department'])\n",
    "                    if dept_match != '':\n",
    "                        print('Dept. match:', dept_match)\n",
    "                    print()\n",
    "                    #print('name similarity ratio', ratio)\n",
    "                    #print('partial ratio', partial_ratio)\n",
    "                    #print('sort_ratio', sort_ratio)\n",
    "                    #print('set_ratio', set_ratio)\n",
    "                    #print('w_ratio', w_ratio)\n",
    "                    researcher_items[researcher_item_index]['orcid'] = orcid_person['orcid']\n",
    "                    researcher_items[researcher_item_index]['orcid_ref1_retrieved_val'] = today\n",
    "                    write_dicts_to_csv(researcher_items, 'wikidata/researchers.csv', fieldnames)\n",
    "                    break\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check new orcid download for people not associated with VU\n",
    "\n",
    "This part of the script looks for people who ORCID said worked at Vanderbilt, but who don't have any employer property in Wikidata saying that they work(ed) at VU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(orcid_people))\n",
    "orcid_list = ''\n",
    "for person in orcid_people[:500]:\n",
    "    orcid_list += '\"' + person['orcid'] + '\"\\n'\n",
    "#print(orcid_list)\n",
    "\n",
    "query_string = '''select distinct ?person ?name ?orcid where {\n",
    "  values ?orcid\n",
    "  {\n",
    "''' + orcid_list +'''  }\n",
    "  ?person wdt:P496 ?orcid.\n",
    "  ?person rdfs:label ?name.\n",
    "  filter(lang(?name) = \"en\")\n",
    "  minus {\n",
    "    {?person wdt:P108 wd:Q29052.}\n",
    "    union\n",
    "    {?person wdt:P108 wd:Q7914455.}\n",
    "  }\n",
    "  }'''\n",
    "\n",
    "#print(query)\n",
    "\n",
    "data = send_sparql_query(query_string)\n",
    "print(json.dumps(data, indent = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
