{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the script can't find the langdetect module even though you installed it at the command line\n",
    "# try this suggestion from\n",
    "# https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "# You only need to do it once ever unless you reinstall Jupyter notebooks.\n",
    "\n",
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install langdetect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artist analysis\n",
    "\n",
    "\n",
    "## Function section\n",
    "\n",
    "This needs to be run before any of the other cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_gallery.ipynb (2020-12-01)\n",
    "# (c) 2020 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from time import sleep\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from copy import deepcopy\n",
    "from langdetect import detect\n",
    "from langdetect import detect_langs\n",
    "import datetime\n",
    "\n",
    "# ----------------\n",
    "# Configuration settings\n",
    "# ----------------\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "# Calculate the reference date retrieved value for all statements\n",
    "whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "dateZ = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "ref_retrieved = dateZ + 'T00:00:00Z' # form 2019-12-05T00:00:00Z as provided by Wikidata, without leading +\n",
    "\n",
    "# ----------------\n",
    "# Utility functions\n",
    "# ----------------\n",
    "\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderBot/1.7.1 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# find non-redundant values for a column or simple list\n",
    "def non_redundant(table, column_key):\n",
    "    non_redundant_list = []\n",
    "    for row in table:\n",
    "        found = False\n",
    "        for test_item in non_redundant_list:\n",
    "            if column_key == '':\n",
    "                if row == test_item:\n",
    "                    found = True\n",
    "                    break\n",
    "            else:\n",
    "                if row[column_key] == test_item:\n",
    "                    found = True\n",
    "                    break\n",
    "        if not found:\n",
    "            if column_key == '':\n",
    "                non_redundant_list.append(row)\n",
    "            else:\n",
    "                non_redundant_list.append(row[column_key])\n",
    "    return non_redundant_list\n",
    "\n",
    "# function to use in sort of simple list\n",
    "def sort_funct(row):\n",
    "    return row\n",
    "\n",
    "# function to use in sort last_first names\n",
    "def sort_last_first(row):\n",
    "    return row['last_first']\n",
    "\n",
    "# function to use in sort by match score\n",
    "def sort_score(row):\n",
    "    return row['score']\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "def extract_local_name(iri):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[len(pieces) - 1]\n",
    "\n",
    "# search label and alias\n",
    "def searchLabelsAtWikidata(string, class_list):\n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id '\n",
    "    query += '''where {\n",
    "  {?id rdfs:label \"''' + string + '''\"@en.}\n",
    "  union\n",
    "  {?id skos:altLabel \"''' + string + '''\"@en.}\n",
    "  '''\n",
    "    for class_index in range(len(class_list)):\n",
    "        if class_index == 0:\n",
    "            query += '''{?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "        else:\n",
    "            query += '''union\n",
    "  {?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "    query += '''}'''\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    # r = requests.get(endpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_qnumber(result['id']['value'])\n",
    "        return_value.append(qid)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def retrieve_gallery_classes():\n",
    "    # create a string for the query\n",
    "    # use Metropolitan Museum of Art because there are too many collections to not specify the collection.\n",
    "    query = '''select distinct ?class ?label where \n",
    "      {\n",
    "      ?item wdt:P195 wd:Q160236.\n",
    "      ?item wdt:P31 ?class.\n",
    "      ?class rdfs:label ?label.\n",
    "      filter(lang(?label) = 'en')\n",
    "      }\n",
    "      order by ?label'''\n",
    "\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    print('sending query')\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    print('results returned')\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_qnumber(result['class']['value'])\n",
    "        label = result['label']['value']\n",
    "        return_value.append({'label': label, 'qid': qid})\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def generateNameAlternatives(name):\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods\n",
    "    name = name.replace('.', '')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "\n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        # make sure first character is alphabetic\n",
    "        # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "        # typical cases are like (Kit) or \"Kit\"\n",
    "        if not piece[0:1].isalpha():\n",
    "            piece = piece[1:len(piece)] # remove the first non-alphabetic character\n",
    "        if len(piece) > 0:\n",
    "            initials.append(piece[0:1])\n",
    "        \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += pieces[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # full name with suffix\n",
    "    if suffix != '':\n",
    "        nameVersion = ''\n",
    "        for pieceNumber in range(0, len(pieces)-1):\n",
    "            nameVersion += pieces[pieceNumber] + ' '\n",
    "        nameVersion += pieces[len(pieces)-1] + suffix\n",
    "        alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first and last name only\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial and last name only\n",
    "    nameVersion = initials[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    nameVersion = initials[0] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with last name\n",
    "    nameVersion = initials[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber]\n",
    "    nameVersion += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def searchNameAtWikidata(name):\n",
    "    nameList = generateNameAlternatives(name)\n",
    "    alternatives = ''\n",
    "    for alternative in nameList:\n",
    "        # get rid of quotes, which will break the query\n",
    "        alternative = alternative.replace('\"', '')\n",
    "        alternative = alternative.replace(\"'\", '')\n",
    "        alternatives += '\"' + alternative + '\"@en\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    results = []\n",
    "    # r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            if 'label' in statement:\n",
    "                name = statement['label']['value']\n",
    "            else:\n",
    "                name = ''\n",
    "            qNumber = vbc.extract_qnumber(wikidataIri)\n",
    "            results.append({'qId': qNumber, 'name': name})\n",
    "    except:\n",
    "        results = [{'error': r.text}]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return results\n",
    "\n",
    "def name_variant_testing(name, variant):\n",
    "    # get rid of periods\n",
    "    name = name.replace('.','')\n",
    "    variant = variant.replace('.','')\n",
    "    \n",
    "    # create first names\n",
    "    name_pieces = name.split(' ')\n",
    "    variant_pieces = variant.split(' ')\n",
    "    last_name = name_pieces[len(name_pieces)-1]\n",
    "    last_variant = variant_pieces[len(variant_pieces)-1]\n",
    "    if len(name_pieces) > 1:\n",
    "        first_names = name[0:-(len(last_name)+1)]\n",
    "    else:\n",
    "        first_names = name     \n",
    "    if len(variant_pieces) > 1:\n",
    "        first_variants = variant[0:-(len(last_variant)+1)]\n",
    "    else:\n",
    "        first_variants = variant      \n",
    "    #print(first_names)\n",
    "    #print(first_variants)\n",
    "    \n",
    "    # compare first names\n",
    "    # I experimented with the different ratios and I think fuzz might be best.\n",
    "    ratio = fuzz.ratio(first_names, first_variants)\n",
    "    #partial_ratio = fuzz.partial_ratio(first_names, first_variants)\n",
    "    #sort_ratio = fuzz.token_sort_ratio(first_names, first_variants)\n",
    "    #set_ratio = fuzz.token_set_ratio(first_names, first_variants)\n",
    "    # print('name similarity ratio', ratio)\n",
    "    #print('partial ratio', partial_ratio)\n",
    "    #print('sort_ratio', sort_ratio)\n",
    "    #print('set_ratio', set_ratio)\n",
    "\n",
    "    return(ratio)\n",
    "\n",
    "def find_surname_givens(name):\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', '')\n",
    "    name = name.replace(',', '')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "def remove_parens(string):\n",
    "    name_string = string.split('(')[0]\n",
    "    return name_string.strip()\n",
    "\n",
    "def remove_description(string):\n",
    "    try:\n",
    "        right_string = string.split('(')[1]\n",
    "        left_string = right_string.split(')')[0]\n",
    "        result = left_string.strip()\n",
    "    except:\n",
    "        result = ''\n",
    "    return result\n",
    "\n",
    "def reverse_names(string):\n",
    "    pieces = string.split(',')\n",
    "    return pieces[1].strip() + ' ' + pieces[0].strip()\n",
    "\n",
    "# Screens for Wikidata items that are potential matches\n",
    "\n",
    "import vb_common_code as vbc\n",
    "retrieve_class_list_query = vbc.Query(pid='P31', uselabel=False, sleep=sparql_sleep)\n",
    "retrieve_birth_date_query = vbc.Query(isitem=False, pid='P569', sleep=sparql_sleep)\n",
    "retrieve_death_date_query = vbc.Query(isitem=False, pid='P570', sleep=sparql_sleep)\n",
    "\n",
    "def human(qId):\n",
    "    screen = True\n",
    "    wdClassList = retrieve_class_list_query.single_property_values_for_item(qId)\n",
    "    # if there is a class property, check if it's a human\n",
    "    if len(wdClassList) != 0:\n",
    "        # if it's not a human\n",
    "        if wdClassList[0] != 'Q5':\n",
    "            #print('*** This item is not a human!')\n",
    "            screen = False\n",
    "    return screen\n",
    "\n",
    "# returns a dictionary of various descriptors of the item with Wikidata ID qId\n",
    "# P106 is occupation, schema:description is filtered to be the English description\n",
    "def searchWikidataDescription(qId):\n",
    "    resultsDict = {}\n",
    "    query = '''select distinct ?description ?orcid ?occupation where {\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' schema:description ?description.\n",
    "            FILTER(lang(?description) = 'en')\n",
    "            }\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = 'en')            \n",
    "            }\n",
    "        optional {wd:'''+ qId + ''' wdt:P496 ?orcid.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the dictionary remains empty\n",
    "            # Only a single description per language is allowed, so there should only be one description\n",
    "            if 'description' in statements[0]:\n",
    "                description = statements[0]['description']['value']\n",
    "            else:\n",
    "                description = ''\n",
    "            resultsDict['description'] = description\n",
    "            \n",
    "            # Only a single ORCID is allowed, so there should only be one orcid value\n",
    "            if 'orcid' in statements[0]:\n",
    "                orcid = statements[0]['orcid']['value']\n",
    "            else:\n",
    "                orcid = ''\n",
    "            resultsDict['orcid'] = orcid\n",
    "            \n",
    "            # if there are multiple statements, that's because there are more than one occupation\n",
    "            occupationList = []\n",
    "            for statement in statements:\n",
    "                if 'occupation' in statement:\n",
    "                    occupationList.append(statement['occupation']['value'])\n",
    "            resultsDict['occupation'] = occupationList\n",
    "    except:\n",
    "        resultsDict = {'error': r.text}\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return resultsDict\n",
    "\n",
    "def determine_era(string):\n",
    "    # dates with no CE or BCE, including empty string\n",
    "    if 'CE' not in string:\n",
    "        value = string\n",
    "        era = 'unknown'      \n",
    "    else:\n",
    "        if 'BCE' in string:\n",
    "            value = string[0:len(string)-3].strip()\n",
    "            era = 'BCE'\n",
    "        else: # string ends with CE\n",
    "            value = string[0:len(string)-2].strip()\n",
    "            era = 'CE'\n",
    "    return value, era\n",
    "\n",
    "def determine_zeros(date):\n",
    "    zero_count = 0\n",
    "    for char_number in range(len(date), 0, -1):\n",
    "        if date[char_number-1] == '0':\n",
    "            zero_count += 1\n",
    "        else:\n",
    "            return zero_count\n",
    "\n",
    "def pad_zeros_left(date_string):\n",
    "    length = len(date_string)\n",
    "    pad = 4-length\n",
    "    return '0' * pad + date_string\n",
    "\n",
    "def sign(era):\n",
    "    if era == 'BCE':\n",
    "        return '-'\n",
    "    elif era == 'CE':\n",
    "        return ''\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def detect_language(string):\n",
    "    try:\n",
    "        lang_list = detect_langs(string)\n",
    "        lang_string = str(lang_list[0])\n",
    "        confidence = float(lang_string[3:])\n",
    "        lang = lang_string[:2]\n",
    "    except: #exceptions occur when no info to decide, e.g. numbers\n",
    "        lang = 'zxx'\n",
    "        confidence = float(0)\n",
    "    return lang, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count works by artists that we added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querying SPARQL endpoint to acquire item metadata\n",
      "done retrieving data\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "filename = 'creators/creators_to_write.csv'\n",
    "new_creators = read_dict(filename)\n",
    "\n",
    "# Create list of Q IDs for artists\n",
    "item_qids = ''\n",
    "for creator in new_creators:\n",
    "    item_qids += 'wd:' + creator['qid'] + '\\n'\n",
    "# remove trailing newline\n",
    "item_qids = item_qids[:len(item_qids)-1]\n",
    "\n",
    "query = '''\n",
    "select distinct ?qid ?artistLabel (COUNT(*) AS ?count) where {\n",
    "\n",
    "  VALUES ?qid\n",
    "{\n",
    "''' + item_qids + '''\n",
    "}\n",
    "\n",
    "  ?work wdt:P170 ?qid.\n",
    "  ?work wdt:P195 wd:Q18563658.\n",
    "  optional {\n",
    "    ?qid rdfs:label ?artistLabel.\n",
    "    filter(lang(?artistLabel)=\"en\")\n",
    "  }\n",
    "}\n",
    "GROUP BY ?qid ?artistLabel\n",
    "ORDER BY DESC(?count) ?artistLabel\n",
    "'''\n",
    "\n",
    "#print(query)\n",
    "\n",
    "print('querying SPARQL endpoint to acquire item metadata')\n",
    "response = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "#print(response.text)\n",
    "data = response.json()\n",
    "\n",
    "# extract the values from the response JSON\n",
    "results = data['results']['bindings']\n",
    "\n",
    "print('done retrieving data')\n",
    "#print(json.dumps(results, indent=2))\n",
    "\n",
    "# ----------------\n",
    "# extract results\n",
    "# ----------------\n",
    "\n",
    "metadata_list_added = []\n",
    "for result in results:\n",
    "    row_dict = {}\n",
    "    row_dict['qid'] = extract_qnumber(result['qid']['value'])\n",
    "    row_dict['artistLabel'] = result['artistLabel']['value']\n",
    "    row_dict['count'] = result['count']['value']\n",
    "    metadata_list_added.append(row_dict)\n",
    "\n",
    "#print(json.dumps(metadata_list_added, indent=2))\n",
    "\n",
    "fieldnames = ['qid', 'artistLabel', 'count']\n",
    "write_dicts_to_csv(metadata_list_added, 'counts_for_artists_added.csv', fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count works by all non-anonymous artists represented in collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querying SPARQL endpoint to acquire item metadata\n",
      "done retrieving data\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "select distinct ?qid ?artistLabel (COUNT(*) AS ?count) where {\n",
    "\n",
    "  ?work wdt:P195 wd:Q18563658.\n",
    "  ?work p:P170 ?statement.\n",
    "  ?statement ps:P170 ?qid.\n",
    "  minus { ?statement pq:P3831 wd:Q4233718.} # remove anonymous artists\n",
    "  optional {\n",
    "    ?qid rdfs:label ?artistLabel.\n",
    "    filter(lang(?artistLabel)=\"en\")\n",
    "  }\n",
    "}\n",
    "GROUP BY ?qid ?artistLabel\n",
    "ORDER BY DESC(?count) ?artistLabel\n",
    "\n",
    "'''\n",
    "\n",
    "#print(query)\n",
    "\n",
    "print('querying SPARQL endpoint to acquire item metadata')\n",
    "response = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "#print(response.text)\n",
    "data = response.json()\n",
    "\n",
    "# extract the values from the response JSON\n",
    "results = data['results']['bindings']\n",
    "\n",
    "print('done retrieving data')\n",
    "#print(json.dumps(results, indent=2))\n",
    "\n",
    "# ----------------\n",
    "# extract results\n",
    "# ----------------\n",
    "\n",
    "metadata_list_all = []\n",
    "for result in results:\n",
    "    row_dict = {}\n",
    "    row_dict['qid'] = extract_qnumber(result['qid']['value'])\n",
    "    row_dict['artistLabel'] = result['artistLabel']['value']\n",
    "    row_dict['count'] = result['count']['value']\n",
    "    metadata_list_all.append(row_dict)\n",
    "\n",
    "#print(json.dumps(metadata_list_all, indent=2))\n",
    "\n",
    "fieldnames = ['qid', 'artistLabel', 'count']\n",
    "write_dicts_to_csv(metadata_list_all, 'counts_for_all_artists.csv', fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Works for artists we didn't add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "metadata_list_nonadded = []\n",
    "\n",
    "for artist in metadata_list_all:\n",
    "    matched = False\n",
    "    for new_artist in metadata_list_added:\n",
    "        if artist['qid'] == new_artist['qid']:\n",
    "            matched = True\n",
    "            break\n",
    "    if not matched:\n",
    "        metadata_list_nonadded.append(artist)\n",
    "\n",
    "fieldnames = ['qid', 'artistLabel', 'count']\n",
    "write_dicts_to_csv(metadata_list_nonadded, 'counts_for_artists_not_added.csv', fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pull additional data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querying SPARQL endpoint to acquire item metadata\n",
      "done retrieving data\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "select distinct ?qid ?artistLabel ?birthdate ?deathdate ?sexgender ?ulan ?birthplace ?birthLabel ?citizenship ?citizenLabel where {\n",
    "\n",
    "  ?work wdt:P195 wd:Q18563658.\n",
    "  ?work p:P170 ?statement.\n",
    "  ?statement ps:P170 ?qid.\n",
    "  minus { ?statement pq:P3831 wd:Q4233718.} # remove anonymous artists\n",
    "\n",
    "  optional {\n",
    "    ?qid rdfs:label ?artistLabel.\n",
    "    filter(lang(?artistLabel)=\"en\")\n",
    "  }\n",
    "  optional { ?qid wdt:P569 ?birthdate.}\n",
    "  optional { ?qid wdt:P570 ?deathdate.}\n",
    "  optional { ?qid wdt:P21 ?sexgender.}\n",
    "  optional { ?qid wdt:P245 ?ulan.}\n",
    "  optional {\n",
    "    ?qid wdt:P19 ?birthplace.\n",
    "    ?birthplace rdfs:label ?birthLabel.\n",
    "    filter(lang(?birthLabel)=\"en\")\n",
    "    }\n",
    "  optional { \n",
    "    ?qid wdt:P27 ?citizenship.\n",
    "    ?citizenship rdfs:label ?citizenLabel.\n",
    "    filter(lang(?citizenLabel)=\"en\")\n",
    "    }\n",
    "\n",
    "}\n",
    "'''\n",
    "\n",
    "#print(query)\n",
    "\n",
    "print('querying SPARQL endpoint to acquire item metadata')\n",
    "response = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "#print(response.text)\n",
    "data = response.json()\n",
    "\n",
    "# extract the values from the response JSON\n",
    "results = data['results']['bindings']\n",
    "\n",
    "print('done retrieving data')\n",
    "#print(json.dumps(results, indent=2))\n",
    "\n",
    "# ----------------\n",
    "# extract results\n",
    "# ----------------\n",
    "\n",
    "metadata_list_all = []\n",
    "for result in results:\n",
    "    row_dict = {}\n",
    "    row_dict['qid'] = extract_qnumber(result['qid']['value'])\n",
    "    \n",
    "    if 'artistLabel' in result:\n",
    "        row_dict['artistLabel'] = result['artistLabel']['value']\n",
    "    else:\n",
    "        row_dict['artistLabel'] = ''\n",
    "        \n",
    "    if 'birthdate' in result:\n",
    "        row_dict['birthdate'] = result['birthdate']['value']\n",
    "    else:\n",
    "        row_dict['birthdate'] = ''\n",
    "        \n",
    "    if 'deathdate' in result:\n",
    "        row_dict['deathdate'] = result['deathdate']['value'] \n",
    "    else:\n",
    "        row_dict['deathdate'] = ''\n",
    "        \n",
    "    if 'sexgender' in result:\n",
    "        row_dict['sexgender'] = result['sexgender']['value']\n",
    "    else:\n",
    "        row_dict['sexgender'] = ''\n",
    "        \n",
    "    if 'ulan' in result:\n",
    "        row_dict['ulan'] = result['ulan']['value']\n",
    "    else:\n",
    "        row_dict['ulan'] = ''\n",
    "        \n",
    "    if 'birthplace' in result:\n",
    "        row_dict['birthplace'] = result['birthplace']['value'] \n",
    "    else:\n",
    "        row_dict['birthplace'] = ''\n",
    "        \n",
    "    if 'birthLabel' in result:\n",
    "        row_dict['birthLabel'] = result['birthLabel']['value'] \n",
    "    else:\n",
    "        row_dict['birthLabel'] = ''\n",
    "        \n",
    "    if 'citizenship' in result:\n",
    "        row_dict['citizenship'] = result['citizenship']['value'] \n",
    "    else:\n",
    "        row_dict['citizenship'] = ''\n",
    "        \n",
    "    if 'citizenLabel' in result:\n",
    "        row_dict['citizenLabel'] = result['citizenLabel']['value']\n",
    "    else:\n",
    "        row_dict['citizenLabel'] = ''\n",
    "\n",
    "    metadata_list_all.append(row_dict)\n",
    "\n",
    "#print(json.dumps(metadata_list_all, indent=2))\n",
    "\n",
    "fieldnames = ['qid', 'artistLabel', 'birthdate', 'deathdate', 'sexgender', 'ulan', 'birthplace', 'birthLabel', 'citizenship', 'citizenLabel']\n",
    "write_dicts_to_csv(metadata_list_all, 'artists_all_metadata.csv', fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aggregate data from different sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "filename = 'counts_for_all_artists.csv'\n",
    "all_artists = read_dict(filename)\n",
    "\n",
    "filename = 'artists_all_metadata.csv'\n",
    "all_artists_metadata = read_dict(filename)\n",
    "\n",
    "filename = 'creators/creators_to_write.csv'\n",
    "new_creators = read_dict(filename)\n",
    "\n",
    "artist_complete_list = []\n",
    "for artist in all_artists:\n",
    "    \n",
    "    matched = False\n",
    "    metadata_match = {} # don't really handle failure to match properly\n",
    "    for metadata in all_artists_metadata:\n",
    "        if artist['qid'] == metadata['qid']:\n",
    "            # If there are multiple lines for the artist in the metadata, the last line will be used\n",
    "            matched = True\n",
    "            metadata_match = metadata\n",
    "    if matched:\n",
    "        metadata_match['count'] = artist['count']\n",
    "    else:\n",
    "        metadata_match['count'] = ''\n",
    "\n",
    "    # Determine if the artist had a new record created by us. If so the 'added' filed will be True\n",
    "    matched = False\n",
    "    for new_creator in new_creators:\n",
    "        if artist['qid'] == new_creator['qid']:\n",
    "            # If there are multiple lines for the artist in the metadata, the last line will be used\n",
    "            matched = True\n",
    "    metadata_match['added'] = matched\n",
    "    \n",
    "    # Do some value cleanup\n",
    "    if metadata_match['birthdate'] != '':\n",
    "        metadata_match['birthdate'] = metadata_match['birthdate'].split('T')[0]\n",
    "    if metadata_match['deathdate'] != '':\n",
    "        metadata_match['deathdate'] = metadata_match['deathdate'].split('T')[0]\n",
    "    if metadata_match['birthplace'] != '':\n",
    "        metadata_match['birthplace'] = extract_local_name(metadata_match['birthplace'])\n",
    "    if metadata_match['citizenship'] != '':\n",
    "        metadata_match['citizenship'] = extract_local_name(metadata_match['citizenship'])\n",
    "\n",
    "    if metadata_match['sexgender'] != '':\n",
    "        if metadata_match['sexgender'] == 'http://www.wikidata.org/entity/Q6581097':\n",
    "            metadata_match['sexgender'] = 'male'\n",
    "        elif metadata_match['sexgender'] == 'http://www.wikidata.org/entity/Q6581072':\n",
    "            metadata_match['sexgender'] = 'female'\n",
    "        else:\n",
    "            metadata_match['sexgender'] = extract_local_name(metadata_match['sexgender'])\n",
    "    \n",
    "    artist_complete_list.append(metadata_match)\n",
    "\n",
    "fieldnames = ['qid', 'artistLabel', 'birthdate', 'deathdate', 'sexgender', 'ulan', 'birthplace', 'birthLabel', 'citizenship', 'citizenLabel', 'count', 'added']\n",
    "write_dicts_to_csv(artist_complete_list, 'artists_master_metadata.csv', fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
