{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# commonsbot.ipynb, a Python script for uploading files and data to Wikimedia Commons using the API.\n",
    "version = '0.4'\n",
    "created = '2022-01-18'\n",
    "\n",
    "# (c) 2022 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "# -----------------------------------------\n",
    "# Version 0.4 change notes: \n",
    "# - Removed double spaces from labels before they are used to generate image filenames.\n",
    "# - Skip over images with raw filenames that contain spaces and log an error for them to be manually removed.\n",
    "# -----------------------------------------\n",
    "\n",
    "# IMPORTANT NOTE: If you hack this script to upload media to Commons, you MUST NOT decrease the delay time between\n",
    "# API writes to less than 5 seconds in order to speed up writing. If you do, then your script isn't \n",
    "# BaskaufCommonsBot and you need to change the user_agent_string to use your own URL and email address.\n",
    "# The same holds true if you make other substantive changes to the way that the script interacts with the API.\n",
    "# This script attempts to respect the \"good citizen\" guidelines for using the API and you should too.\n",
    "\n",
    "# These are recommended delay times to avoid hitting the APIs too frequently and getting blocked\n",
    "sparql_sleep = 0.1 # delay time between calls to Wikidata SPARQL endpoint, probably could be lower (like 0.1)\n",
    "\n",
    "# commons_sleep not used in this script because of built-in-delays between media uploads.\n",
    "commons_sleep = 5 # non-critical edits to commons no faster than this. https://commons.wikimedia.org/wiki/Commons:Bots#Bot_speed\n",
    "\n",
    "read_api_sleep = 0.1\n",
    "\n",
    "# Description of bots on Commons: https://commons.wikimedia.org/wiki/Commons:Bots\n",
    "# See guidelines for operating a bot in Commons: https://commons.wikimedia.org/wiki/Commons:Bots/Requests\n",
    "# Need to decide whether this applies if non autonomous. It probably does.\n",
    "# Bot flag is an indication of community trust and prevents new images/recent changes lists from getting swamped.\n",
    "# It's also an indication of community trust; confirms edits not likely to need manual checking\n",
    "\n",
    "# Generic Commons API reference: https://commons.wikimedia.org/w/api.php\n",
    "\n",
    "# NOTE: this script recycles code from the more full-featured and better tested VanderBot script:\n",
    "# https://github.com/HeardLibrary/linked-data/tree/master/vanderbot\n",
    "\n",
    "# ----------------\n",
    "# Configuration\n",
    "# ----------------\n",
    "\n",
    "# This section contains import statements and function definitions.\n",
    "# It should be run before running other sections of the code\n",
    "\n",
    "import json\n",
    "import requests\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "import sys\n",
    "import re # regex\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import urllib.parse\n",
    "import webbrowser\n",
    "\n",
    "# AWS Python SDK\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "# Hard coded values\n",
    "\n",
    "max_items_to_upload = 1\n",
    "open_browser_tab_after_upload = True\n",
    "\n",
    "# Change working directory to image upload directory\n",
    "os.chdir('/users/baskausj/github/vandycite/gallery_works/image_upload/')\n",
    "\n",
    "# Should be saved in current working directory\n",
    "log_path = 'error_log.txt'\n",
    "\n",
    "local_image_directory_path = 'gallery_digital_image_archive/'\n",
    "path_is_relative_to_home_directory = True\n",
    "\n",
    "local_image_root_directory = '/users/baskausj/gallery_pyramidal_tiffs/'\n",
    "s3_iiif_bucket_name = 'iiif-library-cantaloupe-storage'\n",
    "s3_manifest_bucket_name = 'iiif-manifest.library.vanderbilt.edu'\n",
    "s3_iiif_project_directory = 'gallery'\n",
    "iiif_server_url_root = 'https://iiif.library.vanderbilt.edu/iiif/3/'\n",
    "manifest_iri_stem = 'https://iiif-manifest.library.vanderbilt.edu/'\n",
    "actual_manifest_iri_stem = 'https://iiif-manifest.library.vanderbilt.edu/'\n",
    "\n",
    "user_agent_string = 'BaskaufCommonsBot/' + version + ' (https://github.com/HeardLibrary/linked-data/tree/master/commonsbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "public_domain_categories = [\n",
    "    'artist died before copyright cutoff', \n",
    "    'artist was born before 1800', \n",
    "    'assessed to be out of copyright', \n",
    "    'from style or period that ended prior to copyright cutoff',\n",
    "    'inception prior to copyright cutoff'\n",
    "]\n",
    "copyright_cutoff_date = 1926\n",
    "\n",
    "# Options for filtering by image size\n",
    "size_filter = 'pixsquared' # options: filetype, filesize, pixsquared\n",
    "requrired_filetype = 'tiff' # not implemented (yet)\n",
    "minimum_filesize = 1000\n",
    "minimup_pixel_squared = 1000000\n",
    "\n",
    "templated_institution = 'Vanderbilt University Fine Arts Gallery' # Name used in an existing Institution template\n",
    "source_name = 'Vanderbilt University Fine Arts Gallery'\n",
    "category_strings = ['Vanderbilt University Fine Arts Gallery'] # Commons categories to be added to the image.\n",
    "default_language = 'en'\n",
    "\n",
    "# ------------------------\n",
    "# function definitions\n",
    "# ------------------------\n",
    "\n",
    "# Utility functions\n",
    "\n",
    "# gunction to get local name from an IRI\n",
    "def extract_localname(iri):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[len(pieces)-1] # return the last piece\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "            \n",
    "# Commons identifier/URL conversion functions\n",
    "# There are three identifiers used in Commons:\n",
    "\n",
    "# The most basic one is the filename, unencoded and with file extension.\n",
    "\n",
    "# The Commons web page URL is formed from the filename by prepending a subpath and \"File:\", replacing spaces in the filename with _, and URL-encoding the file name string\n",
    "# The reverse process may be lossy because it assumes that underscores should be turned into spaces and the filename might actuall contain underscores.\n",
    "\n",
    "# The Wikidata IRI identifier for the image is formed from the filename by URL-encoding it and prepending a subpath and \"Special:FilePath/\"\n",
    "# It the reverse process is lossless since it simply reverse URL-encodes the local name part of the IRI.\n",
    "\n",
    "commons_prefix = 'http://commons.wikimedia.org/wiki/Special:FilePath/'\n",
    "commons_page_prefix = 'https://commons.wikimedia.org/wiki/File:'\n",
    "def commons_url_to_filename(url):\n",
    "    # form of URL is: http://commons.wikimedia.org/wiki/Special:FilePath/Castle%20De%20Haar%20%281892-1913%29%20-%20360%C2%B0%20Panorama%20of%20Castle%20%26%20Castle%20Grounds.jpg\n",
    "    string = url.split(commons_prefix)[1] # get local name file part of URL\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_url(filename):\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_prefix + encoded_filename\n",
    "    return url\n",
    "\n",
    "def commons_page_url_to_filename(url):\n",
    "    # form of URL is: https://commons.wikimedia.org/wiki/File:Castle_De_Haar_(1892-1913)_-_360%C2%B0_Panorama_of_Castle_%26_Castle_Grounds.jpg\n",
    "    string = url.split(commons_page_prefix)[1] # get local name file part of URL\n",
    "    string = string.replace('_', ' ')\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_page_url(filename):\n",
    "    filename = filename.replace(' ', '_')\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_page_prefix + encoded_filename\n",
    "    url = url.replace('%28', '(').replace('%29', ')').replace('%2C', ',')\n",
    "    return url\n",
    "\n",
    "\n",
    "# Authentication functions\n",
    "\n",
    "def login(path, relative_to_home):\n",
    "    if relative_to_home:\n",
    "        home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "        full_credentials_path = home + '/' + path\n",
    "    else:\n",
    "        full_credentials_path = path\n",
    "    credentials = retrieve_credentials(full_credentials_path)\n",
    "    \n",
    "    resource_url = '/w/api.php' # default API resource URL for all Wikimedia APIs\n",
    "    endpoint_url = credentials['url'] + resource_url\n",
    "\n",
    "    # Instantiate session\n",
    "    session = requests.Session()\n",
    "    # Set default User-Agent header so you don't have to send it with every request\n",
    "    session.headers.update({'User-Agent': user_agent_string})\n",
    "\n",
    "    # Go through the sequence of steps needed to get get the CSRF token\n",
    "    login_token = get_login_token(endpoint_url, session)\n",
    "    data = session_login(endpoint_url, login_token, credentials['username'], credentials['password'], session)\n",
    "    csrf_token = get_csrf_token(endpoint_url, session)\n",
    "    return {'session': session, 'csrftoken': csrf_token, 'endpoint': endpoint_url}\n",
    "\n",
    "def retrieve_credentials(path):\n",
    "    with open(path, 'rt') as file_object:\n",
    "        line_list = file_object.read().split('\\n')\n",
    "    endpoint_url = line_list[0].split('=')[1]\n",
    "    username = line_list[1].split('=')[1]\n",
    "    password = line_list[2].split('=')[1]\n",
    "    #user_agent = line_list[3].split('=')[1]\n",
    "    credentials = {'url': endpoint_url, 'username': username, 'password': password}\n",
    "    return credentials\n",
    "\n",
    "def get_login_token(apiUrl, session):    \n",
    "    parameters = {\n",
    "        'action':'query',\n",
    "        'meta':'tokens',\n",
    "        'type':'login',\n",
    "        'format':'json'\n",
    "    }\n",
    "    r = session.get(url=apiUrl, params=parameters)\n",
    "    data = r.json()\n",
    "    return data['query']['tokens']['logintoken']\n",
    "\n",
    "def session_login(apiUrl, token, username, password, session):\n",
    "    parameters = {\n",
    "        'action':'login',\n",
    "        'lgname':username,\n",
    "        'lgpassword':password,\n",
    "        'lgtoken':token,\n",
    "        'format':'json'\n",
    "    }\n",
    "    r = session.post(apiUrl, data=parameters)\n",
    "    data = r.json()\n",
    "    return data\n",
    "\n",
    "def get_csrf_token(apiUrl, session):\n",
    "    parameters = {\n",
    "        \"action\": \"query\",\n",
    "        \"meta\": \"tokens\",\n",
    "        \"format\": \"json\"\n",
    "    }\n",
    "    r = session.get(url=apiUrl, params=parameters)\n",
    "    data = r.json()\n",
    "    return data[\"query\"][\"tokens\"][\"csrftoken\"]\n",
    "\n",
    "# Commons identifier conversion functions\n",
    "commons_prefix = 'http://commons.wikimedia.org/wiki/Special:FilePath/'\n",
    "commons_page_prefix = 'https://commons.wikimedia.org/wiki/File:'\n",
    "def commons_url_to_filename(url):\n",
    "    # form of URL is: http://commons.wikimedia.org/wiki/Special:FilePath/Castle%20De%20Haar%20%281892-1913%29%20-%20360%C2%B0%20Panorama%20of%20Castle%20%26%20Castle%20Grounds.jpg\n",
    "    string = url.split(commons_prefix)[1] # get local name file part of URL\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_url(filename):\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_prefix + encoded_filename\n",
    "    return url\n",
    "\n",
    "def commons_page_url_to_filename(url):\n",
    "    # form of URL is: https://commons.wikimedia.org/wiki/File:Castle_De_Haar_(1892-1913)_-_360%C2%B0_Panorama_of_Castle_%26_Castle_Grounds.jpg\n",
    "    string = url.split(commons_page_prefix)[1] # get local name file part of URL\n",
    "    string = string.replace('_', ' ')\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_page_url(filename):\n",
    "    filename = filename.replace(' ', '_')\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_page_prefix + encoded_filename\n",
    "    url = url.replace('%28', '(').replace('%29', ')').replace('%2C', ',')\n",
    "    return url\n",
    "\n",
    "# Data upload functions\n",
    "\n",
    "# Create the date_string for various uncertainty situations. Used in creating template (next function)\n",
    "def create_template_date_string(inception_val, inception_prec, inception_earliest_date_val, inception_earliest_date_prec, inception_latest_date_val, inception_latest_date_prec, inception_sourcing_circumstances):\n",
    "    # See https://commons.wikimedia.org/wiki/Template:Other_date for formatting information\n",
    "    \n",
    "    # Return nothing if there is no inception date\n",
    "    if inception_val == '':\n",
    "        return ''\n",
    "    \n",
    "    if inception_earliest_date_val == '': # no date range\n",
    "        # No VU gallery works have precisions > 9, but it's handled just in case\n",
    "        if inception_prec == '11': # precise to day\n",
    "            date_string = inception_val[:10]\n",
    "        elif inception_prec == '10': # precise to month\n",
    "            date_string = inception_val[:7]\n",
    "        else:\n",
    "            date_string = inception_val[:4] # return the year, regardless of whether precision is year, decade, century, etc.\n",
    "        # Handle circa\n",
    "        if inception_sourcing_circumstances == 'Q5727902': # Q ID for circa\n",
    "            date_string = '{{other date|circa|'+ date_string + '}}'\n",
    "            \n",
    "    else: # date range must be handled\n",
    "        if inception_earliest_date_prec == '11': # precise to day\n",
    "            early_date_string = inception_earliest_date_val[:10]\n",
    "        elif inception_earliest_date_prec == '10': # precise to month\n",
    "            early_date_string = inception_earliest_date_val[:7]\n",
    "        else:\n",
    "            early_date_string = inception_earliest_date_val[:4] # return the year, regardless of whether precision is year, decade, century, etc.\n",
    "            \n",
    "        if inception_latest_date_prec == '11': # precise to day\n",
    "            late_date_string = inception_latest_date_val[:10]\n",
    "        elif inception_latest_date_prec == '10': # precise to month\n",
    "            late_date_string = inception_latest_date_val[:7]\n",
    "        else:\n",
    "            late_date_string = inception_latest_date_val[:4] # return the year, regardless of whether precision is year, decade, century, etc.\n",
    "            \n",
    "        # Handle circa\n",
    "        if inception_sourcing_circumstances == 'Q5727902': # Q ID for circa\n",
    "            date_string = '{{other date|circa|'+ early_date_string + '|'+ late_date_string + '}}'\n",
    "        else:\n",
    "            date_string = '{{other date|between|'+ early_date_string + '|'+ late_date_string + '}}'\n",
    "    \n",
    "    return date_string\n",
    "            \n",
    "# Insert metadata into the Commons Artwork template\n",
    "def create_commons_template(work_qid, label, description, description_language, artist_qid, date_string, width, height, source_name, templated_institution, reference_url, reference_iso_date_string, notes, medium, category_strings):\n",
    "    \n",
    "    # Convert the dateTime formatted string to European style date with month word\n",
    "    datetime_object = datetime.datetime.fromisoformat(reference_iso_date_string[:10])\n",
    "    reference_date = datetime_object.strftime('%d %B %Y')\n",
    "    # Remove leading zero on day if any\n",
    "    if reference_date[0] == '0':\n",
    "        reference_date = reference_date[1:]\n",
    "        \n",
    "    if artist_qid == '':\n",
    "        creator_template = ''\n",
    "    else:\n",
    "        creator_template = '{{ Creator | Wikidata = ' + artist_qid + ' | Option = {{{1|}}} }}'\n",
    "            \n",
    "    page_wikitext = '''\n",
    "=={{int:filedesc}}==\n",
    "{{Artwork\n",
    " |artist             = ''' + creator_template + '''\n",
    " |title              = ''' + \"{{en|'''\" + label + \"'''.}}\" + '''\n",
    " |description        = {{''' + description_language + '''|1=''' + description + '''}}\n",
    " |depicted people    =\n",
    " |depicted place     =\n",
    " |date               = ''' + date_string + '''\n",
    " |medium             = ''' + medium + '''\n",
    " |dimensions         = {{Size|in|''' + width + '|' + height + '''}}\n",
    " |institution        = {{Institution:''' + templated_institution + '''}}\n",
    " |department         =\n",
    " |accession number   = \n",
    " |place of creation  = \n",
    " |place of discovery =\n",
    " |object history     =\n",
    " |exhibition history =\n",
    " |credit line        =\n",
    " |inscriptions       =\n",
    " |notes              = ''' + notes + '''\n",
    " |references         = {{cite web |title=''' + label + ' |url=' + reference_url + ' |accessdate=' + reference_date + '''}}\n",
    " |source             = ''' + source_name + '''\n",
    " |permission         =\n",
    " |other_versions     =\n",
    " |wikidata           = ''' + work_qid + '''\n",
    " |other_fields       =\n",
    "}}\n",
    "\n",
    "=={{int:license-header}}==\n",
    "{{PD-Art|PD-old-100-expired}}\n",
    "\n",
    "'''\n",
    "    # Add all of the categories in the list\n",
    "    for category_string in category_strings:\n",
    "        page_wikitext += '[[Category:' + category_string + ''']]\n",
    "'''\n",
    "    \n",
    "    return page_wikitext\n",
    "\n",
    "# API file Upload example: https://www.mediawiki.org/wiki/API:Upload#POST_request\n",
    "# API Sandbox can be used to generate test JSON, but DO NOT RUN since it actually uploads.\n",
    "# Specifically for uploads, see https://commons.wikimedia.org/wiki/Special:ApiSandbox#action=upload&filename=Wiki.png&url=http%3A//upload.wikimedia.org/wikipedia/en/b/bc/Wiki.png&token=123ABC\n",
    "def upload_file_to_commons(image_filename, commons_filename, directory_path, relative_to_home, session, csrftoken, sleeptime, wikitext):\n",
    "    if relative_to_home:\n",
    "        home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "        directory_path = home + '/' + directory_path\n",
    "\n",
    "    parameters = {\n",
    "        'action': 'upload',\n",
    "        'filename': commons_filename,\n",
    "        'format': 'json',\n",
    "        'token': csrftoken,\n",
    "        'ignorewarnings': 1,\n",
    "        'text': wikitext,\n",
    "        # this is what generates the text in the Description box on user Uploads page and initial edit summary for page\n",
    "        # See https://commons.wikimedia.org/wiki/Commons:First_steps/Quality_and_description#Upload_summary\n",
    "        'comment': 'Uploaded media file and metadata via API'\n",
    "    }\n",
    "    #directory_path = 'Downloads/'\n",
    "    file_path = directory_path + image_filename\n",
    "    file_dict = {'file':(image_filename, open(file_path, 'rb'), 'multipart/form-data')}\n",
    "    #print(parameters)\n",
    "    #print(file_dict)\n",
    "\n",
    "    print('uploading', commons_filename) # This line is important for large TIFF files that will take a while to upload\n",
    "    response = session.post('https://commons.wikimedia.org/w/api.php', files=file_dict, data = parameters)\n",
    "    # Trap for errors. Note: as far as I can tell, no sort of error code or HTTP header gets sent identifying the \n",
    "    # cause of the error. So at this point, just report an error by returning an empty dictionary.\n",
    "    try:\n",
    "        data = response.json()\n",
    "    except:\n",
    "        data = {}\n",
    "    #print(json.dumps(data, indent=2))\n",
    "\n",
    "    # for non-critical applications, do not hit the API rapidly. See notes at the top of the script.\n",
    "    sleep(sleeptime)\n",
    "    return(data)\n",
    "\n",
    "\n",
    "# This function is used in the following function, which needs a page ID rather than a name\n",
    "def get_commons_image_pageid(image_filename):\n",
    "    # get metadata for a photo including from file page\n",
    "    params = {\n",
    "        'action': 'query',\n",
    "        'format': 'json',\n",
    "        'titles': 'File:' + image_filename,\n",
    "        'prop': 'info'\n",
    "    }\n",
    "\n",
    "    response = requests.get('https://commons.wikimedia.org/w/api.php', params=params)\n",
    "    data = response.json()\n",
    "    #print(json.dumps(data, indent=2))\n",
    "    page_dict = data['query']['pages'] # this value is a dict that has the page IDs as keys\n",
    "    page_id_list = list(page_dict.keys()) # the result of the .keys() method is a \"dict_keys\" object, so coerce to a list\n",
    "    page_id = page_id_list[0] # info on only one page was requested, so get item 0\n",
    "    #print('Page ID:',page_id)\n",
    "    \n",
    "    # Don't think I need to add a sleep time for API reads, which are less resource-intensive\n",
    "    # than write operations\n",
    "    # NOTE: appears to return '-1' when it can't find the page.\n",
    "    return page_id\n",
    "\n",
    "# Wikibase edit entity function (upload both caption and all Structured data statements at once)\n",
    "def wbeditentity_upload(commons_session, commons_csrf_token, maxlag, mid, caption, caption_language, property_p_id, value_q_id):\n",
    "    # Code hacked from VanderBot https://github.com/HeardLibrary/linked-data/blob/master/vanderbot/vanderbot.py\n",
    "\n",
    "    # Set up the parameter JSON object that will be passed to the API\n",
    "    parameter_dictionary = {\n",
    "        'action': 'wbeditentity',\n",
    "        'format':'json',\n",
    "        'token': commons_csrf_token,\n",
    "        'id': mid, # use id key instead of new since it already exists\n",
    "        'summary': 'Add caption and structured data via API'\n",
    "        }\n",
    "\n",
    "    # This structure will be encoded as JSON, then used as the value of a \"data\" name in the parameter object\n",
    "    # First create the labels part\n",
    "    data_structure = {\n",
    "        'labels': {\n",
    "            caption_language: {\n",
    "                'language': caption_language,\n",
    "                'value': caption\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Now create a JSON array of the claims (Structured data statements) to be added.\n",
    "    # Currently there is only one, but array items could be created in a loop to add multiple claims at once.\n",
    "    claims_list = []\n",
    "\n",
    "    # start loop here\n",
    "    snak_dict = {\n",
    "        'mainsnak': {\n",
    "            'snaktype': 'value',\n",
    "            'property': property_p_id,\n",
    "            'datatype': 'wikibase-item',\n",
    "            'datavalue': {\n",
    "                'value': {\n",
    "                    'id': value_q_id\n",
    "                    },\n",
    "                'type': 'wikibase-entityid'\n",
    "                }\n",
    "            },\n",
    "        'type': 'statement',\n",
    "        'rank': 'normal'\n",
    "        }\n",
    "    claims_list.append(snak_dict)\n",
    "    # end loop here\n",
    "\n",
    "    # Now add the array of claims to the data structure\n",
    "    data_structure['claims'] = claims_list\n",
    "\n",
    "    #print(json.dumps(data_structure, indent = 2))\n",
    "    #print()\n",
    "\n",
    "    # Confusingly, the data structure has to be encoded as a JSON string before adding as a value of the data name \n",
    "    # in the parameter object, which will itself be encoded as JSON before passing to the API by the requests module.\n",
    "    parameter_dictionary['data'] = json.dumps(data_structure)\n",
    "\n",
    "    # Support maxlag if the API is too busy\n",
    "    if maxlag > 0:\n",
    "        parameter_dictionary['maxlag'] = maxlag\n",
    "\n",
    "    #print(json.dumps(parameter_dictionary, indent = 2))\n",
    "\n",
    "    response = attemptPost('https://commons.wikimedia.org/w/api.php', parameter_dictionary, commons_session)\n",
    "    #response = commons_session.post('https://commons.wikimedia.org/w/api.php', data = parameter_dictionary)\n",
    "    return response\n",
    "\n",
    "\n",
    "# This function attempts to post and handles maxlag errors\n",
    "# Code reused from VanderBot https://github.com/HeardLibrary/linked-data/blob/master/vanderbot/vanderbot.py\n",
    "def attemptPost(apiUrl, parameters, session):\n",
    "    maxRetries = 10\n",
    "    delayLimit = 300\n",
    "    retry = 0\n",
    "    # maximum number of times to retry lagged server = maxRetries\n",
    "    while retry <= maxRetries:\n",
    "        if retry > 0:\n",
    "            print('retry:', retry)\n",
    "        r = session.post(apiUrl, data = parameters)\n",
    "        #print(r.text)\n",
    "        data = r.json()\n",
    "        try:\n",
    "            # check if response is a maxlag error\n",
    "            # see https://www.mediawiki.org/wiki/Manual:Maxlag_parameter\n",
    "            if data['error']['code'] == 'maxlag':\n",
    "                print('Lag of ', data['error']['lag'], ' seconds.')\n",
    "                # recommended delay is basically useless\n",
    "                # recommendedDelay = int(r.headers['Retry-After'])\n",
    "                #if recommendedDelay < 5:\n",
    "                    # recommendation is to wait at least 5 seconds if server is lagged\n",
    "                #    recommendedDelay = 5\n",
    "                recommendedDelay = commons_sleep*2**retry # double the delay with each retry \n",
    "                if recommendedDelay > delayLimit:\n",
    "                    recommendedDelay = delayLimit\n",
    "                if retry != maxRetries:\n",
    "                    print('Waiting ', recommendedDelay , ' seconds.')\n",
    "                    print()\n",
    "                    sleep(recommendedDelay)\n",
    "                retry += 1\n",
    "\n",
    "                # after this, go out of if and try code blocks\n",
    "            else:\n",
    "                # an error code is returned, but it's not maxlag\n",
    "                return data\n",
    "        except:\n",
    "            # if the response doesn't have an error key, it was successful, so return\n",
    "            return data\n",
    "        # here's where execution goes after the delay\n",
    "    # here's where execution goes after maxRetries tries\n",
    "    print('Failed after ' + str(maxRetries) + ' retries.')\n",
    "    exit() # just abort the script\n",
    "    \n",
    "\n",
    "# !!! DEPRECATED IN FAVOR OF THE SINGLE POST USING wbeditentity\n",
    "# Adding the image caption seems to be a hack that uses the Wikibase API command wbsetlabel.\n",
    "# Captions are Wikibase labels (language specific), limit 255 characters length.\n",
    "# See https://commons.wikimedia.org/wiki/Commons:File_captions#Technical\n",
    "def set_commons_image_caption(image_filename, caption, caption_language, session, csrftoken, sleeptime):\n",
    "    parameters = {\n",
    "        'action': 'wbsetlabel',\n",
    "        'format': 'json',\n",
    "        'token': csrftoken,\n",
    "        'site': 'commonswiki',\n",
    "        'title': 'File:' + image_filename,\n",
    "        'value': caption,\n",
    "        'language': caption_language,\n",
    "        'summary': 'Add caption via API'\n",
    "    }\n",
    "\n",
    "    #print(json.dumps(parameters, indent = 2))\n",
    "\n",
    "    response = session.post('https://commons.wikimedia.org/w/api.php', data = parameters)\n",
    "    \n",
    "    data = response.json()\n",
    "    #print(json.dumps(data, indent=2))\n",
    "\n",
    "    sleep(sleeptime)\n",
    "    return(data)\n",
    "\n",
    "# !!! DEPRECATED IN FAVOR OF THE SINGLE POST USING wbeditentity\n",
    "# Code comes from writeStatement() function at https://github.com/HeardLibrary/digital-scholarship/blob/master/code/wikibase/api/load_csv.py\n",
    "# Described in this blog post: http://baskauf.blogspot.com/2019/06/putting-data-into-wikidata-using.html\n",
    "def create_commons_claim(image_filename, property_p_id, value_q_id, session, csrftoken, sleeptime):\n",
    "    wikibase_subject_id = 'M' + get_commons_image_pageid(image_filename)\n",
    "    #property_p_id = 'P180' # depicts\n",
    "    #value_q_id = 'Q384177' # Egyptian Revival (architecture)\n",
    "\n",
    "    stripped_q_number = value_q_id[1:len(value_q_id)] # remove initial \"Q\" from object string\n",
    "    value_dictionary = {\n",
    "        'entity-type': 'item',\n",
    "        'numeric-id': stripped_q_number\n",
    "    }\n",
    "    value_json_string = json.dumps(value_dictionary)\n",
    "\n",
    "    parameters = {\n",
    "        'action':'wbcreateclaim',\n",
    "        'format':'json',\n",
    "        'token': csrftoken,\n",
    "        'entity': wikibase_subject_id,\n",
    "        'snaktype':'value',\n",
    "        'property': property_p_id,\n",
    "        # note: the value of 'value' is a JSON string, not an actual data structure.  \n",
    "        #It will get URL encoded by requests before posting\n",
    "        'value': value_json_string,\n",
    "        'summary': 'Add structured data via API'\n",
    "    }\n",
    "\n",
    "    #print(json.dumps(parameters, indent = 2))\n",
    "    response = session.post('https://commons.wikimedia.org/w/api.php', data = parameters)\n",
    "    data = response.json()\n",
    "    #print(json.dumps(data, indent=2))\n",
    "\n",
    "    sleep(sleeptime)\n",
    "    return data\n",
    "\n",
    "# ---------------------------\n",
    "# Body of main script\n",
    "# ---------------------------\n",
    "\n",
    "# This section contains configuration information and performs necessary logins\n",
    "# It needs to be run once before the rest of the code\n",
    "# No writing is done, so it's \"safe\" to run any time\n",
    "\n",
    "# Set the value of the maxlag parameter to back off when the server is lagged\n",
    "# see https://www.mediawiki.org/wiki/Manual:Maxlag_parameter\n",
    "# The recommended value is 5 seconds.\n",
    "# To not use maxlang, set the value to 0\n",
    "# To test the maxlag handler code, set maxlag to a very low number like .1\n",
    "# If you don't know what you are doing, leave this value alone. In any case, it is rude to use a value greater than 5.\n",
    "\n",
    "maxlag = 5\n",
    "\n",
    "# This section needs to be run prior to running any code that interacts with the Commons API\n",
    "# It generates the CSRF token required to post to the API on behalf of the user whose username and pwd are being used\n",
    "\n",
    "# This is the format of the credentials file. \n",
    "# Username and password are for a bot that you've created.\n",
    "\n",
    "'''\n",
    "endpointUrl=https://test.wikidata.org\n",
    "username=User@bot\n",
    "password=465jli90dslhgoiuhsaoi9s0sj5ki3lo\n",
    "'''\n",
    "\n",
    "# ---------------------------\n",
    "# Load data from CSVs into DataFrames\n",
    "# ---------------------------\n",
    "\n",
    "# Note: setting the index to be the Q ID requires that qid has a unique value for each row. This should be the case.\n",
    "works_metadata = pd.read_csv('/Users/baskausj/github/vandycite/gallery_works/image_upload/commons_images.csv', na_filter=False, dtype = str)\n",
    "works_metadata.set_index('qid', inplace=True)\n",
    "\n",
    "raw_metadata = pd.read_csv('../gallery_works_renamed1.csv', na_filter=False, dtype = str)\n",
    "raw_metadata.set_index('accession_number', inplace=True)\n",
    "\n",
    "image_dimensions = pd.read_csv('image_dimensions.csv', na_filter=False, dtype = str)\n",
    "# Convert some columns to integers\n",
    "image_dimensions[['kilobytes', 'height', 'width']] = image_dimensions[['kilobytes', 'height', 'width']].astype(int)\n",
    "\n",
    "works_classification = pd.read_csv('../../gallery_buchanan/works_classification.csv', na_filter=False, dtype = str)\n",
    "works_classification.set_index('qid', inplace=True)\n",
    "\n",
    "works_ip_status = pd.read_csv('../items_status_abbrev.csv', na_filter=False, dtype = str)\n",
    "works_ip_status.set_index('qid', inplace=True)\n",
    "\n",
    "existing_images = pd.read_csv('commons_images.csv', na_filter=False, dtype = str) # Don't make the Q IDs the index!\n",
    "\n",
    "# For testing purposes, just use the first few rows of the works metadata\n",
    "#test_rows = 70\n",
    "#works_metadata = works_metadata.head(test_rows).copy()\n",
    "\n",
    "# ---------------------------\n",
    "# Commons API Post Authentication (create session and generate CSRF token)\n",
    "# ---------------------------\n",
    "\n",
    "# If credentials file location is relative to current working directory, use subfolders through file name with no leading slash\n",
    "# Example: myproj/credentials/commons_credentials.txt\n",
    "# If credentials file is in current working directory, only filename is necessary\n",
    "# Need to give example for absolute path on Windows - use Unix forward slashes?\n",
    "path = 'commons_credentials.txt'\n",
    "path_is_relative_to_home_directory = True # set to True if relative home directory, False if absolute path or relative to working directory\n",
    "result = login(path, path_is_relative_to_home_directory)\n",
    "# print(result)\n",
    "commons_session = result['session']\n",
    "commons_csrf_token = result['csrftoken']\n",
    "# Commons API endpoint URL is in result['endpoint'], but it is going to be hard coded anyway, so ignore\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# Screen works for appropriate images to upload\n",
    "# ---------------------------\n",
    "\n",
    "# Place limit on number of items to upload in a session by tracking number uploaded\n",
    "items_uploaded = 0\n",
    "\n",
    "# Open an error log to record errors\n",
    "log_object = open(log_path, 'wt', encoding='utf-8')\n",
    "errors = False\n",
    "\n",
    "# The row index is the Q ID and is a string. The work object is the data in the row and is a Pandas series\n",
    "# The items in the row series can be referred to by their labels, which are the column headers, e.g. work['label_en']\n",
    "for index, work in works_metadata.iterrows():\n",
    "    upload_error = False\n",
    "    #print(work['label_en'])\n",
    "    \n",
    "    # Get the remaining metadata from the VanderBot upload CSV\n",
    "    work_qid = index\n",
    "    label = work['label_en']\n",
    "\n",
    "    # The local_filename is the name of the file as it exists locally.\n",
    "    # NOTE: if the image filename contains a space, it will generate an error when the IIIF manifest link is uploaded\n",
    "    # to the Wikidata API. It's better if the images don't have spaces, so the script will just skip over it and \n",
    "    # flag the image to have its name changed manually, rather than automatically changing spaces to underscores \n",
    "    # (potentially causing a naming collision).\n",
    "    if ' ' in work['local_filename']:\n",
    "        print('Raw filename \"' + work['local_filename'] + '\" for ' + work_qid + ' contains spaces that need to be removed manually.')\n",
    "        print('Unallowed spaces in raw filename \"' + work['local_filename'] + '\" for ' + work_qid, file=log_object)\n",
    "        errors = True\n",
    "        continue\n",
    "    else:\n",
    "        local_filename = work['local_filename']\n",
    "\n",
    "    # subdirectory is the directory that contains the local file. It's within the local_image_directory_path. \n",
    "    # Don't include a trailing slash.\n",
    "    # If images are directly in the directory_path, use empty string ('') as the value.\n",
    "    subdirectory = work['directory']\n",
    "\n",
    "    # ----------------\n",
    "    # Upload highres images to IIIF server s3 bucket\n",
    "    # ----------------\n",
    "    \n",
    "    tiff_extensions = ['tif', 'TIF', 'tiff', 'TIFF']\n",
    "    file_extension = local_filename.split('.')[-1]\n",
    "    #print(file_extension)\n",
    "    if file_extension in tiff_extensions:\n",
    "    \n",
    "        # See https://boto3.amazonaws.com/v1/documentation/api/latest/guide/s3.html#uploads\n",
    "        local_file_path = local_image_root_directory + subdirectory + '/' + local_filename\n",
    "        s3_iiif_key = s3_iiif_project_directory + '/' + subdirectory + '/' + local_filename\n",
    "\n",
    "        s3 = boto3.client('s3')\n",
    "        print('Uploading to s3:', local_filename)\n",
    "        s3.upload_file(local_file_path, s3_iiif_bucket_name, s3_iiif_key)\n",
    "\n",
    "        # For the image in the \"iiif-library-cantaloupe-storage\" bucket with the key \"gallery/1979/1979.0264P.tif\"\n",
    "        # the IIIF URL would be https://iiif.library.vanderbilt.edu/iiif/3/gallery%2F1979%2F1979.0264P.tif/full/max/0/default.jpg\n",
    "        print(iiif_server_url_root + s3_iiif_project_directory + '%2F' + subdirectory + '%2F' + local_filename + '/full/1000,/0/default.jpg')\n",
    "    #print()\n",
    "\n",
    "\n",
    "if not errors:\n",
    "    print('No errors occurred.', file=log_object)\n",
    "log_object.close()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
