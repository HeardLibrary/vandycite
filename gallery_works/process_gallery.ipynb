{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from time import sleep\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# ----------------\n",
    "# Configuration settings\n",
    "# ----------------\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "# ----------------\n",
    "# Utility functions\n",
    "# ----------------\n",
    "\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderBot/1.6 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# find non-redundant values for a column\n",
    "def non_redundant(table, column_key):\n",
    "    non_redundant_list = []\n",
    "    for row in table:\n",
    "        found = False\n",
    "        for test_item in non_redundant_list:\n",
    "            if row[column_key] == test_item:\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            non_redundant_list.append(row[column_key])\n",
    "    return non_redundant_list\n",
    "\n",
    "# function to use in sort\n",
    "def sort_funct(row):\n",
    "    return row\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# search label and alias\n",
    "# For whatever reason, if I use the graph pattern\n",
    "\n",
    "# wd:Q21 wdt:P31 ?class.\n",
    "\n",
    "# England is not Q6256 (country)\n",
    "# But if I use the graph pattern\n",
    "\n",
    "#   wd:Q21 p:P31 ?statement.\n",
    "#  ?statement ps:P31 ?class.\n",
    "\n",
    "# it is ??!!\n",
    "def searchLabelsAtWikidata(string, class_list):\n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id '\n",
    "    query += '''where {\n",
    "  {?id rdfs:label \"''' + string + '''\"@en.}\n",
    "  union\n",
    "  {?id skos:altLabel \"''' + string + '''\"@en.}\n",
    "  '''\n",
    "    for class_index in range(len(class_list)):\n",
    "        if class_index == 0:\n",
    "            query += '''{?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "        else:\n",
    "            query += '''union\n",
    "  {?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "    query += '''}'''\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    # r = requests.get(endpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_qnumber(result['id']['value'])\n",
    "        return_value.append(qid)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def retrieve_gallery_classes():\n",
    "    # create a string for the query\n",
    "    # use Metropolitan Museum of Art because there are too many collections to not specify the collection.\n",
    "    query = '''select distinct ?class ?label where \n",
    "      {\n",
    "      ?item wdt:P195 wd:Q160236.\n",
    "      ?item wdt:P31 ?class.\n",
    "      ?class rdfs:label ?label.\n",
    "      filter(lang(?label) = 'en')\n",
    "      }\n",
    "      order by ?label'''\n",
    "\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    print('sending query')\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    print('results returned')\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_qnumber(result['class']['value'])\n",
    "        label = result['label']['value']\n",
    "        return_value.append({'label': label, 'qid': qid})\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract  data\n",
    "\n",
    "This section simplifies the column headers and writes a copy of the data to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'gallery_works.csv'\n",
    "old_works = read_dict(filename)\n",
    "fieldnames = list(old_works[0].keys())\n",
    "print(fieldnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works = []\n",
    "for old_work in old_works:\n",
    "    work = {}\n",
    "    work['ssid'] = old_work['SSID']\n",
    "    work['filename'] = old_work['Filename']\n",
    "    work['title'] = old_work['Title[637073]'] \n",
    "    work['creator'] = old_work['Creator[637071]']\n",
    "    work['date'] = old_work['Date[637076]']\n",
    "    work['classification'] = old_work['Classification[637103]']\n",
    "    work['medium'] = old_work['Medium[637080]']\n",
    "    work['measurements'] = old_work['Measurements[637081]']\n",
    "    work['style_period'] = old_work['Style/Period[637079]']\n",
    "    work['country_culture'] = old_work['Country/Culture[637072]']\n",
    "    work['seals_inscriptions'] = old_work['Seals & Inscriptions[637104]']\n",
    "    work['signature'] = old_work['Signature[637105]']\n",
    "    work['description'] = old_work['Description[637092]']\n",
    "    work['publications'] = old_work['Publications[637106]']\n",
    "    work['exhibitions'] = old_work['Exhibitions[637107]']\n",
    "    work['accession_number'] = old_work['Accession Number[637085]']\n",
    "    work['date_acquired'] = old_work['Date Acquired[637109]']\n",
    "    work['gift_of'] = old_work['Gift of[637110]']\n",
    "    work['purchased_from'] = old_work['Purchased from[637111]']\n",
    "    work['credit_line'] = old_work['Credit Line[637112]']\n",
    "    work['provenance'] = old_work['Provenance[637113]']\n",
    "    work['collection'] = old_work['Collection[637114]']\n",
    "    work['last_change'] = old_work['Last Change[637115]']\n",
    "    work['notes'] = old_work['Notes[637116]']\n",
    "    work['rights'] = old_work['Rights[637099]']\n",
    "    work['media_url'] = old_work['Media URL']\n",
    "    # License and the Artstor fields are not populated\n",
    "        \n",
    "    works.append(work)\n",
    "        \n",
    "fieldnames = list(works[0].keys())\n",
    "write_dicts_to_csv(works, 'gallery_works_renamed.csv', fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate country table\n",
    "\n",
    "Match labels in the country_culture column with labels and aliases in Wikidata for various country-like items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = non_redundant(works, 'country_culture')\n",
    "values.sort(key = sort_funct)\n",
    "if values[0] == '':\n",
    "    values.remove('')\n",
    "print(values)\n",
    "\n",
    "mappings = []\n",
    "for value in values:\n",
    "    result_list = searchLabelsAtWikidata(value, ['Q6256','Q3624078','Q6266'])\n",
    "    print('|' + value + '|', result_list)\n",
    "    if len(result_list) == 1:\n",
    "        qid = result_list[0]\n",
    "    elif len(result_list) > 1:\n",
    "        qid = result_list\n",
    "    else:\n",
    "        qid = ''\n",
    "    mappings.append({'string': value, 'qid': qid})\n",
    "write_dicts_to_csv(mappings, 'country_mappings.csv', ['string', 'qid'])\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate classes\n",
    "\n",
    "This section first gets all of the classes (values for P31 instanceOf) from the Met's collection. Then it tries to match the labels of those items to the values in the classification column of the gallery data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all classes in the Met\n",
    "work_classes = retrieve_gallery_classes()\n",
    "print(work_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = non_redundant(works, 'classification')\n",
    "values.sort(key = sort_funct)\n",
    "if values[0] == '':\n",
    "    values.remove('')\n",
    "print(values)\n",
    "\n",
    "mappings = []\n",
    "for value in values:\n",
    "    found = False\n",
    "    for work_class in work_classes:\n",
    "        if value.lower() == work_class['label'].lower():\n",
    "            found = True\n",
    "            mappings.append({'string': value, 'qid': work_class['qid'], 'label': work_class['label']})\n",
    "    if not found:\n",
    "        mappings.append({'string': value, 'qid': '', 'label': ''})\n",
    "write_dicts_to_csv(mappings, 'classification_mappings.csv', ['string', 'qid', 'label'])\n",
    "print()\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse dimensions data\n",
    "\n",
    "This processes the values of the measurements column and separates them into length and width or length/width/height quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = {}\n",
    "for work_index in range(len(works)):\n",
    "#for work_index in range(0,1):\n",
    "    string = works[work_index]['measurements'].strip()\n",
    "    if string == '':\n",
    "        # no value; set all variables to empty string\n",
    "        height = ''\n",
    "        width = ''\n",
    "        depth = ''\n",
    "        circum = ''\n",
    "    elif ' x ' not in string:\n",
    "        # one dimensional or improperly formatted\n",
    "        pieces = string.split(' ')\n",
    "        try:\n",
    "            value = float(pieces[0])\n",
    "            if pieces[1] != 'in.':\n",
    "                # second part of string not \"in.\"\n",
    "                print(works[work_index]['accession_number'], string)\n",
    "                height = ''\n",
    "                width = ''\n",
    "                depth = ''\n",
    "                circum = ''\n",
    "            else:\n",
    "                height = ''\n",
    "                width = ''\n",
    "                depth = ''\n",
    "                circum = str(value)\n",
    "        except:\n",
    "            # improperly formatted            \n",
    "            # set all variables to empty string\n",
    "            print(works[work_index]['accession_number'], string)\n",
    "            height = ''\n",
    "            width = ''\n",
    "            depth = ''\n",
    "            circum = ''\n",
    "    else:\n",
    "        # the string has an x in it, so it's multidimensional\n",
    "        pieces = string.split('x')\n",
    "        # split the string and get rid of leading and trailing whitespace\n",
    "        for piece_index in range(len(pieces)):\n",
    "            pieces[piece_index] = pieces[piece_index].strip()\n",
    "        # remove the \"in.\" and any spaces from the last piece\n",
    "        pieces[len(pieces)-1] = pieces[len(pieces)-1].split('in')[0].strip()\n",
    "        if len(pieces) == 2:\n",
    "            # two-dimensional work\n",
    "            try:\n",
    "                height = str(float(pieces[0]))\n",
    "                width = str(float(pieces[1]))\n",
    "                depth = ''\n",
    "                circum = ''\n",
    "            except:\n",
    "                print(works[work_index]['accession_number'], string)\n",
    "                height = ''\n",
    "                width = ''\n",
    "                depth = ''\n",
    "                circum = ''                \n",
    "        else:\n",
    "            # three-dimensional work\n",
    "            try:\n",
    "                height = str(float(pieces[0]))\n",
    "                width = str(float(pieces[1]))\n",
    "                depth = str(float(pieces[2]))\n",
    "                circum = ''\n",
    "            except:\n",
    "                print(works[work_index]['accession_number'], string)\n",
    "                height = ''\n",
    "                width = ''\n",
    "                depth = ''\n",
    "                circum = ''\n",
    "    works[work_index]['height'] = height\n",
    "    works[work_index]['width'] = width\n",
    "    works[work_index]['depth'] = depth\n",
    "    works[work_index]['circum'] = circum\n",
    "\n",
    "for work in works:\n",
    "    print(work['height'], work['width'], work['depth'], work['circum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fieldnames = list(works[0].keys())\n",
    "write_dicts_to_csv(works, 'gallery_works_with_dim.csv', fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The followng cell is obsolete but is being left for reference purposes. Since each line represents a separate item, it is no longer necessary to try to group lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_redundant_works = []\n",
    "last_title = ''\n",
    "last_creator = ''\n",
    "last_date = ''\n",
    "n_non_redundant_works = 0\n",
    "for redundant_work in works:\n",
    "    if last_title == redundant_work['Title[637073]'] and last_creator == redundant_work['Creator[637071]']:\n",
    "        # the same work as the previous row\n",
    "        \n",
    "        # add non-redundant fields to the existing lists\n",
    "        ssid.append(redundant_work['SSID'])\n",
    "        filename.append(redundant_work['Filename'])\n",
    "        media_url.append(redundant_work['Media URL'])\n",
    "        accession_number.append(redundant_work['Accession Number[637085]'])\n",
    "        last_change.append(redundant_work['Last Change[637115]'])\n",
    "        date_acquired.append(redundant_work['Date Acquired[637109]'])\n",
    "\n",
    "        # replace the list values in every loop\n",
    "        non_redundant_work['ssid'] = ssid\n",
    "        non_redundant_work['filename'] = filename\n",
    "        non_redundant_work['accession_number'] = accession_number\n",
    "        non_redundant_work['date_acquired'] = date_acquired\n",
    "        non_redundant_work['last_change'] = last_change\n",
    "        non_redundant_work['media_url'] = media_url\n",
    "        \n",
    "        # replace the last value of the non-redundant work with the new value containing added list items\n",
    "        non_redundant_works[n_non_redundant_works -1] = non_redundant_work\n",
    "\n",
    "    else:\n",
    "        # a new work\n",
    "        non_redundant_work = {}\n",
    "        n_non_redundant_works += 1\n",
    "        \n",
    "        last_title = redundant_work['Title[637073]'] \n",
    "        last_creator = redundant_work['Creator[637071]']\n",
    "        \n",
    "        # add non-redundant fields to new lists\n",
    "        ssid = [redundant_work['SSID']]\n",
    "        filename = [redundant_work['Filename']]\n",
    "        accession_number = [redundant_work['Accession Number[637085]']]\n",
    "        date_acquired = [redundant_work['Date Acquired[637109]']]\n",
    "        last_change = [redundant_work['Last Change[637115]']]\n",
    "        media_url = [redundant_work['Media URL']]\n",
    "        \n",
    "        # add redundant fields to work record\n",
    "        non_redundant_work['title'] = redundant_work['Title[637073]'] \n",
    "        non_redundant_work['creator'] = redundant_work['Creator[637071]']\n",
    "        non_redundant_work['date'] = redundant_work['Date[637076]']\n",
    "        non_redundant_work['classification'] = redundant_work['Classification[637103]']\n",
    "        non_redundant_work['medium'] = redundant_work['Medium[637080]']\n",
    "        non_redundant_work['measurements'] = redundant_work['Measurements[637081]']\n",
    "        non_redundant_work['style_period'] = redundant_work['Style/Period[637079]']\n",
    "        non_redundant_work['country_culture'] = redundant_work['Country/Culture[637072]']\n",
    "        non_redundant_work['seals_inscriptions'] = redundant_work['Seals & Inscriptions[637104]']\n",
    "        non_redundant_work['signature'] = redundant_work['Signature[637105]']\n",
    "        non_redundant_work['description'] = redundant_work['Description[637092]']\n",
    "        non_redundant_work['publications'] = redundant_work['Publications[637106]']\n",
    "        non_redundant_work['exhibitions'] = redundant_work['Exhibitions[637107]']\n",
    "        non_redundant_work['gift_of'] = redundant_work['Gift of[637110]']\n",
    "        non_redundant_work['purchased_from'] = redundant_work['Purchased from[637111]']\n",
    "        non_redundant_work['credit_line'] = redundant_work['Credit Line[637112]']\n",
    "        non_redundant_work['provenance'] = redundant_work['Provenance[637113]']\n",
    "        non_redundant_work['collection'] = redundant_work['Collection[637114]']\n",
    "        non_redundant_work['notes'] = redundant_work['Notes[637116]']\n",
    "        non_redundant_work['rights'] = redundant_work['Rights[637099]']\n",
    "        # License and the Artstor fields are not populated\n",
    "        \n",
    "        # replace the list values in every loop\n",
    "        non_redundant_work['ssid'] = ssid\n",
    "        non_redundant_work['filename'] = filename\n",
    "        non_redundant_work['accession_number'] = accession_number\n",
    "        non_redundant_work['date_acquired'] = date_acquired\n",
    "        non_redundant_work['last_change'] = last_change\n",
    "        non_redundant_work['media_url'] = media_url\n",
    "        \n",
    "        # since this is the first time for this non-redundant work, add it to the list\n",
    "        non_redundant_works.append(non_redundant_work)\n",
    "        \n",
    "fieldnames = list(non_redundant_works[0].keys())\n",
    "write_dicts_to_csv(non_redundant_works, 'gallery_works_nr.csv', fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
