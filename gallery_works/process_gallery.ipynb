{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract  data\n",
    "\n",
    "This section simplifies the column headers and writes a copy of the data to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from time import sleep\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "\n",
    "# ----------------\n",
    "# Configuration settings\n",
    "# ----------------\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "# ----------------\n",
    "# Utility functions\n",
    "# ----------------\n",
    "\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderBot/1.6 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# find non-redundant values for a column\n",
    "def non_redundant(table, column_key):\n",
    "    non_redundant_list = []\n",
    "    for row in table:\n",
    "        found = False\n",
    "        for test_item in non_redundant_list:\n",
    "            if row[column_key] == test_item:\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            non_redundant_list.append(row[column_key])\n",
    "    return non_redundant_list\n",
    "\n",
    "# function to use in sort of simple list\n",
    "def sort_funct(row):\n",
    "    return row\n",
    "\n",
    "# function to use in sort last_first names\n",
    "def sort_last_first(row):\n",
    "    return row['last_first']\n",
    "\n",
    "# function to use in sort by match score\n",
    "def sort_score(row):\n",
    "    return row['score']\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# search label and alias\n",
    "# For whatever reason, if I use the graph pattern\n",
    "\n",
    "# wd:Q21 wdt:P31 ?class.\n",
    "\n",
    "# England is not Q6256 (country)\n",
    "# But if I use the graph pattern\n",
    "\n",
    "#   wd:Q21 p:P31 ?statement.\n",
    "#  ?statement ps:P31 ?class.\n",
    "\n",
    "# it is ??!!\n",
    "def searchLabelsAtWikidata(string, class_list):\n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id '\n",
    "    query += '''where {\n",
    "  {?id rdfs:label \"''' + string + '''\"@en.}\n",
    "  union\n",
    "  {?id skos:altLabel \"''' + string + '''\"@en.}\n",
    "  '''\n",
    "    for class_index in range(len(class_list)):\n",
    "        if class_index == 0:\n",
    "            query += '''{?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "        else:\n",
    "            query += '''union\n",
    "  {?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "    query += '''}'''\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    # r = requests.get(endpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_qnumber(result['id']['value'])\n",
    "        return_value.append(qid)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def retrieve_gallery_classes():\n",
    "    # create a string for the query\n",
    "    # use Metropolitan Museum of Art because there are too many collections to not specify the collection.\n",
    "    query = '''select distinct ?class ?label where \n",
    "      {\n",
    "      ?item wdt:P195 wd:Q160236.\n",
    "      ?item wdt:P31 ?class.\n",
    "      ?class rdfs:label ?label.\n",
    "      filter(lang(?label) = 'en')\n",
    "      }\n",
    "      order by ?label'''\n",
    "\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    print('sending query')\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    print('results returned')\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_qnumber(result['class']['value'])\n",
    "        label = result['label']['value']\n",
    "        return_value.append({'label': label, 'qid': qid})\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def generateNameAlternatives(name):\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods\n",
    "    name = name.replace('.', '')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "\n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        # make sure first character is alphabetic\n",
    "        # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "        # typical cases are like (Kit) or \"Kit\"\n",
    "        if not piece[0:1].isalpha():\n",
    "            piece = piece[1:len(piece)] # remove the first non-alphabetic character\n",
    "        if len(piece) > 0:\n",
    "            initials.append(piece[0:1])\n",
    "        \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += pieces[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # full name with suffix\n",
    "    if suffix != '':\n",
    "        nameVersion = ''\n",
    "        for pieceNumber in range(0, len(pieces)-1):\n",
    "            nameVersion += pieces[pieceNumber] + ' '\n",
    "        nameVersion += pieces[len(pieces)-1] + suffix\n",
    "        alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first and last name only\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial and last name only\n",
    "    nameVersion = initials[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    nameVersion = initials[0] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with last name\n",
    "    nameVersion = initials[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber]\n",
    "    nameVersion += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def searchNameAtWikidata(name):\n",
    "    nameList = generateNameAlternatives(name)\n",
    "    alternatives = ''\n",
    "    for alternative in nameList:\n",
    "        # get rid of quotes, which will break the query\n",
    "        alternative = alternative.replace('\"', '')\n",
    "        alternative = alternative.replace(\"'\", '')\n",
    "        alternatives += '\"' + alternative + '\"@en\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    results = []\n",
    "    # r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            if 'label' in statement:\n",
    "                name = statement['label']['value']\n",
    "            else:\n",
    "                name = ''\n",
    "            qNumber = vbc.extract_qnumber(wikidataIri)\n",
    "            results.append({'qId': qNumber, 'name': name})\n",
    "    except:\n",
    "        results = [{'error': r.text}]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return results\n",
    "\n",
    "def name_variant_testing(name, variant):\n",
    "    # get rid of periods\n",
    "    name = name.replace('.','')\n",
    "    variant = variant.replace('.','')\n",
    "    \n",
    "    # create first names\n",
    "    name_pieces = name.split(' ')\n",
    "    variant_pieces = variant.split(' ')\n",
    "    last_name = name_pieces[len(name_pieces)-1]\n",
    "    last_variant = variant_pieces[len(variant_pieces)-1]\n",
    "    if len(name_pieces) > 1:\n",
    "        first_names = name[0:-(len(last_name)+1)]\n",
    "    else:\n",
    "        first_names = name     \n",
    "    if len(variant_pieces) > 1:\n",
    "        first_variants = variant[0:-(len(last_variant)+1)]\n",
    "    else:\n",
    "        first_variants = variant      \n",
    "    #print(first_names)\n",
    "    #print(first_variants)\n",
    "    \n",
    "    # compare first names\n",
    "    # I experimented with the different ratios and I think fuzz might be best.\n",
    "    ratio = fuzz.ratio(first_names, first_variants)\n",
    "    #partial_ratio = fuzz.partial_ratio(first_names, first_variants)\n",
    "    #sort_ratio = fuzz.token_sort_ratio(first_names, first_variants)\n",
    "    #set_ratio = fuzz.token_set_ratio(first_names, first_variants)\n",
    "    # print('name similarity ratio', ratio)\n",
    "    #print('partial ratio', partial_ratio)\n",
    "    #print('sort_ratio', sort_ratio)\n",
    "    #print('set_ratio', set_ratio)\n",
    "\n",
    "    return(ratio)\n",
    "\n",
    "def find_surname_givens(name):\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', '')\n",
    "    name = name.replace(',', '')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "def remove_parens(string):\n",
    "    name_string = string.split('(')[0]\n",
    "    return name_string.strip()\n",
    "\n",
    "def remove_description(string):\n",
    "    try:\n",
    "        right_string = string.split('(')[1]\n",
    "        left_string = right_string.split(')')[0]\n",
    "        result = left_string.strip()\n",
    "    except:\n",
    "        result = ''\n",
    "    return result\n",
    "\n",
    "def reverse_names(string):\n",
    "    pieces = string.split(',')\n",
    "    return pieces[1].strip() + ' ' + pieces[0].strip()\n",
    "\n",
    "# Screens for Wikidata items that are potential matches\n",
    "\n",
    "import vb_common_code as vbc\n",
    "retrieve_class_list_query = vbc.Query(pid='P31', uselabel=False, sleep=sparql_sleep)\n",
    "retrieve_birth_date_query = vbc.Query(isitem=False, pid='P569', sleep=sparql_sleep)\n",
    "retrieve_death_date_query = vbc.Query(isitem=False, pid='P570', sleep=sparql_sleep)\n",
    "\n",
    "def human(qId):\n",
    "    screen = True\n",
    "    wdClassList = retrieve_class_list_query.single_property_values_for_item(qId)\n",
    "    # if there is a class property, check if it's a human\n",
    "    if len(wdClassList) != 0:\n",
    "        # if it's not a human\n",
    "        if wdClassList[0] != 'Q5':\n",
    "            #print('*** This item is not a human!')\n",
    "            screen = False\n",
    "    return screen\n",
    "\n",
    "# returns a dictionary of various descriptors of the item with Wikidata ID qId\n",
    "# P106 is occupation, schema:description is filtered to be the English description\n",
    "def searchWikidataDescription(qId):\n",
    "    resultsDict = {}\n",
    "    query = '''select distinct ?description ?orcid ?occupation where {\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' schema:description ?description.\n",
    "            FILTER(lang(?description) = 'en')\n",
    "            }\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = 'en')            \n",
    "            }\n",
    "        optional {wd:'''+ qId + ''' wdt:P496 ?orcid.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the dictionary remains empty\n",
    "            # Only a single description per language is allowed, so there should only be one description\n",
    "            if 'description' in statements[0]:\n",
    "                description = statements[0]['description']['value']\n",
    "            else:\n",
    "                description = ''\n",
    "            resultsDict['description'] = description\n",
    "            \n",
    "            # Only a single ORCID is allowed, so there should only be one orcid value\n",
    "            if 'orcid' in statements[0]:\n",
    "                orcid = statements[0]['orcid']['value']\n",
    "            else:\n",
    "                orcid = ''\n",
    "            resultsDict['orcid'] = orcid\n",
    "            \n",
    "            # if there are multiple statements, that's because there are more than one occupation\n",
    "            occupationList = []\n",
    "            for statement in statements:\n",
    "                if 'occupation' in statement:\n",
    "                    occupationList.append(statement['occupation']['value'])\n",
    "            resultsDict['occupation'] = occupationList\n",
    "    except:\n",
    "        resultsDict = {'error': r.text}\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return resultsDict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'gallery_works.csv'\n",
    "old_works = read_dict(filename)\n",
    "fieldnames = list(old_works[0].keys())\n",
    "print(fieldnames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "works = []\n",
    "for old_work in old_works:\n",
    "    work = {}\n",
    "    work['ssid'] = old_work['SSID']\n",
    "    work['filename'] = old_work['Filename']\n",
    "    work['title'] = old_work['Title[637073]'] \n",
    "    work['creator'] = old_work['Creator[637071]']\n",
    "    work['date'] = old_work['Date[637076]']\n",
    "    work['classification'] = old_work['Classification[637103]']\n",
    "    work['medium'] = old_work['Medium[637080]']\n",
    "    work['measurements'] = old_work['Measurements[637081]']\n",
    "    work['style_period'] = old_work['Style/Period[637079]']\n",
    "    work['country_culture'] = old_work['Country/Culture[637072]']\n",
    "    work['seals_inscriptions'] = old_work['Seals & Inscriptions[637104]']\n",
    "    work['signature'] = old_work['Signature[637105]']\n",
    "    work['description'] = old_work['Description[637092]']\n",
    "    work['publications'] = old_work['Publications[637106]']\n",
    "    work['exhibitions'] = old_work['Exhibitions[637107]']\n",
    "    work['accession_number'] = old_work['Accession Number[637085]']\n",
    "    work['date_acquired'] = old_work['Date Acquired[637109]']\n",
    "    work['gift_of'] = old_work['Gift of[637110]']\n",
    "    work['purchased_from'] = old_work['Purchased from[637111]']\n",
    "    work['credit_line'] = old_work['Credit Line[637112]']\n",
    "    work['provenance'] = old_work['Provenance[637113]']\n",
    "    work['collection'] = old_work['Collection[637114]']\n",
    "    work['last_change'] = old_work['Last Change[637115]']\n",
    "    work['notes'] = old_work['Notes[637116]']\n",
    "    work['rights'] = old_work['Rights[637099]']\n",
    "    work['media_url'] = old_work['Media URL']\n",
    "    # License and the Artstor fields are not populated\n",
    "        \n",
    "    works.append(work)\n",
    "        \n",
    "fieldnames = list(works[0].keys())\n",
    "write_dicts_to_csv(works, 'gallery_works_renamed.csv', fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate country table\n",
    "\n",
    "Match labels in the country_culture column with labels and aliases in Wikidata for various country-like items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = non_redundant(works, 'country_culture')\n",
    "values.sort(key = sort_funct)\n",
    "if values[0] == '':\n",
    "    values.remove('')\n",
    "print(values)\n",
    "\n",
    "mappings = []\n",
    "for value in values:\n",
    "    result_list = searchLabelsAtWikidata(value, ['Q6256','Q3624078','Q6266'])\n",
    "    print('|' + value + '|', result_list)\n",
    "    if len(result_list) == 1:\n",
    "        qid = result_list[0]\n",
    "    elif len(result_list) > 1:\n",
    "        qid = result_list\n",
    "    else:\n",
    "        qid = ''\n",
    "    mappings.append({'string': value, 'qid': qid})\n",
    "write_dicts_to_csv(mappings, 'country_mappings.csv', ['string', 'qid'])\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate classes\n",
    "\n",
    "This section first gets all of the classes (values for P31 instanceOf) from the Met's collection. Then it tries to match the labels of those items to the values in the classification column of the gallery data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all classes in the Met\n",
    "work_classes = retrieve_gallery_classes()\n",
    "print(work_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = non_redundant(works, 'classification')\n",
    "values.sort(key = sort_funct)\n",
    "if values[0] == '':\n",
    "    values.remove('')\n",
    "print(values)\n",
    "\n",
    "mappings = []\n",
    "for value in values:\n",
    "    found = False\n",
    "    for work_class in work_classes:\n",
    "        if value.lower() == work_class['label'].lower():\n",
    "            found = True\n",
    "            mappings.append({'string': value, 'qid': work_class['qid'], 'label': work_class['label']})\n",
    "    if not found:\n",
    "        mappings.append({'string': value, 'qid': '', 'label': ''})\n",
    "write_dicts_to_csv(mappings, 'classification_mappings.csv', ['string', 'qid', 'label'])\n",
    "print()\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse dimensions data\n",
    "\n",
    "This processes the values of the measurements column and separates them into length and width or length/width/height quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for work_index in range(len(works)):\n",
    "#for work_index in range(0,1):\n",
    "    string = works[work_index]['measurements'].strip()\n",
    "    if string == '':\n",
    "        # no value; set all variables to empty string\n",
    "        height = ''\n",
    "        width = ''\n",
    "        depth = ''\n",
    "        circum = ''\n",
    "    elif ' x ' not in string:\n",
    "        # one dimensional or improperly formatted\n",
    "        pieces = string.split(' ')\n",
    "        try:\n",
    "            value = float(pieces[0])\n",
    "            if pieces[1] != 'in.':\n",
    "                # second part of string not \"in.\"\n",
    "                print(works[work_index]['accession_number'], string)\n",
    "                height = ''\n",
    "                width = ''\n",
    "                depth = ''\n",
    "                circum = ''\n",
    "            else:\n",
    "                height = ''\n",
    "                width = ''\n",
    "                depth = ''\n",
    "                circum = str(value)\n",
    "        except:\n",
    "            # improperly formatted            \n",
    "            # set all variables to empty string\n",
    "            print(works[work_index]['accession_number'], string)\n",
    "            height = ''\n",
    "            width = ''\n",
    "            depth = ''\n",
    "            circum = ''\n",
    "    else:\n",
    "        # the string has an x in it, so it's multidimensional\n",
    "        pieces = string.split('x')\n",
    "        # split the string and get rid of leading and trailing whitespace\n",
    "        for piece_index in range(len(pieces)):\n",
    "            pieces[piece_index] = pieces[piece_index].strip()\n",
    "        # remove the \"in.\" and any spaces from the last piece\n",
    "        pieces[len(pieces)-1] = pieces[len(pieces)-1].split('in')[0].strip()\n",
    "        if len(pieces) == 2:\n",
    "            # two-dimensional work\n",
    "            try:\n",
    "                height = str(float(pieces[0]))\n",
    "                width = str(float(pieces[1]))\n",
    "                depth = ''\n",
    "                circum = ''\n",
    "            except:\n",
    "                print(works[work_index]['accession_number'], string)\n",
    "                height = ''\n",
    "                width = ''\n",
    "                depth = ''\n",
    "                circum = ''                \n",
    "        else:\n",
    "            # three-dimensional work\n",
    "            try:\n",
    "                height = str(float(pieces[0]))\n",
    "                width = str(float(pieces[1]))\n",
    "                depth = str(float(pieces[2]))\n",
    "                circum = ''\n",
    "            except:\n",
    "                print(works[work_index]['accession_number'], string)\n",
    "                height = ''\n",
    "                width = ''\n",
    "                depth = ''\n",
    "                circum = ''\n",
    "    works[work_index]['height'] = height\n",
    "    works[work_index]['width'] = width\n",
    "    works[work_index]['depth'] = depth\n",
    "    works[work_index]['circum'] = circum\n",
    "\n",
    "fieldnames = list(works[0].keys())\n",
    "write_dicts_to_csv(works, 'gallery_works_with_dim.csv', fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse inception dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'gallery_works_with_dim.csv'\n",
    "works = read_dict(filename)\n",
    "fieldnames = list(works[0].keys())\n",
    "print(fieldnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_era(string):\n",
    "    # dates with no CE or BCE, including empty string\n",
    "    if 'CE' not in string:\n",
    "        value = string\n",
    "        era = 'unknown'      \n",
    "    else:\n",
    "        if 'BCE' in string:\n",
    "            value = string[0:len(string)-3].strip()\n",
    "            era = 'BCE'\n",
    "        else: # string ends with CE\n",
    "            value = string[0:len(string)-2].strip()\n",
    "            era = 'CE'\n",
    "    return value, era\n",
    "\n",
    "for work_index in range(len(works)):\n",
    "    string = works[work_index]['date']\n",
    "    \n",
    "    # handle ideosyncratic date values\n",
    "    if string == 'not dated':\n",
    "        string = ''\n",
    "    if string == 'Unknown':\n",
    "        string = ''\n",
    "    if string[0:5] == 'late ':\n",
    "        string = string[5:]\n",
    "    if string[0:4] == 'mid ':\n",
    "        string = string[4:]\n",
    "    # drop parenthetical comments after the dates\n",
    "    if '(' in string:\n",
    "        pieces = string.split('(')\n",
    "        string = pieces[0].strip()\n",
    "    # fix bad century designation\n",
    "    if 'th CE' in string:\n",
    "        pieces = string.split('th CE')\n",
    "        string = pieces[0] + 'th century CE'\n",
    "        \n",
    "\n",
    "    # split dates\n",
    "    date_list = ['', '', ''] # 0 is single date, 1 is beginning of range, 2 is end of range\n",
    "    if '-' in string:\n",
    "        date_list[0] = ''\n",
    "        date_list[1] = string.split('-')[0].strip()\n",
    "        date_list[2] = string.split('-')[1].strip()\n",
    "    elif 'to' in string:\n",
    "        date_list[0] = ''\n",
    "        date_list[1] = string.split('to')[0].strip()\n",
    "        date_list[2] = string.split('to')[1].strip()\n",
    "        # handle special case of \"late ... to early ...\"\n",
    "        if date_list[2][0:6] == 'early ':\n",
    "            date_list[2] = date_list[2][6:]\n",
    "    else:\n",
    "        date_list[0] = string.strip()\n",
    "        date_list[1] = ''\n",
    "        date_list[2] = ''\n",
    "\n",
    "    # extract CE and BCE\n",
    "    for date_index in range(len(date_list)):\n",
    "        date_dict = {}\n",
    "        date_dict['value'], date_dict['era'] = determine_era(date_list[date_index])\n",
    "        date_list[date_index] = date_dict\n",
    "    \n",
    "    # If last date in range has a designation and the first one doesn't, assign it to the first date.\n",
    "    if date_list[1]['value'] != '' and date_list[1]['era'] == 'unknown' and date_list[2]['era'] != 'unknown':\n",
    "        date_list[1]['era'] = date_list[2]['era']\n",
    "        \n",
    "    # For dates with no specified era, assign CE\n",
    "    for date_index in range(len(date_list)):\n",
    "        if date_list[date_index]['value'] != '' and date_list[date_index]['era'] == 'unknown':\n",
    "            date_list[date_index]['era'] = 'CE'\n",
    "    \n",
    "    # Create a date dict to hold more information about the date format\n",
    "    date_dict = {'dates': date_list}\n",
    "    \n",
    "    # Determine if date is circa\n",
    "    date_dict['circa'] = False\n",
    "    for date_index in range(len(date_dict['dates'])):\n",
    "        if date_list[date_index]['value'][0:3] == 'ca.':\n",
    "            date_dict['circa'] = True\n",
    "            date_dict['dates'][date_index]['value'] = date_dict['dates'][date_index]['value'][3:].strip()\n",
    "    \n",
    "    # Determine if values are centuries\n",
    "    date_dict['century'] = False\n",
    "    for date_index in range(len(date_dict['dates'])):\n",
    "        if date_dict['dates'][date_index]['value'][-7:] == 'century':\n",
    "            date_dict['century'] = True\n",
    "            date_dict['dates'][date_index]['value'] = date_dict['dates'][date_index]['value'][:-7].strip()\n",
    "    if date_dict['century']: # if determined to be century values, strip off the \"th\"\n",
    "        for date_index in range(len(date_dict['dates'])):\n",
    "            date_dict['dates'][date_index]['value'] = date_dict['dates'][date_index]['value'][:-2]\n",
    "    # append date dict to works\n",
    "    works[work_index]['inception'] = date_dict\n",
    "\n",
    "# check for bad dates\n",
    "print('Dates with problems that need to be fixed manually')\n",
    "for work_index in range(len(works)):\n",
    "    for date in works[work_index]['inception']['dates']:\n",
    "        if date['value'] != '':\n",
    "            try:\n",
    "                junk = int(date['value'])\n",
    "            except:\n",
    "                print(work_index, works[work_index]['date'])\n",
    "    # check for two-digit second numbers in ranges\n",
    "    if works[work_index]['inception']['dates'][1]['value'] != '' and works[work_index]['inception']['dates'][1]['era'] == 'CE':\n",
    "        if int(works[work_index]['inception']['dates'][1]['value']) > int(works[work_index]['inception']['dates'][2]['value']):\n",
    "            print(work_index, works[work_index]['date'])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_table = []\n",
    "for work in works:\n",
    "    out_dict = {}\n",
    "    out_dict['string'] = work['date']\n",
    "    out_dict['singe_date'] = work['inception']['dates'][0]['value']\n",
    "    out_dict['singe_era'] = work['inception']['dates'][0]['era']\n",
    "    out_dict['first_date'] = work['inception']['dates'][1]['value']\n",
    "    out_dict['first_era'] = work['inception']['dates'][1]['era']\n",
    "    out_dict['second_date'] = work['inception']['dates'][2]['value']\n",
    "    out_dict['second_era'] = work['inception']['dates'][2]['era']\n",
    "    out_dict['circa'] = work['inception']['circa']\n",
    "    out_dict['century'] = work['inception']['century']\n",
    "    out_table.append(out_dict)\n",
    "\n",
    "fieldnames = list(out_table[0].keys())\n",
    "write_dicts_to_csv(out_table, 'test_dates.csv', fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disambiguate creators\n",
    "\n",
    "The following functions are names-related ones from vb3_match_wikidata.py\n",
    "\n",
    "The following cell creates a non-redundant list of processed names by extracting them from the creator field, then reversing them to given name first.\n",
    "\n",
    "Note: don't run this again when there is a creators.csv file already because it will overwrite any data that have been processed by the next script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator_strings = non_redundant(works, 'creator')\n",
    "print(len(creator_strings))creator_data = []\n",
    "for creator_string in creator_strings:\n",
    "    creator_datum = {}\n",
    "    creator_datum['last_first'] = remove_parens(creator_string)\n",
    "    creator_datum['description'] = remove_description(creator_string)\n",
    "    if ',' in creator_datum['last_first']:\n",
    "        creator_datum['name'] = reverse_names(creator_datum['last_first'])\n",
    "    else:\n",
    "        creator_datum['name'] = creator_datum['last_first']\n",
    "    creator_datum['creator_string'] = json.dumps([creator_string], ensure_ascii=False)\n",
    "    creator_data.append(creator_datum)\n",
    "    \n",
    "creator_data.sort(key = sort_last_first)\n",
    "\n",
    "fieldnames = list(creator_data[0].keys())\n",
    "write_dicts_to_csv(creator_data, 'creators.csv', fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script isn't being maintained any more because there is a stand-alone file, `screen_creators.py`, which is run from the command line. It has edits that aren't found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator_data = read_dict('creators.csv')\n",
    "fieldnames = list(creator_data[0].keys())\n",
    "\n",
    "for creator_index in range(len(creator_data)):\n",
    "# for creator_index in range(22,23):\n",
    "    if creator_data[creator_index]['searched'] == '':\n",
    "        match = False\n",
    "        print(creator_data[creator_index]['name'])\n",
    "        print(creator_data[creator_index]['description'])\n",
    "        print()\n",
    "        results = searchNameAtWikidata(creator_data[creator_index]['name'])\n",
    "        if len(results) == 0:\n",
    "            print('No results')\n",
    "            print()\n",
    "            creator_data[creator_index]['matches'] = 'no'\n",
    "        else:\n",
    "            creator_data[creator_index]['matches'] = 'yes'\n",
    "            display_strings = []\n",
    "            for result_index in range(len(results)):\n",
    "                if human(results[result_index]['qId']):\n",
    "                    wikidata_descriptions = searchWikidataDescription(results[result_index]['qId'])\n",
    "                    description = wikidata_descriptions['description']\n",
    "                    if description[0:18] != 'Peerage person ID=':\n",
    "                        \n",
    "                        birthDateList = retrieve_birth_date_query.single_property_values_for_item(results[result_index]['qId'])\n",
    "                        if len(birthDateList) >= 1:\n",
    "                            birth_date = birthDateList[0][0:4]\n",
    "                        else:\n",
    "                            birth_date = ''\n",
    "                        \n",
    "                        deathDateList = retrieve_death_date_query.single_property_values_for_item(results[result_index]['qId'])\n",
    "                        if len(deathDateList) >= 1:\n",
    "                            death_date = deathDateList[0][0:4]\n",
    "                        else:\n",
    "                            death_date = ''\n",
    "                        \n",
    "                        if death_date != '' and birth_date != '':\n",
    "                            dates = birth_date + '-' + death_date\n",
    "                        elif death_date == '' and birth_date != '':\n",
    "                            dates = 'born ' + birth_date\n",
    "                        elif death_date != '' and birth_date == '':\n",
    "                            dates = 'died ' + death_date\n",
    "                        else:\n",
    "                            dates = ''\n",
    "                            \n",
    "                        similarity_score = name_variant_testing(creator_data[creator_index]['name'], results[result_index]['name'])\n",
    "                        # if there is an exact dates match and high name similarity, just assign Q ID\n",
    "                        if dates != '' and dates in creator_data[creator_index]['description'] and int(similarity_score) > 95:\n",
    "                            match = True\n",
    "                            creator_data[creator_index]['qid'] = results[result_index]['qId']\n",
    "                            print('Auto match with', results[result_index]['name'], dates, 'https://www.wikidata.org/wiki/' + results[result_index]['qId'])\n",
    "                            break # kill the results loop\n",
    "                        else:\n",
    "                            occupation = wikidata_descriptions['occupation']\n",
    "                            result_name = results[result_index]['name']\n",
    "                            result_qid = results[result_index]['qId']\n",
    "                            display_strings.append({'qid': result_qid, 'name': result_name, 'dates': dates, 'description': description, 'occupation': occupation, 'score': similarity_score})\n",
    "\n",
    "            if match:\n",
    "                pass\n",
    "            elif len(display_strings) == 0:\n",
    "                print('No results')\n",
    "                print()\n",
    "            else:\n",
    "                display_strings.sort(key = sort_score, reverse = True)\n",
    "                for index in range(len(display_strings)):\n",
    "                    print(index, display_strings[index]['score'], display_strings[index]['name'], 'https://www.wikidata.org/wiki/' + display_strings[index]['qid'])\n",
    "                    print(display_strings[index]['dates'])\n",
    "                    print('Description:', display_strings[index]['description'])\n",
    "                    print('Occupation:', display_strings[index]['occupation'])\n",
    "                    print()\n",
    "                match = input('number of match or Enter for no match')\n",
    "                if match != '':\n",
    "                    creator_data[creator_index]['qid'] = results[result_index]['qId']\n",
    "        creator_data[creator_index]['searched'] = 'yes'\n",
    "        write_dicts_to_csv(creator_data, 'creators.csv', fieldnames)\n",
    "        print()\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
