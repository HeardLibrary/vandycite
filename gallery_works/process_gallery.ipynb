{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine arts gallery data processing script\n",
    "\n",
    "The script starts with a dump from jstor. The Excel file must then be saved as a CSV. NOTE: don't open it without specifying that the accession number is text during the import dialog. Failure to do this will result in the loss of trailing zeros and item mismatches in the future. \n",
    "\n",
    "It is best to avoid opening the CSV files for manual editing and to just let the script do the work. The script reliably opens the file without corrupting the accession number.\n",
    "\n",
    "## Function section\n",
    "\n",
    "This needs to be run before any of the other cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process_gallery.ipynb (2020-12-01)\n",
    "# (c) 2020 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "\n",
    "from pathlib import Path\n",
    "import requests\n",
    "from time import sleep\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from copy import deepcopy\n",
    "from langdetect import detect\n",
    "from langdetect import detect_langs\n",
    "import datetime\n",
    "\n",
    "# ----------------\n",
    "# Configuration settings\n",
    "# ----------------\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "# Calculate the reference date retrieved value for all statements\n",
    "whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "dateZ = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "ref_retrieved = dateZ + 'T00:00:00Z' # form 2019-12-05T00:00:00Z as provided by Wikidata, without leading +\n",
    "\n",
    "# ----------------\n",
    "# Utility functions\n",
    "# ----------------\n",
    "\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderBot/1.6 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# find non-redundant values for a column or simple list\n",
    "def non_redundant(table, column_key):\n",
    "    non_redundant_list = []\n",
    "    for row in table:\n",
    "        found = False\n",
    "        for test_item in non_redundant_list:\n",
    "            if column_key == '':\n",
    "                if row == test_item:\n",
    "                    found = True\n",
    "                    break\n",
    "            else:\n",
    "                if row[column_key] == test_item:\n",
    "                    found = True\n",
    "                    break\n",
    "        if not found:\n",
    "            if column_key == '':\n",
    "                non_redundant_list.append(row)\n",
    "            else:\n",
    "                non_redundant_list.append(row[column_key])\n",
    "    return non_redundant_list\n",
    "\n",
    "# function to use in sort of simple list\n",
    "def sort_funct(row):\n",
    "    return row\n",
    "\n",
    "# function to use in sort last_first names\n",
    "def sort_last_first(row):\n",
    "    return row['last_first']\n",
    "\n",
    "# function to use in sort by match score\n",
    "def sort_score(row):\n",
    "    return row['score']\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# search label and alias\n",
    "# For whatever reason, if I use the graph pattern\n",
    "\n",
    "# wd:Q21 wdt:P31 ?class.\n",
    "\n",
    "# England is not Q6256 (country)\n",
    "# But if I use the graph pattern\n",
    "\n",
    "#   wd:Q21 p:P31 ?statement.\n",
    "#  ?statement ps:P31 ?class.\n",
    "\n",
    "# it is ??!!\n",
    "def searchLabelsAtWikidata(string, class_list):\n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id '\n",
    "    query += '''where {\n",
    "  {?id rdfs:label \"''' + string + '''\"@en.}\n",
    "  union\n",
    "  {?id skos:altLabel \"''' + string + '''\"@en.}\n",
    "  '''\n",
    "    for class_index in range(len(class_list)):\n",
    "        if class_index == 0:\n",
    "            query += '''{?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "        else:\n",
    "            query += '''union\n",
    "  {?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "    query += '''}'''\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    # r = requests.get(endpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_qnumber(result['id']['value'])\n",
    "        return_value.append(qid)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def retrieve_gallery_classes():\n",
    "    # create a string for the query\n",
    "    # use Metropolitan Museum of Art because there are too many collections to not specify the collection.\n",
    "    query = '''select distinct ?class ?label where \n",
    "      {\n",
    "      ?item wdt:P195 wd:Q160236.\n",
    "      ?item wdt:P31 ?class.\n",
    "      ?class rdfs:label ?label.\n",
    "      filter(lang(?label) = 'en')\n",
    "      }\n",
    "      order by ?label'''\n",
    "\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    print('sending query')\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    print('results returned')\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_qnumber(result['class']['value'])\n",
    "        label = result['label']['value']\n",
    "        return_value.append({'label': label, 'qid': qid})\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def generateNameAlternatives(name):\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods\n",
    "    name = name.replace('.', '')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "\n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        # make sure first character is alphabetic\n",
    "        # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "        # typical cases are like (Kit) or \"Kit\"\n",
    "        if not piece[0:1].isalpha():\n",
    "            piece = piece[1:len(piece)] # remove the first non-alphabetic character\n",
    "        if len(piece) > 0:\n",
    "            initials.append(piece[0:1])\n",
    "        \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += pieces[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # full name with suffix\n",
    "    if suffix != '':\n",
    "        nameVersion = ''\n",
    "        for pieceNumber in range(0, len(pieces)-1):\n",
    "            nameVersion += pieces[pieceNumber] + ' '\n",
    "        nameVersion += pieces[len(pieces)-1] + suffix\n",
    "        alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first and last name only\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial and last name only\n",
    "    nameVersion = initials[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    nameVersion = initials[0] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with last name\n",
    "    nameVersion = initials[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber]\n",
    "    nameVersion += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def searchNameAtWikidata(name):\n",
    "    nameList = generateNameAlternatives(name)\n",
    "    alternatives = ''\n",
    "    for alternative in nameList:\n",
    "        # get rid of quotes, which will break the query\n",
    "        alternative = alternative.replace('\"', '')\n",
    "        alternative = alternative.replace(\"'\", '')\n",
    "        alternatives += '\"' + alternative + '\"@en\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    results = []\n",
    "    # r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            if 'label' in statement:\n",
    "                name = statement['label']['value']\n",
    "            else:\n",
    "                name = ''\n",
    "            qNumber = vbc.extract_qnumber(wikidataIri)\n",
    "            results.append({'qId': qNumber, 'name': name})\n",
    "    except:\n",
    "        results = [{'error': r.text}]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return results\n",
    "\n",
    "def name_variant_testing(name, variant):\n",
    "    # get rid of periods\n",
    "    name = name.replace('.','')\n",
    "    variant = variant.replace('.','')\n",
    "    \n",
    "    # create first names\n",
    "    name_pieces = name.split(' ')\n",
    "    variant_pieces = variant.split(' ')\n",
    "    last_name = name_pieces[len(name_pieces)-1]\n",
    "    last_variant = variant_pieces[len(variant_pieces)-1]\n",
    "    if len(name_pieces) > 1:\n",
    "        first_names = name[0:-(len(last_name)+1)]\n",
    "    else:\n",
    "        first_names = name     \n",
    "    if len(variant_pieces) > 1:\n",
    "        first_variants = variant[0:-(len(last_variant)+1)]\n",
    "    else:\n",
    "        first_variants = variant      \n",
    "    #print(first_names)\n",
    "    #print(first_variants)\n",
    "    \n",
    "    # compare first names\n",
    "    # I experimented with the different ratios and I think fuzz might be best.\n",
    "    ratio = fuzz.ratio(first_names, first_variants)\n",
    "    #partial_ratio = fuzz.partial_ratio(first_names, first_variants)\n",
    "    #sort_ratio = fuzz.token_sort_ratio(first_names, first_variants)\n",
    "    #set_ratio = fuzz.token_set_ratio(first_names, first_variants)\n",
    "    # print('name similarity ratio', ratio)\n",
    "    #print('partial ratio', partial_ratio)\n",
    "    #print('sort_ratio', sort_ratio)\n",
    "    #print('set_ratio', set_ratio)\n",
    "\n",
    "    return(ratio)\n",
    "\n",
    "def find_surname_givens(name):\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', '')\n",
    "    name = name.replace(',', '')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "def remove_parens(string):\n",
    "    name_string = string.split('(')[0]\n",
    "    return name_string.strip()\n",
    "\n",
    "def remove_description(string):\n",
    "    try:\n",
    "        right_string = string.split('(')[1]\n",
    "        left_string = right_string.split(')')[0]\n",
    "        result = left_string.strip()\n",
    "    except:\n",
    "        result = ''\n",
    "    return result\n",
    "\n",
    "def reverse_names(string):\n",
    "    pieces = string.split(',')\n",
    "    return pieces[1].strip() + ' ' + pieces[0].strip()\n",
    "\n",
    "# Screens for Wikidata items that are potential matches\n",
    "\n",
    "import vb_common_code as vbc\n",
    "retrieve_class_list_query = vbc.Query(pid='P31', uselabel=False, sleep=sparql_sleep)\n",
    "retrieve_birth_date_query = vbc.Query(isitem=False, pid='P569', sleep=sparql_sleep)\n",
    "retrieve_death_date_query = vbc.Query(isitem=False, pid='P570', sleep=sparql_sleep)\n",
    "\n",
    "def human(qId):\n",
    "    screen = True\n",
    "    wdClassList = retrieve_class_list_query.single_property_values_for_item(qId)\n",
    "    # if there is a class property, check if it's a human\n",
    "    if len(wdClassList) != 0:\n",
    "        # if it's not a human\n",
    "        if wdClassList[0] != 'Q5':\n",
    "            #print('*** This item is not a human!')\n",
    "            screen = False\n",
    "    return screen\n",
    "\n",
    "# returns a dictionary of various descriptors of the item with Wikidata ID qId\n",
    "# P106 is occupation, schema:description is filtered to be the English description\n",
    "def searchWikidataDescription(qId):\n",
    "    resultsDict = {}\n",
    "    query = '''select distinct ?description ?orcid ?occupation where {\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' schema:description ?description.\n",
    "            FILTER(lang(?description) = 'en')\n",
    "            }\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = 'en')            \n",
    "            }\n",
    "        optional {wd:'''+ qId + ''' wdt:P496 ?orcid.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the dictionary remains empty\n",
    "            # Only a single description per language is allowed, so there should only be one description\n",
    "            if 'description' in statements[0]:\n",
    "                description = statements[0]['description']['value']\n",
    "            else:\n",
    "                description = ''\n",
    "            resultsDict['description'] = description\n",
    "            \n",
    "            # Only a single ORCID is allowed, so there should only be one orcid value\n",
    "            if 'orcid' in statements[0]:\n",
    "                orcid = statements[0]['orcid']['value']\n",
    "            else:\n",
    "                orcid = ''\n",
    "            resultsDict['orcid'] = orcid\n",
    "            \n",
    "            # if there are multiple statements, that's because there are more than one occupation\n",
    "            occupationList = []\n",
    "            for statement in statements:\n",
    "                if 'occupation' in statement:\n",
    "                    occupationList.append(statement['occupation']['value'])\n",
    "            resultsDict['occupation'] = occupationList\n",
    "    except:\n",
    "        resultsDict = {'error': r.text}\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return resultsDict\n",
    "\n",
    "def determine_era(string):\n",
    "    # dates with no CE or BCE, including empty string\n",
    "    if 'CE' not in string:\n",
    "        value = string\n",
    "        era = 'unknown'      \n",
    "    else:\n",
    "        if 'BCE' in string:\n",
    "            value = string[0:len(string)-3].strip()\n",
    "            era = 'BCE'\n",
    "        else: # string ends with CE\n",
    "            value = string[0:len(string)-2].strip()\n",
    "            era = 'CE'\n",
    "    return value, era\n",
    "\n",
    "def determine_zeros(date):\n",
    "    zero_count = 0\n",
    "    for char_number in range(len(date), 0, -1):\n",
    "        if date[char_number-1] == '0':\n",
    "            zero_count += 1\n",
    "        else:\n",
    "            return zero_count\n",
    "\n",
    "def pad_zeros_left(date_string):\n",
    "    length = len(date_string)\n",
    "    pad = 4-length\n",
    "    return '0' * pad + date_string\n",
    "\n",
    "def sign(era):\n",
    "    if era == 'BCE':\n",
    "        return '-'\n",
    "    elif era == 'CE':\n",
    "        return ''\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def detect_language(string):\n",
    "    try:\n",
    "        lang_list = detect_langs(string)\n",
    "        lang_string = str(lang_list[0])\n",
    "        confidence = float(lang_string[3:])\n",
    "        lang = lang_string[:2]\n",
    "    except: #exceptions occur when no info to decide, e.g. numbers\n",
    "        lang = 'zxx'\n",
    "        confidence = float(0)\n",
    "    return lang, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial file processing\n",
    "\n",
    "NOTE: When opening the files, be sure to pay attention to the file import dialog. That allows the accession number column to be imported as a string rather than as a number. Importing as a number causes trailing zeros to be dropped.\n",
    "\n",
    "This section simplifies the column headers and writes a copy of the data to a CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "filename = 'gallery_works0.csv'\n",
    "old_works = read_dict(filename)\n",
    "fieldnames = list(old_works[0].keys())\n",
    "\n",
    "# print(fieldnames)\n",
    "\n",
    "ssid_field_name = 'SSID'\n",
    "# need to compensate for Bit Order Mark (BOM) from first character of header.\n",
    "if fieldnames[0][0] == '\\ufeff':\n",
    "    ssid_field_name = '\\ufeff' + ssid_field_name\n",
    "works = []\n",
    "for old_work in old_works:\n",
    "    work = {}\n",
    "    work['ssid'] = old_work[ssid_field_name]\n",
    "    work['filename'] = old_work['Filename']\n",
    "    title = old_work['Title[637073]'].strip()\n",
    "    title = title.replace('\\n', ' ') # replace embedded hard returns with spaces.\n",
    "    work['title'] = title\n",
    "    creator = old_work['Creator[637071]'].strip()\n",
    "    creator = creator.replace('\\n', ' ') # replace embedded hard returns with spaces.\n",
    "    work['creator_string'] = creator\n",
    "    work['date'] = old_work['Date[637076]']\n",
    "    work['classification'] = old_work['Classification[637103]']\n",
    "    work['medium'] = old_work['Medium[637080]']\n",
    "    work['measurements'] = old_work['Measurements[637081]']\n",
    "    work['style_period'] = old_work['Style/Period[637079]']\n",
    "    work['country_culture'] = old_work['Country/Culture[637072]']\n",
    "    work['seals_inscriptions'] = old_work['Seals & Inscriptions[637104]'].strip()\n",
    "    work['signature'] = old_work['Signature[637105]']\n",
    "    work['description'] = old_work['Description[637092]']\n",
    "    work['publications'] = old_work['Publications[637106]']\n",
    "    work['exhibitions'] = old_work['Exhibitions[637107]']\n",
    "    work['accession_number'] = old_work['Accession Number[637085]'].strip()\n",
    "    work['date_acquired'] = old_work['Date Acquired[637109]']\n",
    "    work['gift_of'] = old_work['Gift of[637110]']\n",
    "    work['purchased_from'] = old_work['Purchased from[637111]']\n",
    "    work['credit_line'] = old_work['Credit Line[637112]']\n",
    "    work['provenance'] = old_work['Provenance[637113]']\n",
    "    work['collection'] = old_work['Collection[637114]']\n",
    "    work['last_change'] = old_work['Last Change[637115]']\n",
    "    work['notes'] = old_work['Notes[637116]']\n",
    "    work['rights'] = old_work['Rights[637099]']\n",
    "    work['media_url'] = old_work['Media URL']\n",
    "    # License and the Artstor fields are not populated\n",
    "        \n",
    "    works.append(work)\n",
    "        \n",
    "fieldnames = list(works[0].keys())\n",
    "write_dicts_to_csv(works, 'gallery_works_renamed1.csv', fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse dimensions data\n",
    "\n",
    "This processes the values of the measurements column and separates them into length and width or length/width/height quantities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015.076 various dimensions\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "for work_index in range(len(works)):\n",
    "#for work_index in range(0,1):\n",
    "    string = works[work_index]['measurements'].strip()\n",
    "    if string == '':\n",
    "        # no value; set all variables to empty string\n",
    "        height = ''\n",
    "        width = ''\n",
    "        depth = ''\n",
    "        diameter = ''\n",
    "    elif ' x ' not in string:\n",
    "        # one dimensional or improperly formatted\n",
    "        pieces = string.split(' ')\n",
    "        try:\n",
    "            value = float(pieces[0])\n",
    "            if pieces[1] != 'in.':\n",
    "                # second part of string not \"in.\"\n",
    "                print(works[work_index]['accession_number'], string)\n",
    "                height = ''\n",
    "                width = ''\n",
    "                depth = ''\n",
    "                diameter = ''\n",
    "            else:\n",
    "                height = ''\n",
    "                width = ''\n",
    "                depth = ''\n",
    "                diameter = str(value)\n",
    "        except:\n",
    "            # improperly formatted            \n",
    "            # set all variables to empty string\n",
    "            print(works[work_index]['accession_number'], string)\n",
    "            height = ''\n",
    "            width = ''\n",
    "            depth = ''\n",
    "            diameter = ''\n",
    "    else:\n",
    "        # the string has an x in it, so it's multidimensional\n",
    "        pieces = string.split('x')\n",
    "        # split the string and get rid of leading and trailing whitespace\n",
    "        for piece_index in range(len(pieces)):\n",
    "            pieces[piece_index] = pieces[piece_index].strip()\n",
    "        # remove the \"in.\" and any spaces from the last piece\n",
    "        pieces[len(pieces)-1] = pieces[len(pieces)-1].split('in')[0].strip()\n",
    "        if len(pieces) == 2:\n",
    "            # two-dimensional work\n",
    "            try:\n",
    "                height = str(float(pieces[0]))\n",
    "                width = str(float(pieces[1]))\n",
    "                depth = ''\n",
    "                diameter = ''\n",
    "            except:\n",
    "                print(works[work_index]['accession_number'], string)\n",
    "                height = ''\n",
    "                width = ''\n",
    "                depth = ''\n",
    "                diameter = ''                \n",
    "        else:\n",
    "            # three-dimensional work\n",
    "            try:\n",
    "                height = str(float(pieces[0]))\n",
    "                width = str(float(pieces[1]))\n",
    "                depth = str(float(pieces[2]))\n",
    "                diameter = ''\n",
    "            except:\n",
    "                print(works[work_index]['accession_number'], string)\n",
    "                height = ''\n",
    "                width = ''\n",
    "                depth = ''\n",
    "                diameter = ''\n",
    "    works[work_index]['height'] = height\n",
    "    works[work_index]['width'] = width\n",
    "    works[work_index]['depth'] = depth\n",
    "    works[work_index]['diameter'] = diameter\n",
    "\n",
    "fieldnames = list(works[0].keys())\n",
    "write_dicts_to_csv(works, 'gallery_works_with_dim2.csv', fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse inception dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates with problems that need to be fixed manually\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "filename = 'gallery_works_with_dim2.csv'\n",
    "works = read_dict(filename)\n",
    "# fieldnames = list(works[0].keys())\n",
    "# print(fieldnames)\n",
    "\n",
    "for work_index in range(len(works)):\n",
    "    string = works[work_index]['date']\n",
    "    \n",
    "    # handle ideosyncratic date values\n",
    "    if string == 'not dated':\n",
    "        string = ''\n",
    "    if string == 'Unknown':\n",
    "        string = ''\n",
    "    if string[0:5] == 'late ':\n",
    "        string = string[5:]\n",
    "    if string[0:4] == 'mid ':\n",
    "        string = string[4:]\n",
    "    # drop parenthetical comments after the dates\n",
    "    if '(' in string:\n",
    "        pieces = string.split('(')\n",
    "        string = pieces[0].strip()\n",
    "    # fix bad century designation\n",
    "    if 'th CE' in string:\n",
    "        pieces = string.split('th CE')\n",
    "        string = pieces[0] + 'th century CE'\n",
    "        \n",
    "\n",
    "    # split dates\n",
    "    date_list = ['', '', ''] # 0 is single date, 1 is beginning of range, 2 is end of range\n",
    "    if '-' in string:\n",
    "        date_list[0] = ''\n",
    "        date_list[1] = string.split('-')[0].strip()\n",
    "        date_list[2] = string.split('-')[1].strip()\n",
    "    elif 'to' in string:\n",
    "        date_list[0] = ''\n",
    "        date_list[1] = string.split('to')[0].strip()\n",
    "        date_list[2] = string.split('to')[1].strip()\n",
    "        # handle special case of \"late ... to early ...\"\n",
    "        if date_list[2][0:6] == 'early ':\n",
    "            date_list[2] = date_list[2][6:]\n",
    "    else:\n",
    "        date_list[0] = string.strip()\n",
    "        date_list[1] = ''\n",
    "        date_list[2] = ''\n",
    "\n",
    "    # extract CE and BCE\n",
    "    for date_index in range(len(date_list)):\n",
    "        date_dict = {}\n",
    "        date_dict['value'], date_dict['era'] = determine_era(date_list[date_index])\n",
    "        date_list[date_index] = date_dict\n",
    "    \n",
    "    # If last date in range has a designation and the first one doesn't, assign it to the first date.\n",
    "    if date_list[1]['value'] != '' and date_list[1]['era'] == 'unknown' and date_list[2]['era'] != 'unknown':\n",
    "        date_list[1]['era'] = date_list[2]['era']\n",
    "        \n",
    "    # For dates with no specified era, assign CE\n",
    "    for date_index in range(len(date_list)):\n",
    "        if date_list[date_index]['value'] != '' and date_list[date_index]['era'] == 'unknown':\n",
    "            date_list[date_index]['era'] = 'CE'\n",
    "    \n",
    "    # Create a date dict to hold more information about the date format\n",
    "    date_dict = {'dates': date_list}\n",
    "    \n",
    "    # Determine if date is circa\n",
    "    date_dict['circa'] = False\n",
    "    for date_index in range(len(date_dict['dates'])):\n",
    "        if date_list[date_index]['value'][0:3] == 'ca.':\n",
    "            date_dict['circa'] = True\n",
    "            date_dict['dates'][date_index]['value'] = date_dict['dates'][date_index]['value'][3:].strip()\n",
    "    \n",
    "    # Determine if values are centuries\n",
    "    date_dict['century'] = False\n",
    "    for date_index in range(len(date_dict['dates'])):\n",
    "        if date_dict['dates'][date_index]['value'][-7:] == 'century':\n",
    "            date_dict['century'] = True\n",
    "            date_dict['dates'][date_index]['value'] = date_dict['dates'][date_index]['value'][:-7].strip()\n",
    "    if date_dict['century']: # if determined to be century values, strip off the \"th\"\n",
    "        for date_index in range(len(date_dict['dates'])):\n",
    "            date_dict['dates'][date_index]['value'] = date_dict['dates'][date_index]['value'][:-2]\n",
    "    # append date dict to works\n",
    "    works[work_index]['inception'] = date_dict\n",
    "\n",
    "# check for bad dates\n",
    "print('Dates with problems that need to be fixed manually')\n",
    "for work_index in range(len(works)):\n",
    "    for date in works[work_index]['inception']['dates']:\n",
    "        if date['value'] != '':\n",
    "            try:\n",
    "                junk = int(date['value'])\n",
    "            except:\n",
    "                print(work_index, works[work_index]['date'])\n",
    "    # check for two-digit second numbers in ranges\n",
    "    if works[work_index]['inception']['dates'][1]['value'] != '' and works[work_index]['inception']['dates'][1]['era'] == 'CE':\n",
    "        if int(works[work_index]['inception']['dates'][1]['value']) > int(works[work_index]['inception']['dates'][2]['value']):\n",
    "            print(work_index, works[work_index]['accession_number'], works[work_index]['date'])\n",
    "        \n",
    "# Process dates into form needed by Wikidata\n",
    "for work_index in range(len(works)):\n",
    "#for work_index in range(325, 330):\n",
    "    if works[work_index]['inception']['dates'][1]['value'] != '': # cases with date ranges\n",
    "        # Average ranges\n",
    "        first = works[work_index]['inception']['dates'][1]['value']\n",
    "        first_era = works[work_index]['inception']['dates'][1]['era']\n",
    "        second = works[work_index]['inception']['dates'][2]['value']\n",
    "        second_era = works[work_index]['inception']['dates'][2]['era']\n",
    "        if works[work_index]['inception']['century']:\n",
    "            first = str(int(first) * 100)\n",
    "            second = str(int(second) * 100)\n",
    "        minimum_zeros = min(determine_zeros(first), determine_zeros(second))\n",
    "        factor = 10**minimum_zeros\n",
    "        average = (float(sign(first_era) + first) + float(sign(second_era) + second))/2\n",
    "        if minimum_zeros < 2:\n",
    "            # for years and decades, round to the nearest year\n",
    "            average = int(average +.5)\n",
    "            works[work_index]['inception_prec'] = '9' # precision to year\n",
    "        else:\n",
    "            if works[work_index]['inception']['century']: # date given in centuries\n",
    "                if int(second) - int(first) == 100:\n",
    "                    # if given as \"xth to (x+1)th century\" then use the year between\n",
    "                    average = (int(sign(first_era) + '1') * (int(first) - 100) + float(sign(second_era) + second))/2\n",
    "                    works[work_index]['inception_prec'] = '9' # precision to year\n",
    "                elif int(second) - int(first) == 200:\n",
    "                    # if given as \"xth to (x+2)th century\" then use the century x+1 between\n",
    "                    average = int(average/factor)*factor\n",
    "                    works[work_index]['inception_prec'] = '7' # precision to century\n",
    "                else:\n",
    "                    # for wider ranges, just give the average year\n",
    "                    average = (int(sign(first_era) + '1') * (int(first) - 100) + float(sign(second_era) + second))/2\n",
    "                    works[work_index]['inception_prec'] = '9' # precision to year\n",
    "            else: # date give in year range\n",
    "                if int(second) - int(first) == 100:\n",
    "                    # if given as \"x00 to (x+1)00\" then use the x+1 century. This is good for cases like \"1400-1500\"\n",
    "                    average = int(average/factor + 1)*factor\n",
    "                    works[work_index]['inception_prec'] = '7' # precision to century\n",
    "                else:\n",
    "                    # for ranges like \"x00-(x+2)00\" then use the year in the middle: (x+1)00\n",
    "                    works[work_index]['inception_prec'] = '9' # precision to year\n",
    "        # remove negative sign\n",
    "        average = int(average) # remove any decimals and trailing zeros from the number\n",
    "        if average < 0:\n",
    "            number_string = str(average)[1:]\n",
    "            sign_string = '-'\n",
    "        else: # positive dates aren't stored with signs, they are added by the upload script\n",
    "            number_string = str(average)\n",
    "            sign_string = ''\n",
    "        works[work_index]['inception_val'] = sign_string + pad_zeros_left(number_string) + '-01-01T00:00:00Z'\n",
    "        \n",
    "        # Now set the earliest and latest date values\n",
    "        if works[work_index]['inception']['century']: # date given in centuries\n",
    "            works[work_index]['earliest_date_val'] = sign(works[work_index]['inception']['dates'][1]['era']) + pad_zeros_left(works[work_index]['inception']['dates'][1]['value'] + '00') + '-01-01T00:00:00Z'\n",
    "            works[work_index]['earliest_date_prec'] = '7' # precision to century\n",
    "            works[work_index]['latest_date_val'] = sign(works[work_index]['inception']['dates'][2]['era']) + pad_zeros_left(works[work_index]['inception']['dates'][2]['value'] + '00') + '-01-01T00:00:00Z'\n",
    "            works[work_index]['latest_date_prec'] = '7' # precision to century\n",
    "        else: # date given in years\n",
    "            works[work_index]['earliest_date_val'] = sign(works[work_index]['inception']['dates'][1]['era']) + pad_zeros_left(works[work_index]['inception']['dates'][1]['value']) + '-01-01T00:00:00Z'\n",
    "            works[work_index]['earliest_date_prec'] = '9' # precision to year\n",
    "            works[work_index]['latest_date_val'] = sign(works[work_index]['inception']['dates'][2]['era']) + pad_zeros_left(works[work_index]['inception']['dates'][2]['value']) + '-01-01T00:00:00Z'\n",
    "            works[work_index]['latest_date_prec'] = '9' # precision to year\n",
    "    else: # cases without date ranges\n",
    "        if works[work_index]['inception']['dates'][0]['value'] =='':\n",
    "            works[work_index]['inception_val'] = ''\n",
    "            works[work_index]['inception_prec'] = ''\n",
    "        else:\n",
    "            if works[work_index]['inception']['century']: # date given in centuries\n",
    "                works[work_index]['inception_val'] = sign(works[work_index]['inception']['dates'][0]['era']) + pad_zeros_left(works[work_index]['inception']['dates'][0]['value'] + '00') + '-01-01T00:00:00Z'\n",
    "                works[work_index]['inception_prec'] = '7' # precision to century\n",
    "            else: # date given in years\n",
    "                works[work_index]['inception_val'] = sign(works[work_index]['inception']['dates'][0]['era']) + pad_zeros_left(works[work_index]['inception']['dates'][0]['value']) + '-01-01T00:00:00Z'\n",
    "                works[work_index]['inception_prec'] = '9' # precision to year\n",
    "        \n",
    "        works[work_index]['earliest_date_val'] = ''\n",
    "        works[work_index]['earliest_date_prec'] = ''\n",
    "        works[work_index]['latest_date_val'] = ''\n",
    "        works[work_index]['latest_date_prec'] = ''\n",
    "    \n",
    "    # add statement for sourcing circumstances qualifier P1480 when \"circa\" (Q5727902)\n",
    "    if works[work_index]['inception']['circa']:\n",
    "        works[work_index]['sourcing_circumstances'] = 'Q5727902'\n",
    "    else:\n",
    "        works[work_index]['sourcing_circumstances'] = ''\n",
    "    \n",
    "    '''  \n",
    "    print('date:', works[work_index]['date'])\n",
    "    print('inception:', works[work_index]['inception_val'], works[work_index]['inception_prec'])            \n",
    "    print('earliest date:', works[work_index]['earliest_date_val'], works[work_index]['earliest_date_prec'])\n",
    "    print('latest date:', works[work_index]['latest_date_val'], works[work_index]['latest_date_prec'])\n",
    "    print()\n",
    "    '''\n",
    "\n",
    "fieldnames = list(works[0].keys())\n",
    "write_dicts_to_csv(works, 'gallery_works_with_dates3.csv', fieldnames)\n",
    "\n",
    "'''\n",
    "# output test table for dates for testing, not needed since script is now working.\n",
    "out_table = []\n",
    "for work in works:\n",
    "    out_dict = {}\n",
    "    out_dict['string'] = work['date']\n",
    "    out_dict['singe_date'] = work['inception']['dates'][0]['value']\n",
    "    out_dict['singe_era'] = work['inception']['dates'][0]['era']\n",
    "    out_dict['first_date'] = work['inception']['dates'][1]['value']\n",
    "    out_dict['first_era'] = work['inception']['dates'][1]['era']\n",
    "    out_dict['second_date'] = work['inception']['dates'][2]['value']\n",
    "    out_dict['second_era'] = work['inception']['dates'][2]['era']\n",
    "    out_dict['circa'] = work['inception']['circa']\n",
    "    out_dict['century'] = work['inception']['century']\n",
    "    out_table.append(out_dict)\n",
    "\n",
    "fieldnames = list(out_table[0].keys())\n",
    "write_dicts_to_csv(out_table, 'test_dates.csv', fieldnames)\n",
    "'''\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up acquisition dates\n",
    "\n",
    "This script cleans up the date acquired field. These are mostly years, but there are a few non-year values. However, for old works, the date is when it was acquired by the Peabody gallery. So the actual values for VU should be taken from the accession number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "filename = 'gallery_works_with_dates3.csv'\n",
    "works = read_dict(filename)\n",
    "\n",
    "# This section was used to develop the cleanup routine with a non-redundant list\n",
    "'''\n",
    "for work_index in range(len(works)):\n",
    "    if works[work_index]['date_acquired'].strip() != '':\n",
    "        works[work_index]['date_acquired'] = works[work_index]['date_acquired'].strip()\n",
    "acquisition_dates = non_redundant(works, 'date_acquired')\n",
    "acquisition_dates.sort(key = sort_funct)\n",
    "'''\n",
    "\n",
    "for work_index in range(len(works)):\n",
    "    if works[work_index]['date_acquired'].strip() != '':\n",
    "        date = works[work_index]['date_acquired'].strip()\n",
    "    else:\n",
    "        date = ''\n",
    "\n",
    "    # remove commas\n",
    "    date = date.replace(',', '')\n",
    "    if '/' in date:\n",
    "        pieces = date.split('/')\n",
    "    else:\n",
    "        pieces = date.split(' ')\n",
    "    year = ''\n",
    "    circa = False\n",
    "    for piece in pieces:\n",
    "        if piece == 'ca.':\n",
    "            circa = True\n",
    "        try:\n",
    "            number = int(piece)\n",
    "            if number > 1000:\n",
    "                year = str(number)\n",
    "        except:\n",
    "            pass\n",
    "    if year != '':\n",
    "        works[work_index]['acquired_cleaned'] = year\n",
    "        \n",
    "    # adate['circa'] = circa # don't really know what to do with circa, not an appropriae qualifier for collection\n",
    "    year = works[work_index]['accession_number'][0:4]\n",
    "\n",
    "    if works[work_index]['accession_number'] != '':\n",
    "        year = works[work_index]['accession_number'][0:4]\n",
    "        works[work_index]['collection_start_time_val'] = year + '-01-01T00:00:00Z'\n",
    "        works[work_index]['collection_start_time_prec'] = '9'\n",
    "    else:\n",
    "        works[work_index]['collection_start_time_val'] = ''\n",
    "        works[work_index]['collection_start_time_prec'] = ''\n",
    "        \n",
    "fieldnames = list(works[0].keys())\n",
    "write_dicts_to_csv(works, 'gallery_works_acquisition4.csv', fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build output file for Wikidata upload\n",
    "\n",
    "Note: the relationship between `gallery_works_to_write.csv` and `works_multiprop.csv` is a bit confusing. `gallery_works_to_write.csv` is generated by the script. `works_multiprop.csv` is the file that actually is used for the upload. So `gallery_works_to_write.csv` has to be copied to `works_multiprop.csv` prior to writing or nothing will happen when you attempt to upload. The reconciliation process described after this doesn't exactly make sense unless this copy process happens and `works_multiprop.csv` gets copied back to `gallery_works_to_write.csv` after the upload.\n",
    "\n",
    "The data here needs to be reconciled against data already downloaded from Wikidata using another script (`acquire_wikidata_metadata.py` in the VanderBot directory of the linked-data repo). Those downloaded data are in the `works_multiprop.csv` file. Prior to running that script, take the previous `gallery_works_to_write.csv` file and copy it to `works_multiprop.csv` then run the script. It should update the previous data. In particular, it will get the actual hashes for the value node values. You can look at the diff to see what if anything has changed. The data generated here will be added to that file.\n",
    "\n",
    "**Important note:** If there is a single existing value for a property and that value has been changed to another single value by someone else, the download script will replace that value without comment. That is why either looking at the diff or running the script that does the federated query to detect differences is important. Once the `works_multiprop.csv` file has been changed, there will be no way to know that the data have changed without rummaging around in the version history on GitHub.\n",
    "\n",
    "**Note about country vs. country of origin:** These two properties seem to be used about equally often in the wild. For that reason, we have been writing both. However, there is an important difference in behavior. If the country is England, Wales, Scotland, etc. there is a bot that changes the item to Great Britain while leaving the references the same. See https://www.wikidata.org/wiki/Q104031981 for example. There is no similar change to country of origin. I suppose this implies that \"country\" is the current name of the country, while country of origin could be a historical country, although I don't think this distinction is made anywhere in the documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "There are 261 potential orientation errors\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# open source files\n",
    "\n",
    "filename = 'works_multiprop.csv'\n",
    "items = read_dict(filename)\n",
    "fieldnames = list(items[0].keys())\n",
    "# print(fieldnames)\n",
    "\n",
    "data_columns = []\n",
    "for fieldname in fieldnames:\n",
    "    if '_ref1_referenceUrl' in fieldname:\n",
    "        data_column = {}\n",
    "        data_column['prefix'] = fieldname[:len(fieldname)-18]\n",
    "        if data_column['prefix'] in fieldnames:\n",
    "            data_column['name'] = data_column['prefix']\n",
    "        else: # value node values don't have a column name that is the prefix\n",
    "            data_column['name'] = data_column['prefix'] + '_val'\n",
    "        data_columns.append(data_column)\n",
    "#print(data_columns)\n",
    "\n",
    "filename = 'gallery_works_acquisition4.csv'\n",
    "works = read_dict(filename)\n",
    "\n",
    "# fieldnames = list(works[0].keys())\n",
    "# print(fieldnames)\n",
    "\n",
    "filename = 'creators.csv'\n",
    "creators_raw = read_dict(filename)\n",
    "creators = []\n",
    "for creator in creators_raw:\n",
    "    try:\n",
    "        strings = json.loads(creator['creator_string'])\n",
    "    except:\n",
    "        print(creator)\n",
    "        exit()\n",
    "    for string in strings:\n",
    "        creators.append({'qid': creator['qid'], 'name': creator['name'], 'string': string})\n",
    "\n",
    "filename = 'classification_mappings.csv'\n",
    "classifications = read_dict(filename)\n",
    "\n",
    "filename = 'country_mappings.csv'\n",
    "countries = read_dict(filename)\n",
    "\n",
    "filename = 'creator-multi.csv'\n",
    "creator2 = read_dict(filename)\n",
    "\n",
    "images = read_dict('image_dimensions.csv')\n",
    "\n",
    "\n",
    "# set up error logs\n",
    "missing_creators = []\n",
    "missing_qids = []\n",
    "missing_classifications = []\n",
    "missing_countries = []\n",
    "\n",
    "output = deepcopy(items)\n",
    "\n",
    "count = 0\n",
    "orientation_error_count = 0\n",
    "\n",
    "for work in works:\n",
    "    if count % 1000 == 0:\n",
    "        print(count)\n",
    "    dic = {}\n",
    "    \n",
    "    # Screening section. Several screens are applied to suppress a record from being written\n",
    "    suppressed = False\n",
    "    \n",
    "    # Prevent existing items from being modified by data from Artstor\n",
    "    found = False\n",
    "    for item in items:\n",
    "        if item['inventory_number'] == work['accession_number']:\n",
    "            found = True\n",
    "            break\n",
    "    # If the accession number of the work matches an existing item, at this point we will just skip it.\n",
    "    # At some point in the future, we would want to check for corrected or updated information.\n",
    "    if found:\n",
    "        suppressed = True\n",
    "    \n",
    "    dic['inventory_number'] = work['accession_number']\n",
    "\n",
    "    # For now, suppress all works with missing creators\n",
    "    found = False\n",
    "    for creator in creators:\n",
    "        if work['creator_string'] == creator['string']:\n",
    "            artist_name = creator['name']\n",
    "            dic['creator'] = creator['qid']\n",
    "            found = True\n",
    "            break\n",
    "    if not found:\n",
    "        suppressed = True\n",
    "        missing_creators.append({'inventory_number': \"#\" + dic['inventory_number'], 'string': json.dumps([work['creator_string']])})\n",
    "        artist_name = work['creator_string']\n",
    "        dic['creator'] = ''\n",
    "    \n",
    "    if creator['qid'] == '':\n",
    "        #suppressed = True\n",
    "        missing_qids.append({'inventory_number': \"#\" + dic['inventory_number'], 'string': artist_name})\n",
    "    \n",
    "    # Suppress works with labels that are too long\n",
    "    if len(work['title']) > 150:\n",
    "        suppressed = True\n",
    "    \n",
    "    # Supress works that aren't classified with a genre since they won't have instance_of\n",
    "    if work['classification'] == '':\n",
    "        suppressed = True\n",
    "\n",
    "    # Suppress works where the dimensions may be reversed (longest x shortest instead of height x width)\n",
    "    if work['accession_number'] != '': # there's only one case of this\n",
    "        if work['height'] != '': # skip the check if measurements not available\n",
    "            if work['depth'] == '': # don't check 3 dimensional objects\n",
    "                kind = work['classification'].lower()\n",
    "                # only check 2-D type works\n",
    "                if 'photograph' in kind or 'print' in kind or 'painting' in kind or 'drawing' in kind or 'graphic' in kind or 'book' in kind:\n",
    "                    # we only need to check portrait aspect images\n",
    "                    if float(work['height']) > float(work['width']):\n",
    "                        for image in images:\n",
    "                            if work['accession_number'] == image['accession']:\n",
    "                                if float(image['height']) <= float(image['width']):\n",
    "                                    orientation_error_count += 1\n",
    "                                    suppressed = True\n",
    "                                    #print(work['accession_number'], work['height'], work['width'], work['classification'])\n",
    "                                    #print('https://library.artstor.org/#/asset/' + work['ssid'])\n",
    "\n",
    "    # For now, suppress all works with \"diameters\" because a lot of them are lengths\n",
    "    if work['diameter'] != '':\n",
    "        suppressed = True\n",
    "        \n",
    "    # Generate the lines for all non-suppressed works\n",
    "    if not suppressed:\n",
    "        dic['inventory_number_collection'] = 'Q18563658' # Fine Arts Gallery\n",
    "        dic['label_en'] = work['title']\n",
    "        \n",
    "        # title\n",
    "        # The title column is hard-coded as English, so suspected non-English titles \n",
    "        # should be suppressed\n",
    "        lang, prec = detect_language(work['title'])\n",
    "        if lang == 'en' and prec > 0.99:\n",
    "            dic['title'] = work['title']\n",
    "        else:\n",
    "            dic['title'] = ''\n",
    "        dic['title_subtitle'] = ''\n",
    "        \n",
    "        # find second creator if there is one; only used for the description. 2nd creator item will be added\n",
    "        # as part of a second table\n",
    "        found = False\n",
    "        for creator in creator2:\n",
    "            if work['creator_string'] == json.loads(creator['creator_string'])[0]:\n",
    "                second_artist_name = creator['name']\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            missing_creators.append({'inventory_number': \"#\" + dic['inventory_number'], 'string': \"2 \" + json.dumps([work['creator_string']])})\n",
    "            second_artist_name = ''\n",
    "        \n",
    "        # instance of\n",
    "        found = False\n",
    "        for classification in classifications:\n",
    "            if work['classification'] == classification['string']:\n",
    "                genre_string = classification['label']\n",
    "                dic['instance_of'] = classification['qid']\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            missing_classifications.append({'inventory_number': \"#\" + dic['inventory_number'], 'string': work['classification']})\n",
    "            genre_string = ''\n",
    "            dic['instance_of'] = ''\n",
    "            \n",
    "        if second_artist_name == '':\n",
    "            if 'attributed to' in artist_name:\n",
    "                dic['description_en'] = genre_string + ' ' + artist_name\n",
    "            else:\n",
    "                dic['description_en'] = genre_string + ' by ' + artist_name\n",
    "        else:\n",
    "            dic['description_en'] = genre_string + ' by ' + artist_name + ' and ' + second_artist_name\n",
    "        dic['inception_val'] = work['inception_val']\n",
    "        dic['inception_prec'] = work['inception_prec']\n",
    "        dic['inception_earliest_date_val'] = work['earliest_date_val']\n",
    "        dic['inception_earliest_date_prec'] = work['earliest_date_prec']\n",
    "        dic['inception_latest_date_val'] = work['latest_date_val']\n",
    "        dic['inception_latest_date_prec'] = work['latest_date_prec']\n",
    "        dic['inception_sourcing_circumstances'] = work['sourcing_circumstances']\n",
    "        \n",
    "        # country\n",
    "        found = False\n",
    "        for country in countries:\n",
    "            if work['country_culture'] == country['string']:\n",
    "                country_culture = work['country_culture']\n",
    "                dic['country'] = country['qid']\n",
    "                dic['country_of_origin'] = country['qid']\n",
    "                found = True\n",
    "                break\n",
    "        if not found:\n",
    "            missing_countries.append({'inventory_number': \"#\" + dic['inventory_number'], 'string': work['country_culture']})\n",
    "            country_culture = ''\n",
    "            dic['country'] = ''\n",
    "            dic['country_of_origin'] = ''\n",
    "        \n",
    "        dic['height_val'] = work['height']\n",
    "        if work['height'] != '':\n",
    "            dic['height_unit'] = 'Q218593'\n",
    "        else:\n",
    "            dic['height_unit'] = ''\n",
    "            \n",
    "        dic['width_val'] = work['width']\n",
    "        if work['width'] != '':\n",
    "            dic['width_unit'] = 'Q218593'\n",
    "        else:\n",
    "            dic['width_unit'] = ''\n",
    "            \n",
    "        dic['thickness_val'] = work['depth']\n",
    "        if work['depth'] != '':\n",
    "            dic['thickness_unit'] = 'Q218593'\n",
    "        else:\n",
    "            dic['thickness_unit'] = ''\n",
    "        \n",
    "        dic['diameter_val'] = work['diameter']\n",
    "        if work['diameter'] != '':\n",
    "            dic['diameter_unit'] = 'Q218593'\n",
    "        else:\n",
    "            dic['diameter_unit'] = ''\n",
    "        \n",
    "        dic['collection'] = 'Q18563658' # Fine Arts Gallery\n",
    "        # !!! Can we get this from the inventory number if missing?\n",
    "        dic['collection_start_time_val'] = work['collection_start_time_val']\n",
    "        dic['collection_start_time_prec'] = work['collection_start_time_prec']\n",
    "        dic['location'] = 'Q18563658' # Vanderbilt University Fine Arts Gallery\n",
    "\n",
    "        # generate references\n",
    "        for column in data_columns:\n",
    "            try: # some columns are passed through without values and will generate errors\n",
    "                if dic[column['name']] != '':\n",
    "                    dic[column['prefix'] + '_ref1_referenceUrl'] = 'https://library.artstor.org/#/asset/' + work['ssid']\n",
    "                    dic[column['prefix'] + '_ref1_retrieved_val'] =  ref_retrieved\n",
    "                    dic[column['prefix'] + '_ref1_retrieved_prec'] =  '11'\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "        output.append(dic)\n",
    "    count += 1\n",
    "\n",
    "# output data\n",
    "fieldnames = list(output[0].keys())\n",
    "write_dicts_to_csv(output, 'gallery_works_to_write_dup.csv', fieldnames)\n",
    "\n",
    "# write error logs\n",
    "fieldnames = ['inventory_number', 'string']\n",
    "write_dicts_to_csv(missing_creators, 'missing_creators.csv', fieldnames)\n",
    "write_dicts_to_csv(missing_qids, 'missing_qids.csv', fieldnames)\n",
    "write_dicts_to_csv(missing_classifications, 'missing_classifications.csv', fieldnames)\n",
    "write_dicts_to_csv(missing_countries, 'missing_countries.csv', fieldnames)\n",
    "\n",
    "\n",
    "print('There are', orientation_error_count, 'potential orientation errors')\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove duplicate label/description combinations\n",
    "\n",
    "Wikidata does not allow writing any records if their label/description combinations are the same as an existing item. So we need to not try to write records that are duplicates locally. This cell eliminates local duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 314\n",
      "10 / 304\n",
      "20 / 287\n",
      "30 / 277\n",
      "40 / 267\n",
      "50 / 257\n",
      "60 / 247\n",
      "70 / 237\n",
      "80 / 227\n",
      "90 / 217\n",
      "100 / 207\n",
      "110 / 197\n",
      "120 / 187\n",
      "130 / 172\n",
      "140 / 161\n",
      "150 / 147\n",
      "160 / 137\n",
      "170 / 127\n",
      "180 / 117\n",
      "190 / 107\n",
      "200 / 92\n",
      "210 / 80\n",
      "220 / 70\n",
      "230 / 60\n",
      "240 / 50\n",
      "250 / 39\n",
      "260 / 28\n",
      "270 / 18\n",
      "280 / 8\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "filename = 'gallery_works_to_write_dup.csv'\n",
    "temp_works = read_dict(filename)\n",
    "#for work in dup_works:\n",
    "#    print(work['label_en'])\n",
    "#print()\n",
    "\n",
    "# sort out the works already in Wikidata with Q IDs from those to be written (without Q IDs)\n",
    "dup_works = []\n",
    "qid_works = []\n",
    "for work in temp_works:\n",
    "    if work['qid'] == '':\n",
    "        dup_works.append(work)\n",
    "    else:\n",
    "        qid_works.append(work)\n",
    "\n",
    "works = []\n",
    "count = 0\n",
    "while len(dup_works) > 0:\n",
    "    remaining = []\n",
    "    if count % 10 == 0: # make something show up so we know it's working\n",
    "        print(count, '/', len(dup_works))\n",
    "    match = False\n",
    "    done = False\n",
    "    \n",
    "    # first, check those to be written against those already written to Wikidata\n",
    "    #print('checking', dup_works[0]['label_en'], 'against qid')\n",
    "    for qid_work_index in range(len(qid_works)):\n",
    "        if qid_works[qid_work_index]['label_en'] == dup_works[0]['label_en'] and qid_works[qid_work_index]['description_en'] == dup_works[0]['description_en']:\n",
    "            done = True\n",
    "            #print('match against qid')\n",
    "            \n",
    "            # Deduplicate by appending the accession number to the end of the description\n",
    "            dup_works[0]['description_en'] = dup_works[0]['description_en'] + ' ' + dup_works[0]['inventory_number']\n",
    "            qid_works[qid_work_index]['description_en'] = qid_works[qid_work_index]['description_en'] + ' ' + qid_works[qid_work_index]['inventory_number']\n",
    "\n",
    "            works.append(dup_works[0])\n",
    "            del dup_works[0]\n",
    "            remaining = deepcopy(dup_works)\n",
    "            break # stop checking against existing works\n",
    "\n",
    "    if not done:\n",
    "        #print('no match, checking against others')\n",
    "        # Then check against all the other ones to be written\n",
    "        for work_index in range(len(dup_works)-1, 0, -1):\n",
    "            #print(count, dup_works[work_index]['label_en'], '|', dup_works[0]['label_en'])\n",
    "            if dup_works[work_index]['label_en'] == dup_works[0]['label_en'] and dup_works[work_index]['description_en'] == dup_works[0]['description_en']:\n",
    "                match = True\n",
    "                #print('match against others')\n",
    "\n",
    "                # Deduplicate by appending the accession number to the end of the description\n",
    "                dup_works[work_index]['description_en'] = dup_works[work_index]['description_en'] + ' ' + dup_works[work_index]['inventory_number']\n",
    "                \n",
    "                works.append(dup_works[work_index])\n",
    "                del dup_works[work_index]\n",
    "            else:\n",
    "                remaining.append(dup_works[work_index])\n",
    "        if match:\n",
    "            # Deduplicate the tested work if it matched with any other unpublished ones\n",
    "            dup_works[0]['description_en'] = dup_works[0]['description_en'] + ' ' + dup_works[0]['inventory_number']\n",
    "        works.append(dup_works[0])\n",
    "    #print('length of remaining', len(remaining))\n",
    "    dup_works = deepcopy(remaining)\n",
    "    count += 1\n",
    "    #print()\n",
    "        \n",
    "works_to_write = qid_works + works\n",
    "\n",
    "fieldnames = list(works_to_write[0].keys())\n",
    "write_dicts_to_csv(works_to_write, 'gallery_works_to_write.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell checks items against Wikidata labels and descriptions (hacked from vb5_check_labels_descriptions.py)\n",
    "\n",
    "Probably good to do a sort by qid, label, description before running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10\n",
      "20\n",
      "30\n",
      "40\n",
      "50\n",
      "60\n",
      "70\n",
      "80\n",
      "90\n",
      "100\n",
      "110\n",
      "120\n",
      "130\n",
      "140\n",
      "150\n",
      "160\n",
      "170\n",
      "180\n",
      "190\n",
      "200\n",
      "210\n",
      "220\n",
      "230\n",
      "240\n",
      "250\n",
      "260\n",
      "270\n",
      "280\n",
      "290\n",
      "300\n",
      "310\n",
      "320\n",
      "330\n",
      "340\n",
      "350\n",
      "360\n",
      "370\n",
      "380\n",
      "390\n",
      "400\n",
      "410\n",
      "420\n",
      "430\n",
      "440\n",
      "450\n",
      "460\n",
      "470\n",
      "480\n",
      "490\n",
      "500\n",
      "510\n",
      "520\n",
      "530\n",
      "540\n",
      "550\n",
      "560\n",
      "570\n",
      "580\n",
      "590\n",
      "600\n",
      "610\n",
      "620\n",
      "630\n",
      "640\n",
      "650\n",
      "660\n",
      "670\n",
      "680\n",
      "690\n",
      "700\n",
      "710\n",
      "720\n",
      "730\n",
      "740\n",
      "750\n",
      "760\n",
      "770\n",
      "780\n",
      "790\n",
      "800\n",
      "810\n",
      "820\n",
      "830\n",
      "840\n",
      "850\n",
      "860\n",
      "870\n",
      "880\n",
      "890\n",
      "900\n",
      "910\n",
      "920\n",
      "930\n",
      "940\n",
      "950\n",
      "960\n",
      "970\n",
      "980\n",
      "990\n",
      "1000\n",
      "1010\n",
      "1020\n",
      "1030\n",
      "1040\n",
      "1050\n",
      "1060\n",
      "1070\n",
      "1080\n",
      "1090\n",
      "1100\n",
      "1110\n",
      "1120\n",
      "1130\n",
      "1140\n",
      "1150\n",
      "1160\n",
      "1170\n",
      "1180\n",
      "1190\n",
      "1200\n",
      "1210\n",
      "1220\n",
      "1230\n",
      "1240\n",
      "1250\n",
      "1260\n",
      "1270\n",
      "1280\n",
      "1290\n",
      "1300\n",
      "1310\n",
      "1320\n",
      "1330\n",
      "1340\n",
      "1350\n",
      "1360\n",
      "1370\n",
      "1380\n",
      "1390\n",
      "1400\n",
      "1410\n",
      "1420\n",
      "1430\n",
      "1440\n",
      "1450\n",
      "1460\n",
      "1470\n",
      "1480\n",
      "1490\n",
      "1500\n",
      "1510\n",
      "1520\n",
      "1530\n",
      "1540\n",
      "1550\n",
      "1560\n",
      "1570\n",
      "1580\n",
      "1590\n",
      "1600\n",
      "1610\n",
      "1620\n",
      "1630\n",
      "1640\n",
      "1650\n",
      "1660\n",
      "1670\n",
      "1680\n",
      "1690\n",
      "1700\n",
      "1710\n",
      "1720\n",
      "1730\n",
      "1740\n",
      "1750\n",
      "1760\n",
      "1770\n",
      "1780\n",
      "1790\n",
      "1800\n",
      "1810\n",
      "1820\n",
      "1830\n",
      "1840\n",
      "1850\n",
      "1860\n",
      "1870\n",
      "1880\n",
      "1890\n",
      "1900\n",
      "1910\n",
      "1920\n",
      "1930\n",
      "1940\n",
      "1950\n",
      "1960\n",
      "1970\n",
      "1980\n",
      "1990\n",
      "2000\n",
      "2010\n",
      "2020\n",
      "2030\n",
      "2040\n",
      "2050\n",
      "2060\n",
      "2070\n",
      "2080\n",
      "2090\n",
      "2100\n",
      "2110\n",
      "2120\n",
      "2130\n",
      "2140\n",
      "2150\n",
      "2160\n",
      "2170\n",
      "2180\n",
      "2190\n",
      "2200\n",
      "2210\n",
      "2220\n",
      "2230\n",
      "2240\n",
      "2250\n",
      "2260\n",
      "2270\n",
      "2280\n",
      "2290\n",
      "2300\n",
      "2310\n",
      "2320\n",
      "2330\n",
      "2340\n",
      "2350\n",
      "2360\n",
      "2370\n",
      "2380\n",
      "2390\n",
      "2400\n",
      "2410\n",
      "2420\n",
      "2430\n",
      "2440\n",
      "2450\n",
      "2460\n",
      "2470\n",
      "2480\n",
      "2490\n",
      "2500\n",
      "2510\n",
      "2520\n",
      "2530\n",
      "2540\n",
      "2550\n",
      "2560\n",
      "2570\n",
      "2580\n",
      "2590\n",
      "2600\n",
      "2610\n",
      "2620\n",
      "2630\n",
      "2640\n",
      "2650\n",
      "2660\n",
      "2670\n",
      "2680\n",
      "2690\n",
      "2700\n",
      "2710\n",
      "2720\n",
      "2730\n",
      "2740\n",
      "2750\n",
      "2760\n",
      "2770\n",
      "2780\n",
      "2790\n",
      "2800\n",
      "2810\n",
      "2820\n",
      "2830\n",
      "2840\n",
      "2850\n",
      "2860\n",
      "2870\n",
      "2880\n",
      "2890\n",
      "2900\n",
      "2910\n",
      "2920\n",
      "2930\n",
      "2940\n",
      "2950\n",
      "2960\n",
      "2970\n",
      "2980\n",
      "2990\n",
      "3000\n",
      "3010\n",
      "3020\n",
      "3030\n",
      "3040\n",
      "3050\n",
      "3060\n",
      "3070\n",
      "3080\n",
      "3090\n",
      "3100\n",
      "3110\n",
      "3120\n",
      "3130\n",
      "3140\n",
      "3150\n",
      "3160\n",
      "3170\n",
      "3180\n",
      "3190\n",
      "3200\n",
      "3210\n",
      "3220\n",
      "3230\n",
      "3240\n",
      "3250\n",
      "3260\n",
      "3270\n",
      "3280\n",
      "3290\n",
      "3300\n",
      "3310\n",
      "3320\n",
      "3330\n",
      "3340\n",
      "3350\n",
      "3360\n",
      "3370\n",
      "3380\n",
      "3390\n",
      "3400\n",
      "3410\n",
      "3420\n",
      "3430\n",
      "3440\n",
      "3450\n",
      "3460\n",
      "3470\n",
      "3480\n",
      "3490\n",
      "3500\n",
      "3510\n",
      "3520\n",
      "3530\n",
      "3540\n",
      "3550\n",
      "3560\n",
      "3570\n",
      "3580\n",
      "3590\n",
      "3600\n",
      "3610\n",
      "3620\n",
      "3630\n",
      "3640\n",
      "3650\n",
      "3660\n",
      "3670\n",
      "3680\n",
      "3690\n",
      "3700\n",
      "3710\n",
      "3720\n",
      "3730\n",
      "3740\n",
      "3750\n",
      "3760\n",
      "3770\n",
      "3780\n",
      "3790\n",
      "3800\n",
      "3810\n",
      "3820\n",
      "3830\n",
      "3840\n",
      "3850\n",
      "3860\n",
      "3870\n",
      "3880\n",
      "3890\n",
      "3900\n",
      "3910\n",
      "3920\n",
      "3930\n",
      "3940\n",
      "3950\n",
      "3960\n",
      "3970\n",
      "3980\n",
      "3990\n",
      "4000\n",
      "4010\n",
      "4020\n",
      "4030\n",
      "4040\n",
      "4050\n",
      "4060\n",
      "4070\n",
      "4080\n",
      "4090\n",
      "4100\n",
      "4110\n",
      "4120\n",
      "4130\n",
      "4140\n",
      "4150\n",
      "4160\n",
      "4170\n",
      "4180\n",
      "4190\n",
      "4200\n",
      "4210\n",
      "4220\n",
      "4230\n",
      "4240\n",
      "4250\n",
      "4260\n",
      "4270\n",
      "4280\n",
      "4290\n",
      "4300\n",
      "4310\n",
      "4320\n",
      "4330\n",
      "4340\n",
      "4350\n",
      "4360\n",
      "4370\n",
      "4380\n",
      "4390\n",
      "4400\n",
      "4410\n",
      "4420\n",
      "4430\n",
      "4440\n",
      "4450\n",
      "4460\n",
      "4470\n",
      "4480\n",
      "4490\n",
      "4500\n",
      "4510\n",
      "4520\n",
      "4530\n",
      "4540\n",
      "4550\n",
      "4560\n",
      "4570\n",
      "4580\n",
      "4590\n",
      "4600\n",
      "4610\n",
      "4620\n",
      "4630\n",
      "4640\n",
      "4650\n",
      "4660\n",
      "4670\n",
      "4680\n",
      "4690\n",
      "4700\n",
      "4710\n",
      "4720\n",
      "4730\n",
      "4740\n",
      "4750\n",
      "4760\n",
      "4770\n",
      "4780\n",
      "4790\n",
      "4800\n",
      "4810\n",
      "4820\n",
      "4830\n",
      "4840\n",
      "4850\n",
      "4860\n",
      "4870\n",
      "4880\n",
      "4890\n",
      "4900\n",
      "4910\n",
      "4920\n",
      "4930\n",
      "4940\n",
      "4950\n",
      "4960\n",
      "4970\n",
      "4980\n",
      "4990\n",
      "5000\n",
      "5010\n",
      "5020\n",
      "5030\n",
      "5040\n",
      "5050\n",
      "5060\n",
      "5070\n",
      "5080\n",
      "5090\n",
      "5100\n",
      "5110\n",
      "5120\n",
      "5130\n",
      "5140\n",
      "5150\n",
      "5160\n",
      "5170\n",
      "5180\n",
      "5190\n",
      "5200\n",
      "5210\n",
      "5220\n",
      "5230\n",
      "5240\n",
      "5250\n",
      "5260\n",
      "5270\n",
      "5280\n",
      "5290\n",
      "5300\n",
      "5310\n",
      "5320\n",
      "5330\n",
      "5340\n",
      "5350\n",
      "5360\n",
      "5370\n",
      "5380\n",
      "5390\n",
      "5400\n",
      "5410\n",
      "5420\n",
      "5430\n",
      "5440\n",
      "5450\n",
      "5460\n",
      "5470\n",
      "5480\n",
      "5490\n",
      "5500\n",
      "5510\n",
      "5520\n",
      "5530\n",
      "5540\n",
      "5550\n",
      "5560\n",
      "5570\n",
      "5580\n",
      "5590\n",
      "5600\n",
      "5610\n",
      "5620\n",
      "5630\n",
      "5640\n",
      "5650\n",
      "5660\n",
      "5670\n",
      "5680\n",
      "5690\n",
      "5700\n",
      "5710\n",
      "5720\n",
      "5730\n",
      "5740\n",
      "5750\n",
      "5760\n",
      "5770\n",
      "5780\n",
      "5790\n",
      "5800\n",
      "5810\n",
      "5820\n",
      "5830\n",
      "5840\n",
      "5850\n",
      "5860\n",
      "5870\n",
      "5880\n",
      "5890\n",
      "5900\n",
      "5910\n",
      "5920\n",
      "5930\n",
      "5940\n",
      "5950\n",
      "5960\n",
      "5970\n",
      "5980\n",
      "5990\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "filename = 'gallery_works_to_write.csv'\n",
    "works = read_dict(filename)\n",
    "\n",
    "cleared = []\n",
    "duplicates = []\n",
    "for work_index in range(len(works)):\n",
    "    if work_index % 10 == 0: # make something show up so we know it's working\n",
    "        print(work_index)\n",
    "    if works[work_index]['qid'] == '':\n",
    "        # Have to do really weird stuff with quotes to avoid problems with strings that contain them\n",
    "        # Still could have problems if any label starts or ends with a single quote.\n",
    "        \n",
    "        # Also, there is still some problem with diacritics. A cedilla failes to match when sent by this script,\n",
    "        # but when pasted in the Query Service box, it returns a result.\n",
    "        query = \"\"\"select distinct ?item where {\n",
    "          ?item rdfs:label '''\"\"\" + works[work_index]['label_en'] + \"\"\"'''@en.\n",
    "          ?item schema:description '''\"\"\" + works[work_index]['description_en'] + \"\"\"'''@en.\n",
    "          }\"\"\"\n",
    "\n",
    "        #print('Checking label: \"' + works[work_index]['label_en'] + '\", description: \"' + works[work_index]['description_en'] + '\"')\n",
    "        \n",
    "        r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "        try:\n",
    "            data = r.json()\n",
    "            results = data['results']['bindings']\n",
    "            #print(results)\n",
    "\n",
    "            if len(results) > 0:\n",
    "                match = extract_qnumber(results[0]['item']['value'])\n",
    "                print('Row ' + str(work_index + 2) + ' is the same as ' + match)\n",
    "                works[work_index]['description_en'] = works[work_index]['description_en'] + ' ' + works[work_index]['inventory_number']\n",
    "            cleared.append(works[work_index])\n",
    "        except:\n",
    "            print(r.text)\n",
    "    else:\n",
    "        cleared.append(works[work_index])\n",
    "\n",
    "fieldnames = list(cleared[0].keys())\n",
    "write_dicts_to_csv(cleared, 'gallery_works_to_write.csv', fieldnames)\n",
    "#fieldnames = list(duplicates[0].keys())\n",
    "#write_dicts_to_csv(duplicates, 'gallery_works_duplicates.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------\n",
    "\n",
    "# STOP HERE\n",
    "\n",
    "# --------------\n",
    "\n",
    "The following scripts are for one-time use and have already been run!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate classes\n",
    "\n",
    "This section first gets all of the classes (values for P31 instanceOf) from the Met's collection. Then it tries to match the labels of those items to the values in the classification column of the gallery data. The source file for the `works` list can really be any of the series because the `classification` field isn't pre-processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of all classes in the Met\n",
    "work_classes = retrieve_gallery_classes()\n",
    "# print(work_classes)\n",
    "\n",
    "filename = 'gallery_works_with_dim2.txt'\n",
    "works = read_dict(filename)\n",
    "\n",
    "values = non_redundant(works, 'classification')\n",
    "values.sort(key = sort_funct)\n",
    "if values[0] == '':\n",
    "    values.remove('')\n",
    "print(values)\n",
    "\n",
    "mappings = []\n",
    "for value in values:\n",
    "    found = False\n",
    "    for work_class in work_classes:\n",
    "        if value.lower() == work_class['label'].lower():\n",
    "            found = True\n",
    "            mappings.append({'string': value, 'qid': work_class['qid'], 'label': work_class['label']})\n",
    "    if not found:\n",
    "        mappings.append({'string': value, 'qid': '', 'label': ''})\n",
    "write_dicts_to_csv(mappings, 'classification_mappings.csv', ['string', 'qid', 'label'])\n",
    "print()\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate country table\n",
    "\n",
    "Match labels in the country_culture column with labels and aliases in Wikidata for various country-like items. NOTE: this requires hand-processing after generation, so it shouldn't be re-run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = non_redundant(works, 'country_culture')\n",
    "values.sort(key = sort_funct)\n",
    "if values[0] == '':\n",
    "    values.remove('')\n",
    "print(values)\n",
    "\n",
    "mappings = []\n",
    "for value in values:\n",
    "    result_list = searchLabelsAtWikidata(value, ['Q6256','Q3624078','Q6266'])\n",
    "    print('|' + value + '|', result_list)\n",
    "    if len(result_list) == 1:\n",
    "        qid = result_list[0]\n",
    "    elif len(result_list) > 1:\n",
    "        qid = result_list\n",
    "    else:\n",
    "        qid = ''\n",
    "    mappings.append({'string': value, 'qid': qid})\n",
    "write_dicts_to_csv(mappings, 'country_mappings.csv', ['string', 'qid'])\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process medium field\n",
    "\n",
    "The medium field could form the basis of the description field, but is also used to generate the material used (P186) values.\n",
    "\n",
    "Currently, the `medium.csv` file isn't really used for anything, but it could be used in conjunction with the materials dictionary to describe the materials in the object. This is an area for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It doesn't really matter which source file is used, since there is no pre-processing done on this column\n",
    "filename = 'gallery_works_with_dates.txt'\n",
    "works = read_dict(filename)\n",
    "fieldnames = list(works[0].keys())\n",
    "# for field in fieldnames:\n",
    "#    print(field)\n",
    "\n",
    "for work_index in range(len(works)):\n",
    "    if works[work_index]['medium'].strip() != '':\n",
    "        works[work_index]['medium'] = works[work_index]['medium'].strip()\n",
    "medium_strings = non_redundant(works, 'medium')\n",
    "medium_strings.sort(key = sort_funct)\n",
    "out_table = []\n",
    "materials_list = []\n",
    "for string in medium_strings:\n",
    "    #print(string)\n",
    "    out_dict = {}\n",
    "    out_dict['string'] = string\n",
    "    out_dict['material'] = []\n",
    "\n",
    "    pieces = string.split(' ')\n",
    "    if len(pieces) == 1:\n",
    "        out_dict['material1'] = string\n",
    "        out_dict['material'].append(string)\n",
    "        out_dict['material2'] = ''\n",
    "        out_dict['material'].append('')\n",
    "    elif not ' on ' in string and ' and ' in string:\n",
    "        pieces = string.split(' and ')\n",
    "        out_dict['material1'] = pieces[0]\n",
    "        out_dict['material'].append(pieces[0])\n",
    "        out_dict['material2'] = pieces[1]\n",
    "        out_dict['material'].append(pieces[1])\n",
    "    else:\n",
    "        out_dict['material1'] = ''\n",
    "        out_dict['material'].append('')\n",
    "        out_dict['material2'] = ''\n",
    "        out_dict['material'].append('')\n",
    "        \n",
    "    if ' on ' in string:\n",
    "        pieces = string.split(' on ')\n",
    "        out_dict['medium'] = pieces[0]\n",
    "        out_dict['material'].append(pieces[0])\n",
    "        out_dict['surface'] = pieces[1]\n",
    "        out_dict['material'].append(pieces[1])\n",
    "    else:\n",
    "        out_dict['medium'] = ''\n",
    "        out_dict['material'].append('')\n",
    "        out_dict['surface'] = ''\n",
    "        out_dict['material'].append('')\n",
    "        \n",
    "    #print(out_dict['material'])\n",
    "    #print()\n",
    "    out_table.append(out_dict)\n",
    "    for material in out_dict['material']:\n",
    "        materials_list.append(material.lower().strip())\n",
    "\n",
    "fieldnames = list(out_table[0].keys())\n",
    "write_dicts_to_csv(out_table, 'medium.csv', fieldnames)\n",
    "\n",
    "materials_list = non_redundant(materials_list, '')\n",
    "materials_list.sort(key = sort_funct)\n",
    "for material in materials_list:\n",
    "    print(material)\n",
    "\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is a one-time script to generate a non-redundant list of material strings. It later gets hand-edited, so don't re-run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load and process materials list derived from above. The materials.csv file was manually created by copy and paste\n",
    "# of the list at the end of the previous script.\n",
    "filename = 'materials.csv'\n",
    "creator_data = read_dict(filename)\n",
    "fieldnames = list(creator_data[0].keys())\n",
    "\n",
    "# This is a hack of the creators disambiguation routine below\n",
    "for creator_index in range(len(creator_data)):\n",
    "#for creator_index in range(5):\n",
    "    if creator_data[creator_index]['searched'] == '':\n",
    "        match = False\n",
    "        print(creator_data[creator_index]['material'])\n",
    "        print()\n",
    "        results = searchNameAtWikidata(creator_data[creator_index]['material'])\n",
    "        if len(results) == 0:\n",
    "            print('No results')\n",
    "            print()\n",
    "            creator_data[creator_index]['matches'] = 'no'\n",
    "        elif len(results) == 1:\n",
    "            print(results)\n",
    "            print('match with', searchWikidataDescription(results[0]['qId'])['description'])\n",
    "            creator_data[creator_index]['qid'] = results[0]['qId']\n",
    "        else:\n",
    "            creator_data[creator_index]['matches'] = 'yes'\n",
    "            display_strings = []\n",
    "            for result_index in range(len(results)):\n",
    "                wikidata_descriptions = searchWikidataDescription(results[result_index]['qId'])\n",
    "                description = wikidata_descriptions['description']\n",
    "\n",
    "                similarity_score = name_variant_testing(creator_data[creator_index]['material'], results[result_index]['name'])\n",
    "                # if there is an exact dates match and high name similarity, just assign Q ID\n",
    "\n",
    "                result_name = results[result_index]['name']\n",
    "                result_qid = results[result_index]['qId']\n",
    "                display_strings.append({'qid': result_qid, 'name': result_name, 'description': description, 'score': similarity_score})\n",
    "\n",
    "            display_strings.sort(key = sort_score, reverse = True)\n",
    "            for index in range(len(display_strings)):\n",
    "                print(index, display_strings[index]['score'], display_strings[index]['name'], 'https://www.wikidata.org/wiki/' + display_strings[index]['qid'])\n",
    "                print('Description:', display_strings[index]['description'])\n",
    "                print()\n",
    "            match = input('number of match or Enter for no match')\n",
    "            if match != '':\n",
    "                creator_data[creator_index]['qid'] = results[result_index]['qId']\n",
    "        creator_data[creator_index]['searched'] = 'yes'\n",
    "        write_dicts_to_csv(creator_data, 'materials.csv', fieldnames)\n",
    "        print()\n",
    "        print()\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Disambiguate creators\n",
    "\n",
    "The following functions are names-related ones from vb3_match_wikidata.py\n",
    "\n",
    "The following cell creates a non-redundant list of processed names by extracting them from the creator field, then reversing them to given name first.\n",
    "\n",
    "Note: don't run this again when there is a creators.csv file already because it will overwrite any data that have been processed by the next script!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator_strings = non_redundant(works, 'creator_string')\n",
    "print(len(creator_strings))creator_data = []\n",
    "for creator_string in creator_strings:\n",
    "    creator_datum = {}\n",
    "    creator_datum['last_first'] = remove_parens(creator_string)\n",
    "    creator_datum['description'] = remove_description(creator_string)\n",
    "    if ',' in creator_datum['last_first']:\n",
    "        creator_datum['name'] = reverse_names(creator_datum['last_first'])\n",
    "    else:\n",
    "        creator_datum['name'] = creator_datum['last_first']\n",
    "    creator_datum['creator_string'] = json.dumps([creator_string], ensure_ascii=False)\n",
    "    creator_data.append(creator_datum)\n",
    "    \n",
    "creator_data.sort(key = sort_last_first)\n",
    "\n",
    "fieldnames = list(creator_data[0].keys())\n",
    "write_dicts_to_csv(creator_data, 'creators.csv', fieldnames)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following script isn't being maintained any more because there is a stand-alone file, `screen_creators.py`, which is run from the command line. It has edits that aren't found here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creator_data = read_dict('creators.csv')\n",
    "fieldnames = list(creator_data[0].keys())\n",
    "\n",
    "for creator_index in range(len(creator_data)):\n",
    "# for creator_index in range(22,23):\n",
    "    if creator_data[creator_index]['searched'] == '':\n",
    "        match = False\n",
    "        print(creator_data[creator_index]['name'])\n",
    "        print(creator_data[creator_index]['description'])\n",
    "        print()\n",
    "        results = searchNameAtWikidata(creator_data[creator_index]['name'])\n",
    "        if len(results) == 0:\n",
    "            print('No results')\n",
    "            print()\n",
    "            creator_data[creator_index]['matches'] = 'no'\n",
    "        else:\n",
    "            creator_data[creator_index]['matches'] = 'yes'\n",
    "            display_strings = []\n",
    "            for result_index in range(len(results)):\n",
    "                if human(results[result_index]['qId']):\n",
    "                    wikidata_descriptions = searchWikidataDescription(results[result_index]['qId'])\n",
    "                    description = wikidata_descriptions['description']\n",
    "                    if description[0:18] != 'Peerage person ID=':\n",
    "                        \n",
    "                        birthDateList = retrieve_birth_date_query.single_property_values_for_item(results[result_index]['qId'])\n",
    "                        if len(birthDateList) >= 1:\n",
    "                            birth_date = birthDateList[0][0:4]\n",
    "                        else:\n",
    "                            birth_date = ''\n",
    "                        \n",
    "                        deathDateList = retrieve_death_date_query.single_property_values_for_item(results[result_index]['qId'])\n",
    "                        if len(deathDateList) >= 1:\n",
    "                            death_date = deathDateList[0][0:4]\n",
    "                        else:\n",
    "                            death_date = ''\n",
    "                        \n",
    "                        if death_date != '' and birth_date != '':\n",
    "                            dates = birth_date + '-' + death_date\n",
    "                        elif death_date == '' and birth_date != '':\n",
    "                            dates = 'born ' + birth_date\n",
    "                        elif death_date != '' and birth_date == '':\n",
    "                            dates = 'died ' + death_date\n",
    "                        else:\n",
    "                            dates = ''\n",
    "                            \n",
    "                        similarity_score = name_variant_testing(creator_data[creator_index]['name'], results[result_index]['name'])\n",
    "                        # if there is an exact dates match and high name similarity, just assign Q ID\n",
    "                        if dates != '' and dates in creator_data[creator_index]['description'] and int(similarity_score) > 95:\n",
    "                            match = True\n",
    "                            creator_data[creator_index]['qid'] = results[result_index]['qId']\n",
    "                            print('Auto match with', results[result_index]['name'], dates, 'https://www.wikidata.org/wiki/' + results[result_index]['qId'])\n",
    "                            break # kill the results loop\n",
    "                        else:\n",
    "                            occupation = wikidata_descriptions['occupation']\n",
    "                            result_name = results[result_index]['name']\n",
    "                            result_qid = results[result_index]['qId']\n",
    "                            display_strings.append({'qid': result_qid, 'name': result_name, 'dates': dates, 'description': description, 'occupation': occupation, 'score': similarity_score})\n",
    "\n",
    "            if match:\n",
    "                pass\n",
    "            elif len(display_strings) == 0:\n",
    "                print('No results')\n",
    "                print()\n",
    "            else:\n",
    "                display_strings.sort(key = sort_score, reverse = True)\n",
    "                for index in range(len(display_strings)):\n",
    "                    print(index, display_strings[index]['score'], display_strings[index]['name'], 'https://www.wikidata.org/wiki/' + display_strings[index]['qid'])\n",
    "                    print(display_strings[index]['dates'])\n",
    "                    print('Description:', display_strings[index]['description'])\n",
    "                    print('Occupation:', display_strings[index]['occupation'])\n",
    "                    print()\n",
    "                match = input('number of match or Enter for no match')\n",
    "                if match != '':\n",
    "                    creator_data[creator_index]['qid'] = results[result_index]['qId']\n",
    "        creator_data[creator_index]['searched'] = 'yes'\n",
    "        write_dicts_to_csv(creator_data, 'creators.csv', fieldnames)\n",
    "        print()\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check image dimensions\n",
    "\n",
    "The `get_image_dimensions.ipynb` script was used to generate the `image_dimensions.csv` file.\n",
    "\n",
    "The code was developed here but moved for screening purposes to the output file-generating script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "works = read_dict('gallery_works_acquisition4.csv')\n",
    "\n",
    "images = read_dict('image_dimensions.csv')\n",
    "\n",
    "image_list = []\n",
    "count = 0\n",
    "#for work_index in range(500):\n",
    "for work_index in range(len(works)):\n",
    "    if works[work_index]['accession_number'] != '': # there's only one case of this\n",
    "        if works[work_index]['height'] != '': # skip the check if measurements not available\n",
    "            if works[work_index]['depth'] == '': # don't check 3 dimensional objects\n",
    "                kind = works[work_index]['classification'].lower()\n",
    "                if 'photograph' in kind or 'print' in kind or 'painting' in kind or 'drawing' in kind or 'graphic' in kind or 'book' in kind:\n",
    "                \n",
    "                    # we only need to check portrait aspect images\n",
    "                    if float(works[work_index]['height']) > float(works[work_index]['width']):\n",
    "                        match = False\n",
    "                        for image in images:\n",
    "                            if works[work_index]['accession_number'] == image['accession']:\n",
    "                                match = True\n",
    "                                if float(image['height']) <= float(image['width']):\n",
    "                                    image_dict = {}\n",
    "                                    count += 1\n",
    "                                    print(work_index, works[work_index]['accession_number'], works[work_index]['height'], works[work_index]['width'], works[work_index]['classification'])\n",
    "                                    print('https://library.artstor.org/#/asset/' + works[work_index]['ssid'])\n",
    "                                    image_dict['artstor_url'] = 'https://library.artstor.org/#/asset/' + works[work_index]['ssid']\n",
    "                                    image_dict['accession'] = '/ ' + works[work_index]['accession_number']\n",
    "                                    image_dict['height'] = works[work_index]['height']\n",
    "                                    image_dict['width'] = works[work_index]['width']\n",
    "                                    image_dict['classification'] = works[work_index]['classification']\n",
    "                                    image_list.append(image_dict)\n",
    "                                break\n",
    "                        if not match:\n",
    "                            #print('no image match')\n",
    "                            pass\n",
    "\n",
    "fieldnames = list(image_list[0].keys())\n",
    "write_dicts_to_csv(image_list, 'possible_dimension_reversals.csv', fieldnames)\n",
    "print('There are', count, 'potential orientation errors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process creators to be created\n",
    "\n",
    "The creators who weren't disambiguated against existing items need to be created. This cell processes the leftover lines from the `creator.csv` file that didn't have Q ID matches.\n",
    "\n",
    "**Important note:** Somewhere in this process, the `creator_string` values got changed from valid JSON with double quotes to having single quotes. That necessitated annoying manual processing of some records. Don't go through this process again without carefully watching to see where this went wrong!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dates(string):\n",
    "    try:\n",
    "        birthyear = str(int(string[:4]))\n",
    "    except:\n",
    "        birthyear = ''\n",
    "    if '-' in string:\n",
    "        pieces = string.split('-')\n",
    "        try:\n",
    "            deathyear = str(int(pieces[1][:4]))\n",
    "        except:\n",
    "            deathyear = ''\n",
    "    else:\n",
    "        deathyear = ''\n",
    "        \n",
    "    return birthyear, deathyear\n",
    "    \n",
    "    \n",
    "creators = read_dict('creators-to-upload.csv')\n",
    "\n",
    "for creator_index in range(len(creators)):\n",
    "    birthyear = ''\n",
    "    deathyear = ''\n",
    "    description_text = ''\n",
    "    birthcity_text = ''\n",
    "    birthstate_text = ''\n",
    "    #print(creators[creator_index]['description'])\n",
    "    string = creators[creator_index]['description']\n",
    "    if ',' in string:\n",
    "        pieces = string.split(',')\n",
    "        birthyear, deathyear = parse_dates(pieces[0])\n",
    "        if birthyear != '':\n",
    "            pieces[1] = pieces[1].strip()\n",
    "            if pieces[1][:5] == 'born ':\n",
    "                birthcity_text = pieces[1][5:]\n",
    "                if len(pieces) > 2:\n",
    "                    birthstate_text = pieces[2]\n",
    "    else:\n",
    "        birthyear, deathyear = parse_dates(string)\n",
    "        pieces = [string]\n",
    "        \n",
    "    # If a birthyear wasn't found in the first piece, assume the first piece is a description and look for born in 2nd piece\n",
    "    if birthyear == '' and len(pieces) > 1:\n",
    "        pieces[1] = pieces[1].strip()\n",
    "        if pieces[1][:5] == 'born ':\n",
    "            try:\n",
    "                birthyear = str(int(pieces[1][5:9]))\n",
    "                description_text = pieces[0]\n",
    "            except:\n",
    "                birthyear = ''\n",
    "        else:\n",
    "            birthyear, deathyear = parse_dates(pieces[1])\n",
    "            if birthyear != '':\n",
    "                description_text = pieces[0]\n",
    "            else:\n",
    "                if pieces[0][:5] == 'born ':\n",
    "                    birthcity_text = pieces[0][5:]\n",
    "                    birthstate_text = pieces[1]\n",
    "    else:\n",
    "        pass\n",
    "        \n",
    "    #print(birthyear, '/', deathyear, '/', birthcity_text, '/', birthstate_text, '/', description_text, '/', string)\n",
    "    creators[creator_index]['born'] = birthyear\n",
    "    creators[creator_index]['died'] = deathyear\n",
    "    creators[creator_index]['birth_city'] = birthcity_text\n",
    "    creators[creator_index]['birth_state'] = birthstate_text\n",
    "    creators[creator_index]['wikidata_description'] = description_text\n",
    "\n",
    "fieldnames = list(creators[0].keys())\n",
    "write_dicts_to_csv(creators, 'creators-to-upload-plus.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "creators = read_dict('creators-to-upload-plus.csv')\n",
    "\n",
    "for creator_index in range(len(creators)):\n",
    "    strings = json.loads(creators[creator_index]['creator_string'])\n",
    "    creators[creator_index]['creator_string'] = strings\n",
    "\n",
    "filename = 'classification_mappings.csv'\n",
    "classifications = read_dict(filename)\n",
    "\n",
    "works = read_dict('gallery_works_acquisition4.csv')\n",
    "\n",
    "for creator_index in range(len(creators)):\n",
    "#for creator_index in range(10):\n",
    "    found = False\n",
    "    for work in works:\n",
    "        for creator_string in creators[creator_index]['creator_string']:\n",
    "            if work['creator_string'] == creator_string:\n",
    "                for classification in classifications:\n",
    "                    if work['classification'] == classification['string']:\n",
    "                        found = True\n",
    "                        work_type = classification['label']\n",
    "                        #print(creators[creator_index]['name'], work_type)\n",
    "                        break\n",
    "        if found:\n",
    "            break\n",
    "            \n",
    "    if work_type == 'painting':\n",
    "        artist_type = 'painter'\n",
    "    elif work_type == 'sculpture':\n",
    "        artist_type = 'sculptor'\n",
    "    elif work_type == 'work of art':\n",
    "        artist_type = 'artist'\n",
    "    elif work_type == \"artist's book\":\n",
    "        artist_type = 'book artist'\n",
    "    elif work_type == 'ceramics':\n",
    "        artist_type = 'ceramic artist'\n",
    "    elif work_type == 'furniture':\n",
    "        artist_type = 'furniture designer'\n",
    "    elif work_type == 'print':\n",
    "        artist_type = 'printmaker'\n",
    "    elif work_type == 'drawing':\n",
    "        artist_type = 'drawer'\n",
    "    elif work_type == 'photograph':\n",
    "        artist_type = 'photographer'\n",
    "    elif work_type == 'pottery':\n",
    "        artist_type = 'potter'\n",
    "    elif work_type == 'printing block':\n",
    "        artist_type = 'printmaker'\n",
    "    elif work_type == 'scroll painting':\n",
    "        artist_type = 'painter'\n",
    "    elif work_type == 'furniture':\n",
    "        artist_type = 'furniture designer'\n",
    "    else:\n",
    "        artist_type = 'artist'\n",
    "        \n",
    "    #print(creators[creator_index]['name'], artist_type)\n",
    "    creators[creator_index]['artist_type'] = artist_type\n",
    "    if creators[creator_index]['wikidata_description'] == '':\n",
    "        creators[creator_index]['wikidata_description'] = artist_type\n",
    "\n",
    "fieldnames = list(creators[0].keys())\n",
    "write_dicts_to_csv(creators, 'creators-to-upload-complete.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match creators to upload list with Collector Systems dump\n",
    "\n",
    "2021-01-26 Kali gave me a dump that has overlapping data with the JSTOR dump, so they need to be matched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_artists = read_dict('creators/artists_from_collector_systems.csv')\n",
    "\n",
    "for cs_artist_index in range(len(cs_artists)):\n",
    "    years = cs_artists[cs_artist_index]['ArtistYears'].strip().split('-')\n",
    "    if len(years) == 0:\n",
    "        cs_artists[cs_artist_index]['born'] = ''\n",
    "        cs_artists[cs_artist_index]['died'] = ''\n",
    "    elif len(years) == 1:\n",
    "        cs_artists[cs_artist_index]['born'] = years[0].strip()\n",
    "        cs_artists[cs_artist_index]['died'] = ''\n",
    "    else:\n",
    "        if years[0].strip() == '':\n",
    "            cs_artists[cs_artist_index]['born'] = ''\n",
    "        else:\n",
    "            cs_artists[cs_artist_index]['born'] = years[0].strip()\n",
    "        if years[1].strip() == '':\n",
    "            cs_artists[cs_artist_index]['died'] = ''\n",
    "        else:\n",
    "            cs_artists[cs_artist_index]['died'] = years[1].strip()\n",
    "            \n",
    "fieldnames = list(cs_artists[0].keys())\n",
    "write_dicts_to_csv(cs_artists, 'creators/cs_artists_dates.csv', fieldnames)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after the first step, I manually deleted some non-conforming cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs_artists = read_dict('creators/cs_artists_dates.csv')\n",
    "creators = read_dict('creators/creators-to-upload-complete-sorted.csv')\n",
    "\n",
    "for creator_index in range(len(creators)):\n",
    "#for creator_index in range(6):\n",
    "    found = False\n",
    "    # Check upload artist names agains CS names\n",
    "    for cs_artist in cs_artists:\n",
    "        ratio = fuzz.ratio(cs_artist['ArtistFull'].strip(), creators[creator_index]['name'])\n",
    "        if ratio > 80:\n",
    "            found = True\n",
    "            break\n",
    "\n",
    "    # Check upload artist names agains CS names\n",
    "    if not found:\n",
    "        for cs_artist in cs_artists:\n",
    "            ratio = fuzz.ratio(cs_artist['AliasFull'].strip(), creators[creator_index]['name'])\n",
    "            if ratio > 80:\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "    if found:\n",
    "        qc = str(ratio) + ' ' + cs_artist['ArtistFull'].strip() + ' / ' + cs_artist['AliasFull'].strip() + ' / ' + creators[creator_index]['name']\n",
    "        if cs_artist['born'] != creators[creator_index]['born']:\n",
    "            qc += ' / ' + cs_artist['born'] + '-' + cs_artist['died'] + ' / ' + creators[creator_index]['born'] + '-' + creators[creator_index]['died']\n",
    "        print(qc)\n",
    "        creators[creator_index]['cs_name'] = cs_artist['ArtistFull'].strip()\n",
    "        creators[creator_index]['cs_alias'] = cs_artist['AliasFull'].strip()\n",
    "        creators[creator_index]['cs_born'] = cs_artist['born']\n",
    "        creators[creator_index]['cs_died'] = cs_artist['died']\n",
    "        creators[creator_index]['cs_nationality'] = cs_artist['ArtistNationality'].strip()\n",
    "        creators[creator_index]['ulan'] = cs_artist['ulan'].strip()\n",
    "        if cs_artist['ArtistGender'].strip() != '':\n",
    "            creators[creator_index]['gender'] = cs_artist['ArtistGender'].strip().lower()[0]\n",
    "        else:\n",
    "            creators[creator_index]['gender'] = ''\n",
    "        creators[creator_index]['qc'] = qc\n",
    "    else:\n",
    "        creators[creator_index]['cs_name'] = ''\n",
    "        creators[creator_index]['cs_alias'] = ''\n",
    "        creators[creator_index]['cs_born'] = ''\n",
    "        creators[creator_index]['cs_died'] = ''\n",
    "        creators[creator_index]['cs_nationality'] = ''\n",
    "        creators[creator_index]['ulan'] = ''\n",
    "        creators[creator_index]['gender'] = ''\n",
    "        creators[creator_index]['qc'] = ''\n",
    "    #print(creators[creator_index])\n",
    "    #print()\n",
    "\n",
    "fieldnames = ['name', 'cs_name', 'cs_alias', 'wikidata_description', 'born', 'cs_born', 'died', 'cs_died', 'birthplace', 'deathplace', 'gender', 'ulan', 'cs_nationality', 'artist_type', 'birth_city', 'birth_state', 'description', 'last_first', 'creator_string', 'qc']\n",
    "write_dicts_to_csv(creators, 'creators/combined_artists.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the names were matched, we need to see if any of the ULAN identifiers are linked to existing Wikidata items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create a list of strings into a single string. Each strung is surrounded by double quotes\n",
    "# as required for RDF/Turtle syntax i.e. SPARQL syntax, and separated by newlines.\n",
    "def create_string_values_list(list):\n",
    "    string_values = ''  # VALUES list for query\n",
    "    for record in list:\n",
    "        string = record['ulan']\n",
    "        string_values += '\"' + string + '\"\\n'\n",
    "\n",
    "    # remove trailing newline\n",
    "    string_values = string_values[:len(string_values)-1]\n",
    "    return string_values\n",
    "\n",
    "def build_qid_query(screen):\n",
    "    query = '''select distinct ?qid ?ulan ?label where\n",
    "    {'''\n",
    "    query += '''\n",
    "    VALUES ?ulan\n",
    "        {\n",
    "''' + screen + '''\n",
    "        }'''\n",
    "\n",
    "    query += '''\n",
    "    ?qid wdt:P245 ?ulan.\n",
    "    ?qid rdfs:label ?label.\n",
    "    filter(lang(?label) = 'en')    \n",
    "    }'''   \n",
    "    return query\n",
    "\n",
    "def send_sparql_query(query_string, request_header):\n",
    "    response = requests.post(endpoint, data=query_string.encode('utf-8'), headers=request_header)\n",
    "    #print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "    data = response.json()\n",
    "\n",
    "    # Extract the values from the response JSON\n",
    "    results = data['results']['bindings']\n",
    "    \n",
    "    # You can delete the print statement if the queries are short. However, for large/long queries,\n",
    "    # it's good to let the user know what's going on.\n",
    "    print('done retrieving data')\n",
    "    #print(json.dumps(results, indent=2))\n",
    "    return(results)\n",
    "\n",
    "# Extracts the local name part of an IRI, e.g. a qNumber from a Wikidata IRI\n",
    "def extract_local_name(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    last_piece = len(pieces)\n",
    "    return pieces[last_piece - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ulan_artists = read_dict('creators/artist_ulan.csv')\n",
    "values = create_string_values_list(ulan_artists)\n",
    "query = build_qid_query(values)\n",
    "\n",
    "print('querying SPARQL endpoint to acquire entity counts')\n",
    "results = send_sparql_query(query, requestheader)\n",
    "#print(json.dumps(results, indent=2))\n",
    "\n",
    "# Extract labels from the results and match them to their IDs and counts.\n",
    "output_list = []\n",
    "for interim_result in ulan_artists:\n",
    "    found = False\n",
    "    for result in results:\n",
    "        final_result = {}\n",
    "        if result['ulan']['value'] == interim_result['ulan']:\n",
    "            found = True\n",
    "            final_result['qid'] = extract_local_name(result['qid']['value'])\n",
    "            final_result['label'] = result['label']['value']\n",
    "            final_result['name'] = interim_result['name']\n",
    "            final_result['ulan'] = interim_result['ulan']\n",
    "            final_result['creator_string'] = interim_result['creator_string']\n",
    "            break\n",
    "    if not found:\n",
    "        final_result['qid'] = ''\n",
    "        final_result['label'] = ''\n",
    "        final_result['name'] = interim_result['name']\n",
    "        final_result['ulan'] = interim_result['ulan']\n",
    "        final_result['creator_string'] = interim_result['creator_string']\n",
    "    output_list.append(final_result)\n",
    "\n",
    "fieldnames = list(output_list[0].keys())\n",
    "write_dicts_to_csv(output_list, 'creators/qid_ulan_table.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next stage was to update previous files: add Q IDs to creators.csv for newly discovered qids, remove \"new\" creators that weren't new (because they had Q IDs) from the combined_artists.csv.\n",
    "\n",
    "I then did a lot of manual cleaning of the data and discovered a lot of duplicates by Googling (especially reversed Japanese and Chinese transliterations).\n",
    "\n",
    "Next look up the occupation Q ID by matching with the artist_type string. Also set the sex or gender Q ID from the gender string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occupations = read_dict('creators/occupations_qids.csv')\n",
    "new_creators = read_dict('creators/91cleaned_artists.csv')\n",
    "\n",
    "output_list = []\n",
    "for creator in new_creators:\n",
    "    found = False\n",
    "    for occupation in occupations:\n",
    "        if occupation['artist_type'] == creator['artist_type']:\n",
    "            creator['occupation'] = occupation['qid']\n",
    "            break\n",
    "    if creator['gender'] == 'm':\n",
    "        creator['sex'] = 'Q6581097'\n",
    "    elif creator['gender'] == 'f':\n",
    "        creator['sex'] = 'Q6581072'\n",
    "    else:\n",
    "        creator['sex'] = ''\n",
    "    output_list.append(creator)\n",
    "\n",
    "fieldnames = list(output_list[0].keys())\n",
    "write_dicts_to_csv(output_list, 'creators/92cleaned_artists.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding these last columns, I cleaned up the `config.json` file to contain all the properties needed to generate the new creator items. Then I ran `convert_json_to_metadata_schema.py` to generate the `csv-metadata.json` file for the upload and `acquire_wikidata_metadata.py` with one old Q ID to generate the column headers. I then copied and pasted columns from the cleaned data into that CSV. \n",
    "\n",
    "I hacked `vb5_check_labels_descriptions.py` to check for duplicate label/description combinations. Then I did the upload.\n",
    "\n",
    "Next step, match the creator_strings column between the `creators_to_write.csv` file and `creators.csv` to add the newly created Q IDs to `creators.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_creators = read_dict('creators.csv')\n",
    "new_creators = read_dict('creators/94new_creator_qids_creator_strings.csv')\n",
    "\n",
    "for new_creator in new_creators:\n",
    "    found = False\n",
    "    for old_creator_id in range(len(old_creators)):\n",
    "        #if old_creators[old_creator_id]['creator_string'] == new_creator['creator_string'].replace(\"'\", '\"'):\n",
    "        if old_creators[old_creator_id]['creator_string'].replace('\"', \"'\") == new_creator['creator_string']:\n",
    "            found = True\n",
    "            if old_creators[old_creator_id]['qid'] != '':\n",
    "                print('Warning: old QID', old_creators[old_creator_id]['qid'], 'for', old_creators[old_creator_id]['last_first'], 'is being replaced by', new_creator['qid'])\n",
    "            old_creators[old_creator_id]['qid'] = new_creator['qid']\n",
    "            break\n",
    "    if not found:\n",
    "        print('Warning: there was no match for new QID', new_creator['qid'], 'for', new_creator['label_en'])\n",
    "\n",
    "\n",
    "#fieldnames = list(output_list[0].keys())\n",
    "fieldnames = ['qid','searched','matches','after_work_by','school_of','workshop_of','last_first','description','name','creator_string']\n",
    "write_dicts_to_csv(old_creators, 'creators-out.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
