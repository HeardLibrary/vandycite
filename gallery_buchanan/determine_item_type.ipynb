{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine arts gallery item type determination\n",
    "\n",
    "This is mostly to sort out 3D and 2D items, but also to figure out which 2D items are posters and possibly other categories\n",
    "\n",
    "## Function section\n",
    "\n",
    "This needs to be run before any of the other cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from time import sleep\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from copy import deepcopy\n",
    "from langdetect import detect\n",
    "from langdetect import detect_langs\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import RegexpParser\n",
    "\n",
    "# ----------------\n",
    "# Configuration settings\n",
    "# ----------------\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "dots_sleep = 1 # number of seconds to wait between calls to ParallelDots API\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "# Calculate the reference date retrieved value for all statements\n",
    "whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "dateZ = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "ref_retrieved = dateZ + 'T00:00:00Z' # form 2019-12-05T00:00:00Z as provided by Wikidata, without leading +\n",
    "\n",
    "# ----------------\n",
    "# Utility functions\n",
    "# ----------------\n",
    "\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderBot/1.6 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def load_credential(filename, directory):\n",
    "    cred = ''\n",
    "    # to change the script to look for the credential in the working directory, change the value of home to empty string\n",
    "    if directory == 'home':\n",
    "        home = str(Path.home()) #gets path to home directory; works for both Win and Mac\n",
    "        credential_path = home + '/' + filename\n",
    "    else:\n",
    "        directory = 'working'\n",
    "        credential_path = filename\n",
    "    try:\n",
    "        with open(credential_path, 'rt', encoding='utf-8') as file_object:\n",
    "            cred = file_object.read()\n",
    "    except:\n",
    "        print(filename + ' file not found - is it in your ' + directory + ' directory?')\n",
    "        exit()\n",
    "    return(cred)\n",
    "\n",
    "# find non-redundant values for a column or simple list\n",
    "def non_redundant(table, column_key):\n",
    "    non_redundant_list = []\n",
    "    for row in table:\n",
    "        found = False\n",
    "        for test_item in non_redundant_list:\n",
    "            if column_key == '':\n",
    "                if row == test_item:\n",
    "                    found = True\n",
    "                    break\n",
    "            else:\n",
    "                if row[column_key] == test_item:\n",
    "                    found = True\n",
    "                    break\n",
    "        if not found:\n",
    "            if column_key == '':\n",
    "                non_redundant_list.append(row)\n",
    "            else:\n",
    "                non_redundant_list.append(row[column_key])\n",
    "    return non_redundant_list\n",
    "\n",
    "# function to use in sort of simple list\n",
    "def sort_funct(row):\n",
    "    return row\n",
    "\n",
    "# function to use in sort last_first names\n",
    "def sort_last_first(row):\n",
    "    return row['last_first']\n",
    "\n",
    "# function to use in sort by match score\n",
    "def sort_score(row):\n",
    "    return row['score']\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# search label and alias\n",
    "# For whatever reason, if I use the graph pattern\n",
    "\n",
    "# wd:Q21 wdt:P31 ?class.\n",
    "\n",
    "# England is not Q6256 (country)\n",
    "# But if I use the graph pattern\n",
    "\n",
    "#   wd:Q21 p:P31 ?statement.\n",
    "#  ?statement ps:P31 ?class.\n",
    "\n",
    "# it is ??!!\n",
    "def searchLabelsAtWikidata(string, class_list):\n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id '\n",
    "    query += '''where {\n",
    "  {?id rdfs:label \"''' + string + '''\"@en.}\n",
    "  union\n",
    "  {?id skos:altLabel \"''' + string + '''\"@en.}\n",
    "  '''\n",
    "    for class_index in range(len(class_list)):\n",
    "        if class_index == 0:\n",
    "            query += '''{?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "        else:\n",
    "            query += '''union\n",
    "  {?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "    query += '''}'''\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    # r = requests.get(endpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_qnumber(result['id']['value'])\n",
    "        return_value.append(qid)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def retrieve_gallery_classes():\n",
    "    # create a string for the query\n",
    "    # use Metropolitan Museum of Art because there are too many collections to not specify the collection.\n",
    "    query = '''select distinct ?class ?label where \n",
    "      {\n",
    "      ?item wdt:P195 wd:Q160236.\n",
    "      ?item wdt:P31 ?class.\n",
    "      ?class rdfs:label ?label.\n",
    "      filter(lang(?label) = 'en')\n",
    "      }\n",
    "      order by ?label'''\n",
    "\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    print('sending query')\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    print('results returned')\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_qnumber(result['class']['value'])\n",
    "        label = result['label']['value']\n",
    "        return_value.append({'label': label, 'qid': qid})\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def generateNameAlternatives(name):\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods\n",
    "    name = name.replace('.', '')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "\n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        # make sure first character is alphabetic\n",
    "        # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "        # typical cases are like (Kit) or \"Kit\"\n",
    "        if not piece[0:1].isalpha():\n",
    "            piece = piece[1:len(piece)] # remove the first non-alphabetic character\n",
    "        if len(piece) > 0:\n",
    "            initials.append(piece[0:1])\n",
    "        \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += pieces[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # full name with suffix\n",
    "    if suffix != '':\n",
    "        nameVersion = ''\n",
    "        for pieceNumber in range(0, len(pieces)-1):\n",
    "            nameVersion += pieces[pieceNumber] + ' '\n",
    "        nameVersion += pieces[len(pieces)-1] + suffix\n",
    "        alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first and last name only\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial and last name only\n",
    "    nameVersion = initials[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    nameVersion = initials[0] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with last name\n",
    "    nameVersion = initials[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber]\n",
    "    nameVersion += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def searchNameAtWikidata(name):\n",
    "    nameList = generateNameAlternatives(name)\n",
    "    alternatives = ''\n",
    "    for alternative in nameList:\n",
    "        # get rid of quotes, which will break the query\n",
    "        alternative = alternative.replace('\"', '')\n",
    "        alternative = alternative.replace(\"'\", '')\n",
    "        alternatives += '\"' + alternative + '\"@en\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    results = []\n",
    "    # r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            if 'label' in statement:\n",
    "                name = statement['label']['value']\n",
    "            else:\n",
    "                name = ''\n",
    "            qNumber = vbc.extract_qnumber(wikidataIri)\n",
    "            results.append({'qId': qNumber, 'name': name})\n",
    "    except:\n",
    "        results = [{'error': r.text}]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return results\n",
    "\n",
    "def name_variant_testing(name, variant):\n",
    "    # get rid of periods\n",
    "    name = name.replace('.','')\n",
    "    variant = variant.replace('.','')\n",
    "    \n",
    "    # create first names\n",
    "    name_pieces = name.split(' ')\n",
    "    variant_pieces = variant.split(' ')\n",
    "    last_name = name_pieces[len(name_pieces)-1]\n",
    "    last_variant = variant_pieces[len(variant_pieces)-1]\n",
    "    if len(name_pieces) > 1:\n",
    "        first_names = name[0:-(len(last_name)+1)]\n",
    "    else:\n",
    "        first_names = name     \n",
    "    if len(variant_pieces) > 1:\n",
    "        first_variants = variant[0:-(len(last_variant)+1)]\n",
    "    else:\n",
    "        first_variants = variant      \n",
    "    #print(first_names)\n",
    "    #print(first_variants)\n",
    "    \n",
    "    # compare first names\n",
    "    # I experimented with the different ratios and I think fuzz might be best.\n",
    "    ratio = fuzz.ratio(first_names, first_variants)\n",
    "    #partial_ratio = fuzz.partial_ratio(first_names, first_variants)\n",
    "    #sort_ratio = fuzz.token_sort_ratio(first_names, first_variants)\n",
    "    #set_ratio = fuzz.token_set_ratio(first_names, first_variants)\n",
    "    # print('name similarity ratio', ratio)\n",
    "    #print('partial ratio', partial_ratio)\n",
    "    #print('sort_ratio', sort_ratio)\n",
    "    #print('set_ratio', set_ratio)\n",
    "\n",
    "    return(ratio)\n",
    "\n",
    "def find_surname_givens(name):\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', '')\n",
    "    name = name.replace(',', '')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "def remove_parens(string):\n",
    "    name_string = string.split('(')[0]\n",
    "    return name_string.strip()\n",
    "\n",
    "def remove_description(string):\n",
    "    try:\n",
    "        right_string = string.split('(')[1]\n",
    "        left_string = right_string.split(')')[0]\n",
    "        result = left_string.strip()\n",
    "    except:\n",
    "        result = ''\n",
    "    return result\n",
    "\n",
    "def reverse_names(string):\n",
    "    pieces = string.split(',')\n",
    "    return pieces[1].strip() + ' ' + pieces[0].strip()\n",
    "\n",
    "# Screens for Wikidata items that are potential matches\n",
    "\n",
    "import vb_common_code as vbc\n",
    "retrieve_class_list_query = vbc.Query(pid='P31', uselabel=False, sleep=sparql_sleep)\n",
    "retrieve_birth_date_query = vbc.Query(isitem=False, pid='P569', sleep=sparql_sleep)\n",
    "retrieve_death_date_query = vbc.Query(isitem=False, pid='P570', sleep=sparql_sleep)\n",
    "\n",
    "def human(qId):\n",
    "    screen = True\n",
    "    wdClassList = retrieve_class_list_query.single_property_values_for_item(qId)\n",
    "    # if there is a class property, check if it's a human\n",
    "    if len(wdClassList) != 0:\n",
    "        # if it's not a human\n",
    "        if wdClassList[0] != 'Q5':\n",
    "            #print('*** This item is not a human!')\n",
    "            screen = False\n",
    "    return screen\n",
    "\n",
    "# returns a dictionary of various descriptors of the item with Wikidata ID qId\n",
    "# P106 is occupation, schema:description is filtered to be the English description\n",
    "def searchWikidataDescription(qId):\n",
    "    resultsDict = {}\n",
    "    query = '''select distinct ?description ?orcid ?occupation where {\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' schema:description ?description.\n",
    "            FILTER(lang(?description) = 'en')\n",
    "            }\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = 'en')            \n",
    "            }\n",
    "        optional {wd:'''+ qId + ''' wdt:P496 ?orcid.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the dictionary remains empty\n",
    "            # Only a single description per language is allowed, so there should only be one description\n",
    "            if 'description' in statements[0]:\n",
    "                description = statements[0]['description']['value']\n",
    "            else:\n",
    "                description = ''\n",
    "            resultsDict['description'] = description\n",
    "            \n",
    "            # Only a single ORCID is allowed, so there should only be one orcid value\n",
    "            if 'orcid' in statements[0]:\n",
    "                orcid = statements[0]['orcid']['value']\n",
    "            else:\n",
    "                orcid = ''\n",
    "            resultsDict['orcid'] = orcid\n",
    "            \n",
    "            # if there are multiple statements, that's because there are more than one occupation\n",
    "            occupationList = []\n",
    "            for statement in statements:\n",
    "                if 'occupation' in statement:\n",
    "                    occupationList.append(statement['occupation']['value'])\n",
    "            resultsDict['occupation'] = occupationList\n",
    "    except:\n",
    "        resultsDict = {'error': r.text}\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return resultsDict\n",
    "\n",
    "def determine_era(string):\n",
    "    # dates with no CE or BCE, including empty string\n",
    "    if 'CE' not in string:\n",
    "        value = string\n",
    "        era = 'unknown'      \n",
    "    else:\n",
    "        if 'BCE' in string:\n",
    "            value = string[0:len(string)-3].strip()\n",
    "            era = 'BCE'\n",
    "        else: # string ends with CE\n",
    "            value = string[0:len(string)-2].strip()\n",
    "            era = 'CE'\n",
    "    return value, era\n",
    "\n",
    "def determine_zeros(date):\n",
    "    zero_count = 0\n",
    "    for char_number in range(len(date), 0, -1):\n",
    "        if date[char_number-1] == '0':\n",
    "            zero_count += 1\n",
    "        else:\n",
    "            return zero_count\n",
    "\n",
    "def pad_zeros_left(date_string):\n",
    "    length = len(date_string)\n",
    "    pad = 4-length\n",
    "    return '0' * pad + date_string\n",
    "\n",
    "def sign(era):\n",
    "    if era == 'BCE':\n",
    "        return '-'\n",
    "    elif era == 'CE':\n",
    "        return ''\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def detect_language(string):\n",
    "    try:\n",
    "        lang_list = detect_langs(string)\n",
    "        lang_string = str(lang_list[0])\n",
    "        confidence = float(lang_string[3:])\n",
    "        lang = lang_string[:2]\n",
    "    except: #exceptions occur when no info to decide, e.g. numbers\n",
    "        lang = 'zxx'\n",
    "        confidence = float(0)\n",
    "    return lang, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Loads all of the gallery metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'works_multiprop.csv'\n",
    "works = read_dict(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# work of art types are all or nearly all 3D objects of diverse kinds not narrowly classified. They would be good\n",
    "# targets for better typing using text analysis\n",
    "known_3d_types = [\n",
    "    {'name': 'ceramics', 'qid': 'Q13464614'},\n",
    "    {'name': 'sculpture', 'qid': 'Q860861'},\n",
    "    {'name': 'textile', 'qid': 'Q28823'},\n",
    "    {'name': 'coin', 'qid': 'Q41207'},\n",
    "    {'name': 'stone', 'qid': 'Q22731'},\n",
    "    {'name': 'weapon', 'qid': 'Q728'},\n",
    "    {'name': 'container', 'qid': 'Q987767'},\n",
    "    {'name': 'mask', 'qid': 'Q161524'},\n",
    "    {'name': 'furniture', 'qid': 'Q14745'},\n",
    "    {'name': 'shield', 'qid': 'Q131559'},\n",
    "    {'name': 'plaque', 'qid': 'Q4364339'},\n",
    "    {'name': 'tool', 'qid': 'Q39546'},\n",
    "    {'name': 'necklace', 'qid': 'Q189299'},\n",
    "    {'name': 'vase', 'qid': 'Q191851'},\n",
    "    {'name': 'utensil', 'qid': 'Q1357761'},\n",
    "    {'name': 'kylix', 'qid': 'Q668349'},\n",
    "    {'name': 'relief sculpture', 'qid': 'Q245117'},\n",
    "    {'name': 'African doll', 'qid': 'Q4689996'},\n",
    "    {'name': 'pottery', 'qid': 'Q11642'},\n",
    "    {'name': 'bracelet', 'qid': 'Q201664'},\n",
    "    {'name': 'shoe', 'qid': 'Q22676'},\n",
    "    {'name': 'yarn', 'qid': 'Q49007'},\n",
    "    {'name': 'musical instrument', 'qid': 'Q34379'},\n",
    "    {'name': 'headdress', 'qid': 'Q28972621'},\n",
    "    {'name': 'bowl', 'qid': 'Q153988'},\n",
    "    {'name': 'compass', 'qid': 'Q34735'},\n",
    "    {'name': 'amulet', 'qid': 'Q131557'},\n",
    "    {'name': 'clock', 'qid': 'Q376'},\n",
    "    {'name': 'earring', 'qid': 'Q168456'},\n",
    "    {'name': 'doll', 'qid': 'Q168658'},\n",
    "    {'name': 'embroidery', 'qid': 'Q18281'},\n",
    "    {'name': 'artificial physical object', 'qid': 'Q8205328'},\n",
    "    {'name': 'currency', 'qid': 'Q8142'},\n",
    "    {'name': 'quilt', 'qid': 'Q1064538'},\n",
    "    {'name': 'stained glass', 'qid': 'Q1473346'},\n",
    "    {'name': 'printing block', 'qid': 'Q20820214'},\n",
    "    {'name': 'work of art', 'qid': 'Q838948'}\n",
    "]\n",
    "\n",
    "# Note: image types are generally book illustrations. In their case the text \"from\" generally separates the image\n",
    "# description from the containing work\n",
    "known_2d_types = [\n",
    "    {'name': 'photograph', 'qid': 'Q125191'},\n",
    "    {'name': 'painting', 'qid': 'Q3305213'},\n",
    "    {'name': 'drawing', 'qid': 'Q93184'},\n",
    "    {'name': 'scroll painting', 'qid': 'Q19969434'},\n",
    "    {'name': 'manuscript', 'qid': 'Q87167'},\n",
    "    {'name': 'etching print', 'qid': 'Q18218093'},\n",
    "    {'name': 'document', 'qid': 'Q49848'},\n",
    "    {'name': 'panel', 'qid': 'Q1348059'},\n",
    "    {'name': 'woodcut print', 'qid': 'Q18219090'},\n",
    "    {'name': 'artists book', 'qid': 'Q1062404'},\n",
    "    {'name': 'image', 'qid': 'Q478798'},\n",
    "]\n",
    "\n",
    "# Note: the single audio recording type is included here since its label is its text.\n",
    "known_poster_types = [\n",
    "    {'name': 'poster', 'qid': 'Q429785'},\n",
    "    {'name': 'audio recording', 'qid': 'Q3302947'}\n",
    "]\n",
    "\n",
    "# print types are all 2D, but are a mixture of artistic prints and posters\n",
    "# We should try to do OCR to discover if there is text in the image and if so, does it match the label.\n",
    "ambiguous_types = [\n",
    "    {'name': 'print', 'qid': 'Q11060274'}\n",
    "]\n",
    "\n",
    "works_list = []\n",
    "for work in works:\n",
    "    label = work['label_en']\n",
    "    #print(label)\n",
    "\n",
    "    work_dict = {'qid': work['qid'], 'label': label}\n",
    "    \n",
    "    dimension = ''\n",
    "    \n",
    "    height = work['height_val']\n",
    "    width = work['width_val']\n",
    "    thickness = work['thickness_val']\n",
    "    diameter = work['diameter_val'] # not useful since some 2D posters have diameters of circular features\n",
    "    #print('qid', work['qid'])\n",
    "    #print('height', height, '/ width', width, '/ thickness', thickness, '/ diameter', diameter)\n",
    "    \n",
    "    '''\n",
    "    # These rules turn out to be superfluous since basically all 3D works can be identified by their type\n",
    "    if diameter != '':\n",
    "        dimension = '3D'\n",
    "    if thickness != '':\n",
    "        dimension = '3D'\n",
    "    if height != '' and width == '': # some 3D objects just have height specified\n",
    "        dimension = '3D'\n",
    "    '''\n",
    "    \n",
    "    for type in known_3d_types:\n",
    "        if type['qid'] == work['instance_of']:\n",
    "            work_dict['type'] = type['name']\n",
    "            work_dict['dimension'] = '3D'\n",
    "    for type in known_2d_types:\n",
    "        if type['qid'] == work['instance_of']:\n",
    "            work_dict['type'] = type['name']\n",
    "            work_dict['dimension'] = '2D'\n",
    "    for type in known_poster_types:\n",
    "        if type['qid'] == work['instance_of']:\n",
    "            work_dict['type'] = type['name']\n",
    "            work_dict['dimension'] = 'poster'\n",
    "    for type in ambiguous_types:\n",
    "        if type['qid'] == work['instance_of']:\n",
    "            work_dict['type'] = type['name']\n",
    "            work_dict['dimension'] = 'ambiguous'\n",
    "    \n",
    "    #print()\n",
    "\n",
    "    works_list.append(work_dict)\n",
    "#print(json.dumps(works_list, indent = 2))\n",
    "fieldnames = ['qid', 'dimension', 'type', 'label']\n",
    "write_dicts_to_csv(works_list, 'works_classification.csv', fieldnames)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
