{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# google_cloud_vision.ipynb, a Jupyter notebook for analyzing images using the Google Cloud Vision API\n",
    "version = '0.1.0'\n",
    "created = '2023-03-27'\n",
    "\n",
    "# (c) 2023 Vanderbilt University. This program is released under a GNU General Public License v3.0 http://www.gnu.org/licenses/gpl-3.0\n",
    "# Author: Steve Baskauf\n",
    "# For more information, see https://github.com/HeardLibrary/linked-data/tree/master/image_analysis\n",
    "\n",
    "# This script carries out three major tasks:\n",
    "# 1. It downloads images from the Vanderbilt University Libraries IIIF server at a standard resolution of 1000 pixels in \n",
    "# the shortest dimension. If the image is already smaller than 1000 pixels in the shortest dimension, then the image is\n",
    "# downloaded at the original resolution.\n",
    "# 2. It analyzes the images using the Google Cloud Vision API using the FACE_DETECTION, LABEL_DETECTION, OBJECT_LOCALIZATION, and\n",
    "# TEXT_DETECTION features. The results are saved to CSV files.\n",
    "# 3. Optionally, it can generate an IIIF annotation file for each image that can be used to display the results of the\n",
    "# object localization analysis of the image.\n",
    "\n",
    "# -----------------------------------------\n",
    "# Version 0.1.0 change notes (2023-03-27):\n",
    "# - Initial version\n",
    "# -----------------------------------------\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "import shutil # high-level file operations\n",
    "from PIL import Image\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "computer_username = 'baskausj' # baskauf for home computer, baskausj for work computer\n",
    "\n",
    "# Load the image data into a dataframe\n",
    "base_path = '/Users/'+ computer_username + '/github/vandycite/gallery_buchanan/image_analysis/'\n",
    "download_path = '/Users/'+ computer_username + '/Downloads/'\n",
    "metadata_path = '/Users/'+ computer_username + '/github/vandycite/gallery_buchanan/'\n",
    "past_upload_metadata_path = '/Users/'+ computer_username + '/github/vandycite/gallery_works/image_upload/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determine which images are prints that have not yet been analyzed.\n",
    "\n",
    "Our priority as of 2023-09-15 is to start analyzing works that might have text on them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the metadata on all works\n",
    "metadata_df = pd.read_csv(metadata_path + 'works_multiprop.csv', dtype=str, na_filter=False)\n",
    "# Set the index to the work qid\n",
    "metadata_df = metadata_df.set_index('qid')\n",
    "\n",
    "# Load the works classification metadata\n",
    "classification_df = pd.read_csv(metadata_path + 'works_classification.csv', dtype=str, na_filter=False)\n",
    "# Set the index to the work qid\n",
    "classification_df = classification_df.set_index('qid')\n",
    "\n",
    "# Join the two dataframes based on the qid value\n",
    "metadata_df = metadata_df.join(classification_df, how='left')\n",
    "\n",
    "# Filter for rows that have a value of \"print\" in the \"type\" column\n",
    "metadata_df = metadata_df[metadata_df['type'] == 'print']\n",
    "\n",
    "# Create a list of accession numbers from the inventory_number column\n",
    "accession_list = metadata_df['inventory_number'].tolist()\n",
    "\n",
    "\n",
    "# The combined_images.csv file is a list of all images that have been downloaded from the IIIF server for analysis.\n",
    "\n",
    "# Load the combined_images.csv file\n",
    "combined_images_df = pd.read_csv(base_path + 'combined_images.csv', dtype=str, na_filter=False)\n",
    "# Creat a list of accession_number values\n",
    "combined_images_list = combined_images_df['accession_number'].tolist()\n",
    "\n",
    "# Create a list of accession numbers that have not yet been downloaded\n",
    "download_list = [x for x in accession_list if x not in combined_images_list]\n",
    "\n",
    "print(len(download_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to sort through the image metadata to figure out which of these accessioned works have images that have been classified as \"primary\" (i.e. they have a single \"best\" image suitable for putting into the IIIF server). As of 2023-09-15, not all of the classifying has been finished, but most of the unclassified ones were either 3D works or works that are parts of sets. So there should be at least a thousand that are ready to do now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the data for all images (not just the downloaded ones).\n",
    "all_image_data_df = pd.read_csv(past_upload_metadata_path + 'images_use_combined_images.csv', dtype=str, na_filter=False)\n",
    "all_image_data_df.head()\n",
    "\n",
    "# Loop through the list of download accession numbers and pull the rows from the all_image_data_df dataframe\n",
    "# that match the accession number. The local_identifier column in the all_image_data_df dataframe has the accession number.\n",
    "\n",
    "# Filter the all_image_data_df dataframe to only include rows that have a value of \"primary\" in the rank column.\n",
    "all_image_data_df = all_image_data_df[all_image_data_df['rank'] == 'primary']\n",
    "\n",
    "# Create an empty dataframe to hold the rows pulled from the all_image_data_df dataframe. Use the same column names.\n",
    "download_image_data_df = pd.DataFrame(columns=all_image_data_df.columns)\n",
    "\n",
    "for accession_number in download_list:\n",
    "    # Get the rows from the all_image_data_df dataframe that match the accession number\n",
    "    download_image_data_df = download_image_data_df.append(all_image_data_df.loc[all_image_data_df['local_identifier'] == accession_number], ignore_index=True)\n",
    "\n",
    "print(len(download_image_data_df))\n",
    "\n",
    "# Save the download_image_data_df dataframe to a CSV file\n",
    "download_image_data_df.to_csv(base_path + 'download_image_data.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to copy these images to another directory so that they can be uploaded to the IIIF server. If they are TIFFs, they should be converted to tiled pyramidal tiffs first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_magick_convert_tiff(in_path: str, out_path: str, log_path: str):\n",
    "    \"\"\"Convert a TIFF file to a pyramidal tiled TIFF file using ImageMagick.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    in_path: path to the input TIFF file\n",
    "    out_path: path to the output pyramidal tiled TIFF file\n",
    "    log_path: path to a text log file to which warnings, errors, and other output will be appended.\n",
    "\n",
    "    Based on practical experience, most errors can be ignored.\"\"\"\n",
    "    # Note: need to enclose file paths in quotes because filenames sometimes include spaces.\n",
    "    # 256x256 is the tile size and seems to be a good choice for IIIF viewers. \n",
    "    # The depth 8 setting should not be changed as it is necessary to view all of the tiles in Gimp and \n",
    "    # for preview to work properly in Mac Preview.\n",
    "    # The number of images included in the pyramid will depend on the filesize of the original image.\n",
    "    command_string = 'convert \"' + in_path + '\" -define tiff:tile-geometry=256x256 -depth 8 ptif:\"' + out_path + '\" 2>> \"' + log_path + '\"'\n",
    "    #print(command_string)\n",
    "    os.system(command_string)\n",
    "\n",
    "# Set the path to the directory containing the TIFF files to be converted\n",
    "# NOTE: this script will convert all TIFF files in the directory, and will ignore other filetypes. \n",
    "# The path should end with a slash.\n",
    "in_base_dir = '/Users/baskausj/gallery_digital_image_archive/'\n",
    "\n",
    "# Set the path to the directory where the converted files will be written. The path should end with a slash.\n",
    "out_dir = '/Users/baskausj/gallery_pyramidal_tiffs/prints_analysis/'\n",
    "\n",
    "# Open the download_image_data.csv file\n",
    "download_image_data_df = pd.read_csv(base_path + 'download_image_data.csv', dtype=str, na_filter=False)\n",
    "\n",
    "# Set the path to the log file\n",
    "log_path = out_dir + 'log.txt'\n",
    "\n",
    "\n",
    "# Loop through the files download_image_data_df dataframe\n",
    "for index, row in download_image_data_df.iterrows():\n",
    "    # Get the filename from the local_filename column\n",
    "    filename = row['local_filename']\n",
    "    print(index, filename)\n",
    "\n",
    "    # Set the path to the input file\n",
    "    in_path = in_base_dir + row['subdir'] + '/' + row['local_filename']\n",
    "    # Set the path to the output file\n",
    "    out_path = out_dir + filename\n",
    "    # Write the file name to the log file\n",
    "    with open(log_path, 'a') as log_file:\n",
    "        log_file.write(filename + '\\n')\n",
    "\n",
    "    # Check to see if the file is a TIFF file\n",
    "    if filename.lower().endswith('.tif') or filename.lower().endswith('.tiff'):\n",
    "        # Convert the file\n",
    "        image_magick_convert_tiff(in_path, out_path, log_path)\n",
    "    else:\n",
    "        # Copy the file\n",
    "        shutil.copyfile(in_path, out_path)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Completing analysis of all images (2024-03-19)\n",
    "\n",
    "It doesn't look like I was careful about consistently saving the same files that record which images have been analyzed. However, there are two files that contain the accession numbers of images that have been done: `combined_images.csv` contains the round 1 data and `accession_dimensions.csv` contains the round 2 data. So the accession numbers in those files should be excluded from the final analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the combined_images.csv file\n",
    "round1_df = pd.read_csv(base_path + 'combined_images.csv', dtype=str, na_filter=False)\n",
    "\n",
    "# Load the accession_dimensions.csv file\n",
    "round2_df = pd.read_csv(base_path + 'accession_dimensions.csv', dtype=str, na_filter=False)\n",
    "\n",
    "# Create a list of accession numbers from the accession_number column of the two dataframes\n",
    "accession_number_list = round1_df['accession_number'].tolist() + round2_df['accession_number'].tolist()\n",
    "\n",
    "# Get the data for all images (not just the downloaded ones).\n",
    "all_image_data_df = pd.read_csv(past_upload_metadata_path + 'images_use_combined_images.csv', dtype=str, na_filter=False)\n",
    "all_image_data_df.head()\n",
    "\n",
    "# The local_identifier column in the all_image_data_df dataframe has the accession number.\n",
    "\n",
    "# Filter the all_image_data_df dataframe to only include rows that have a value of \"primary\" in the rank column.\n",
    "all_image_data_df = all_image_data_df[all_image_data_df['rank'] == 'primary']\n",
    "\n",
    "# Create an empty dataframe to hold the rows pulled from the all_image_data_df dataframe. Use the same column names.\n",
    "download_image_data_df = pd.DataFrame(columns=all_image_data_df.columns)\n",
    "\n",
    "for accession_number in accession_number_list:\n",
    "    # remove the row with the accession_number from the DataFrame\n",
    "    all_image_data_df = all_image_data_df[all_image_data_df.local_identifier != accession_number]\n",
    "    \n",
    "print(len(all_image_data_df))\n",
    "\n",
    "# Save the download_image_data_df dataframe to a CSV file\n",
    "download_image_data_df.to_csv(base_path + 'images_to_process.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the images to a local directory\n",
    "\n",
    "NOTE: this only needs to be done once for a given set of images. Once they are loaded into the bucket it doesn't need to be run again.\n",
    "\n",
    "This code uses a list of accession numbers (found as a column in a CSV file) to generate IIIF Image API (v2) URLs for JPEG images that are 1000 pixels in the shortest dimension, then download them into a local directory.\n",
    "\n",
    "After generating and downloading the images, they need to be uploaded to the Google Cloud bucket used in the Vision analysis.\n",
    "\n",
    "NOTE: There are two versions of this code. Only one or the other of the following cells should be run!\n",
    "\n",
    "The *first version* is for images that were already loaded into the IIIF server during the Commons upload and therefore they had manifests already generated. \n",
    "\n",
    "The *second version* also generates smaller JPEGs using the Image API, but gets its metadata from the `download_image_data.csv` file output from a cell above rather than from manifests. The files are in a single directory rather than in directories organized by year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!!\n",
    "# First version\n",
    "# !!!!!!!!!!!!!\n",
    "\n",
    "# Load the source image data into a dataframe\n",
    "source_image_dataframe = pd.read_csv(base_path + 'combined_images.csv', dtype=str)\n",
    "# Set the commons_id column as the index\n",
    "source_image_dataframe = source_image_dataframe.set_index('commons_id')\n",
    "\n",
    "#source_image_dataframe.head()\n",
    "\n",
    "# Import CSV data as a dataframe.\n",
    "accession_dataframe = pd.read_csv(base_path + 'accession_numbers_to_analyze.csv', dtype=str)\n",
    "\n",
    "# Test with a single row\n",
    "#accession_dataframe = accession_dataframe.head(1)\n",
    "\n",
    "\n",
    "# Create a dataframe to hold the accession numbers and dimensions\n",
    "accession_dimensions_dataframe = pd.DataFrame(columns=['accession_number', 'height', 'width'])\n",
    "\n",
    "bad_image_list = []\n",
    "\n",
    "# Loop through the dataframe rows and download the images.\n",
    "for index, row in accession_dataframe.iterrows():\n",
    "    accession_number = row['accession_number']\n",
    "    print(accession_number)\n",
    "    \n",
    "    # Look up the image data in the source image dataframe.\n",
    "    # In cases where there are two images, we want the primary image.\n",
    "    image_series = source_image_dataframe.loc[(source_image_dataframe['accession_number'] == accession_number) & (source_image_dataframe['rank'] == 'primary')]\n",
    "    manifest_url = image_series['iiif_manifest'][0]\n",
    "\n",
    "    # get the manifest from the manifest url\n",
    "    manifest = requests.get(manifest_url).json()\n",
    "    #print(json.dumps(manifest, indent=2))\n",
    "    service_url = manifest['sequences'][0]['canvases'][0]['images'][0]['resource']['service']['@id']\n",
    "    # Because of the error in original manifests, replace version 3 with version 2 in the URL.\n",
    "    #service_url = service_url.replace('/3/', '/2/') # This is no longer needed because the manifests have been fixed.\n",
    "    #print('service_url', service_url)\n",
    "\n",
    "    # Determine the maximum and minimum dimensions of the image.\n",
    "    height = image_series['height'][0]\n",
    "    #print('height', height)\n",
    "    width = image_series['width'][0]\n",
    "    #print('width', width)\n",
    "    shortest_dimension = min(int(height), int(width))\n",
    "    longest_dimension = max(int(height), int(width))\n",
    "    #print('shortest_dimension', shortest_dimension)\n",
    "\n",
    "    # We want to know what the largest dimension needs to be for the shortest dimension to be 1000 pixels.\n",
    "    # If that calculation makes the longest dimension longer than the actual longest dimension, \n",
    "    # then we want to use the actual longest dimension.\n",
    "    # If the shortest dimension is already less than 1000 pixels, then we will just use the longest dimension as is.\n",
    "    if shortest_dimension > 1000:\n",
    "        size = int(1000 * (longest_dimension / shortest_dimension))\n",
    "        if size > longest_dimension:\n",
    "            size = longest_dimension\n",
    "    else:\n",
    "        size = longest_dimension\n",
    "    #print('size', size)\n",
    "\n",
    "    # construct the image url using the \"!\" size option, that keeps the aspect ratio but sizes to the maximum dimension.\n",
    "    image_url = service_url + '/full/!' + str(size) + ',' + str(size) + '/0/default.jpg'\n",
    "    print('image_url', image_url)\n",
    "    print()\n",
    "        \n",
    "    # retrieve the image from the IIIF server\n",
    "    image_object = requests.get(image_url, stream=True).raw\n",
    "\n",
    "    # Save the image to a file\n",
    "    with open(download_path + 'google_vision_images/' + accession_number + '.jpg', 'wb') as out_file:\n",
    "        shutil.copyfileobj(image_object, out_file)\n",
    "        # Force the file to be written to disk\n",
    "        out_file.flush()\n",
    "    \n",
    "    # Find the image dimensions\n",
    "    # Open the image file from disk\n",
    "    with open(download_path + 'google_vision_images/' + accession_number + '.jpg', 'rb') as image_file:\n",
    "        reduced_width = 0\n",
    "        reduced_height = 0\n",
    "        try:\n",
    "            image = Image.open(image_file)\n",
    "            reduced_width, reduced_height = image.size\n",
    "            #print('reduced_width', reduced_width)\n",
    "            #print('reduced_height', reduced_height)\n",
    "        except:\n",
    "            print('bad image')\n",
    "            bad_image_list.append(accession_number)\n",
    "            continue\n",
    "    \n",
    "    # Display the image\n",
    "    #image.show()\n",
    "\n",
    "    # Add the accession number and dimensions to the dataframe\n",
    "    accession_dimensions_dataframe = accession_dimensions_dataframe.append({'accession_number': accession_number, 'max_height': height, 'max_width': width, 'height': reduced_height, 'width': reduced_width}, ignore_index=True)\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "accession_dimensions_dataframe.to_csv(base_path + 'accession_dimensions.csv', index=False)\n",
    "\n",
    "print('bad_image_list', bad_image_list)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!!!!!!!!!!!\n",
    "# Second version\n",
    "# !!!!!!!!!!!!!\n",
    "\n",
    "# Load the source image data into a dataframe\n",
    "source_image_dataframe = pd.read_csv(base_path + 'download_image_data.csv', dtype=str, na_filter=False)\n",
    "\n",
    "# Test with a single row\n",
    "#source_image_dataframe = source_image_dataframe.head(1)\n",
    "\n",
    "\n",
    "# Create a dataframe to hold the accession numbers and dimensions\n",
    "accession_dimensions_dataframe = pd.DataFrame(columns=['accession_number', 'height', 'width'])\n",
    "\n",
    "bad_image_list = []\n",
    "\n",
    "# Loop through the dataframe rows and download the images.\n",
    "for index, row in source_image_dataframe.iterrows():\n",
    "    accession_number = row['local_identifier']\n",
    "    print(accession_number)\n",
    "\n",
    "    # Determine the maximum and minimum dimensions of the image.\n",
    "    height = row['height']\n",
    "    #print('height', height)\n",
    "    width = row['width']\n",
    "    #print('width', width)\n",
    "    shortest_dimension = min(int(height), int(width))\n",
    "    longest_dimension = max(int(height), int(width))\n",
    "    #print('shortest_dimension', shortest_dimension)\n",
    "\n",
    "    # We want to know what the largest dimension needs to be for the shortest dimension to be 1000 pixels.\n",
    "    # If that calculation makes the longest dimension longer than the actual longest dimension, \n",
    "    # then we want to use the actual longest dimension.\n",
    "    # If the shortest dimension is already less than 1000 pixels, then we will just use the longest dimension as is.\n",
    "    if shortest_dimension > 1000:\n",
    "        size = int(1000 * (longest_dimension / shortest_dimension))\n",
    "        if size > longest_dimension:\n",
    "            size = longest_dimension\n",
    "    else:\n",
    "        size = longest_dimension\n",
    "    #print('size', size)\n",
    "\n",
    "    # Construct the service_url from the image API root URL, the storage directory path, and the filename.\n",
    "    service_url = 'https://iiif.library.vanderbilt.edu/iiif/2/gallery%2Fanalysis%2F' + row['local_filename']\n",
    "\n",
    "    # construct the image url using the \"!\" size option, that keeps the aspect ratio but sizes to the maximum dimension.\n",
    "    image_url = service_url + '/full/!' + str(size) + ',' + str(size) + '/0/default.jpg'\n",
    "    print('image_url', image_url)\n",
    "    print()\n",
    "        \n",
    "    # retrieve the image from the IIIF server\n",
    "    image_object = requests.get(image_url, stream=True).raw\n",
    "\n",
    "    # Save the image to a file\n",
    "    with open(download_path + 'google_vision_images/' + accession_number + '.jpg', 'wb') as out_file:\n",
    "        shutil.copyfileobj(image_object, out_file)\n",
    "        # Force the file to be written to disk\n",
    "        out_file.flush()\n",
    "    \n",
    "    # Find the image dimensions\n",
    "    # Open the image file from disk\n",
    "    with open(download_path + 'google_vision_images/' + accession_number + '.jpg', 'rb') as image_file:\n",
    "        reduced_width = 0\n",
    "        reduced_height = 0\n",
    "        try:\n",
    "            image = Image.open(image_file)\n",
    "            reduced_width, reduced_height = image.size\n",
    "            #print('reduced_width', reduced_width)\n",
    "            #print('reduced_height', reduced_height)\n",
    "        except:\n",
    "            print('bad image')\n",
    "            bad_image_list.append(accession_number)\n",
    "            continue\n",
    "    \n",
    "    # Display the image\n",
    "    #image.show()\n",
    "\n",
    "    # Add the accession number and dimensions to the dataframe\n",
    "    accession_dimensions_dataframe = accession_dimensions_dataframe.append({'accession_number': accession_number, 'max_height': height, 'max_width': width, 'height': reduced_height, 'width': reduced_width}, ignore_index=True)\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "accession_dimensions_dataframe.to_csv(base_path + 'accession_dimensions.csv', index=False)\n",
    "\n",
    "print('bad_image_list', bad_image_list)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Cloud Vision image analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is part of google_cloud_vision.ipynb\n",
    "# For licensing and other information, see https://github.com/HeardLibrary/linked-data/tree/master/image_analysis\n",
    "\n",
    "# Here's the landing page for Google Cloud Vision\n",
    "# https://cloud.google.com/vision/\n",
    "# From it you can try the api by dragging and dropping an image into the browser. You can then \n",
    "# view the JSON response, which was helpful at first to understand the structure of the response.\n",
    "\n",
    "# The following tutorial contains critical information about enabling the API and creating a role\n",
    "# for the service account to allow it access. This is followed by creating a service account key.\n",
    "# https://cloud.google.com/vision/docs/detect-labels-image-client-libraries\n",
    "\n",
    "# I didn't actually do this tutorial, but it was useful to understand the order of operations that\n",
    "# needed to be done prior to writing to the API.\n",
    "# https://www.cloudskillsboost.google/focuses/2457?parent=catalog&utm_source=vision&utm_campaign=cloudapi&utm_medium=webpage\n",
    "# Because I'm using the Python client library, the part about setting up the request body was irrelevant. \n",
    "# But the stuff about uploading the files to the bucket, making it publicly accessible, etc. was helpful.\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Tuple, Any, Optional\n",
    "\n",
    "# Imports the Google Cloud client library\n",
    "# Reference for Google Cloud Vision Python client https://cloud.google.com/python/docs/reference/vision/latest\n",
    "from google.cloud import vision\n",
    "from google.cloud import vision_v1\n",
    "from google.cloud.vision_v1 import AnnotateImageResponse\n",
    "\n",
    "# Import from Google oauth library\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "def extract_object_localization_data(accession_number: str, annotation: List[Dict[str, Any]], width: int, height: int) -> Dict[str, Any]:\n",
    "    \"\"\"Extract the object localization data from a hit and turn it into a dict to be added as a row in the dataframe.\"\"\"\n",
    "    #print('annotation', annotation)\n",
    "    description = annotation['name']\n",
    "    score = annotation['score']\n",
    "    left_x = annotation['boundingPoly']['normalizedVertices'][0]['x']\n",
    "    top_y = annotation['boundingPoly']['normalizedVertices'][0]['y']\n",
    "    right_x = annotation['boundingPoly']['normalizedVertices'][2]['x']\n",
    "    bottom_y = annotation['boundingPoly']['normalizedVertices'][2]['y']\n",
    "    #print('description', description)\n",
    "    #print('score', score)\n",
    "    #print('left_x', left_x)\n",
    "    #print('top_y', top_y)\n",
    "    #print('right_x', right_x)\n",
    "    #print('bottom_y', bottom_y)\n",
    "    #print()\n",
    "\n",
    "    row = {'accession_number': accession_number, 'description': description, 'score': score, 'rel_left_x': left_x, 'rel_right_x': right_x, 'rel_top_y': top_y, 'rel_bottom_y': bottom_y, 'abs_left_x': round(left_x * width), 'abs_right_x': round(right_x * width), 'abs_top_y': round(top_y * height), 'abs_bottom_y': round(bottom_y * height)}\n",
    "    return row\n",
    "\n",
    "def extract_face_detection_data(accession_number: str, annotation: List[Dict[str, Any]], width: int, height: int) -> Dict[str, Any]:\n",
    "    \"\"\"Extract the face detection data from a hit and turn it into a dict to be added as a row in the dataframe.\"\"\"\n",
    "    score = annotation['detectionConfidence']\n",
    "    left_x = annotation['boundingPoly']['vertices'][0]['x']\n",
    "    top_y = annotation['boundingPoly']['vertices'][0]['y']\n",
    "    right_x = annotation['boundingPoly']['vertices'][2]['x']\n",
    "    bottom_y = annotation['boundingPoly']['vertices'][2]['y']\n",
    "    roll_angle = annotation['rollAngle']\n",
    "    pan_angle = annotation['panAngle']\n",
    "    tilt_angle = annotation['tiltAngle']\n",
    "    landmarking_confidence = annotation['landmarkingConfidence']\n",
    "    joy_likelihood = annotation['joyLikelihood']\n",
    "    sorrow_likelihood = annotation['sorrowLikelihood']\n",
    "    anger_likelihood = annotation['angerLikelihood']\n",
    "    surprise_likelihood = annotation['surpriseLikelihood']\n",
    "    under_exposed_likelihood = annotation['underExposedLikelihood']\n",
    "    blurred_likelihood = annotation['blurredLikelihood']\n",
    "    headwear_likelihood = annotation['headwearLikelihood']\n",
    "\n",
    "    row = {'accession_number': accession_number, 'score': score, \n",
    "           'rel_left_x': left_x / width, 'rel_right_x': right_x / width, 'rel_top_y': top_y / height, 'rel_bottom_y': bottom_y /height,\n",
    "           'abs_left_x': left_x, 'abs_right_x': right_x, 'abs_top_y': top_y, 'abs_bottom_y': bottom_y,\n",
    "           'roll_angle': roll_angle, 'pan_angle': pan_angle, 'tilt_angle': tilt_angle,\n",
    "           'landmarking_confidence': landmarking_confidence, 'joy_likelihood': joy_likelihood, \n",
    "           'sorrow_likelihood': sorrow_likelihood, 'anger_likelihood': anger_likelihood, \n",
    "           'surprise_likelihood': surprise_likelihood, 'under_exposed_likelihood': under_exposed_likelihood,\n",
    "           'blurred_likelihood': blurred_likelihood, 'headwear_likelihood': headwear_likelihood}\n",
    "    return row\n",
    "\n",
    "def extract_label_detection_data(accession_number: str, annotation: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "    \"\"\"Extract the label detection data from a hit and turn it into a dict to be added as a row in the dataframe.\"\"\"\n",
    "    mid = annotation['mid']\n",
    "    description = annotation['description']\n",
    "    score = annotation['score']\n",
    "    topicality = annotation['topicality']\n",
    "    row = {'accession_number': accession_number, 'mid': mid, 'description': description, 'score': score, 'topicality': topicality}\n",
    "    return row\n",
    "\n",
    "def extract_text_detection_data(accession_number: str, annotation: List[Dict[str, Any]], width: int, height: int) -> Dict[str, Any]:\n",
    "    \"\"\"Extract the text detection data from a hit and turn it into a dict to be added as a row in the dataframe.\"\"\"\n",
    "    locale = annotation['locale']\n",
    "    description = annotation['description']\n",
    "    left_x = annotation['boundingPoly']['vertices'][0]['x']\n",
    "    top_y = annotation['boundingPoly']['vertices'][0]['y']\n",
    "    right_x = annotation['boundingPoly']['vertices'][2]['x']\n",
    "    bottom_y = annotation['boundingPoly']['vertices'][2]['y']\n",
    "    row = {'accession_number': accession_number, 'locale': locale, 'description': description, \n",
    "           'rel_left_x': left_x / width, 'rel_right_x': right_x / width, 'rel_top_y': top_y / height, 'rel_bottom_y': bottom_y / height,\n",
    "           'abs_left_x': left_x, 'abs_right_x': right_x, 'abs_top_y': top_y, 'abs_bottom_y': bottom_y,\n",
    "           }\n",
    "    return row\n",
    "\n",
    "# Customize for your own computer\n",
    "user_dir = 'baskausj' # Enter your user directory name here\n",
    "base_path = '/Users/baskausj/github/vandycite/gallery_buchanan/image_analysis/' # Location of the accession number data file\n",
    "annotations_base_url = 'https://baskaufs.github.io/iiif/baskauf/'\n",
    "\n",
    "# Set the path to the service account key\n",
    "key_path = '/Users/' + user_dir + '/image-analysis-376619-193859a33600.json'\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Retrieve the service key, create a credentials object, then use it to authenticate and create a `client` object.\n",
    "# ---------------------------------------------\n",
    "\n",
    "# Create a credentials object from the service account key\n",
    "credentials = service_account.Credentials.from_service_account_file(\n",
    "    key_path, scopes=[\"https://www.googleapis.com/auth/cloud-platform\"],\n",
    ")\n",
    "\n",
    "# API documentation https://cloud.google.com/python/docs/reference/vision/latest/google.cloud.vision_v1.services.image_annotator.ImageAnnotatorClient#methods\n",
    "# The first two versions have no arguments and the credentials are loaded from the environment variable.\n",
    "#client = vision.ImageAnnotatorClient()\n",
    "# Used this specific v1 to get the JSON conversion to work\n",
    "#client = vision_v1.ImageAnnotatorClient()\n",
    "# Use this line instead of the one above to load the credentials directly from the file\n",
    "client = vision_v1.ImageAnnotatorClient(credentials=credentials)\n",
    "\n",
    "# Load the source data from a CSV. The critical column needed here is the `accession_number` column, since it is the one \n",
    "# that was used to construct the image file name for the uploaded test images.\n",
    "accession_dataframe = pd.read_csv(base_path + 'accession_dimensions.csv', dtype=str)\n",
    "#accession_dataframe.head()\n",
    "\n",
    "# -----------------------------------\n",
    "# Loop through all of the accession numbers and perform the analysis on each of the images.\n",
    "# -----------------------------------\n",
    "\n",
    "# I did these 100 at a time and stored the intermediate results in case I had to stop or if the process was interrupted.\n",
    "\n",
    "\n",
    "# Create a new dataframe to hold the annotations\n",
    "object_localization_dataframe = pd.DataFrame(columns=['accession_number', 'description', 'score', 'rel_left_x', 'rel_right_x', 'rel_top_y', 'rel_bottom_y', 'abs_left_x', 'abs_right_x', 'abs_top_y', 'abs_bottom_y'])\n",
    "face_detection_dataframe = pd.DataFrame(columns=['accession_number', 'score', 'rel_left_x', 'rel_right_x', 'rel_top_y', 'rel_bottom_y', 'abs_left_x', 'abs_right_x', 'abs_top_y', 'abs_bottom_y', 'roll_angle', 'pan_angle', 'tilt_angle', 'landmarking, confidence', 'joy_likelihood', 'sorrow_likelihood', 'anger_likelihood', 'surprise_likelihood', 'under_exposed_likelihood', 'blurred_likelihood', 'headwear_likelihood'])\n",
    "label_detection_dataframe = pd.DataFrame(columns=['accession_number', 'mid', 'description', 'score', 'topicality'])\n",
    "text_detection_dataframe = pd.DataFrame(columns=['accession_number', 'locale', 'description', 'rel_left_x', 'rel_right_x', 'rel_top_y', 'rel_bottom_y', 'abs_left_x', 'abs_right_x', 'abs_top_y', 'abs_bottom_y'])\n",
    "\n",
    "# Loop through the dataframe rows and analyze the images.\n",
    "for index, row in accession_dataframe.iterrows():\n",
    "    accession_number = row['accession_number']\n",
    "    print('accession_number', accession_number)\n",
    "    width = int(row['width'])\n",
    "    height = int(row['height'])\n",
    "\n",
    "    # To access the images, they should be stored in a Google Cloud Storage bucket that is set up for public access.\n",
    "    # It's also possible to use a publicly accessible URL, but that seems to be unreliable.\n",
    "    # The storage costs for a few images are negligible.\n",
    "\n",
    "    # Construct the path to the image file\n",
    "    image_uri = 'gs://vu-gallery/' + accession_number + '.jpg'\n",
    "    #print('image_uri', image_uri)\n",
    "    \n",
    "    # Here is the API documentation for the Feature object.\n",
    "    # https://cloud.google.com/vision/docs/reference/rest/v1/Feature\n",
    "    #analysis_type = vision.Feature.Type.FACE_DETECTION\n",
    "    #analysis_type = vision.Feature.Type.LABEL_DETECTION\n",
    "    #analysis_type = vision.Feature.Type.OBJECT_LOCALIZATION\n",
    "\n",
    "    # This API documentation isn't exactly the one for the .annotate_image method, but it's close enough.\n",
    "    # https://cloud.google.com/vision/docs/reference/rest/v1/projects.images/annotate\n",
    "    # In particular, it links to the AnnotateImageRequest object, which is what we need to pass to the annotate_image method.\n",
    "    response = client.annotate_image({\n",
    "    'image': {'source': {'image_uri': image_uri}},\n",
    "    'features': [\n",
    "        {'type_': vision.Feature.Type.OBJECT_LOCALIZATION},\n",
    "        {'type_': vision.Feature.Type.FACE_DETECTION},\n",
    "        {'type_': vision.Feature.Type.LABEL_DETECTION},\n",
    "        {'type_': vision.Feature.Type.TEXT_DETECTION}  \n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # The API response is a protobuf object, which is not JSON serializable.\n",
    "    # So we need to convert it to a JSON serializable object.\n",
    "    # Solution from https://stackoverflow.com/a/65728119\n",
    "    response_json = AnnotateImageResponse.to_json(response)\n",
    "\n",
    "    # The structure of the response is detailed in the API documentation here:\n",
    "    # https://cloud.google.com/vision/docs/reference/rest/v1/AnnotateImageResponse\n",
    "    # The various bits are detailed for each feature type.\n",
    "    # Here's the documentation for entity annotations, with a link to the BoundingPoly object.\n",
    "    # https://cloud.google.com/vision/docs/reference/rest/v1/AnnotateImageResponse#EntityAnnotation\n",
    "    response_struct = json.loads(response_json)\n",
    "\n",
    "    # Object localization\n",
    "    # -------------------\n",
    "\n",
    "    for annotation in response_struct['localizedObjectAnnotations']:\n",
    "        row = extract_object_localization_data(accession_number, annotation, width, height)\n",
    "        object_localization_dataframe = object_localization_dataframe.append(row, ignore_index=True)\n",
    "    \n",
    "    # Write the annotations to a CSV file after every image in case the process is interrupted.\n",
    "    object_localization_dataframe.to_csv(base_path + 'object_localization.csv', index=False)\n",
    "    \n",
    "    # Face detection\n",
    "    # --------------\n",
    "    '''\n",
    "    analysis_type = vision.Feature.Type.FACE_DETECTION\n",
    "    response = client.annotate_image({\n",
    "    'image': {'source': {'image_uri': image_uri}},\n",
    "    'features': [{'type_': analysis_type}]\n",
    "    })\n",
    "    response_json = AnnotateImageResponse.to_json(response)\n",
    "    response_struct = json.loads(response_json)\n",
    "    '''\n",
    "    for annotation in response_struct['faceAnnotations']:\n",
    "        row = extract_face_detection_data(accession_number, annotation, width, height)\n",
    "        face_detection_dataframe = face_detection_dataframe.append(row, ignore_index=True)\n",
    "    \n",
    "    # Write the annotations to a CSV file after every image in case the process is interrupted.\n",
    "    face_detection_dataframe.to_csv(base_path + 'face_detection.csv', index=False)\n",
    "    \n",
    "    # Label detection\n",
    "    # ---------------\n",
    "    '''\n",
    "    analysis_type = vision.Feature.Type.LABEL_DETECTION\n",
    "    response = client.annotate_image({\n",
    "    'image': {'source': {'image_uri': image_uri}},\n",
    "    'features': [{'type_': analysis_type}]\n",
    "    })\n",
    "    response_json = AnnotateImageResponse.to_json(response)\n",
    "    response_struct = json.loads(response_json)\n",
    "    # print(json.dumps(response_struct, indent=2))\n",
    "    '''\n",
    "    for annotation in response_struct['labelAnnotations']:\n",
    "        row = extract_label_detection_data(accession_number, annotation)\n",
    "        label_detection_dataframe = label_detection_dataframe.append(row, ignore_index=True)\n",
    "    \n",
    "    # Write the annotations to a CSV file after every image in case the process is interrupted.\n",
    "    label_detection_dataframe.to_csv(base_path + 'label_detection.csv', index=False)\n",
    "    \n",
    "    # Text detection\n",
    "    # --------------\n",
    "    '''\n",
    "    analysis_type = vision.Feature.Type.TEXT_DETECTION\n",
    "    response = client.annotate_image({\n",
    "    'image': {'source': {'image_uri': image_uri}},\n",
    "    'features': [{'type_': analysis_type}]\n",
    "    })\n",
    "    response_json = AnnotateImageResponse.to_json(response)\n",
    "    response_struct = json.loads(response_json)\n",
    "    #print(json.dumps(response_struct, indent=2))\n",
    "    '''\n",
    "    for annotation in response_struct['textAnnotations']:\n",
    "        row = extract_text_detection_data(accession_number, annotation, width, height)\n",
    "        text_detection_dataframe = text_detection_dataframe.append(row, ignore_index=True)\n",
    "\n",
    "    # Write the annotations to a CSV file after every image in case the process is interrupted.\n",
    "    text_detection_dataframe.to_csv(base_path + 'text_detection.csv', index=False)\n",
    "\n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create IIIF annotation file\n",
    "\n",
    "To create the annotations, we need to convert the relative dimensions to the absolute pixel dimensions based on the canvas size.\n",
    "\n",
    "The canvas size is given as the dimensions of the full-sized image, which is reported as `max_height` and `max_width` in the dimensions CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is part of google_cloud_vision.ipynb\n",
    "# For licensing and other information, see https://github.com/HeardLibrary/linked-data/tree/master/image_analysis\n",
    "\n",
    "# object_localization.csv contains the results of the object localization analysis\n",
    "object_localization_dataframe = pd.read_csv(base_path + 'object_localization.csv')\n",
    "# accession_dimensions.csv is a temporary file that contains the dimensions of the full-size images (max_height and max_width)\n",
    "# as retrieved from the IIIF manifest\n",
    "accession_dataframe = pd.read_csv(base_path + 'accession_dimensions.csv', dtype=str)\n",
    "\n",
    "# Loop through each accession number and create an annotation for each localized object.\n",
    "for image_index, image_row in accession_dataframe.iterrows():\n",
    "    print('Processing image ' + str(image_index + 1) + ' of ' + str(len(accession_dataframe)))\n",
    "    # Build the resources list for the annotations.\n",
    "    resources = []\n",
    "    \n",
    "    # Loop through each object in the image.\n",
    "    for object_index, object_row in object_localization_dataframe.iterrows():\n",
    "        if object_row['accession_number'] != image_row['accession_number']:\n",
    "            continue\n",
    "\n",
    "        # Create a W3C fragment selector for the annotation.\n",
    "        # https://www.w3.org/TR/annotation-model/#fragment-selector\n",
    "        # Calculate the upper left x and y in absolute canvas coordinates.\n",
    "        x = str(round(object_row['rel_left_x'] * float(image_row['max_width'])))\n",
    "        y = str(round(object_row['rel_top_y'] * float(image_row['max_height'])))\n",
    "\n",
    "        # Calculate the width and height in absolute canvas coordinates.\n",
    "        width = str(round((object_row['rel_right_x'] - object_row['rel_left_x']) * float(image_row['max_width'])))\n",
    "        height = str(round((object_row['rel_bottom_y'] - object_row['rel_top_y']) * float(image_row['max_height'])))\n",
    "\n",
    "        fragment_selector = 'xywh=' + x + ',' + y + ',' + width + ',' + height\n",
    "\n",
    "        # Build the annotation.\n",
    "        on_value = {\n",
    "            '@type': 'oa:SpecificResource',\n",
    "            'full': 'https://iiif-manifest.library.vanderbilt.edu/gallery/' + image_row['accession_number'].split('.')[0] + '/' + image_row['accession_number'] + '.json_1',\n",
    "            'selector': {\n",
    "                'type': 'oa:FragmentSelector',\n",
    "                'value': fragment_selector\n",
    "            },\n",
    "            'within': {\n",
    "                '@id': 'https://iiif-manifest.library.vanderbilt.edu/gallery/' + image_row['accession_number'].split('.')[0] + '/' + image_row['accession_number'] + '.json',\n",
    "                '@type': 'sc:Manifest'\n",
    "            }\n",
    "        }\n",
    "        resource_value = {\n",
    "            '@type': 'dctypes:Text',\n",
    "            'format': 'text/plain',\n",
    "            'chars': object_row['description']\n",
    "        }\n",
    "\n",
    "        annotation = {\n",
    "            '@context': 'http://iiif.io/api/presentation/2/context.json',\n",
    "            '@id': 'https://iiif-manifest.library.vanderbilt.edu/gallery/' + image_row['accession_number'].split('.')[0] + '/' + image_row['accession_number'] + '/annotation/' + str(object_index),\n",
    "            '@type': 'oa:Annotation',\n",
    "            'motivation': [\n",
    "                'oa:commenting'\n",
    "            ],\n",
    "            'on': on_value,\n",
    "            'resource': [\n",
    "                resource_value\n",
    "            ]\n",
    "        }\n",
    "        resources.append(annotation)\n",
    "    \n",
    "    annotations = {\n",
    "        \"@context\": \"http://www.shared-canvas.org/ns/context.json\",\n",
    "        \"@id\": annotations_base_url + image_row['accession_number'].split('.')[0] + \"/\" + image_row['accession_number'] + \"_annotations.json\",\n",
    "        \"@type\": \"sc:AnnotationList\",\n",
    "        \"resources\": resources\n",
    "    }\n",
    "\n",
    "    # Write the annotations to a JSON file.\n",
    "    with open(base_path + 'annotations/' + image_row['accession_number'] + '_annotations.json', 'w') as outfile:\n",
    "        output_text = json.dumps(annotations, indent=2)\n",
    "        outfile.write(output_text)\n",
    "\n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add the link from the manifest to the annotation URL\n",
    "\n",
    "The annotations file can't be applied to the manifest unless the manifest has a link to it's web address. So an `otherContent` link must be added to the canvas that's being annotated. The link URL has to be a real URL that dereferences, since the annotations have to be retrieved on the fly when the viewer applies the annotations to the canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step through each image in the accession dimensions CSV file.\n",
    "for image_index, image_row in accession_dataframe.iterrows():\n",
    "    print(image_row['accession_number'])\n",
    "\n",
    "    # Look up the manifest URL for the image in the source image dataframe.\n",
    "    manifest_url = source_image_dataframe.loc[source_image_dataframe['accession_number'] == image_row['accession_number'], 'iiif_manifest'].iloc[0]\n",
    "    \n",
    "    # Get the manifest JSON.\n",
    "    manifest_response = requests.get(manifest_url)\n",
    "    manifest_json = manifest_response.json()\n",
    "    \n",
    "    # Create otherContent dictionary.\n",
    "    other_content = [\n",
    "        {\n",
    "        '@id': annotations_base_url + image_row['accession_number'].split('.')[0] + \"/\" + image_row['accession_number'] + \"_annotations.json\",\n",
    "        '@type': 'sc:AnnotationList'\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Add the otherContent dictionary to the manifest.\n",
    "    manifest_json['sequences'][0]['canvases'][0]['otherContent'] = other_content\n",
    "\n",
    "    # Write the manifest to a JSON file.\n",
    "    with open(base_path + 'manifests/' + image_row['accession_number'] + '.json', 'w') as outfile:\n",
    "        text = json.dumps(manifest_json, indent=4)\n",
    "        outfile.write(text)\n",
    "\n",
    "print('done')\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenate the DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "base_path = '/Users/baskausj/github/vandycite/gallery_buchanan/image_analysis/'\n",
    "\n",
    "# Get the directory names for the subdirectories of the output directory.\n",
    "subdirectories = os.listdir(base_path + 'output/')\n",
    "subdirectories.remove('.DS_Store')\n",
    "# Sort the subdirectories ascending.\n",
    "subdirectories.sort()\n",
    "\n",
    "# Create dataframes for each analysis type.\n",
    "face_detection_dataframe = pd.DataFrame()\n",
    "label_detection_dataframe = pd.DataFrame()\n",
    "text_detection_dataframe = pd.DataFrame()\n",
    "object_localization_dataframe = pd.DataFrame()\n",
    "\n",
    "# Loop through each subdirectory, load each CSV file, and concatenate the face_detection.csv files into a single CSV file.\n",
    "for subdirectory in subdirectories:\n",
    "    print('Processing subdirectory ' + subdirectory)\n",
    "    # Load the CSV files.\n",
    "    face_detection_dataframe = face_detection_dataframe.append(pd.read_csv(base_path + 'output/' + subdirectory + '/face_detection.csv', dtype=str))\n",
    "    label_detection_dataframe = label_detection_dataframe.append(pd.read_csv(base_path + 'output/' + subdirectory + '/label_detection.csv', dtype=str))\n",
    "    text_detection_dataframe = text_detection_dataframe.append(pd.read_csv(base_path + 'output/' + subdirectory + '/text_detection.csv', dtype=str))\n",
    "    object_localization_dataframe = object_localization_dataframe.append(pd.read_csv(base_path + 'output/' + subdirectory + '/object_localization.csv', dtype=str))\n",
    "\n",
    "# Write the concatenated CSV files.\n",
    "face_detection_dataframe.to_csv(base_path + 'output/face_detection.csv', index=False)\n",
    "label_detection_dataframe.to_csv(base_path + 'output/label_detection.csv', index=False)\n",
    "text_detection_dataframe.to_csv(base_path + 'output/text_detection.csv', index=False)\n",
    "object_localization_dataframe.to_csv(base_path + 'output/object_localization.csv', index=False)\n",
    "\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a table of detected object labels and the IIIF server URLs that would display the bounded regions\n",
    "\n",
    "This code is a hack of the `Create IIIF annotation file` cell in this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is part of google_cloud_vision.ipynb\n",
    "# For licensing and other information, see https://github.com/HeardLibrary/linked-data/tree/master/image_analysis\n",
    "\n",
    "# object_localization.csv contains the results of the object localization analysis\n",
    "object_localization_dataframe = pd.read_csv(base_path + 'object_localization.csv')\n",
    "# accession_dimensions.csv is a temporary file that contains the dimensions of the full-size images (max_height and max_width)\n",
    "# as retrieved from the IIIF manifest\n",
    "accession_dataframe = pd.read_csv(base_path + 'accession_dimensions.csv', dtype=str)\n",
    "\n",
    "# Open the download_image_data.csv file. This file is needed to associate the accession numbers with the file names (i.e. to get the file extension).\n",
    "download_image_data_df = pd.read_csv(base_path + 'download_image_data.csv', dtype=str, na_filter=False)\n",
    "\n",
    "# For testing, just use the first 10 rows of the dataframe\n",
    "#accession_dataframe = accession_dataframe.head(10)\n",
    "\n",
    "# Create a dataframe to hold the data on detected objects.\n",
    "object_localization_output_dataframe = pd.DataFrame(columns=['accession_number', 'status', 'description', 'image_url'])\n",
    "\n",
    "\n",
    "# Loop through each accession number and create an row for each localized object.\n",
    "for image_index, image_row in accession_dataframe.iterrows():\n",
    "    print()\n",
    "    print('Processing image ' + str(image_index + 1) + ' of ' + str(len(accession_dataframe)))\n",
    "\n",
    "    # Extract the year from the first part of the accession number (before the period).\n",
    "    year = image_row['accession_number'].split('.')[0]\n",
    "\n",
    "    # Look up the local filename of the image from the download_image_data dataframe.\n",
    "    # NOTE: This assumes that there is a single image for each local identifier (i.e. accession number).\n",
    "    local_filename = download_image_data_df.loc[download_image_data_df['local_identifier'] == image_row['accession_number'], 'local_filename'].iloc[0]\n",
    "\n",
    "    # Create the base IIIF 3.0 URL including the server and identifier components.\n",
    "    base_url = 'https://iiif.library.vanderbilt.edu/iiif/2/gallery%2F' + year + '%2F' + local_filename + '/'\n",
    "\n",
    "    print('base_url', base_url)\n",
    "\n",
    "    # Perform an HTTP GET to see if the image exists on the IIIF server\n",
    "    response = requests.get(base_url + 'info.json')\n",
    "    if response.status_code == 200:\n",
    "        status = 'uploaded'\n",
    "    else:\n",
    "        status = 'local'\n",
    "    print(status)\n",
    "    \n",
    "    # Loop through each object in the image.\n",
    "    for object_index, object_row in object_localization_dataframe.iterrows():\n",
    "        if object_row['accession_number'] != image_row['accession_number']:\n",
    "            continue\n",
    "        '''\n",
    "        # Create a W3C fragment selector for the annotation.\n",
    "        # https://www.w3.org/TR/annotation-model/#fragment-selector\n",
    "        # Calculate the upper left x and y in absolute canvas coordinates.\n",
    "        x = str(round(object_row['rel_left_x'] * float(image_row['max_width'])))\n",
    "        y = str(round(object_row['rel_top_y'] * float(image_row['max_height'])))\n",
    "        '''\n",
    "\n",
    "        # Set the x and y to be percents of the full image width and height.\n",
    "        x = str(object_row['rel_left_x'] * 100)\n",
    "        y = str(object_row['rel_top_y'] * 100)\n",
    "\n",
    "        '''\n",
    "        # Calculate the width and height in absolute canvas coordinates.\n",
    "        width = str(round((object_row['rel_right_x'] - object_row['rel_left_x']) * float(image_row['max_width'])))\n",
    "        height = str(round((object_row['rel_bottom_y'] - object_row['rel_top_y']) * float(image_row['max_height'])))\n",
    "        '''\n",
    "\n",
    "        # Set the width and height to be percents of the full image width and height.\n",
    "        width = str((object_row['rel_right_x'] - object_row['rel_left_x']) * 100)\n",
    "        height = str((object_row['rel_bottom_y'] - object_row['rel_top_y']) * 100)\n",
    "\n",
    "        # Substitute the pct: string for the absolute fragment selector.\n",
    "        #fragment_selector = 'xywh=' + x + ',' + y + ',' + width + ',' + height\n",
    "        selector = 'pct:' + x + ',' + y + ',' + width + ',' + height\n",
    "\n",
    "        # Add the selector to the base URL to specify the region of the image.\n",
    "        url = base_url + selector + '/full/0/default.jpg'\n",
    "        print('url', url)\n",
    "        print('description', object_row['description'])\n",
    "\n",
    "        # Add the accession number, description, and image URL to the dataframe.\n",
    "        object_localization_output_dataframe = object_localization_output_dataframe.append({'accession_number': image_row['accession_number'], 'status': status, 'description': object_row['description'], 'image_url': url}, ignore_index=True)\n",
    "\n",
    "    # Write the dataframe to a CSV file.\n",
    "    object_localization_output_dataframe.to_csv(base_path + 'object_localization_image_urls.csv', index=False)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "8f96c65e2c1d4fcba82e9525c1be2fd15c6a14102f9c31bd3457b5f48c526190"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
