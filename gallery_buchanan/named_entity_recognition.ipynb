{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine arts gallery Named Entity Recognition (NER) tests\n",
    "\n",
    "\n",
    "## 2021 fall tests\n",
    "\n",
    "These are some of the NER products that seem to be widely used: NLTK, Spacy, and ParallelDots.\n",
    "\n",
    "## Function section\n",
    "\n",
    "This needs to be run before any of the other cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from time import sleep\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from copy import deepcopy\n",
    "from langdetect import detect\n",
    "from langdetect import detect_langs\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "import paralleldots\n",
    "\n",
    "# ----------------\n",
    "# Configuration settings\n",
    "# ----------------\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "dots_sleep = 1 # number of seconds to wait between calls to ParallelDots API\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "# Calculate the reference date retrieved value for all statements\n",
    "whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "dateZ = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "ref_retrieved = dateZ + 'T00:00:00Z' # form 2019-12-05T00:00:00Z as provided by Wikidata, without leading +\n",
    "\n",
    "# ----------------\n",
    "# Utility functions\n",
    "# ----------------\n",
    "\n",
    "def generate_utc_date():\n",
    "    whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "    date_z = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "    return date_z\n",
    "\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderBot/1.6 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def load_credential(filename, directory):\n",
    "    cred = ''\n",
    "    # to change the script to look for the credential in the working directory, change the value of home to empty string\n",
    "    if directory == 'home':\n",
    "        home = str(Path.home()) #gets path to home directory; works for both Win and Mac\n",
    "        credential_path = home + '/' + filename\n",
    "    else:\n",
    "        directory = 'working'\n",
    "        credential_path = filename\n",
    "    try:\n",
    "        with open(credential_path, 'rt', encoding='utf-8') as file_object:\n",
    "            cred = file_object.read()\n",
    "    except:\n",
    "        print(filename + ' file not found - is it in your ' + directory + ' directory?')\n",
    "        exit()\n",
    "    return(cred)\n",
    "\n",
    "# find non-redundant values for a column or simple list\n",
    "def non_redundant(table, column_key):\n",
    "    non_redundant_list = []\n",
    "    for row in table:\n",
    "        found = False\n",
    "        for test_item in non_redundant_list:\n",
    "            if column_key == '':\n",
    "                if row == test_item:\n",
    "                    found = True\n",
    "                    break\n",
    "            else:\n",
    "                if row[column_key] == test_item:\n",
    "                    found = True\n",
    "                    break\n",
    "        if not found:\n",
    "            if column_key == '':\n",
    "                non_redundant_list.append(row)\n",
    "            else:\n",
    "                non_redundant_list.append(row[column_key])\n",
    "    return non_redundant_list\n",
    "\n",
    "# function to use in sort of simple list\n",
    "def sort_funct(row):\n",
    "    return row\n",
    "\n",
    "# function to use in sort last_first names\n",
    "def sort_last_first(row):\n",
    "    return row['last_first']\n",
    "\n",
    "# function to use in sort by match score\n",
    "def sort_score(row):\n",
    "    return row['score']\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# search label and alias\n",
    "# For whatever reason, if I use the graph pattern\n",
    "\n",
    "# wd:Q21 wdt:P31 ?class.\n",
    "\n",
    "# England is not Q6256 (country)\n",
    "# But if I use the graph pattern\n",
    "\n",
    "#   wd:Q21 p:P31 ?statement.\n",
    "#  ?statement ps:P31 ?class.\n",
    "\n",
    "# it is ??!!\n",
    "def searchLabelsAtWikidata(string, class_list):\n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id '\n",
    "    query += '''where {\n",
    "  {?id rdfs:label \"''' + string + '''\"@en.}\n",
    "  union\n",
    "  {?id skos:altLabel \"''' + string + '''\"@en.}\n",
    "  '''\n",
    "    for class_index in range(len(class_list)):\n",
    "        if class_index == 0:\n",
    "            query += '''{?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "        else:\n",
    "            query += '''union\n",
    "  {?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "    query += '''}'''\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    # r = requests.get(endpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_qnumber(result['id']['value'])\n",
    "        return_value.append(qid)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def retrieve_gallery_classes():\n",
    "    # create a string for the query\n",
    "    # use Metropolitan Museum of Art because there are too many collections to not specify the collection.\n",
    "    query = '''select distinct ?class ?label where \n",
    "      {\n",
    "      ?item wdt:P195 wd:Q160236.\n",
    "      ?item wdt:P31 ?class.\n",
    "      ?class rdfs:label ?label.\n",
    "      filter(lang(?label) = 'en')\n",
    "      }\n",
    "      order by ?label'''\n",
    "\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    print('sending query')\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    print('results returned')\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_qnumber(result['class']['value'])\n",
    "        label = result['label']['value']\n",
    "        return_value.append({'label': label, 'qid': qid})\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def generateNameAlternatives(name):\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods\n",
    "    name = name.replace('.', '')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "\n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        # make sure first character is alphabetic\n",
    "        # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "        # typical cases are like (Kit) or \"Kit\"\n",
    "        if not piece[0:1].isalpha():\n",
    "            piece = piece[1:len(piece)] # remove the first non-alphabetic character\n",
    "        if len(piece) > 0:\n",
    "            initials.append(piece[0:1])\n",
    "        \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += pieces[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # full name with suffix\n",
    "    if suffix != '':\n",
    "        nameVersion = ''\n",
    "        for pieceNumber in range(0, len(pieces)-1):\n",
    "            nameVersion += pieces[pieceNumber] + ' '\n",
    "        nameVersion += pieces[len(pieces)-1] + suffix\n",
    "        alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first and last name only\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial and last name only\n",
    "    nameVersion = initials[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    nameVersion = initials[0] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with last name\n",
    "    nameVersion = initials[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber]\n",
    "    nameVersion += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def searchNameAtWikidata(name):\n",
    "    nameList = generateNameAlternatives(name)\n",
    "    alternatives = ''\n",
    "    for alternative in nameList:\n",
    "        # get rid of quotes, which will break the query\n",
    "        alternative = alternative.replace('\"', '')\n",
    "        alternative = alternative.replace(\"'\", '')\n",
    "        alternatives += '\"' + alternative + '\"@en\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    results = []\n",
    "    # r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            if 'label' in statement:\n",
    "                name = statement['label']['value']\n",
    "            else:\n",
    "                name = ''\n",
    "            qNumber = vbc.extract_qnumber(wikidataIri)\n",
    "            results.append({'qId': qNumber, 'name': name})\n",
    "    except:\n",
    "        results = [{'error': r.text}]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return results\n",
    "\n",
    "def name_variant_testing(name, variant):\n",
    "    # get rid of periods\n",
    "    name = name.replace('.','')\n",
    "    variant = variant.replace('.','')\n",
    "    \n",
    "    # create first names\n",
    "    name_pieces = name.split(' ')\n",
    "    variant_pieces = variant.split(' ')\n",
    "    last_name = name_pieces[len(name_pieces)-1]\n",
    "    last_variant = variant_pieces[len(variant_pieces)-1]\n",
    "    if len(name_pieces) > 1:\n",
    "        first_names = name[0:-(len(last_name)+1)]\n",
    "    else:\n",
    "        first_names = name     \n",
    "    if len(variant_pieces) > 1:\n",
    "        first_variants = variant[0:-(len(last_variant)+1)]\n",
    "    else:\n",
    "        first_variants = variant      \n",
    "    #print(first_names)\n",
    "    #print(first_variants)\n",
    "    \n",
    "    # compare first names\n",
    "    # I experimented with the different ratios and I think fuzz might be best.\n",
    "    ratio = fuzz.ratio(first_names, first_variants)\n",
    "    #partial_ratio = fuzz.partial_ratio(first_names, first_variants)\n",
    "    #sort_ratio = fuzz.token_sort_ratio(first_names, first_variants)\n",
    "    #set_ratio = fuzz.token_set_ratio(first_names, first_variants)\n",
    "    # print('name similarity ratio', ratio)\n",
    "    #print('partial ratio', partial_ratio)\n",
    "    #print('sort_ratio', sort_ratio)\n",
    "    #print('set_ratio', set_ratio)\n",
    "\n",
    "    return(ratio)\n",
    "\n",
    "def find_surname_givens(name):\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', '')\n",
    "    name = name.replace(',', '')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "def remove_parens(string):\n",
    "    name_string = string.split('(')[0]\n",
    "    return name_string.strip()\n",
    "\n",
    "def remove_description(string):\n",
    "    try:\n",
    "        right_string = string.split('(')[1]\n",
    "        left_string = right_string.split(')')[0]\n",
    "        result = left_string.strip()\n",
    "    except:\n",
    "        result = ''\n",
    "    return result\n",
    "\n",
    "def reverse_names(string):\n",
    "    pieces = string.split(',')\n",
    "    return pieces[1].strip() + ' ' + pieces[0].strip()\n",
    "\n",
    "# Screens for Wikidata items that are potential matches\n",
    "\n",
    "import vb_common_code as vbc\n",
    "retrieve_class_list_query = vbc.Query(pid='P31', uselabel=False, sleep=sparql_sleep)\n",
    "retrieve_birth_date_query = vbc.Query(isitem=False, pid='P569', sleep=sparql_sleep)\n",
    "retrieve_death_date_query = vbc.Query(isitem=False, pid='P570', sleep=sparql_sleep)\n",
    "\n",
    "def human(qId):\n",
    "    screen = True\n",
    "    wdClassList = retrieve_class_list_query.single_property_values_for_item(qId)\n",
    "    # if there is a class property, check if it's a human\n",
    "    if len(wdClassList) != 0:\n",
    "        # if it's not a human\n",
    "        if wdClassList[0] != 'Q5':\n",
    "            #print('*** This item is not a human!')\n",
    "            screen = False\n",
    "    return screen\n",
    "\n",
    "# returns a dictionary of various descriptors of the item with Wikidata ID qId\n",
    "# P106 is occupation, schema:description is filtered to be the English description\n",
    "def searchWikidataDescription(qId):\n",
    "    resultsDict = {}\n",
    "    query = '''select distinct ?description ?orcid ?occupation where {\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' schema:description ?description.\n",
    "            FILTER(lang(?description) = 'en')\n",
    "            }\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = 'en')            \n",
    "            }\n",
    "        optional {wd:'''+ qId + ''' wdt:P496 ?orcid.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the dictionary remains empty\n",
    "            # Only a single description per language is allowed, so there should only be one description\n",
    "            if 'description' in statements[0]:\n",
    "                description = statements[0]['description']['value']\n",
    "            else:\n",
    "                description = ''\n",
    "            resultsDict['description'] = description\n",
    "            \n",
    "            # Only a single ORCID is allowed, so there should only be one orcid value\n",
    "            if 'orcid' in statements[0]:\n",
    "                orcid = statements[0]['orcid']['value']\n",
    "            else:\n",
    "                orcid = ''\n",
    "            resultsDict['orcid'] = orcid\n",
    "            \n",
    "            # if there are multiple statements, that's because there are more than one occupation\n",
    "            occupationList = []\n",
    "            for statement in statements:\n",
    "                if 'occupation' in statement:\n",
    "                    occupationList.append(statement['occupation']['value'])\n",
    "            resultsDict['occupation'] = occupationList\n",
    "    except:\n",
    "        resultsDict = {'error': r.text}\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return resultsDict\n",
    "\n",
    "def determine_era(string):\n",
    "    # dates with no CE or BCE, including empty string\n",
    "    if 'CE' not in string:\n",
    "        value = string\n",
    "        era = 'unknown'      \n",
    "    else:\n",
    "        if 'BCE' in string:\n",
    "            value = string[0:len(string)-3].strip()\n",
    "            era = 'BCE'\n",
    "        else: # string ends with CE\n",
    "            value = string[0:len(string)-2].strip()\n",
    "            era = 'CE'\n",
    "    return value, era\n",
    "\n",
    "def determine_zeros(date):\n",
    "    zero_count = 0\n",
    "    for char_number in range(len(date), 0, -1):\n",
    "        if date[char_number-1] == '0':\n",
    "            zero_count += 1\n",
    "        else:\n",
    "            return zero_count\n",
    "\n",
    "def pad_zeros_left(date_string):\n",
    "    length = len(date_string)\n",
    "    pad = 4-length\n",
    "    return '0' * pad + date_string\n",
    "\n",
    "def sign(era):\n",
    "    if era == 'BCE':\n",
    "        return '-'\n",
    "    elif era == 'CE':\n",
    "        return ''\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def detect_language(string):\n",
    "    try:\n",
    "        lang_list = detect_langs(string)\n",
    "        lang_string = str(lang_list[0])\n",
    "        confidence = float(lang_string[3:])\n",
    "        lang = lang_string[:2]\n",
    "    except: #exceptions occur when no info to decide, e.g. numbers\n",
    "        lang = 'zxx'\n",
    "        confidence = float(0)\n",
    "    return lang, confidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "These data need to be loaded before running any of the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'works_multiprop.csv'\n",
    "works = read_dict(filename)\n",
    "\n",
    "# See https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da \n",
    "# for codes used in output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "NOTE: seems to depend very heavily on capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = []\n",
    "\n",
    "for work in works[3361:3362]:\n",
    "    print('raw text:', work['label_en'])\n",
    "    work_dict = {'qid': work['qid'], 'label_en': work['label_en'], 'inventory_number': work['inventory_number']}\n",
    "    ne_list = []\n",
    "    # https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list\n",
    "    tokens = nltk.word_tokenize(work['label_en'])\n",
    "    print('tokens:', tokens)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    print('tagged:', tagged_tokens)\n",
    "    named_entity_chunks = nltk.ne_chunk(tagged_tokens)\n",
    "    print('NE chunks:', named_entity_chunks)\n",
    "    print()\n",
    "    \n",
    "    for chunk in named_entity_chunks:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ne_dict = {'ne_label': chunk.label()}\n",
    "            # A chunk is some kind of iterable of tuples\n",
    "            # Each tuple contains (word, noun_descriptor)\n",
    "            ne_string = chunk[0][0] # 0th tuple, word\n",
    "            # Iterate through the rest of the tuples in the chunk\n",
    "            for additional_tuple in chunk[1:len(chunk)]:\n",
    "                ne_string += ' ' + additional_tuple[0]\n",
    "            ne_dict['ne_string'] = ne_string\n",
    "            ne_list.append(ne_dict)\n",
    "\n",
    "            print(chunk.label(), ' '.join(c[0] for c in chunk))\n",
    "    print('NE list:', ne_list)\n",
    "    work_dict['ne_list'] = ne_list\n",
    "    \n",
    "    ''' did some tests and found it basically doesn't work if lowercase\n",
    "    print()\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(work['label_en'].lower()))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(chunk.label(), ' '.join(c[0] for c in chunk))\n",
    "    '''\n",
    "    print('-----------')\n",
    "    output_list.append(work_dict)\n",
    "    print()\n",
    "#print(json.dumps(output_list, indent = 2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you want a diagram of the NER chunks\n",
    "# It will open in a separate window that must be closed before any cell can be run again.\n",
    "# Sometimes it opens under other windows and you must click on its icon in the dock to make\n",
    "# it come to the frong.\n",
    "named_entity_chunks.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy \n",
    "\n",
    "Results were fairly similar between the cleaned up capitalization and the raw label strings\n",
    "\n",
    "That makes me think that Spacy is relatively insensite to capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list = []\n",
    "for work in works[3361:3362]:\n",
    "    print(work['label_en'])\n",
    "    work_dict = {'qid': work['qid'], 'label_en': work['label_en'], 'inventory_number': work['inventory_number']}\n",
    "    \n",
    "    # https://www.analyticsvidhya.com/blog/2021/06/nlp-application-named-entity-recognition-ner-in-python-with-spacy/\n",
    "    result = nlp(work['label_en'])\n",
    "    for entity in result.ents:\n",
    "        print(entity.text, entity.label_)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # results were somewhat worse when using all lower case\n",
    "    print('converted to lower case:')\n",
    "    result = nlp(work['label_en'].lower())\n",
    "    for entity in result.ents:\n",
    "        print(entity.text, entity.label_)\n",
    "    print()\n",
    "\n",
    "    print('-----------')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Dots API\n",
    "\n",
    "NOTE: Seems to be relatively insensitive to case\n",
    "\n",
    "Note: If you don't have a Parallel Dots API key, you can't run this test.\n",
    "\n",
    "Cleaned test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: ParallelDots is a commercial product with a free tier. But it requires an account and API key to use.\n",
    "# If you don't have an account, comment out this section.\n",
    "\n",
    "key = load_credential('paralleldots_api_key.txt', 'home')\n",
    "paralleldots.set_api_key(key)\n",
    "\n",
    "output_list = []\n",
    "for work in works[3361:3362]:\n",
    "    print(work['label_en'])\n",
    "    work_dict = {'qid': work['qid'], 'label_en': work['label_en'], 'inventory_number': work['inventory_number']}\n",
    "    named_entities = paralleldots.ner(work['label_en'])\n",
    "    work_dict['named_entities'] = named_entities\n",
    "    output_list.append(work_dict)\n",
    "    sleep(dots_sleep)\n",
    "out_text = json.dumps(output_list, indent = 2)\n",
    "print(out_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case test\n",
    "output_list = []\n",
    "for work in works[3361:3362]:\n",
    "    print(work['label_en'])\n",
    "    work_dict = {'qid': work['qid'], 'label_en': work['label_en'], 'inventory_number': work['inventory_number']}\n",
    "    named_entities = paralleldots.ner(work['label_en'].lower())\n",
    "    work_dict['named_entities'] = named_entities\n",
    "    output_list.append(work_dict)\n",
    "    sleep(dots_sleep)\n",
    "out_text = json.dumps(output_list, indent = 2)\n",
    "print(out_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Stanza, NER by Stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined code \n",
    "\n",
    "This script is the final product putting together the test scripts above. The configuration cell must be run before this one, but that's the only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    with open('start_work.txt', 'rt', encoding='utf-8') as fileObject:\n",
    "        start_work = int(fileObject.read())\n",
    "except:\n",
    "    start_work = 0\n",
    "print(start_work)\n",
    "print(type(start_work))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WARNING: there is a daily limit for the Parallel Dots free tier, so don't run this script over and over\n",
    "# or with a large dataset without commenting out the Parallel Dots part of the code\n",
    "# Free tier daily limits are 2000 hits, rate limit 60 hits per minute. NOTE: the limit is\n",
    "# hits, not API calls. So there is no easily predictable way to know how many itereations will be run.\n",
    "\n",
    "def perform_ner_analysis(works, start_work):\n",
    "    output_list = []\n",
    "    # This determines the starting record for the session. When the 2000 hit limit for Parallel Dots is reached,\n",
    "    # an error will be thrown. At that point, the loop will end and the data will be written.\n",
    "    work_number = start_work\n",
    "    for work in works[start_work:]:\n",
    "        work_number += 1 # when the loop fails, this will be recorded in the filename so that we know where to resume next day\n",
    "        print('raw text:', work['label_en'])\n",
    "        work_dict = {'qid': work['qid'], 'label_en': work['label_en'], 'inventory_number': work['inventory_number']}\n",
    "\n",
    "        # NLP\n",
    "        ne_list = []\n",
    "        # https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list\n",
    "        tokens = nltk.word_tokenize(work['label_en'])\n",
    "        # print('tokens:', tokens)\n",
    "        tagged_tokens = nltk.pos_tag(tokens)\n",
    "        # print('tagged:', tagged_tokens)\n",
    "        named_entity_chunks = nltk.ne_chunk(tagged_tokens)\n",
    "        # print('NE chunks:', named_entity_chunks)\n",
    "\n",
    "        for chunk in named_entity_chunks:\n",
    "            if hasattr(chunk, 'label'):\n",
    "                ne_dict = {'label': chunk.label()}\n",
    "                # A chunk is some kind of iterable of tuples\n",
    "                # Each tuple contains (word, noun_descriptor)\n",
    "                ne_string = chunk[0][0] # 0th tuple, word\n",
    "                # Iterate through the rest of the tuples in the chunk\n",
    "                for additional_tuple in chunk[1:len(chunk)]:\n",
    "                    ne_string += ' ' + additional_tuple[0]\n",
    "                ne_dict['string'] = ne_string\n",
    "                ne_list.append(ne_dict)\n",
    "\n",
    "                # print(chunk.label(), ' '.join(c[0] for c in chunk))\n",
    "        # print('NE list:', ne_list)\n",
    "        work_dict['nltk'] = ne_list\n",
    "        # print(json.dumps(work_dict, indent = 2))\n",
    "\n",
    "        # Spacy\n",
    "        # See https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da \n",
    "        # for codes used in output by Spacy\n",
    "        ne_list = []\n",
    "        result = nlp(work['label_en'])\n",
    "        for entity in result.ents:\n",
    "            ne_list.append({'label': entity.label_, 'string': entity.text})\n",
    "        work_dict['spacy'] = ne_list\n",
    "\n",
    "        # Parallel Dots API\n",
    "        try:\n",
    "            named_entities = paralleldots.ner(work['label_en'])\n",
    "        except:\n",
    "            # avoid script crashes when there's an error with the API\n",
    "            print('API error for', work['qid'], work['label_en'])\n",
    "            work_dict['parallel_dots'] = [{'label': 'error', 'string': '', 'confidence_score': ''}]\n",
    "            output_list.append(work_dict)\n",
    "            continue # go to start of loop and do next item\n",
    "\n",
    "        # Convert the Parallel Dots JSON structure to match the others\n",
    "        try: # an error will be thrown if the API data is an error message instead of having an 'entities' key (daily limit)\n",
    "            temp_list = []\n",
    "            for entity in named_entities['entities']:\n",
    "                temp_list.append({'label': entity['category'], 'string': entity['name'], 'confidence_score': entity['confidence_score']})\n",
    "            work_dict['parallel_dots'] = temp_list\n",
    "        except:\n",
    "            break # When the daily limit is exceeded, the loop will terminate and go on to the output code\n",
    "\n",
    "        sleep(dots_sleep) # This is a delay to prevent hitting the API to frequently and getting blocked\n",
    "\n",
    "        # Append all of the collected data to the accumulation list\n",
    "        output_list.append(work_dict)\n",
    "\n",
    "    work_number -= 1 # decrement because the last one failed\n",
    "    if work_number > start_work: # Do not output unless at least one work was processed.\n",
    "        output_text = json.dumps(output_list, indent = 2)\n",
    "\n",
    "        # the file name includes the first record number and the record number after the last record (i.e. Python range style)\n",
    "        out_file_path = 'named_er' + '_' + str(start_work) + '_' + str(work_number) + '.json'\n",
    "        with open(out_file_path, 'wt', encoding='utf-8') as file_object:\n",
    "            file_object.write(output_text)\n",
    "\n",
    "        with open('start_work.txt', 'wt', encoding='utf-8') as fileObject:\n",
    "            fileObject.write(str(work_number)) # Next time, start with the record after the last successfully analyzed record\n",
    "        success = True\n",
    "    else:\n",
    "        print('No works analyzed.')\n",
    "        success = False\n",
    "\n",
    "    print('done')\n",
    "    return success\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time checked: 2021-10-04T03:01:54.097635\n",
      "Date last run: 2021-10-02\n",
      "UTC date now is: 2021-10-04\n",
      "raw text: Scroll weight with an inlaid chrysanthemum design in mother of pearl\n",
      "No works analyzed.\n",
      "done\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Note: ParallelDots is a commercial product with a free tier. But it requires an account and API key to use.\n",
    "key = load_credential('paralleldots_api_key.txt', 'home')\n",
    "paralleldots.set_api_key(key)\n",
    "\n",
    "filename = 'works_multiprop.csv'\n",
    "works = read_dict(filename)\n",
    "\n",
    "try:\n",
    "    with open('start_work.txt', 'rt', encoding='utf-8') as fileObject:\n",
    "        start_work = int(fileObject.read())\n",
    "except:\n",
    "    start_work = 0\n",
    "\n",
    "while True: # infinite loop\n",
    "    print('Time checked:', datetime.datetime.utcnow().isoformat())\n",
    "\n",
    "    try:\n",
    "        # Look to see the last date the script was run\n",
    "        with open('last_run.txt', 'rt', encoding='utf-8') as fileObject:\n",
    "            date_last_run = fileObject.read()\n",
    "    except:\n",
    "        date_last_run = '2021-01-01' # a date in the past\n",
    "    print('Date last run:', date_last_run)\n",
    "\n",
    "    date_now_utc = generate_utc_date()\n",
    "    print('UTC date now is:', date_now_utc)\n",
    "\n",
    "    if date_now_utc > date_last_run:\n",
    "        success = perform_ner_analysis(works, start_work)\n",
    "        \n",
    "        if success:\n",
    "            with open('last_run.txt', 'wt', encoding='utf-8') as fileObject:\n",
    "                # If analysis occurrred successfully, update the last_run date in the file with today's date\n",
    "                fileObject.write(generate_utc_date())\n",
    "\n",
    "    print()\n",
    "    # wait an hour before checking again\n",
    "    sleep(3600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
