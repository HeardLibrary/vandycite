{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine arts gallery Named Entity Recognition (NER) tests\n",
    "\n",
    "\n",
    "## 2021 fall tests\n",
    "\n",
    "These are some of the NER products that seem to be widely used: NLTK, Spacy, and ParallelDots.\n",
    "\n",
    "## Function section\n",
    "\n",
    "This needs to be run before any of the other cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from time import sleep\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from copy import deepcopy\n",
    "from langdetect import detect\n",
    "from langdetect import detect_langs\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "import paralleldots\n",
    "\n",
    "# ----------------\n",
    "# Configuration settings\n",
    "# ----------------\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "dots_sleep = 1 # number of seconds to wait between calls to ParallelDots API\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "# Calculate the reference date retrieved value for all statements\n",
    "whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "dateZ = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "ref_retrieved = dateZ + 'T00:00:00Z' # form 2019-12-05T00:00:00Z as provided by Wikidata, without leading +\n",
    "\n",
    "# ----------------\n",
    "# Utility functions\n",
    "# ----------------\n",
    "\n",
    "\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderBot/1.6 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def load_credential(filename, directory):\n",
    "    cred = ''\n",
    "    # to change the script to look for the credential in the working directory, change the value of home to empty string\n",
    "    if directory == 'home':\n",
    "        home = str(Path.home()) #gets path to home directory; works for both Win and Mac\n",
    "        credential_path = home + '/' + filename\n",
    "    else:\n",
    "        directory = 'working'\n",
    "        credential_path = filename\n",
    "    try:\n",
    "        with open(credential_path, 'rt', encoding='utf-8') as file_object:\n",
    "            cred = file_object.read()\n",
    "    except:\n",
    "        print(filename + ' file not found - is it in your ' + directory + ' directory?')\n",
    "        exit()\n",
    "    return(cred)\n",
    "\n",
    "# find non-redundant values for a column or simple list\n",
    "def non_redundant(table, column_key):\n",
    "    non_redundant_list = []\n",
    "    for row in table:\n",
    "        found = False\n",
    "        for test_item in non_redundant_list:\n",
    "            if column_key == '':\n",
    "                if row == test_item:\n",
    "                    found = True\n",
    "                    break\n",
    "            else:\n",
    "                if row[column_key] == test_item:\n",
    "                    found = True\n",
    "                    break\n",
    "        if not found:\n",
    "            if column_key == '':\n",
    "                non_redundant_list.append(row)\n",
    "            else:\n",
    "                non_redundant_list.append(row[column_key])\n",
    "    return non_redundant_list\n",
    "\n",
    "# function to use in sort of simple list\n",
    "def sort_funct(row):\n",
    "    return row\n",
    "\n",
    "# function to use in sort last_first names\n",
    "def sort_last_first(row):\n",
    "    return row['last_first']\n",
    "\n",
    "# function to use in sort by match score\n",
    "def sort_score(row):\n",
    "    return row['score']\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# search label and alias\n",
    "# For whatever reason, if I use the graph pattern\n",
    "\n",
    "# wd:Q21 wdt:P31 ?class.\n",
    "\n",
    "# England is not Q6256 (country)\n",
    "# But if I use the graph pattern\n",
    "\n",
    "#   wd:Q21 p:P31 ?statement.\n",
    "#  ?statement ps:P31 ?class.\n",
    "\n",
    "# it is ??!!\n",
    "def searchLabelsAtWikidata(string, class_list):\n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id '\n",
    "    query += '''where {\n",
    "  {?id rdfs:label \"''' + string + '''\"@en.}\n",
    "  union\n",
    "  {?id skos:altLabel \"''' + string + '''\"@en.}\n",
    "  '''\n",
    "    for class_index in range(len(class_list)):\n",
    "        if class_index == 0:\n",
    "            query += '''{?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "        else:\n",
    "            query += '''union\n",
    "  {?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "    query += '''}'''\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    # r = requests.get(endpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_qnumber(result['id']['value'])\n",
    "        return_value.append(qid)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def retrieve_gallery_classes():\n",
    "    # create a string for the query\n",
    "    # use Metropolitan Museum of Art because there are too many collections to not specify the collection.\n",
    "    query = '''select distinct ?class ?label where \n",
    "      {\n",
    "      ?item wdt:P195 wd:Q160236.\n",
    "      ?item wdt:P31 ?class.\n",
    "      ?class rdfs:label ?label.\n",
    "      filter(lang(?label) = 'en')\n",
    "      }\n",
    "      order by ?label'''\n",
    "\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    print('sending query')\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    print('results returned')\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_qnumber(result['class']['value'])\n",
    "        label = result['label']['value']\n",
    "        return_value.append({'label': label, 'qid': qid})\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def generateNameAlternatives(name):\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods\n",
    "    name = name.replace('.', '')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "\n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        # make sure first character is alphabetic\n",
    "        # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "        # typical cases are like (Kit) or \"Kit\"\n",
    "        if not piece[0:1].isalpha():\n",
    "            piece = piece[1:len(piece)] # remove the first non-alphabetic character\n",
    "        if len(piece) > 0:\n",
    "            initials.append(piece[0:1])\n",
    "        \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += pieces[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # full name with suffix\n",
    "    if suffix != '':\n",
    "        nameVersion = ''\n",
    "        for pieceNumber in range(0, len(pieces)-1):\n",
    "            nameVersion += pieces[pieceNumber] + ' '\n",
    "        nameVersion += pieces[len(pieces)-1] + suffix\n",
    "        alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first and last name only\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial and last name only\n",
    "    nameVersion = initials[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    nameVersion = initials[0] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with last name\n",
    "    nameVersion = initials[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber]\n",
    "    nameVersion += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def searchNameAtWikidata(name):\n",
    "    nameList = generateNameAlternatives(name)\n",
    "    alternatives = ''\n",
    "    for alternative in nameList:\n",
    "        # get rid of quotes, which will break the query\n",
    "        alternative = alternative.replace('\"', '')\n",
    "        alternative = alternative.replace(\"'\", '')\n",
    "        alternatives += '\"' + alternative + '\"@en\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    results = []\n",
    "    # r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            if 'label' in statement:\n",
    "                name = statement['label']['value']\n",
    "            else:\n",
    "                name = ''\n",
    "            qNumber = vbc.extract_qnumber(wikidataIri)\n",
    "            results.append({'qId': qNumber, 'name': name})\n",
    "    except:\n",
    "        results = [{'error': r.text}]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return results\n",
    "\n",
    "def name_variant_testing(name, variant):\n",
    "    # get rid of periods\n",
    "    name = name.replace('.','')\n",
    "    variant = variant.replace('.','')\n",
    "    \n",
    "    # create first names\n",
    "    name_pieces = name.split(' ')\n",
    "    variant_pieces = variant.split(' ')\n",
    "    last_name = name_pieces[len(name_pieces)-1]\n",
    "    last_variant = variant_pieces[len(variant_pieces)-1]\n",
    "    if len(name_pieces) > 1:\n",
    "        first_names = name[0:-(len(last_name)+1)]\n",
    "    else:\n",
    "        first_names = name     \n",
    "    if len(variant_pieces) > 1:\n",
    "        first_variants = variant[0:-(len(last_variant)+1)]\n",
    "    else:\n",
    "        first_variants = variant      \n",
    "    #print(first_names)\n",
    "    #print(first_variants)\n",
    "    \n",
    "    # compare first names\n",
    "    # I experimented with the different ratios and I think fuzz might be best.\n",
    "    ratio = fuzz.ratio(first_names, first_variants)\n",
    "    #partial_ratio = fuzz.partial_ratio(first_names, first_variants)\n",
    "    #sort_ratio = fuzz.token_sort_ratio(first_names, first_variants)\n",
    "    #set_ratio = fuzz.token_set_ratio(first_names, first_variants)\n",
    "    # print('name similarity ratio', ratio)\n",
    "    #print('partial ratio', partial_ratio)\n",
    "    #print('sort_ratio', sort_ratio)\n",
    "    #print('set_ratio', set_ratio)\n",
    "\n",
    "    return(ratio)\n",
    "\n",
    "def find_surname_givens(name):\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', '')\n",
    "    name = name.replace(',', '')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "def remove_parens(string):\n",
    "    name_string = string.split('(')[0]\n",
    "    return name_string.strip()\n",
    "\n",
    "def remove_description(string):\n",
    "    try:\n",
    "        right_string = string.split('(')[1]\n",
    "        left_string = right_string.split(')')[0]\n",
    "        result = left_string.strip()\n",
    "    except:\n",
    "        result = ''\n",
    "    return result\n",
    "\n",
    "def reverse_names(string):\n",
    "    pieces = string.split(',')\n",
    "    return pieces[1].strip() + ' ' + pieces[0].strip()\n",
    "\n",
    "# Screens for Wikidata items that are potential matches\n",
    "\n",
    "import vb_common_code as vbc\n",
    "retrieve_class_list_query = vbc.Query(pid='P31', uselabel=False, sleep=sparql_sleep)\n",
    "retrieve_birth_date_query = vbc.Query(isitem=False, pid='P569', sleep=sparql_sleep)\n",
    "retrieve_death_date_query = vbc.Query(isitem=False, pid='P570', sleep=sparql_sleep)\n",
    "\n",
    "def human(qId):\n",
    "    screen = True\n",
    "    wdClassList = retrieve_class_list_query.single_property_values_for_item(qId)\n",
    "    # if there is a class property, check if it's a human\n",
    "    if len(wdClassList) != 0:\n",
    "        # if it's not a human\n",
    "        if wdClassList[0] != 'Q5':\n",
    "            #print('*** This item is not a human!')\n",
    "            screen = False\n",
    "    return screen\n",
    "\n",
    "# returns a dictionary of various descriptors of the item with Wikidata ID qId\n",
    "# P106 is occupation, schema:description is filtered to be the English description\n",
    "def searchWikidataDescription(qId):\n",
    "    resultsDict = {}\n",
    "    query = '''select distinct ?description ?orcid ?occupation where {\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' schema:description ?description.\n",
    "            FILTER(lang(?description) = 'en')\n",
    "            }\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = 'en')            \n",
    "            }\n",
    "        optional {wd:'''+ qId + ''' wdt:P496 ?orcid.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the dictionary remains empty\n",
    "            # Only a single description per language is allowed, so there should only be one description\n",
    "            if 'description' in statements[0]:\n",
    "                description = statements[0]['description']['value']\n",
    "            else:\n",
    "                description = ''\n",
    "            resultsDict['description'] = description\n",
    "            \n",
    "            # Only a single ORCID is allowed, so there should only be one orcid value\n",
    "            if 'orcid' in statements[0]:\n",
    "                orcid = statements[0]['orcid']['value']\n",
    "            else:\n",
    "                orcid = ''\n",
    "            resultsDict['orcid'] = orcid\n",
    "            \n",
    "            # if there are multiple statements, that's because there are more than one occupation\n",
    "            occupationList = []\n",
    "            for statement in statements:\n",
    "                if 'occupation' in statement:\n",
    "                    occupationList.append(statement['occupation']['value'])\n",
    "            resultsDict['occupation'] = occupationList\n",
    "    except:\n",
    "        resultsDict = {'error': r.text}\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return resultsDict\n",
    "\n",
    "def determine_era(string):\n",
    "    # dates with no CE or BCE, including empty string\n",
    "    if 'CE' not in string:\n",
    "        value = string\n",
    "        era = 'unknown'      \n",
    "    else:\n",
    "        if 'BCE' in string:\n",
    "            value = string[0:len(string)-3].strip()\n",
    "            era = 'BCE'\n",
    "        else: # string ends with CE\n",
    "            value = string[0:len(string)-2].strip()\n",
    "            era = 'CE'\n",
    "    return value, era\n",
    "\n",
    "def determine_zeros(date):\n",
    "    zero_count = 0\n",
    "    for char_number in range(len(date), 0, -1):\n",
    "        if date[char_number-1] == '0':\n",
    "            zero_count += 1\n",
    "        else:\n",
    "            return zero_count\n",
    "\n",
    "def pad_zeros_left(date_string):\n",
    "    length = len(date_string)\n",
    "    pad = 4-length\n",
    "    return '0' * pad + date_string\n",
    "\n",
    "def sign(era):\n",
    "    if era == 'BCE':\n",
    "        return '-'\n",
    "    elif era == 'CE':\n",
    "        return ''\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def detect_language(string):\n",
    "    try:\n",
    "        lang_list = detect_langs(string)\n",
    "        lang_string = str(lang_list[0])\n",
    "        confidence = float(lang_string[3:])\n",
    "        lang = lang_string[:2]\n",
    "    except: #exceptions occur when no info to decide, e.g. numbers\n",
    "        lang = 'zxx'\n",
    "        confidence = float(0)\n",
    "    return lang, confidence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "These data need to be loaded before running any of the following cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'works_multiprop.csv'\n",
    "works = read_dict(filename)\n",
    "\n",
    "# See https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da \n",
    "# for codes used in output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLTK\n",
    "\n",
    "NOTE: seems to depend very heavily on capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw text: Sculpture Project from the Studio at Palma de Mallorca, from Joaquim Gomis Photographs 1941-1981.\n",
      "tokens: ['Sculpture', 'Project', 'from', 'the', 'Studio', 'at', 'Palma', 'de', 'Mallorca', ',', 'from', 'Joaquim', 'Gomis', 'Photographs', '1941-1981', '.']\n",
      "tagged: [('Sculpture', 'NN'), ('Project', 'NN'), ('from', 'IN'), ('the', 'DT'), ('Studio', 'NNP'), ('at', 'IN'), ('Palma', 'NNP'), ('de', 'FW'), ('Mallorca', 'NNP'), (',', ','), ('from', 'IN'), ('Joaquim', 'NNP'), ('Gomis', 'NNP'), ('Photographs', 'NNP'), ('1941-1981', 'CD'), ('.', '.')]\n",
      "NE chunks: (S\n",
      "  (GPE Sculpture/NN)\n",
      "  (ORGANIZATION Project/NN)\n",
      "  from/IN\n",
      "  the/DT\n",
      "  (ORGANIZATION Studio/NNP)\n",
      "  at/IN\n",
      "  (GSP Palma/NNP)\n",
      "  de/FW\n",
      "  (GPE Mallorca/NNP)\n",
      "  ,/,\n",
      "  from/IN\n",
      "  (PERSON Joaquim/NNP Gomis/NNP)\n",
      "  Photographs/NNP\n",
      "  1941-1981/CD\n",
      "  ./.)\n",
      "\n",
      "GPE Sculpture\n",
      "ORGANIZATION Project\n",
      "ORGANIZATION Studio\n",
      "GSP Palma\n",
      "GPE Mallorca\n",
      "PERSON Joaquim Gomis\n",
      "NE list: [{'ne_label': 'GPE', 'ne_string': 'Sculpture'}, {'ne_label': 'ORGANIZATION', 'ne_string': 'Project'}, {'ne_label': 'ORGANIZATION', 'ne_string': 'Studio'}, {'ne_label': 'GSP', 'ne_string': 'Palma'}, {'ne_label': 'GPE', 'ne_string': 'Mallorca'}, {'ne_label': 'PERSON', 'ne_string': 'Joaquim Gomis'}]\n",
      "-----------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output_list = []\n",
    "\n",
    "for work in works[3361:3362]:\n",
    "    print('raw text:', work['label_en'])\n",
    "    work_dict = {'qid': work['qid'], 'label_en': work['label_en'], 'inventory_number': work['inventory_number']}\n",
    "    ne_list = []\n",
    "    # https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list\n",
    "    tokens = nltk.word_tokenize(work['label_en'])\n",
    "    print('tokens:', tokens)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    print('tagged:', tagged_tokens)\n",
    "    named_entity_chunks = nltk.ne_chunk(tagged_tokens)\n",
    "    print('NE chunks:', named_entity_chunks)\n",
    "    print()\n",
    "    \n",
    "    for chunk in named_entity_chunks:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ne_dict = {'ne_label': chunk.label()}\n",
    "            # A chunk is some kind of iterable of tuples\n",
    "            # Each tuple contains (word, noun_descriptor)\n",
    "            ne_string = chunk[0][0] # 0th tuple, word\n",
    "            # Iterate through the rest of the tuples in the chunk\n",
    "            for additional_tuple in chunk[1:len(chunk)]:\n",
    "                ne_string += ' ' + additional_tuple[0]\n",
    "            ne_dict['ne_string'] = ne_string\n",
    "            ne_list.append(ne_dict)\n",
    "\n",
    "            print(chunk.label(), ' '.join(c[0] for c in chunk))\n",
    "    print('NE list:', ne_list)\n",
    "    work_dict['ne_list'] = ne_list\n",
    "    \n",
    "    ''' did some tests and found it basically doesn't work if lowercase\n",
    "    print()\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(work['label_en'].lower()))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "            print(chunk.label(), ' '.join(c[0] for c in chunk))\n",
    "    '''\n",
    "    print('-----------')\n",
    "    output_list.append(work_dict)\n",
    "    print()\n",
    "#print(json.dumps(output_list, indent = 2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if you want a diagram of the NER chunks\n",
    "# It will open in a separate window that must be closed before any cell can be run again.\n",
    "# Sometimes it opens under other windows and you must click on its icon in the dock to make\n",
    "# it come to the frong.\n",
    "named_entity_chunks.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spacy \n",
    "\n",
    "Results were fairly similar between the cleaned up capitalization and the raw label strings\n",
    "\n",
    "That makes me think that Spacy is relatively insensite to capitalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sculpture Project from the Studio at Palma de Mallorca, from Joaquim Gomis Photographs 1941-1981.\n",
      "Palma de Mallorca FAC\n",
      "Joaquim Gomis Photographs PERSON\n",
      "1941-1981 DATE\n",
      "\n",
      "converted to lower case:\n",
      "palma de mallorca ORG\n",
      "1941-1981 DATE\n",
      "\n",
      "-----------\n"
     ]
    }
   ],
   "source": [
    "output_list = []\n",
    "for work in works[3361:3362]:\n",
    "    print(work['label_en'])\n",
    "    work_dict = {'qid': work['qid'], 'label_en': work['label_en'], 'inventory_number': work['inventory_number']}\n",
    "    \n",
    "    # https://www.analyticsvidhya.com/blog/2021/06/nlp-application-named-entity-recognition-ner-in-python-with-spacy/\n",
    "    result = nlp(work['label_en'])\n",
    "    for entity in result.ents:\n",
    "        print(entity.text, entity.label_)\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    # results were somewhat worse when using all lower case\n",
    "    print('converted to lower case:')\n",
    "    result = nlp(work['label_en'].lower())\n",
    "    for entity in result.ents:\n",
    "        print(entity.text, entity.label_)\n",
    "    print()\n",
    "\n",
    "    print('-----------')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parallel Dots API\n",
    "\n",
    "NOTE: Seems to be relatively insensitive to case\n",
    "\n",
    "Note: If you don't have a Parallel Dots API key, you can't run this test.\n",
    "\n",
    "Cleaned test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: ParallelDots is a commercial product with a free tier. But it requires an account and API key to use.\n",
    "# If you don't have an account, comment out this section.\n",
    "\n",
    "key = load_credential('paralleldots_api_key.txt', 'home')\n",
    "paralleldots.set_api_key(key)\n",
    "\n",
    "output_list = []\n",
    "for work in works[3361:3362]:\n",
    "    print(work['label_en'])\n",
    "    work_dict = {'qid': work['qid'], 'label_en': work['label_en'], 'inventory_number': work['inventory_number']}\n",
    "    named_entities = paralleldots.ner(work['label_en'])\n",
    "    work_dict['named_entities'] = named_entities\n",
    "    output_list.append(work_dict)\n",
    "    sleep(dots_sleep)\n",
    "out_text = json.dumps(output_list, indent = 2)\n",
    "print(out_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lower case test\n",
    "output_list = []\n",
    "for work in works[3361:3362]:\n",
    "    print(work['label_en'])\n",
    "    work_dict = {'qid': work['qid'], 'label_en': work['label_en'], 'inventory_number': work['inventory_number']}\n",
    "    named_entities = paralleldots.ner(work['label_en'].lower())\n",
    "    work_dict['named_entities'] = named_entities\n",
    "    output_list.append(work_dict)\n",
    "    sleep(dots_sleep)\n",
    "out_text = json.dumps(output_list, indent = 2)\n",
    "print(out_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Stanza, NER by Stanford"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combined code \n",
    "\n",
    "This script is the final product putting together the test scripts above. The configuration cell must be run before this one, but that's the only one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw text: Hagi ware cup\n",
      "raw text: Hair comb with Indian and European figures\n",
      "raw text: Hakone, from \"The Fifty-three Stations of the Tokaido\" (reproduction)\n",
      "raw text: Half Mile Walk\n",
      "raw text: Half-Reclining Nude\n",
      "raw text: Halt the Hun! Buy U.S. Government Bonds. Third Liberty Loan\n",
      "raw text: Hamas militant, Gaza\n",
      "raw text: Handling Stone in the shape of a pomegranate branch with mature and immature fruit and blossom\n",
      "raw text: Handscroll with a Mountain Landscape, a Building, and Rocks\n",
      "raw text: Handscroll with Design of Birds and Birds on Branches\n",
      "raw text: Handscroll with Design of Two Trees\n",
      "raw text: Handwarmer with design of flowers (kyo-yaki ware)\n",
      "raw text: Hanging scroll in Bingata cloth depicting Okinawan fishing village with boats and anchor\n",
      "raw text: Hanging vase with incised abstract design\n",
      "raw text: Harbor Scene\n",
      "raw text: Harbor Scene with a Fort\n",
      "raw text: Harbour Scene\n",
      "raw text: Harwichport, Cape Cod\n",
      "raw text: Has She a Heart\n",
      "raw text: Hasshu Gafu (Collection of Eight Ming Picture Albums), Volumes 5, 7 and 8\n",
      "raw text: Hazy Meadow\n",
      "raw text: Head in Profile\n",
      "raw text: Head of Buddha\n",
      "raw text: Head of John the Baptist\n",
      "raw text: Heads\n",
      "raw text: Heavenly Bird\n",
      "raw text: Heel of the Boot\n",
      "raw text: Henryk Tomaszewski, Wroctawski Teatr Pantomimy, \"Spor\"\n",
      "raw text: Heraldic Tinctures, from American Abstract Artists 60th Anniversary Print Portfolio\n",
      "raw text: Herbstmonat, Septembre (September), from a suite of allegorical prints illustrating the months of the year with text in German and French\n",
      "raw text: Het Hoog-En Lager-Huys van Engelandt\n",
      "raw text: Heumonat, Juillet (July), from a suite of allegorical prints illustrating the months of the year with text in German and French\n",
      "raw text: Hexagonal Censer with a Floral Design and Chain\n",
      "raw text: Hexagonal Seal Ink Box with a Butterfly Family Crest on the Lid\n",
      "raw text: High-footed bowl\n",
      "raw text: Homage to N.S. \"Your thoughts don't have words-/ Like signal esoteric sips-\"\n",
      "raw text: Homenage a Mi Hijo\n",
      "raw text: Hommage à J.S. Bach (Homage to J.S. Bach)\n",
      "raw text: Homme à la cigarette (Self-portrait)\n",
      "raw text: Hornŭng, Fevrier. (February), from a suite of allegorical prints illustrating the months of the year with text in German and French\n",
      "raw text: Horse and Rider\n",
      "raw text: Horses in the Snow\n",
      "raw text: Hotel de Ville, Eglise St. Gervais et Pont d' Arcole\n",
      "raw text: House by a Stream\n",
      "raw text: House Cat\n",
      "raw text: How to Make Love to a Sound, from the Revolutions Per Minute (The Art Record)\n",
      "raw text: Human head\n",
      "raw text: Hundred Guilder Print\n",
      "raw text: Ibiza\n",
      "raw text: Ida Rubinstein\n",
      "raw text: Il Giorno (Day), from Paesaggi (Landscapes)\n",
      "raw text: Illustration from Le Solitaire by Andre' Verdet\n",
      "raw text: Illustration from the Mustard Seed Garden\n",
      "raw text: Illustration from Toreros\n",
      "raw text: Ils se sauvent\n",
      "raw text: Ils sont vollez\n",
      "raw text: Imari ware planter\n",
      "raw text: In Confinement (after the painting by Marten de Vos)\n",
      "raw text: In the Land of Oo-Bla-Dee\n",
      "raw text: In the Wood\n",
      "raw text: Incense burner base with four seated Buddhas with mandorlas\n",
      "raw text: Incense container (Kogo) in the form of a mythical beast\n",
      "raw text: Incense container in the shape of a heart  with a design of a woman and a boy and a girl\n",
      "raw text: Industry and Idleness: plate 11. The Idle 'Prentice Executed at Tyburn\n",
      "raw text: Industry and Idleness: plate 12. The Industrious 'Prentice Lord Mayor of London\n",
      "raw text: Initial Rites\n",
      "raw text: Ink Rubbing from a Temple  (Two Apsaras in a Landscape)\n",
      "raw text: Ink Rubbing of Reliefs from the Offering Shrine of Wu Liang\n",
      "raw text: Ink stone with a design of grapes and grape leaf\n",
      "raw text: Innocent Cat\n",
      "raw text: Inro  (Medicine Case) with a Design of Autumn Flowers and Insects\n",
      "raw text: Inro  (Medicine Case) with a Design of Autumn Flowers, an Ojime (Closing Bead), and a Netsuke (Toggle) in the Form of a Noh Mask\n",
      "raw text: Inro  (Medicine Case) with a Design of Boats, Mountains, and Pine Trees-(See Notes)\n",
      "raw text: Inro  (Medicine Case) with a Design of Pine Trees, Ojime (Closing Bead), and Netsuke (Toggle)\n",
      "raw text: Inro (Medicine Container) with a Landscape Design on Both Sides and a Netsuke (Toggle) in the Form of a Man Standing with Right Foot Raised\n",
      "raw text: Inro with a design of a snail and bamboo with a drawstring bead in the form of a double gourd, with bone netsuke with inscription\n",
      "raw text: Inro with ivory drawstring bead (ojime) and ivory netsuke with a design of a fruit\n",
      "raw text: Interior Scene\n",
      "raw text: Internal Sound, from the Revolutions Per Minute (The Art Record)\n",
      "raw text: Interweaving Rhythms, from American Abstract Artists 50th Anniversary Print Portfolio\n",
      "raw text: Iraqi Kurd refugees, Southern Turkey, Gulf War\n",
      "raw text: Irish\n",
      "raw text: Iron Deer\n",
      "raw text: Iron-Origin-Gold-Origin\n",
      "raw text: Irriguible, Galeria Carl van der Voort, Ibiza\n",
      "raw text: Ishibe: Megawa Village (Ishibe, Megawa no sato)\n",
      "raw text: Ishiyakushi: Ishiyakushi Temple (Ishiyakushi, Ishiyakushi-ji)\n",
      "raw text: It is Twilight Still in Japan; Speed the Dawn\n",
      "raw text: Italian Lake Scene\n",
      "raw text: Ivy League\n",
      "raw text: Izumi Bridge in Rain (Izumi-bashi no ame)\n",
      "raw text: Izumo-yaki ware plate with a flower design in brown and white glazes\n",
      "raw text: J'ai besoin de quitter Paris-., from Album du Siége\n",
      "raw text: Jade drinking cup\n",
      "raw text: Jade sculpture of little boy riding on the back of carp\n",
      "raw text: Jade vase with deer, crane, and bat designs\n",
      "raw text: Jade wedding drinking cup\n",
      "raw text: James and Taylor, from the series, I Am Unbeatable\n",
      "raw text: James McBey\n",
      "raw text: Jamup & Honey are on a CD\n",
      "raw text: Janice, Boulder Colo 1987\n",
      "raw text: Jar\n",
      "raw text: Jar with a raised scroll design at base with celadon glaze\n",
      "raw text: Jar with jun (white) glaze\n",
      "raw text: Jar with ormolu decoration\n",
      "raw text: Jar with straight neck and two loop handles with eight sets of five vertical ribs\n",
      "raw text: Jar with upper body brown glaze\n",
      "raw text: Je me suis mis dans les volontaires! from Album du Siége\n",
      "raw text: Jean Cocteau and Sphinx, les Baux\n",
      "raw text: Jenner, Janvier. (January), from a suite of allegorical prints illustrating the months of the year with text in German and French\n",
      "raw text: Jesus Raising Jairus' Daughter, surrounded by Peter, James and John\n",
      "raw text: Jeune Dame a la Promenade, No. 6\n",
      "raw text: Jeune Fille à La Chevelure Longue (Young Girl with Long Hair)\n",
      "raw text: Jewelry box with inlaid scene from Greek mythology\n",
      "raw text: Jian ware dark glazed bowl\n",
      "raw text: Jizhou ware conical bowl with olive green glaze\n",
      "raw text: Jizhou ware tea bowl with stylized plum blossoms\n",
      "raw text: Jō and Uba at Takasago - The Immortal Couple, from the series Yoshitoshi ryakuga (Sketches by Yoshitoshi)\n",
      "raw text: Jonah II 10: \"The Lord spoke unto the fish, it vomited out Jonah upon the dry land.\"\n",
      "raw text: Journey\n",
      "raw text: Juin - On Tond les Moutons\n",
      "raw text: Jun ware bowl with brown rim glaze pattern fading to blue on the interior and exterior, with drip pattern on exterior\n",
      "raw text: Jüngling am Abend (Youth in the Evening)\n",
      "raw text: Juno and Argus, Guardian of the Jupitern Herds\n",
      "raw text: K'o-ssu tapestry silk roundels depicting flowers in vases\n",
      "raw text: K'o-ssu tapestry silk roundels depicting flowers in vases\n",
      "raw text: Kabuki Actors\n",
      "raw text: Kabuki Theater Signboard Showing an Actor in a Feature Role\n",
      "raw text: Kama (cast iron tea kettle) with Paulownia leaves and flower design\n",
      "raw text: Kanaya, from \"The Fifty-three Stations of the Tokaido\"\n",
      "raw text: Kanzashi (Woman's Hair Ornament) (See Notes)\n",
      "raw text: Kanzashi (Woman's Hair Ornament) Comb with a Design of Branches of Flowers and Leaves\n",
      "raw text: Kanzashi (Woman's Hair Ornament) Comb with a Design of Geese Flying Over Dew-Covered Grasses\n",
      "raw text: Kanzashi (Woman's Hair Ornament) Comb with a Design of Kikyo (Chinese Balloon Flowers)\n",
      "raw text: Kanzashi (Woman's Hair Ornament) Comb with Inlaid and Perforated Floral Motifs\n",
      "raw text: Kanzashi (Woman's Hair Ornament); Comb with a Design of Water and Chrysanthemums\n",
      "raw text: Kanzashi (Woman's Hair Ornament); Comb with Appliqué Design of Lilies and an Ivory Motif of the Sun Rising Over Waves\n",
      "raw text: Karatsu ware cup with an abstract bird design\n",
      "raw text: Karen, Minneapolis\n",
      "raw text: Kashira (pummel cap) of a Japanese sword\n",
      "raw text: Kashira (Sword Handle Pommel) with a Design of a Wind God\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw text: Kayari (insect repellent incense burner) with green glazes\n",
      "raw text: Kettle for tea with a knob in the shape of two No masks from Morioka\n",
      "raw text: Kimono\n",
      "raw text: Kimono coat with a design of peonies and cherry blossoms done in the Yuzen dye technique\n",
      "raw text: Kinchakuzan Hukuzanji Ni Hiromaru Shishu, Volume 3\n",
      "raw text: Kingfisher Perched on a Reed (hanging scroll)\n",
      "raw text: Knackers\n",
      "raw text: Kneeling female figure in prayerful attitude with green glaze\n",
      "raw text: Kogo (incense container) in the Form of a Lion\n",
      "raw text: Kogo (Incense Container) in the Form of a Pine Cone\n",
      "raw text: Kogo (Incense container) in the shape of a boar's head\n",
      "raw text: Kogo (Incense Container) with a Design of a Chrysanthemum and Paulownia Leaves and Flowers\n",
      "raw text: Kogo (Incense Container) with a Design of a Family Crest with Three Leaves\n",
      "raw text: Kogo (Incense Container) with a Design of a Lion Dog on the Lid and Peonies on the Bottom\n",
      "raw text: Kogo (Incense Container) with a Design of Bamboo Grass\n",
      "raw text: Kogo (Incense Container) with a Flame-Shaped Lid\n",
      "raw text: Kogo (Incense Container) with a Floral Motif\n",
      "raw text: Kogo (Incense Container) with Comma-Shaped Design on Lid\n",
      "raw text: Kogo (Lidded Incense Container)\n",
      "raw text: Kosavar-Albanian Refugee, Albanian Border with Kosovo\n",
      "raw text: Kosovar-Albanian Refugees During the War in Kosovo, Kosovo-Albanian Border\n",
      "raw text: Kosovar-Albanian refugees, Macedonia\n",
      "raw text: Kuan type jar with handles with green crackle glaze\n",
      "raw text: Kuan Yin with Mandorla\n",
      "raw text: Kurdish Refugees from Iraq, the Gulf War, Southern Turkey\n",
      "raw text: Kusatsu, from \"The Fifty-three Stations of the Tokaido\"°\n",
      "raw text: L'Apparition\n",
      "raw text: L'Education Fait Tout (Education is the Key to All Success), after Honoré Fragonard\n",
      "raw text: L'empire c'est la paix. (The Empire means peace.), from Album du Siége\n",
      "raw text: L'incendie (The Fire) reproduction\n",
      "raw text: La Astronomie\n",
      "raw text: La Colegiata, Toro\n",
      "raw text: La Dialectique\n",
      "raw text: La Eojida de Ibiza\n",
      "raw text: La Forêt\n",
      "raw text: La Grande Verte et Noire\n",
      "raw text: La Halte\n",
      "raw text: La Jurisprudence\n",
      "raw text: La Malheureuse Family Calas\n",
      "raw text: La Notte (Night), from Paesaggi (Landscapes)\n",
      "raw text: La Physique\n",
      "raw text: La Plaine Prés Du Lac (The Plains near the Lake)\n",
      "raw text: La Pomme\n",
      "raw text: La Rhétorique\n",
      "raw text: La Sculpture\n",
      "raw text: La Théology\n",
      "raw text: Labels of Names of Donors in Osaka\n",
      "raw text: Lacquer box with inlaid coins and a design of lucky symbols with a staff and gourd\n",
      "raw text: Lacquer pipe case with pipe, drawstring bead (ojimi) with geometric\n",
      "raw text: Laissez appuyer moa-, from Album du Siége\n",
      "raw text: Lake George\n",
      "raw text: Lake Landscape with Dock\n",
      "raw text: Lamp with a human mask on lid\n",
      "raw text: Landscape\n",
      "raw text: Landscape\n",
      "raw text: Landscape\n",
      "raw text: Landscape\n",
      "raw text: Landscape\n",
      "raw text: Landscape and river\n",
      "raw text: Landscape II/Landschap II\n",
      "raw text: Landscape Painting\n",
      "raw text: Landscape with Cows\n",
      "raw text: Landscape with Figures\n",
      "raw text: Landscape with Figures\n",
      "raw text: Landscape with Figures in Boat\n",
      "raw text: Landscape with Harbor and Men on Horseback\n",
      "raw text: Landscape with Moon\n",
      "raw text: Landscape with Shepherds\n",
      "raw text: Landscape with Two Figures\n",
      "raw text: Landscape, Purple Sky\n",
      "raw text: Lapwing, Vanellus cristatus, (Meyer)\n",
      "raw text: Large eagle pendant\n",
      "raw text: Large Green Swiss\n",
      "raw text: Large plate with auspicious symbols\n",
      "raw text: Large type two-headed dragon Ch'ing (also referred to as \"chime\" or \"Bridge Money\")\n",
      "raw text: Last of Six Images for J\n",
      "raw text: Laughing - Crying\n",
      "raw text: Le Bon Genre, #110; Jeu de Bague Volante\n",
      "raw text: Le Bon Genre, #5; La Promenade sous le Berceau\n",
      "raw text: Le Bouquet (from \"Croquis Parisiens\" by J.K. Huysmans)\n",
      "raw text: Le danger de manger, from Album du Siége\n",
      "raw text: Le Fantôme\n",
      "raw text: Le Galop de Chasse\n",
      "raw text: Le Louvre et la Seine\n",
      "raw text: Le Portique (The Portico), from the suite \"La Vida es Sueño\" (Life is a Dream)\n",
      "raw text: Le revers de la Médaille de Sainte-Hélène. (The reverse side of the Medal of Sainte-Hélène.), from Album du Siége\n",
      "raw text: Le sang je ne veux pas le voir\n",
      "raw text: Least Bittern\n",
      "raw text: Leaving Home\n",
      "raw text: Leaving the Shop\n",
      "raw text: Leeds Woods\n",
      "raw text: Les fuyards se dissimulant, from Album du Siége\n",
      "raw text: Les Mans\n",
      "raw text: Les Plaisirs du Seigneur\n",
      "raw text: Les Prussiens m'ont donne un ciagre! from Album du Siége\n",
      "raw text: Letter-Hebrew-1997, from American Abstract Artists 60th Anniversary Print Portfolio\n",
      "raw text: Leur retour chez eux\n",
      "raw text: Lidded bowl with a saucer (rose medallion ware)\n",
      "raw text: Lidded container with a design of the Seven Lucky Gods\n",
      "raw text: Lidded jar\n",
      "raw text: Lidded jar with grey-brown underglaze painting\n",
      "raw text: Lidded rectangular box with a design in blue underglaze of a dog\n",
      "raw text: Lidded seal ink container with a design in blue underglaze of a bird and plants\n",
      "raw text: Lidded seal ink container with a design of prawns and leaves\n",
      "raw text: Lidded Seal Ink Container with Floral Pattern\n",
      "raw text: Lidded soy sauce pot with a design in blue underglaze of a boat, reeds and birds\n",
      "raw text: Lidded suwankolok ware jar with a design in black slip of a combination of floral and geometric patterns with gold lacquer repair\n",
      "raw text: Lidded suwankolok ware jar with a design in blue underglaze of a bird and water plants with geometric patterns\n",
      "raw text: Lidded teacup with a design in blue and red underglaze of peonies with gold enamel\n",
      "raw text: Lidded teapot with a design of Taoist immortal Shoulau and a boy in a landscape with a large peach and five orange bats\n",
      "raw text: Light Grove\n",
      "raw text: Limitless, from American Abstract Artists 75th Anniversary Print Portfolio\n",
      "raw text: Lincoln, London & North Eastern Railway of England and Scotland\n",
      "raw text: Lion bobble-head toy\n",
      "raw text: Lion dogs sword ornaments\n",
      "raw text: Lippincott Apple Butter\n",
      "raw text: Lisa and Garth, Plato's Retreat\n",
      "raw text: Little Boatyard, Venice (copy)\n",
      "raw text: Little Sandpiper\n",
      "raw text: Lletra O (Letter O)\n",
      "raw text: Location Plan\n",
      "raw text: Logo Suite Grey, H\n",
      "raw text: Lolita Perturbed\n",
      "raw text: Longquan celadon ware charger\n",
      "raw text: Longquan celadon ware dish with twin-fish motif\n",
      "raw text: Love\n",
      "raw text: Love (trial proof)\n",
      "raw text: Lovers in a Garden\n",
      "raw text: Lucía\n",
      "raw text: Luminous Cat\n",
      "raw text: Lycoris (Higanbana)\n",
      "raw text: Madame Putois, j'aurais tout de meme jamais, from Album du Siége\n",
      "raw text: Mädchen am Sonnenstrand (Maiden on Sunny Beach)\n",
      "raw text: Maintenant j'adore les gens greles, from Album du Siége\n",
      "raw text: Makou Castle Battle Scene at Houko Island (Sino-Japanese War)\n",
      "raw text: Male figure riding in a chariot\n",
      "raw text: Man and Two Wives\n",
      "raw text: Man Cutting Bread\n",
      "raw text: Man fighting with a demon\n",
      "raw text: Man in a Boat and Man in a Building Over Water\n",
      "raw text: Man in a Boat Viewing a Waterfall\n",
      "raw text: Man in Building, Viewing Water\n",
      "raw text: Man in Water Holding Edge of Boat and a Warrior's Helmet\n",
      "raw text: Man seated on a platform with three figures below\n",
      "raw text: Man with a Staff Viewing a Waterfall\n",
      "raw text: Man with a Staff Walking Towards Buildings in Hills\n",
      "raw text: Map of Edo\n",
      "raw text: Marine Life\n",
      "raw text: Marine view with three-masted black schooner in misty weather\n",
      "raw text: Marine View, Teutonic Rescuing the Crew of the Josie Reeves\n",
      "raw text: Marine view, white brigantine, pilot boat #2\n",
      "raw text: Mark Wynn at the Tennessee Medical Association Exhibition, \"Living With the Enemy\"\n",
      "raw text: Marly le roi, France\n",
      "raw text: Mashiko ware plate with an abstract design in brown, black and cream colored glazes\n",
      "raw text: Mask of a Tengu with inscription on inside\n",
      "raw text: Mask, Representing an Antelope\n",
      "raw text: Massacre of the Innocents, after Jacopo Tintoretto\n",
      "raw text: Matthew 7:19\n",
      "raw text: Maze, from American Abstract Artists 60th Anniversary Print Portfolio\n",
      "raw text: Mbuya (initiation mask)\n",
      "raw text: Mechanical Wolf\n",
      "raw text: Medicine steamer\n",
      "raw text: Meleager et Atalanta\n",
      "raw text: Melon shaped hagi ware dish with an inlaid design\n",
      "raw text: Mercado #1\n",
      "raw text: Mercury and Argus\n",
      "raw text: Mickey Rourke, from the series Tribeca, 10013\n",
      "raw text: Military Escort - NOT RECEIVED\n",
      "raw text: Millennium Moment, from American Abstract Artists 60th Anniversary Print Portfolio\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw text: Minamoto Yoshitsune Parting with his Wife Shizuka at Mt. Yoshino\n",
      "raw text: Miniature pot\n",
      "raw text: Miniature Sake Drinking Box\n",
      "raw text: Mirror with a design of flying cranes, tortoise, pine tree with inscription in lacquered case with a design of a pine tree and bamboo\n",
      "raw text: Mirror with a foliate rim with a design of phoenixes and flowers\n",
      "raw text: Mirror with concentric rings with characters\n",
      "raw text: Mirror with design of paulownia, three circles and three lozenges\n",
      "raw text: Mirror with eight petaled rim with a design of a phoenix\n",
      "raw text: Mirror with Engraved Design of the Buddhist Deity Zao Gongen\n",
      "raw text: Mishima ware mizusashi (lidded water jar) for the tea ceremony with inlay design of dragons and clouds with abstract borders with green glaze\n",
      "raw text: Miya, from \"The Fifty-three Stations of the Tokaido\" (reproduction)\n",
      "raw text: Model in landscape\n",
      "raw text: Modern Art Poster\n",
      "raw text: Modern Man Followed By the Ghosts of His Meat\n",
      "raw text: Modes Parisiennes en prévision-, from Album du Siége\n",
      "raw text: Moi, je suis ravitaille!-, from Album du Siége\n",
      "raw text: Momotarō Goes to Devil's Island from the series Yoshitoshi ryakuga (Sketches by Yoshitoshi)\n",
      "raw text: Mont St. Michel: Interior of the Knights Hall\n",
      "raw text: Moonlight Nymphs\n",
      "raw text: Moonrise on the Ternin\n",
      "raw text: Morning\n",
      "raw text: Morse Code (Gattinara, Italy)\n",
      "raw text: Mort de la Madeleine\n",
      "raw text: Mosaic\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# WARNING: there is a daily limit for the Parallel Dots free tier, so don't run this script over and over\n",
    "# or with a large dataset without commenting out the Parallel Dots part of the code\n",
    "# Free tier daily limits are 2000 hits, rate limit 60 hits per minute. NOTE: the limit is\n",
    "# hits, not API calls. So there is no easily predictable way to know how many itereations will be run.\n",
    "\n",
    "# Note: ParallelDots is a commercial product with a free tier. But it requires an account and API key to use.\n",
    "key = load_credential('paralleldots_api_key.txt', 'home')\n",
    "paralleldots.set_api_key(key)\n",
    "\n",
    "filename = 'works_multiprop.csv'\n",
    "works = read_dict(filename)\n",
    "output_list = []\n",
    "\n",
    "# This determines the starting record for the session. When the 2000 hit limit for Parallel Dots is reached,\n",
    "# an error will be thrown. At that point, the loop will end and the data will be written.\n",
    "start_work = 665\n",
    "work_number = start_work\n",
    "for work in works[start_work:]:\n",
    "    work_number += 1 # when the loop fails, this will be recorded in the filename so that we know where to resume next day\n",
    "    print('raw text:', work['label_en'])\n",
    "    work_dict = {'qid': work['qid'], 'label_en': work['label_en'], 'inventory_number': work['inventory_number']}\n",
    "    \n",
    "    # NLP\n",
    "    ne_list = []\n",
    "    # https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list\n",
    "    tokens = nltk.word_tokenize(work['label_en'])\n",
    "    # print('tokens:', tokens)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    # print('tagged:', tagged_tokens)\n",
    "    named_entity_chunks = nltk.ne_chunk(tagged_tokens)\n",
    "    # print('NE chunks:', named_entity_chunks)\n",
    "    \n",
    "    for chunk in named_entity_chunks:\n",
    "        if hasattr(chunk, 'label'):\n",
    "            ne_dict = {'label': chunk.label()}\n",
    "            # A chunk is some kind of iterable of tuples\n",
    "            # Each tuple contains (word, noun_descriptor)\n",
    "            ne_string = chunk[0][0] # 0th tuple, word\n",
    "            # Iterate through the rest of the tuples in the chunk\n",
    "            for additional_tuple in chunk[1:len(chunk)]:\n",
    "                ne_string += ' ' + additional_tuple[0]\n",
    "            ne_dict['string'] = ne_string\n",
    "            ne_list.append(ne_dict)\n",
    "\n",
    "            # print(chunk.label(), ' '.join(c[0] for c in chunk))\n",
    "    # print('NE list:', ne_list)\n",
    "    work_dict['nltk'] = ne_list\n",
    "    # print(json.dumps(work_dict, indent = 2))\n",
    "    \n",
    "    # Spacy\n",
    "    # See https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da \n",
    "    # for codes used in output by Spacy\n",
    "    ne_list = []\n",
    "    result = nlp(work['label_en'])\n",
    "    for entity in result.ents:\n",
    "        ne_list.append({'label': entity.label_, 'string': entity.text})\n",
    "    work_dict['spacy'] = ne_list\n",
    "    \n",
    "    # Parallel Dots API\n",
    "    named_entities = paralleldots.ner(work['label_en'])\n",
    "    \n",
    "    # Convert the Parallel Dots JSON structure to match the others\n",
    "    try: # an error will be thrown if the API data is an error message instead of having an 'entities' key (daily limit)\n",
    "        temp_list = []\n",
    "        for entity in named_entities['entities']:\n",
    "            temp_list.append({'label': entity['category'], 'string': entity['name'], 'confidence_score': entity['confidence_score']})\n",
    "        work_dict['parallel_dots'] = temp_list\n",
    "    except:\n",
    "        break # When the daily limit is exceeded, the loop will terminate and go on to the output code\n",
    "    \n",
    "    sleep(dots_sleep) # This is a delay to prevent hitting the API to frequently and getting blocked\n",
    "\n",
    "    # Append all of the collected data to the accumulation list\n",
    "    output_list.append(work_dict)\n",
    "    \n",
    "output_text = json.dumps(output_list, indent = 2)\n",
    "\n",
    "# the file name includes the first record number and the record number after the last record (i.e. Python range style)\n",
    "out_file_path = 'named_er' + '_' + str(start_work) + '_' + str(work_number) + '.json'\n",
    "with open(out_file_path, 'wt', encoding='utf-8') as file_object:\n",
    "    file_object.write(output_text)\n",
    "print('done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Error': 'Daily Limit Exceeded. Please upgrade your account from your user dashboard at https://user.apis.paralleldots.com/user_dashboard', 'code': 403}\n"
     ]
    }
   ],
   "source": [
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "output_text = json.dumps(output_list, indent = 2)\n",
    "\n",
    "out_file_path = 'named_er' + '_' + str(start_work) + '_' + str(end_work) + '.json'\n",
    "with open(out_file_path, 'wt', encoding='utf-8') as file_object:\n",
    "    file_object.write(output_text)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
