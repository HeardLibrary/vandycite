{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_version = '0.0.1'\n",
    "version_modified = '2024-03-18'\n",
    "\n",
    "# -------------------\n",
    "# Imports\n",
    "# -------------------\n",
    "\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# -------------------\n",
    "# Global variables\n",
    "# -------------------\n",
    "\n",
    "# create a base prompt that will be used for all questions\n",
    "BASE_PROMPT = \"\"\"Below is a CSV file that contains titles of artworks. Each title is delineated by a comma. For each artwork title, give me the list of:\n",
    "- object named entity\n",
    "- location named entity\n",
    "- person named entity\n",
    "- genre named entity\n",
    "- miscellaneous named entity.\n",
    "For each title, format the output as shown in the example given below with the following keys:\n",
    "- TITLE for the title of the artwork\n",
    "- OBJECT for organization named entity\n",
    "- LOCATION for location named entity\n",
    "- PERSON for person named entity\n",
    "- GENRE for genre named entity\n",
    "- MISCELLANEOUS for miscellaneous named entity.\n",
    "Example below: \n",
    "- Title 1: \"Portrait of the Artist Pablo Picasso with a Guitar in Paris\"\n",
    "- Output:\n",
    "{\n",
    "    \"TITLE\": \"Portrait of the Artist Pablo Picasso with a Guitar in Paris\",\n",
    "    \"OBJECT\": [\"guitar\"],\n",
    "    \"LOCATION\": [\"Paris\"],\n",
    "    \"PERSON\": [\"Pablo Picasso\"],\n",
    "    \"GENRE\": [\"portrait\"],\n",
    "    \"MISCELLANEOUS\": [\"artist\"]\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "OPENAI_QUERY_PARAMS = {\n",
    "    \"model\": \"gpt-3.5-turbo\",\n",
    "    \"temperature\": 0,\n",
    "    \"max_tokens\": 1024\n",
    "}\n",
    "\n",
    "# add your API key here\n",
    "openai_key_filename = 'open_ai_api_key_text_analysis.txt'\n",
    "\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "with open(home + '/' + openai_key_filename, 'r') as file:\n",
    "    api_key_string = file.read().strip() # remove any leading or trailing white space or newlines\n",
    "\n",
    "CLIENT = OpenAI(api_key=api_key_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------\n",
    "# Function definitions\n",
    "# -------------------\n",
    "\n",
    "def ask_openai(prompt: str, base_prompt=BASE_PROMPT, openai_query_params=OPENAI_QUERY_PARAMS) -> str:\n",
    "    \"\"\"Send a request to OpenAI's ChatGPT API to do entity recognition. The prompt should be a sentence or paragraph of text\n",
    "    on which you want to perform NER.\n",
    "    \n",
    "    The function returns a JSON-formatted string with the named entities extracted from the input text.\n",
    "    \"\"\"\n",
    "    response = CLIENT.chat.completions.create(\n",
    "        messages=[\n",
    "        {\n",
    "            \"role\": \"system\", \n",
    "            \"content\": \"You are a smart and intelligent Named Entity Recognition (NER) system whose job is to extract entities from the title of an artwork. You will look for people, location names, and common objects. You will also look for genres of visual art and any miscellaneous entities. The labels in your output should not include words that are not in the title.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": base_prompt + prompt\n",
    "        }        \n",
    "    ],\n",
    "        **openai_query_params\n",
    "    )\n",
    "    \n",
    "    return(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"TITLE\": \"Vanderbilt University is a private research university in Nashville, Tennessee. It was founded in 1873.\",\n",
      "    \"OBJECT\": [\"university\"],\n",
      "    \"LOCATION\": [\"Nashville\", \"Tennessee\"],\n",
      "    \"PERSON\": [],\n",
      "    \"GENRE\": [\"research\"],\n",
      "    \"MISCELLANEOUS\": [\"Vanderbilt University\", \"1873\"]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# example \n",
    "example_text = \"Vanderbilt University is a private research university in Nashville, Tennessee. It was founded in 1873.\"\n",
    "print(ask_openai(example_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON\n",
    "\n",
    "endpoint_url = \"https://query.wikidata.org/sparql\"\n",
    "\n",
    "\n",
    "def get_results(endpoint_url, tag):\n",
    "    query = \"\"\"SELECT (count(distinct ?item) as ?count) WHERE{{  \n",
    "      ?item ?depicts \"{tag}\"@en.  \n",
    "      ?article schema:about ?item .\n",
    "      ?article schema:inLanguage \"en\" .\n",
    "      ?article schema:isPartOf <https://en.wikipedia.org/>.\t   \n",
    "    }}\"\"\".format(tag=tag)\n",
    "    user_agent = \"WDQS-example Python/%s.%s\" % (sys.version_info[0], sys.version_info[1])\n",
    "    sparql = SPARQLWrapper(endpoint_url, agent=user_agent)\n",
    "    sparql.setQuery(query)\n",
    "    sparql.setReturnFormat(JSON)\n",
    "    return sparql.query().convert()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NER on Artwork Titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting artwork titles for NER\n",
    "title_pd = pd.read_csv(home + '/GitHub/vandycite/gallery_buchanan/works_multiprop.csv')\n",
    "url_pd = pd.read_csv(home + '/GitHub/vandycite/gallery_buchanan/image_analysis/object_localization_image_urls.csv')\n",
    "\n",
    "# delete all columns except english label and inventory_number\n",
    "title_pd = title_pd[['label_en', 'inventory_number']]\n",
    "\n",
    "# only keep rows of title_pd with inventory_number in url_pd and status = uploaded\n",
    "url_pd = url_pd[url_pd['status'] == 'uploaded']\n",
    "title_pd = title_pd[title_pd['inventory_number'].isin(url_pd['accession_number'])]\n",
    "\n",
    "# isolate titles only to send to OpenAI\n",
    "#title_pd_slice = title_pd['label_en']\n",
    "# TODO: isolate first 15 titles only to send to OpenAI (current token limit might cut off output otherwise)\n",
    "title_pd_slice = title_pd['label_en'].head(15)\n",
    "title_pd_slice = title_pd_slice[~title_pd_slice.str.contains(\"Untitled\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_output = ask_openai(title_pd_slice.to_csv(index=False, header=False))\n",
    "\n",
    "ner_output_list = ner_output.split('}\\n')\n",
    "ner_output_list = [x + '}' for x in ner_output_list]\n",
    "# drop extra '}' from last element\n",
    "ner_output_list[-1] = ner_output_list[-1][:-1]\n",
    "ner_output_json = [json.loads(ner_output_list[i]) for i in range(len(ner_output_list))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking NER Output in Wikidata Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_output_df = pd.DataFrame(columns=['title', 'ner_output', 'wikidata_item', 'no_item'])\n",
    "\n",
    "# iterate through ner_output_json, extract title and ner labels to dataframe\n",
    "for i in range(len(ner_output_json)):\n",
    "    ner_output_df.at[i, 'title'] = ner_output_json[i]['TITLE']\n",
    "    ner_output_df.at[i, 'wikidata_item'] = []\n",
    "    ner_output_df.at[i, 'no_item'] = []\n",
    "    # add the keys that are not title as ner_output in the dataframe\n",
    "    ner_labels = []\n",
    "    for key in ner_output_json[i].keys():\n",
    "        if key != 'TITLE':\n",
    "            ner_labels.extend(ner_output_json[i][key])\n",
    "    ner_output_df.at[i, 'ner_output'] = ner_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(ner_output_df)):\n",
    "    # loop through each label in ner_output\n",
    "    for label in ner_output_df.at[i, 'ner_output']:\n",
    "        results = get_results(endpoint_url, label)\n",
    "        if results[\"results\"][\"bindings\"][0][\"count\"][\"value\"] != '0':\n",
    "            # add label to wikidata_item list\n",
    "            ner_output_df.at[i, 'wikidata_item'].append(label)\n",
    "        else:\n",
    "            # add label to no_item list\n",
    "            ner_output_df.at[i, 'no_item'].append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through rows, only keep unique list items in wikidata_item\n",
    "for index, row in ner_output_df.iterrows():\n",
    "    # convert lists to string\n",
    "    ner_output_df.at[index, 'wikidata_item'] = ', '.join(ner_output_df.at[index, 'wikidata_item'])\n",
    "    ner_output_df.at[index, 'no_item'] = ', '.join(ner_output_df.at[index, 'no_item'])\n",
    "\n",
    "ner_output_df.head()\n",
    "#ner_output_df.to_csv('title_ner_image_wikilabels.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object Detection on Google Vision Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the object_localization_image_urls.csv file into a pandas dataframe\n",
    "object_data = pd.read_csv('object_localization_image_urls.csv', na_filter=False, dtype = str)\n",
    "# object_data.head()\n",
    "\n",
    "\n",
    "prompt = \"What’s the main subject of this artwork?\" # basic prompt\n",
    "#prompt = 'State what is present in this image. Provide only a name for what is depicted, but do not provide details, such as the direction it is oriented.' # specific prompt\n",
    "\n",
    "object_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loop through each row in the dataframe\n",
    "for index, row in object_data.iterrows():\n",
    "    \n",
    "    # If the row already has a GPT description, skip it\n",
    "    #if row['gpt_description'] != '':\n",
    "        #continue\n",
    "    \n",
    "    print('Processing image', index + 1, 'of', len(object_data))\n",
    "    # Get the time at the start of the request\n",
    "    start_time = time.time()\n",
    "\n",
    "    image_url = row['image_url']\n",
    "\n",
    "    incomplete = True\n",
    "    tries = 0\n",
    "    while incomplete:\n",
    "        tries += 1\n",
    "        try:\n",
    "            response = CLIENT.chat.completions.create(\n",
    "              model=\"gpt-4-vision-preview\",\n",
    "              messages=[\n",
    "                {\n",
    "                  \"role\": \"system\", \n",
    "                  \"content\": \"You are an intelligent and concise multi-modal model whose job is to identify what is depicted in an artwork.\"\n",
    "                }, \n",
    "                {\n",
    "                  \"role\": \"system\", \n",
    "                  \"content\": \"I will provide you with artworks that each depict an unknown subject. For each artwork, give me your three best guesses for what the main subject is. Do not consider the type of artwork; only focus on the person or object being depicted in the artwork. Each guess should be two words long.\"\n",
    "                },\n",
    "                {\n",
    "                  \"role\": \"user\",\n",
    "                  \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                    {\n",
    "                      \"type\": \"image_url\",\n",
    "                      \"image_url\": {\n",
    "                        \"url\": image_url,\n",
    "                      },\n",
    "                    },\n",
    "                  ],\n",
    "                }\n",
    "              ],\n",
    "              max_tokens=300,\n",
    "            )\n",
    "            incomplete = False\n",
    "            #print(response)\n",
    "        except Exception as e:\n",
    "            # Print the error message\n",
    "            print(e)\n",
    "            if tries > 5:\n",
    "                print('Error after 5 tries. Skipping this image.')\n",
    "                break\n",
    "            print('Error, waiting 10 seconds.')\n",
    "            time.sleep(10)\n",
    "            print('Retrying.')\n",
    "\n",
    "    if not incomplete: # Only save the response if the request was successful\n",
    "        print(image_url)\n",
    "        # Extract the response from the API\n",
    "        gpt_description = response.choices[0].message.content\n",
    "        print(gpt_description)\n",
    "        total_tokens = response.usage.total_tokens\n",
    "\n",
    "        # Get the time at the end of the request\n",
    "        end_time = time.time()\n",
    "        # Calculate the elapsed time\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        # Add the response to the dataframe\n",
    "        object_data.at[index, 'gpt_description'] = gpt_description\n",
    "        object_data.at[index, 'total_tokens'] = total_tokens\n",
    "        object_data.at[index, 'elapsed_time'] = elapsed_time\n",
    "\n",
    "        # Save the dataframe to a CSV file after each iteration in case it crashes\n",
    "        object_data.to_csv('object_localization_image_urls1.csv', index=False)\n",
    "\n",
    "print('Done.')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load csv, preserve line breaks\n",
    "gptv_output = pd.read_csv('object_localization_image_urls1.csv', na_filter=False, dtype = str)\n",
    "gptv_output.head()\n",
    "\n",
    "# add column wikidata_item, make each value a list\n",
    "gptv_output['wikidata_item'] = ''\n",
    "gptv_output['no_item'] = ''\n",
    "gptv_output['wikidata_item'] = gptv_output['wikidata_item'].apply(lambda x: [])\n",
    "gptv_output['no_item'] = gptv_output['no_item'].apply(lambda x: [])\n",
    "\n",
    "# remove rows that have '[' in the gpt_description, reassign index to remaining rows\n",
    "gptv_output = gptv_output[~gptv_output['gpt_description'].str.contains('\\[')].reset_index(drop=True)\n",
    "# remove numberings like '1.' from gpt_description\n",
    "gptv_output['gpt_description'] = gptv_output['gpt_description'].str.replace(r'\\d+\\. ', '')\n",
    "# make all gpt_description lowercase\n",
    "gptv_output['gpt_description'] = gptv_output['gpt_description'].str.lower()\n",
    "# replace new line characters with commas and a space\n",
    "gptv_output['gpt_description'] = gptv_output['gpt_description'].str.replace('\\n', ', ')\n",
    "# split gpt_description into a list\n",
    "gptv_output['gpt_description'] = gptv_output['gpt_description'].str.split(', ')\n",
    "# if any item starts with a non-alphabetical character, remove it\n",
    "gptv_output['gpt_description'] = gptv_output['gpt_description'].replace(r'^\\W+ ', '', regex=True)\n",
    "\n",
    "gptv_output.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking GPT-V Output in Wikidata Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through rows of gptv_output\n",
    "for index, row in gptv_output.iterrows():\n",
    "    # turn gpt_description into a list\n",
    "    print('querying item', index+1, 'of', len(gptv_output))\n",
    "     # loop through each label in the list\n",
    "    for label in row['gpt_description']:\n",
    "        results = get_results(endpoint_url, label)\n",
    "        if results[\"results\"][\"bindings\"][0][\"count\"][\"value\"] != '0':\n",
    "            # add label to wikidata_item list\n",
    "            gptv_output.at[index, 'wikidata_item'].append(label)\n",
    "            continue\n",
    "         # if label is not in wikidata, split label into two words and search again\n",
    "        split_label = label.split(' ')\n",
    "        results = get_results(endpoint_url, split_label[1])\n",
    "        if results[\"results\"][\"bindings\"][0][\"count\"][\"value\"] != '0':\n",
    "            # add label to wikidata_item list\n",
    "            gptv_output.at[index, 'wikidata_item'].append(split_label[1])\n",
    "        else:\n",
    "            # add label to no_item list\n",
    "            gptv_output.at[index, 'no_item'].append(label)\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through rows, only keep unique list items in wikidata_item\n",
    "for index, row in gptv_output.iterrows():\n",
    "    gptv_output.at[index, 'wikidata_item'] = list(set(gptv_output.at[index, 'wikidata_item']))\n",
    "    # convert lists to string\n",
    "    gptv_output.at[index, 'wikidata_item'] = ', '.join(gptv_output.at[index, 'wikidata_item'])\n",
    "    gptv_output.at[index, 'no_item'] = ', '.join(gptv_output.at[index, 'no_item'])\n",
    "\n",
    "gptv_output.to_csv('object_localization_image_wikilabels.csv', index=False)\n",
    "gptv_output.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
