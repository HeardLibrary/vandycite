{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code to set up NLTK\n",
    "\n",
    "These are one-time actions that need to be done to install packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine arts gallery chunking tests\n",
    "\n",
    "Based on Chapter 7 of Natural Language Processing with Python\n",
    "https://www.nltk.org/book/ch07.html\n",
    "\n",
    "NOTE: the NLTK setup must be done before running this notebook!\n",
    "\n",
    "## Function section\n",
    "\n",
    "This needs to be run before any of the other cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import RegexpParser\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import requests\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# read a CSV from a URL into a list of dictionaries\n",
    "def url_csv_to_list_of_dicts(url):\n",
    "    response = requests.get(url)\n",
    "    file_text = response.text.splitlines()\n",
    "    file_rows = csv.DictReader(file_text)\n",
    "    list_of_dicts = []\n",
    "    for row in file_rows:\n",
    "        list_of_dicts.append(row)\n",
    "    return list_of_dicts\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "\n",
    "Loads gallery metadata (Wikidata Q ID, label, description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this code to load a CSV table from a URL\n",
    "url = 'https://gist.githubusercontent.com/baskaufs/f76c243a4a4ad94d0dd00cdcaca6d8df/raw/3410f020df72cdbdf65d81fed8d0c344c66e7e5b/gallery_works.csv'\n",
    "works = url_csv_to_list_of_dicts(url)\n",
    "\n",
    "# Use this code to load a CSV table from local file\n",
    "#filename = 'works_multiprop.csv'\n",
    "#works = read_dict(filename)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select a work and examine the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some works to try: 36, 6293, 6560, 6789\n",
    "work_number = 6293\n",
    "print(json.dumps(works[work_number], indent=2))\n",
    "print()\n",
    "print(works[work_number]['label_en'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform tokenization and tagging\n",
    "\n",
    "See https://www.nltk.org/book/ch03.html about tokenization.\n",
    "\n",
    "See https://www.nltk.org/book/ch05.html about tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "work = works[work_number]\n",
    "tokens = nltk.word_tokenize(work['label_en'])\n",
    "print('tokens:', tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print('tagged:', tagged_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chunking text\n",
    "\n",
    "The commented out `grammar` assignments offer alternative sets of rules for doing the chunking.\n",
    "\n",
    "See https://www.h2kinfosys.com/blog/part-of-speech-tagging-chunking-with-nltk/ for codes used to tag the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grammar= \"\"\"chunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\" # test pattern\n",
    "\n",
    "# grammar = \"NP: {<DT>?<JJ>*<NN>}\" # noun phrase detection\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ.*>*<NN.*>+}\" # modified NP detection\n",
    "\n",
    "#grammar = r\"\"\"\n",
    "#  NP: {<DT|PP\\$>?<JJ>*<NN>.*}   # chunk determiner/possessive, adjectives and noun\n",
    "#      {<NNP>+}                # chunk sequences of proper nouns\n",
    "#\"\"\"\n",
    "  \t\n",
    "# Chinking example:\n",
    "#grammar = r\"\"\"\n",
    "#  NP:\n",
    "#    {<.*>+}          # Chunk everything\n",
    "#    }<VBD|IN>+{      # Chink sequences of VBD and IN\n",
    "#  \"\"\"\n",
    "\n",
    "chunker = RegexpParser(grammar)\n",
    "print(\"Chunker summary:\", chunker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = chunker.parse(tagged_tokens)\n",
    "print(\"chunks:\",chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you run the following cell on a local Jupyter notebook, the diagram pops up in a separate window.  That window must be closed to stop the cell from running in order to be able re-run the chunking cell again.\n",
    "\n",
    "Sometimes the popup is below other windows and you may need to click on the \"python\" icon in the task bar to bring it to the front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named entity recognition\n",
    "\n",
    "Seems to be heavily dependent on capitalization, so not that great for titles\n",
    "\n",
    "Code hacked from https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires the \"Perform tokenization and tagging\" cell to be run first.\n",
    "# Try with row 6293\n",
    "named_entity_chunks = nltk.ne_chunk(tagged_tokens)\n",
    "print('NE chunks:', named_entity_chunks)\n",
    "print()\n",
    "\n",
    "ne_list = []\n",
    "for chunk in named_entity_chunks:\n",
    "    if hasattr(chunk, 'label'):\n",
    "        ne_dict = {'ne_label': chunk.label()}\n",
    "        # A chunk is some kind of iterable of tuples\n",
    "        # Each tuple contains (word, noun_descriptor)\n",
    "        ne_string = chunk[0][0] # 0th tuple, word\n",
    "        # Iterate through the rest of the tuples in the chunk\n",
    "        for additional_tuple in chunk[1:len(chunk)]:\n",
    "            ne_string += ' ' + additional_tuple[0]\n",
    "        ne_dict['ne_string'] = ne_string\n",
    "        ne_list.append(ne_dict)\n",
    "\n",
    "        # Print results for humans to see\n",
    "        print(chunk.label(), ' '.join(c[0] for c in chunk))\n",
    "\n",
    "# List of dictionaries format for subsequent use or output as a CSV\n",
    "print()\n",
    "print('NE list:', ne_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell if running locally and you want a diagram of the NER chunks.\n",
    "# It will open in a separate window that must be closed before any cell can be run again.\n",
    "# Sometimes it opens under other windows and you must click on its icon in the dock to make\n",
    "# it come to the frong.\n",
    "named_entity_chunks.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
