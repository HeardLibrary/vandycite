{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine arts gallery label language detection\n",
    "\n",
    "This is a script to attempt to sort out the Gallery works labels by language, particularly to detect which ones are in English\n",
    "\n",
    "## Function section\n",
    "\n",
    "This needs to be run before any of the other cells\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from time import sleep\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from copy import deepcopy\n",
    "from langdetect import detect\n",
    "from langdetect import detect_langs\n",
    "import datetime\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import RegexpParser\n",
    "\n",
    "# ----------------\n",
    "# Configuration settings\n",
    "# ----------------\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "dots_sleep = 1 # number of seconds to wait between calls to ParallelDots API\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "# Calculate the reference date retrieved value for all statements\n",
    "whole_time_string_z = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "dateZ = whole_time_string_z.split('T')[0] # form 2019-12-05\n",
    "ref_retrieved = dateZ + 'T00:00:00Z' # form 2019-12-05T00:00:00Z as provided by Wikidata, without leading +\n",
    "\n",
    "# ----------------\n",
    "# Utility functions\n",
    "# ----------------\n",
    "\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderBot/1.6 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    request_header_dictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return request_header_dictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def load_credential(filename, directory):\n",
    "    cred = ''\n",
    "    # to change the script to look for the credential in the working directory, change the value of home to empty string\n",
    "    if directory == 'home':\n",
    "        home = str(Path.home()) #gets path to home directory; works for both Win and Mac\n",
    "        credential_path = home + '/' + filename\n",
    "    else:\n",
    "        directory = 'working'\n",
    "        credential_path = filename\n",
    "    try:\n",
    "        with open(credential_path, 'rt', encoding='utf-8') as file_object:\n",
    "            cred = file_object.read()\n",
    "    except:\n",
    "        print(filename + ' file not found - is it in your ' + directory + ' directory?')\n",
    "        exit()\n",
    "    return(cred)\n",
    "\n",
    "# find non-redundant values for a column or simple list\n",
    "def non_redundant(table, column_key):\n",
    "    non_redundant_list = []\n",
    "    for row in table:\n",
    "        found = False\n",
    "        for test_item in non_redundant_list:\n",
    "            if column_key == '':\n",
    "                if row == test_item:\n",
    "                    found = True\n",
    "                    break\n",
    "            else:\n",
    "                if row[column_key] == test_item:\n",
    "                    found = True\n",
    "                    break\n",
    "        if not found:\n",
    "            if column_key == '':\n",
    "                non_redundant_list.append(row)\n",
    "            else:\n",
    "                non_redundant_list.append(row[column_key])\n",
    "    return non_redundant_list\n",
    "\n",
    "# function to use in sort of simple list\n",
    "def sort_funct(row):\n",
    "    return row\n",
    "\n",
    "# function to use in sort last_first names\n",
    "def sort_last_first(row):\n",
    "    return row['last_first']\n",
    "\n",
    "# function to use in sort by match score\n",
    "def sort_score(row):\n",
    "    return row['score']\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# search label and alias\n",
    "# For whatever reason, if I use the graph pattern\n",
    "\n",
    "# wd:Q21 wdt:P31 ?class.\n",
    "\n",
    "# England is not Q6256 (country)\n",
    "# But if I use the graph pattern\n",
    "\n",
    "#   wd:Q21 p:P31 ?statement.\n",
    "#  ?statement ps:P31 ?class.\n",
    "\n",
    "# it is ??!!\n",
    "def searchLabelsAtWikidata(string, class_list):\n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id '\n",
    "    query += '''where {\n",
    "  {?id rdfs:label \"''' + string + '''\"@en.}\n",
    "  union\n",
    "  {?id skos:altLabel \"''' + string + '''\"@en.}\n",
    "  '''\n",
    "    for class_index in range(len(class_list)):\n",
    "        if class_index == 0:\n",
    "            query += '''{?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "        else:\n",
    "            query += '''union\n",
    "  {?id p:P31 ?statement.\n",
    "  ?statement ps:P31 wd:''' + class_list[class_index] + '''.}\n",
    "  '''\n",
    "    query += '''}'''\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    # r = requests.get(endpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_qnumber(result['id']['value'])\n",
    "        return_value.append(qid)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def retrieve_gallery_classes():\n",
    "    # create a string for the query\n",
    "    # use Metropolitan Museum of Art because there are too many collections to not specify the collection.\n",
    "    query = '''select distinct ?class ?label where \n",
    "      {\n",
    "      ?item wdt:P195 wd:Q160236.\n",
    "      ?item wdt:P31 ?class.\n",
    "      ?class rdfs:label ?label.\n",
    "      filter(lang(?label) = 'en')\n",
    "      }\n",
    "      order by ?label'''\n",
    "\n",
    "    #print(query)\n",
    "\n",
    "    return_value = []\n",
    "    print('sending query')\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=generate_header_dictionary(accept_media_type))\n",
    "    print('results returned')\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        qid = extract_qnumber(result['class']['value'])\n",
    "        label = result['label']['value']\n",
    "        return_value.append({'label': label, 'qid': qid})\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    \n",
    "    return return_value\n",
    "\n",
    "def generateNameAlternatives(name):\n",
    "    # treat commas as if they were spaces\n",
    "    name = name.replace(',', ' ')\n",
    "    # get rid of periods\n",
    "    name = name.replace('.', '')\n",
    "\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # Remove \", Jr.\", \"III\", etc. from end of name\n",
    "    if pieces[len(pieces)-1] == 'Jr':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ', Jr.'\n",
    "    elif pieces[len(pieces)-1] == 'II':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' II'\n",
    "    elif pieces[len(pieces)-1] == 'III':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' III'\n",
    "    elif pieces[len(pieces)-1] == 'IV':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' IV'\n",
    "    elif pieces[len(pieces)-1] == 'V':\n",
    "        pieces = pieces[0:len(pieces)-1]\n",
    "        suffix = ' V'\n",
    "    elif len(pieces) > 3 and pieces[len(pieces)-2] == 'the' and pieces[len(pieces)-1] == 'elder':\n",
    "        pieces = pieces[0:len(pieces)-2]\n",
    "        suffix = ' the elder'\n",
    "    else:\n",
    "        suffix = ''\n",
    "\n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        # make sure first character is alphabetic\n",
    "        # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "        # typical cases are like (Kit) or \"Kit\"\n",
    "        if not piece[0:1].isalpha():\n",
    "            piece = piece[1:len(piece)] # remove the first non-alphabetic character\n",
    "        if len(piece) > 0:\n",
    "            initials.append(piece[0:1])\n",
    "        \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += pieces[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # full name with suffix\n",
    "    if suffix != '':\n",
    "        nameVersion = ''\n",
    "        for pieceNumber in range(0, len(pieces)-1):\n",
    "            nameVersion += pieces[pieceNumber] + ' '\n",
    "        nameVersion += pieces[len(pieces)-1] + suffix\n",
    "        alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first and last name only\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial and last name only\n",
    "    nameVersion = initials[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    nameVersion = initials[0] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with last name\n",
    "    nameVersion = initials[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber]\n",
    "    nameVersion += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def searchNameAtWikidata(name):\n",
    "    nameList = generateNameAlternatives(name)\n",
    "    alternatives = ''\n",
    "    for alternative in nameList:\n",
    "        # get rid of quotes, which will break the query\n",
    "        alternative = alternative.replace('\"', '')\n",
    "        alternative = alternative.replace(\"'\", '')\n",
    "        alternatives += '\"' + alternative + '\"@en\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    results = []\n",
    "    # r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            if 'label' in statement:\n",
    "                name = statement['label']['value']\n",
    "            else:\n",
    "                name = ''\n",
    "            qNumber = vbc.extract_qnumber(wikidataIri)\n",
    "            results.append({'qId': qNumber, 'name': name})\n",
    "    except:\n",
    "        results = [{'error': r.text}]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return results\n",
    "\n",
    "def name_variant_testing(name, variant):\n",
    "    # get rid of periods\n",
    "    name = name.replace('.','')\n",
    "    variant = variant.replace('.','')\n",
    "    \n",
    "    # create first names\n",
    "    name_pieces = name.split(' ')\n",
    "    variant_pieces = variant.split(' ')\n",
    "    last_name = name_pieces[len(name_pieces)-1]\n",
    "    last_variant = variant_pieces[len(variant_pieces)-1]\n",
    "    if len(name_pieces) > 1:\n",
    "        first_names = name[0:-(len(last_name)+1)]\n",
    "    else:\n",
    "        first_names = name     \n",
    "    if len(variant_pieces) > 1:\n",
    "        first_variants = variant[0:-(len(last_variant)+1)]\n",
    "    else:\n",
    "        first_variants = variant      \n",
    "    #print(first_names)\n",
    "    #print(first_variants)\n",
    "    \n",
    "    # compare first names\n",
    "    # I experimented with the different ratios and I think fuzz might be best.\n",
    "    ratio = fuzz.ratio(first_names, first_variants)\n",
    "    #partial_ratio = fuzz.partial_ratio(first_names, first_variants)\n",
    "    #sort_ratio = fuzz.token_sort_ratio(first_names, first_variants)\n",
    "    #set_ratio = fuzz.token_set_ratio(first_names, first_variants)\n",
    "    # print('name similarity ratio', ratio)\n",
    "    #print('partial ratio', partial_ratio)\n",
    "    #print('sort_ratio', sort_ratio)\n",
    "    #print('set_ratio', set_ratio)\n",
    "\n",
    "    return(ratio)\n",
    "\n",
    "def find_surname_givens(name):\n",
    "    # Get rid of periods and commas\n",
    "    name = name.replace('.', '')\n",
    "    name = name.replace(',', '')\n",
    "    \n",
    "    # Split name\n",
    "    pieces = name.split(' ')\n",
    "    # Must be at least a surname and something else\n",
    "    if len(pieces) <= 1:\n",
    "        return False\n",
    "    \n",
    "    # Make sure first character is alphabetic\n",
    "    # only fixes the case where there is one alphanumeric, but more than one is rare\n",
    "    # typical cases are like (Kit) or \"Kit\"    \n",
    "    for piece_index in range(len(pieces)):\n",
    "        if not pieces[piece_index][0:1].isalpha(): \n",
    "            pieces[piece_index] = pieces[piece_index][1:len(pieces)] # remove the first non-alphabetic character\n",
    "    # Now get rid of any empty strings; could also be caused by double spaces\n",
    "    for piece in pieces:\n",
    "        if len(piece) == 0: # there's nothing left, get rid of piece\n",
    "            pieces.remove('')\n",
    "            \n",
    "    # Get rid of \", Jr.\", \"III\", etc.\n",
    "    if 'Jr' in pieces:\n",
    "        pieces.remove('Jr')\n",
    "    if 'Sr' in pieces:\n",
    "        pieces.remove('Sr')\n",
    "    if 'II' in pieces:\n",
    "        pieces.remove('II')\n",
    "    if 'III' in pieces:\n",
    "        pieces.remove('III')\n",
    "    if 'IV' in pieces:\n",
    "        pieces.remove('IV')\n",
    "    if 'V' in pieces:\n",
    "        pieces.remove('V')\n",
    "    \n",
    "    # Not interested unless there are at least two pieces\n",
    "    if len(pieces) == 1:\n",
    "        return False\n",
    "    \n",
    "    # Put all but last piece together again\n",
    "    given_names = ''\n",
    "    for piece in pieces[0:len(pieces)-2]:\n",
    "        given_names += piece + ' '\n",
    "    given_names += pieces[len(pieces)-2]\n",
    "    \n",
    "    return {'given': given_names, 'family': pieces[len(pieces)-1]}\n",
    "\n",
    "def remove_parens(string):\n",
    "    name_string = string.split('(')[0]\n",
    "    return name_string.strip()\n",
    "\n",
    "def remove_description(string):\n",
    "    try:\n",
    "        right_string = string.split('(')[1]\n",
    "        left_string = right_string.split(')')[0]\n",
    "        result = left_string.strip()\n",
    "    except:\n",
    "        result = ''\n",
    "    return result\n",
    "\n",
    "def reverse_names(string):\n",
    "    pieces = string.split(',')\n",
    "    return pieces[1].strip() + ' ' + pieces[0].strip()\n",
    "\n",
    "# Screens for Wikidata items that are potential matches\n",
    "\n",
    "import vb_common_code as vbc\n",
    "retrieve_class_list_query = vbc.Query(pid='P31', uselabel=False, sleep=sparql_sleep)\n",
    "retrieve_birth_date_query = vbc.Query(isitem=False, pid='P569', sleep=sparql_sleep)\n",
    "retrieve_death_date_query = vbc.Query(isitem=False, pid='P570', sleep=sparql_sleep)\n",
    "\n",
    "def human(qId):\n",
    "    screen = True\n",
    "    wdClassList = retrieve_class_list_query.single_property_values_for_item(qId)\n",
    "    # if there is a class property, check if it's a human\n",
    "    if len(wdClassList) != 0:\n",
    "        # if it's not a human\n",
    "        if wdClassList[0] != 'Q5':\n",
    "            #print('*** This item is not a human!')\n",
    "            screen = False\n",
    "    return screen\n",
    "\n",
    "# returns a dictionary of various descriptors of the item with Wikidata ID qId\n",
    "# P106 is occupation, schema:description is filtered to be the English description\n",
    "def searchWikidataDescription(qId):\n",
    "    resultsDict = {}\n",
    "    query = '''select distinct ?description ?orcid ?occupation where {\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' schema:description ?description.\n",
    "            FILTER(lang(?description) = 'en')\n",
    "            }\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = 'en')            \n",
    "            }\n",
    "        optional {wd:'''+ qId + ''' wdt:P496 ?orcid.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the dictionary remains empty\n",
    "            # Only a single description per language is allowed, so there should only be one description\n",
    "            if 'description' in statements[0]:\n",
    "                description = statements[0]['description']['value']\n",
    "            else:\n",
    "                description = ''\n",
    "            resultsDict['description'] = description\n",
    "            \n",
    "            # Only a single ORCID is allowed, so there should only be one orcid value\n",
    "            if 'orcid' in statements[0]:\n",
    "                orcid = statements[0]['orcid']['value']\n",
    "            else:\n",
    "                orcid = ''\n",
    "            resultsDict['orcid'] = orcid\n",
    "            \n",
    "            # if there are multiple statements, that's because there are more than one occupation\n",
    "            occupationList = []\n",
    "            for statement in statements:\n",
    "                if 'occupation' in statement:\n",
    "                    occupationList.append(statement['occupation']['value'])\n",
    "            resultsDict['occupation'] = occupationList\n",
    "    except:\n",
    "        resultsDict = {'error': r.text}\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(sparql_sleep)\n",
    "    return resultsDict\n",
    "\n",
    "def determine_era(string):\n",
    "    # dates with no CE or BCE, including empty string\n",
    "    if 'CE' not in string:\n",
    "        value = string\n",
    "        era = 'unknown'      \n",
    "    else:\n",
    "        if 'BCE' in string:\n",
    "            value = string[0:len(string)-3].strip()\n",
    "            era = 'BCE'\n",
    "        else: # string ends with CE\n",
    "            value = string[0:len(string)-2].strip()\n",
    "            era = 'CE'\n",
    "    return value, era\n",
    "\n",
    "def determine_zeros(date):\n",
    "    zero_count = 0\n",
    "    for char_number in range(len(date), 0, -1):\n",
    "        if date[char_number-1] == '0':\n",
    "            zero_count += 1\n",
    "        else:\n",
    "            return zero_count\n",
    "\n",
    "def pad_zeros_left(date_string):\n",
    "    length = len(date_string)\n",
    "    pad = 4-length\n",
    "    return '0' * pad + date_string\n",
    "\n",
    "def sign(era):\n",
    "    if era == 'BCE':\n",
    "        return '-'\n",
    "    elif era == 'CE':\n",
    "        return ''\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "def detect_language(string):\n",
    "    try:\n",
    "        lang_list = detect_langs(string)\n",
    "        lang_string = str(lang_list[0])\n",
    "        confidence = float(lang_string[3:])\n",
    "        lang = lang_string[:2]\n",
    "    except: #exceptions occur when no info to decide, e.g. numbers\n",
    "        lang = 'zxx'\n",
    "        confidence = float(0)\n",
    "    return lang, confidence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "\n",
    "Loads all of the gallery metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'works_multiprop.csv'\n",
    "works = read_dict(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'qid': 'Q102961843', 'label_en': 'Autumn, Sowing of Grain', 'description_en': 'image by Artist Unknown', 'inventory_number_uuid': 'AAC54739-960E-4E03-9DEF-6A21D0EAF430', 'inventory_number': '1979.0138', 'inventory_number_collection': 'Q18563658', 'inventory_number_ref1_hash': 'c607a636ef4f13b44e87bfce1b0a5b86b25b572c', 'inventory_number_ref1_statedIn': '', 'inventory_number_ref1_referenceUrl': 'https://library.artstor.org/#/asset/26754349', 'inventory_number_ref1_retrieved_nodeId': '0f364e2fcaf6b3e5870f19adb2cba00e', 'inventory_number_ref1_retrieved_val': '2020-11-30T00:00:00Z', 'inventory_number_ref1_retrieved_prec': '11', 'title_uuid': 'B9A9055C-9AAC-4951-9B8C-0891707EA44F', 'title': 'Autumn, Sowing of Grain', 'title_subtitle': '', 'title_ref1_hash': 'c607a636ef4f13b44e87bfce1b0a5b86b25b572c', 'title_ref1_statedIn': '', 'title_ref1_referenceUrl': 'https://library.artstor.org/#/asset/26754349', 'title_ref1_retrieved_nodeId': '0f364e2fcaf6b3e5870f19adb2cba00e', 'title_ref1_retrieved_val': '2020-11-30T00:00:00Z', 'title_ref1_retrieved_prec': '11', 'creator_uuid': '8417DF67-94B4-4E08-84B8-D96BA7B71C6F', 'creator': 'Q4233718', 'creator_ref1_hash': 'c607a636ef4f13b44e87bfce1b0a5b86b25b572c', 'creator_ref1_statedIn': '', 'creator_ref1_referenceUrl': 'https://library.artstor.org/#/asset/26754349', 'creator_ref1_retrieved_nodeId': '0f364e2fcaf6b3e5870f19adb2cba00e', 'creator_ref1_retrieved_val': '2020-11-30T00:00:00Z', 'creator_ref1_retrieved_prec': '11', 'instance_of_uuid': '65964E15-4725-4641-9057-3A345136A495', 'instance_of': 'Q478798', 'instance_of_ref1_hash': 'c607a636ef4f13b44e87bfce1b0a5b86b25b572c', 'instance_of_ref1_statedIn': '', 'instance_of_ref1_referenceUrl': 'https://library.artstor.org/#/asset/26754349', 'instance_of_ref1_retrieved_nodeId': '0f364e2fcaf6b3e5870f19adb2cba00e', 'instance_of_ref1_retrieved_val': '2020-11-30T00:00:00Z', 'instance_of_ref1_retrieved_prec': '11', 'inception_uuid': '', 'inception_nodeId': '', 'inception_val': '', 'inception_prec': '', 'inception_earliest_date_nodeId': '', 'inception_earliest_date_val': '', 'inception_earliest_date_prec': '', 'inception_latest_date_nodeId': '', 'inception_latest_date_val': '', 'inception_latest_date_prec': '', 'inception_sourcing_circumstances': '', 'inception_ref1_hash': '', 'inception_ref1_statedIn': '', 'inception_ref1_referenceUrl': '', 'inception_ref1_retrieved_nodeId': '', 'inception_ref1_retrieved_val': '', 'inception_ref1_retrieved_prec': '', 'country_uuid': 'FD0C525F-B55E-4B7A-A214-2B6F85514CBF', 'country': 'Q145', 'country_ref1_hash': 'c607a636ef4f13b44e87bfce1b0a5b86b25b572c', 'country_ref1_statedIn': '', 'country_ref1_referenceUrl': 'https://library.artstor.org/#/asset/26754349', 'country_ref1_retrieved_nodeId': '0f364e2fcaf6b3e5870f19adb2cba00e', 'country_ref1_retrieved_val': '2020-11-30T00:00:00Z', 'country_ref1_retrieved_prec': '11', 'country_of_origin_uuid': 'DA566E2D-856A-4117-8620-65D400586C9D', 'country_of_origin': 'Q21', 'country_of_origin_ref1_hash': 'c607a636ef4f13b44e87bfce1b0a5b86b25b572c', 'country_of_origin_ref1_statedIn': '', 'country_of_origin_ref1_referenceUrl': 'https://library.artstor.org/#/asset/26754349', 'country_of_origin_ref1_retrieved_nodeId': '0f364e2fcaf6b3e5870f19adb2cba00e', 'country_of_origin_ref1_retrieved_val': '2020-11-30T00:00:00Z', 'country_of_origin_ref1_retrieved_prec': '11', 'height_uuid': '07A11EFB-E37D-4278-A4E5-B39194D86B3B', 'height_nodeId': 'a81a0d0df44bd20752b69d67d2ad15e7', 'height_val': '9.5', 'height_unit': 'Q218593', 'height_ref1_hash': 'c607a636ef4f13b44e87bfce1b0a5b86b25b572c', 'height_ref1_statedIn': '', 'height_ref1_referenceUrl': 'https://library.artstor.org/#/asset/26754349', 'height_ref1_retrieved_nodeId': '0f364e2fcaf6b3e5870f19adb2cba00e', 'height_ref1_retrieved_val': '2020-11-30T00:00:00Z', 'height_ref1_retrieved_prec': '11', 'width_uuid': 'B2F6284C-54BC-4725-925A-15153C90997C', 'width_nodeId': 'a882999ab80a4444a81845a7a61e9a05', 'width_val': '12.5', 'width_unit': 'Q218593', 'width_ref1_hash': 'c607a636ef4f13b44e87bfce1b0a5b86b25b572c', 'width_ref1_statedIn': '', 'width_ref1_referenceUrl': 'https://library.artstor.org/#/asset/26754349', 'width_ref1_retrieved_nodeId': '0f364e2fcaf6b3e5870f19adb2cba00e', 'width_ref1_retrieved_val': '2020-11-30T00:00:00Z', 'width_ref1_retrieved_prec': '11', 'thickness_uuid': '', 'thickness_nodeId': '', 'thickness_val': '', 'thickness_unit': '', 'thickness_ref1_hash': '', 'thickness_ref1_statedIn': '', 'thickness_ref1_referenceUrl': '', 'thickness_ref1_retrieved_nodeId': '', 'thickness_ref1_retrieved_val': '', 'thickness_ref1_retrieved_prec': '', 'diameter_uuid': '', 'diameter_nodeId': '', 'diameter_val': '', 'diameter_unit': '', 'diameter_ref1_hash': '', 'diameter_ref1_statedIn': '', 'diameter_ref1_referenceUrl': '', 'diameter_ref1_retrieved_nodeId': '', 'diameter_ref1_retrieved_val': '', 'diameter_ref1_retrieved_prec': '', 'inscription_uuid': '', 'inscription': '', 'inscription_ref1_hash': '', 'inscription_ref1_statedIn': '', 'inscription_ref1_referenceUrl': '', 'inscription_ref1_retrieved_nodeId': '', 'inscription_ref1_retrieved_val': '', 'inscription_ref1_retrieved_prec': '', 'copyright_status_uuid': '', 'copyright_status': '', 'copyright_status_applies_to_jurisdiction': '', 'copyright_status_determination_method': '', 'copyright_status_ref1_hash': '', 'copyright_status_ref1_statedIn': '', 'copyright_status_ref1_referenceUrl': '', 'copyright_status_ref1_retrieved_nodeId': '', 'copyright_status_ref1_retrieved_val': '', 'copyright_status_ref1_retrieved_prec': '', 'image_uuid': '', 'image': '', 'image_ref1_hash': '', 'image_ref1_statedIn': '', 'image_ref1_referenceUrl': '', 'image_ref1_retrieved_nodeId': '', 'image_ref1_retrieved_val': '', 'image_ref1_retrieved_prec': '', 'collection_uuid': '4D361FD2-AE49-4B4A-985E-6E43714F78A8', 'collection': 'Q18563658', 'collection_start_time_nodeId': '378d534a05a3fbe28245ad24b80e70be', 'collection_start_time_val': '1979-01-01T00:00:00Z', 'collection_start_time_prec': '9', 'collection_end_time_nodeId': '', 'collection_end_time_val': '', 'collection_end_time_prec': '', 'collection_ref1_hash': 'c607a636ef4f13b44e87bfce1b0a5b86b25b572c', 'collection_ref1_statedIn': '', 'collection_ref1_referenceUrl': 'https://library.artstor.org/#/asset/26754349', 'collection_ref1_retrieved_nodeId': '0f364e2fcaf6b3e5870f19adb2cba00e', 'collection_ref1_retrieved_val': '2020-11-30T00:00:00Z', 'collection_ref1_retrieved_prec': '11', 'location_uuid': '03F23B74-3DBD-4D8F-8D50-E3FA516BB310', 'location': 'Q18563658', 'location_ref1_hash': 'c607a636ef4f13b44e87bfce1b0a5b86b25b572c', 'location_ref1_statedIn': '', 'location_ref1_referenceUrl': 'https://library.artstor.org/#/asset/26754349', 'location_ref1_retrieved_nodeId': '0f364e2fcaf6b3e5870f19adb2cba00e', 'location_ref1_retrieved_val': '2020-11-30T00:00:00Z', 'location_ref1_retrieved_prec': '11'}\n"
     ]
    }
   ],
   "source": [
    "print(works[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
