{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "To use the Neptune SPARQL endpoint, open the SSH tunnel using\n",
    "\n",
    "```\n",
    "ssh neptune -N\n",
    "```\n",
    "\n",
    "which reads the `~/.ssh/config` file containing:\n",
    "\n",
    "```\n",
    "host neptune-demo\n",
    " ForwardAgent yes\n",
    " User ec2-user # when using Amazon Linux\n",
    " HostName <your-ec2-address>\n",
    " IdentitiesOnly yes\n",
    " IdentityFile ~/.ssh/<your-ec2-key-file>.pem\n",
    " LocalForward 8182 <cluster-endpoint-neptune>:8182\n",
    " ```\n",
    " \n",
    " This allows access to the server through port 8182. \n",
    " \n",
    " NOTE: at least on my computer, the Terminal window does not produce a prompt after executing this command. However, it does seem to work, since I get a response when I issue this command in a different Terminal window:\n",
    " \n",
    "```\n",
    "curl https://triplestore1.cluster-cml0hq81gymg.us-east-1.neptune.amazonaws.com:8182/status\n",
    "```\n",
    "\n",
    "NOTE: The dump of `AATOut_WikidataCoref.nt` from Getty was missing the period at the end of each triple. I had to add it before the file would load into Neptune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "from time import sleep\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "endpoint = 'https://triplestore1.cluster-cml0hq81gymg.us-east-1.neptune.amazonaws.com:8182/sparql'\n",
    "wdqs_endpoint = 'https://query.wikidata.org/sparql'\n",
    "user_agent_header = 'thesaurus_crosswalk/0.1 (https://github.com/HeardLibrary/linked-data/; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "\n",
    "sparql_request_header = {\n",
    "        'Accept' : 'application/json',\n",
    "#        'Content-Type': 'application/sparql-query',\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "\n",
    "# Low level functions\n",
    "\n",
    "def send_sparql_query(query_string, endpoint):\n",
    "    \"\"\"Sends a SPARQL query to an endpoint URL. Argument is the query string, returns a list of results.\"\"\"\n",
    "    # You can delete the two print statements if the queries are short. However, for large/long queries,\n",
    "    # it's good to let the user know what's going on.\n",
    "    #print('querying SPARQL endpoint to acquire item metadata')\n",
    "    #response = requests.post(endpoint, data=query_string.encode('utf-8'), headers=sparql_request_header)\n",
    "    response = requests.post(endpoint, data=dict(query=query_string), headers=sparql_request_header)\n",
    "    #print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "    try:\n",
    "        data = response.json()\n",
    "    except:\n",
    "        print(response.text)\n",
    "\n",
    "    # Extract the values from the response JSON\n",
    "    results = data['results']['bindings']\n",
    "    \n",
    "    #print('done retrieving data')\n",
    "    #print(json.dumps(results, indent=2))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def extract_local_name(iri):\n",
    "    \"\"\"Extracts the local name part of an IRI, e.g. a qNumber from a Wikidata IRI. Argument is the IRI, returns the local name string.\"\"\"\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    last_piece = len(pieces)\n",
    "    return pieces[last_piece - 1]\n",
    "\n",
    "# Write list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# Read from a CSV file into a list of dictionaries\n",
    "def read_dicts_from_csv(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts\n",
    "\n",
    "## Find out what graphs are in the triplestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = '''select distinct ?graph where {\n",
    "graph ?graph {?s ?o ?p.}\n",
    "}'''\n",
    "\n",
    "results = send_sparql_query(query_string, endpoint)\n",
    "for result in results:\n",
    "    graph_iri = result['graph']['value']\n",
    "    print(graph_iri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine frequency of unique noun values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_parts = pd.read_csv('3d_parts.csv', na_filter=False, dtype = str)\n",
    "#print(label_parts.head())\n",
    "\n",
    "# Count the unique values of modified nouns. result is a Pandas series with the noun as the index and count as value\n",
    "nouns = label_parts['noun-modified'].value_counts()\n",
    "\n",
    "# Build a data frame from the series\n",
    "#for noun, freq in nouns.iteritems():\n",
    "#    id_dict = {'noun': noun, 'freq': freq}\n",
    "\n",
    "thesaurus_ids = pd.DataFrame({'noun': nouns.index, 'freq': nouns.values})\n",
    "#thesaurus_ids = thesaurus_ids.head(3).copy() # uncomment to test on a few nouns\n",
    "thesaurus_ids.head()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Neptune and the WD Query Service to match nouns to labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Neptune to match each noun with Nomenclature and get any crosswalks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesauri_ids = []\n",
    "\n",
    "for index, concept in thesaurus_ids.iterrows():\n",
    "    id_dict = {'noun': concept['noun'], 'freq': concept['freq']}\n",
    "    \n",
    "    test_label = concept['noun'].title() # Nomenclature labels have the first letters of their labels capitalized\n",
    "    print(test_label)\n",
    "    query_string = '''prefix skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "    select distinct ?iri ?otherConcept ?prefLabel\n",
    "    from <http://nomenclature_2022-02-02>\n",
    "    where {\n",
    "    {?iri skos:prefLabel \"''' + test_label + '''\"@en.}\n",
    "    union\n",
    "    {?iri skos:altLabel \"''' + test_label + '''\"@en.}\n",
    "    optional {\n",
    "    ?iri skos:prefLabel ?prefLabel.\n",
    "    filter(lang(?prefLabel)=\"en\")\n",
    "    }\n",
    "    optional {?iri skos:exactMatch ?otherConcept.}\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    #print(query_string)\n",
    "\n",
    "    results = send_sparql_query(query_string, endpoint)\n",
    "    found = False\n",
    "    for result in results:\n",
    "        iri = result['iri']['value']\n",
    "        print(iri)\n",
    "        nomenclature_id = extract_local_name(iri)\n",
    "        id_dict['nomenclature'] = nomenclature_id\n",
    "        id_dict['n_pref'] = result['prefLabel']['value']\n",
    "        if 'otherConcept' in result:\n",
    "            other = result['otherConcept']['value']\n",
    "            if 'http://vocab.getty.edu/aat/' in other:\n",
    "                print(other)\n",
    "                id_dict['getty'] = extract_local_name(other)\n",
    "            elif 'http://www.wikidata.org/entity/' in other:\n",
    "                print(other)\n",
    "                id_dict['wikidata'] = extract_local_name(other)\n",
    "                \n",
    "    if not('nomenclature' in id_dict):\n",
    "        id_dict['nomenclature'] = ''\n",
    "    if not('n_pref' in id_dict):\n",
    "        id_dict['n_pref'] = ''\n",
    "    if not('getty' in id_dict):\n",
    "        id_dict['getty'] = ''\n",
    "    if not('wikidata' in id_dict):\n",
    "        id_dict['wikidata'] = ''\n",
    "    print()\n",
    "    thesauri_ids.append(id_dict)\n",
    "    \n",
    "#print(json.dumps(thesauri_ids, indent = 2))\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Neptune to match any nouns with Getty if they weren't already known from the Nomenclature query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for concept_index in range(len(thesauri_ids)):\n",
    "    # Do the AAT lookup only if it's not already known from the Nomenclature search\n",
    "    # If it is found, try to also get the equivalent Wikidata concept\n",
    "    if thesauri_ids[concept_index]['getty'] == '':\n",
    "        noun = thesauri_ids[concept_index]['noun']\n",
    "        print(noun)\n",
    "        query_string = '''prefix skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "        prefix skosxl: <http://www.w3.org/2008/05/skos-xl#>\n",
    "        select distinct ?iri ?otherConcept \n",
    "        from <http://AATOut_2Terms>\n",
    "        from <http://AATOut_WikidataCoref>\n",
    "        where {\n",
    "        {?iri skosxl:prefLabel ?labelObject.}\n",
    "        union\n",
    "        {?iri skosxl:altLabel ?labelObject.}\n",
    "        ?labelObject skosxl:literalForm \"''' + noun +'''\"@en.\n",
    "        optional {?iri skos:exactMatch ?otherConcept.}\n",
    "        }'''\n",
    "\n",
    "        #print(query_string)\n",
    "\n",
    "        results = send_sparql_query(query_string, endpoint)\n",
    "        #print(json.dumps(results, indent = 2))\n",
    "\n",
    "        found = False\n",
    "        for result in results:\n",
    "            iri = result['iri']['value']\n",
    "            getty_id = extract_local_name(iri)\n",
    "            found = True\n",
    "            print(getty_id)\n",
    "            thesauri_ids[concept_index]['getty'] = getty_id\n",
    "            \n",
    "            if 'otherConcept' in result:\n",
    "                other = result['otherConcept']['value']\n",
    "                if 'http://www.wikidata.org/entity/' in other:\n",
    "                    print(other)\n",
    "                    thesauri_ids[concept_index]['wikidata'] = extract_local_name(other)\n",
    "            \n",
    "        if not found:\n",
    "            print('no match')\n",
    "        print()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query Neptune for Getty preferred labels and the Wikidata Query service to find Wikidata preferred labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for concept_index in range(len(thesauri_ids)):\n",
    "    print(thesauri_ids[concept_index]['noun'])\n",
    "    # Do the AAT label lookup only if we know the Getty ID\n",
    "    if thesauri_ids[concept_index]['getty'] != '':\n",
    "        query_string = '''prefix skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "        prefix skosxl: <http://www.w3.org/2008/05/skos-xl#>\n",
    "        select distinct ?prefLabel \n",
    "        from <http://AATOut_2Terms>\n",
    "        where {\n",
    "\n",
    "        optional {\n",
    "        <http://vocab.getty.edu/aat/''' + thesauri_ids[concept_index]['getty'] +'''> skosxl:prefLabel ?labelObject.\n",
    "        ?labelObject skosxl:literalForm ?prefLabel.\n",
    "        filter(lang(?prefLabel)=\"en\")\n",
    "        }\n",
    "\n",
    "        }'''\n",
    "\n",
    "        #print(query_string)\n",
    "\n",
    "        results = send_sparql_query(query_string, endpoint)\n",
    "        #print(json.dumps(results, indent = 2))\n",
    "\n",
    "        try:\n",
    "            for result in results:\n",
    "                thesauri_ids[concept_index]['g_pref'] = result['prefLabel']['value']\n",
    "        except:\n",
    "            thesauri_ids[concept_index]['g_pref'] = ''\n",
    "    else:\n",
    "        thesauri_ids[concept_index]['g_pref'] = ''\n",
    "        \n",
    "    # Do the Wikidata label lookup only if we know the Q ID\n",
    "    if thesauri_ids[concept_index]['wikidata'] != '':\n",
    "        query_string = '''prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        select distinct ?prefLabel \n",
    "        where {\n",
    "\n",
    "        optional {\n",
    "        <http://www.wikidata.org/entity/''' + thesauri_ids[concept_index]['wikidata'] +'''> rdfs:label ?prefLabel.\n",
    "        filter(lang(?prefLabel)=\"en\")\n",
    "        }\n",
    "\n",
    "        }'''\n",
    "\n",
    "        #print(query_string)\n",
    "\n",
    "        results = send_sparql_query(query_string, wdqs_endpoint)\n",
    "        #print(json.dumps(results, indent = 2))\n",
    "\n",
    "        try:\n",
    "            for result in results:\n",
    "                thesauri_ids[concept_index]['w_pref'] = result['prefLabel']['value']\n",
    "        except:\n",
    "            thesauri_ids[concept_index]['w_pref'] = ''\n",
    "    else:\n",
    "        thesauri_ids[concept_index]['w_pref'] = ''\n",
    "    sleep(0.2)\n",
    "        \n",
    "#print(json.dumps(thesauri_ids, indent = 2))\n",
    "\n",
    "write_dicts_to_csv(thesauri_ids, 'thesauri_ids.csv', ['noun', 'freq', 'nomenclature', 'n_pref', 'wikidata', 'w_pref', 'getty', 'g_pref'])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query Neptune to find superclasses in each classification system\n",
    "\n",
    "## Nomenclature superclasses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'thesauri_ids.csv'\n",
    "thesauri_ids = read_dicts_from_csv(filename)\n",
    "\n",
    "node_list = []\n",
    "edge_list = []\n",
    "\n",
    "node_iris = []\n",
    "\n",
    "for noun in thesauri_ids:\n",
    "    if noun['nomenclature'] != '':\n",
    "        print(noun['noun'])\n",
    "        \n",
    "        query_string = '''prefix skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "        select distinct ?broader_subject ?broader_object\n",
    "        from <http://nomenclature_2022-02-02>\n",
    "        where {\n",
    "        <https://nomenclature.info/nom/''' + noun['nomenclature'] + '''> skos:broader* ?broader_subject.\n",
    "        ?broader_subject skos:broader ?broader_object.\n",
    "        }\n",
    "        '''\n",
    "        #print(query_string)\n",
    "\n",
    "        results = send_sparql_query(query_string, endpoint)\n",
    "        #print(json.dumps(results, indent=2))\n",
    "        \n",
    "        for result in results:\n",
    "            subject_iri = result['broader_subject']['value']\n",
    "            object_iri = result['broader_object']['value']\n",
    "            node_iris += [subject_iri, object_iri]\n",
    "            subject_id = extract_local_name(subject_iri)\n",
    "            object_id = extract_local_name(object_iri)\n",
    "            edge_list.append( (object_id, subject_id, {'classification_system': 'nomenclature'}) )\n",
    "            #print(subject_id, object_id)\n",
    "\n",
    "# Create non-redundant list of node IRIs\n",
    "node_iris = list(set(node_iris))\n",
    "#print(node_iris)\n",
    "\n",
    "# Create a text list of node IRIs for a query\n",
    "value_string = ''\n",
    "for node_iri in node_iris:\n",
    "    value_string += '<' + node_iri + '>\\n'\n",
    "#print(value_string)\n",
    "\n",
    "query_string = '''prefix skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "select distinct ?subject ?prefLabel\n",
    "from <http://nomenclature_2022-02-02>\n",
    "where {\n",
    "values ?subject {\n",
    "''' + value_string + '''\n",
    "}\n",
    "?subject skos:prefLabel ?prefLabel.\n",
    "filter(lang(?prefLabel)=\"en\")\n",
    "}\n",
    "'''\n",
    "#print(query_string)\n",
    "\n",
    "results = send_sparql_query(query_string, endpoint)\n",
    "#print(json.dumps(results, indent=2))\n",
    "\n",
    "for result in results:\n",
    "    subject_iri = result['subject']['value']\n",
    "    label = result['prefLabel']['value']\n",
    "    node_id = extract_local_name(subject_iri)\n",
    "    #print(node_id, label)\n",
    "    node_list.append( (node_id, {'label_string': label}) )\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directed graph\n",
    "G = nx.DiGraph()\n",
    "G.add_nodes_from(node_list)\n",
    "G.add_edges_from(edge_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ugly plot\n",
    "fig = plt.figure(figsize=(40,10))\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "node_labels = nx.get_node_attributes(G,'label_string')\n",
    "nx.draw(G, with_labels=True, labels = node_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gml(G, 'nomenclature.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
