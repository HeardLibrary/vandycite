{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preliminaries\n",
    "\n",
    "To use the Neptune SPARQL endpoint, open the SSH tunnel using\n",
    "\n",
    "```\n",
    "ssh neptune -N\n",
    "```\n",
    "\n",
    "which reads the `~/.ssh/config` file containing:\n",
    "\n",
    "```\n",
    "host neptune-demo\n",
    " ForwardAgent yes\n",
    " User ec2-user # when using Amazon Linux\n",
    " HostName <your-ec2-address>\n",
    " IdentitiesOnly yes\n",
    " IdentityFile ~/.ssh/<your-ec2-key-file>.pem\n",
    " LocalForward 8182 <cluster-endpoint-neptune>:8182\n",
    " ```\n",
    " \n",
    " This allows access to the server through port 8182. \n",
    " \n",
    " NOTE: at least on my computer, the Terminal window does not produce a prompt after executing this command. However, it does seem to work, since I get a response when I issue this command in a different Terminal window:\n",
    " \n",
    "```\n",
    "curl https://triplestore1.cluster-cml0hq81gymg.us-east-1.neptune.amazonaws.com:8182/status\n",
    "```\n",
    "\n",
    "NOTE: The dump of `AATOut_WikidataCoref.nt` from Getty was missing the period at the end of each triple. I had to add it before the file would load into Neptune."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import csv\n",
    "from time import sleep\n",
    "\n",
    "endpoint = 'https://triplestore1.cluster-cml0hq81gymg.us-east-1.neptune.amazonaws.com:8182/sparql'\n",
    "wdqs_endpoint = 'https://query.wikidata.org/sparql'\n",
    "user_agent_header = 'thesaurus_crosswalk/0.1 (https://github.com/HeardLibrary/linked-data/; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "\n",
    "sparql_request_header = {\n",
    "        'Accept' : 'application/json',\n",
    "#        'Content-Type': 'application/sparql-query',\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "\n",
    "# Low level functions\n",
    "\n",
    "def send_sparql_query(query_string, endpoint):\n",
    "    \"\"\"Sends a SPARQL query to an endpoint URL. Argument is the query string, returns a list of results.\"\"\"\n",
    "    # You can delete the two print statements if the queries are short. However, for large/long queries,\n",
    "    # it's good to let the user know what's going on.\n",
    "    #print('querying SPARQL endpoint to acquire item metadata')\n",
    "    #response = requests.post(endpoint, data=query_string.encode('utf-8'), headers=sparql_request_header)\n",
    "    response = requests.post(endpoint, data=dict(query=query_string), headers=sparql_request_header)\n",
    "    #print(response.text) # uncomment to view the raw response, e.g. if you are getting an error\n",
    "    try:\n",
    "        data = response.json()\n",
    "    except:\n",
    "        print(response.text)\n",
    "\n",
    "    # Extract the values from the response JSON\n",
    "    results = data['results']['bindings']\n",
    "    \n",
    "    #print('done retrieving data')\n",
    "    #print(json.dumps(results, indent=2))\n",
    "    \n",
    "    return results\n",
    "\n",
    "def extract_local_name(iri):\n",
    "    \"\"\"Extracts the local name part of an IRI, e.g. a qNumber from a Wikidata IRI. Argument is the IRI, returns the local name string.\"\"\"\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    last_piece = len(pieces)\n",
    "    return pieces[last_piece - 1]\n",
    "\n",
    "# Write list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scripts\n",
    "\n",
    "## Find out what graphs are in the triplestore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_string = '''select distinct ?graph where {\n",
    "graph ?graph {?s ?o ?p.}\n",
    "}'''\n",
    "\n",
    "results = send_sparql_query(query_string, endpoint)\n",
    "for result in results:\n",
    "    graph_iri = result['graph']['value']\n",
    "    print(graph_iri)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query the Nomenclature thesaurus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('noun_list.txt', 'rt', encoding='utf-8') as file_object:\n",
    "    nouns_text = file_object.read()\n",
    "    descriptive_nouns = nouns_text.split('\\n')\n",
    "\n",
    "print(len(descriptive_nouns))\n",
    "print(descriptive_nouns[:10])\n",
    "    \n",
    "#descriptive_nouns = ['plate', 'burner', 'bowl', 'mug', 'jar', 'vase', 'figure', 'ferrule', 'amulet', 'dish', 'coin', 'cup', 'buckle']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thesauri_ids = []\n",
    "\n",
    "for noun in descriptive_nouns:\n",
    "    id_dict = {'noun': noun}\n",
    "    \n",
    "    test_label = noun.title() # Nomenclature labels have the first letters of their labels capitalized\n",
    "    print(test_label)\n",
    "    query_string = '''prefix skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "    select distinct ?iri ?otherConcept ?prefLabel\n",
    "    from <http://nomenclature_2022-02-02>\n",
    "    where {\n",
    "    {?iri skos:prefLabel \"''' + test_label + '''\"@en.}\n",
    "    union\n",
    "    {?iri skos:altLabel \"''' + test_label + '''\"@en.}\n",
    "    optional {\n",
    "    ?iri skos:prefLabel ?prefLabel.\n",
    "    filter(lang(?prefLabel)=\"en\")\n",
    "    }\n",
    "    optional {?iri skos:exactMatch ?otherConcept.}\n",
    "    }\n",
    "    '''\n",
    "\n",
    "    #print(query_string)\n",
    "\n",
    "    results = send_sparql_query(query_string, endpoint)\n",
    "    found = False\n",
    "    for result in results:\n",
    "        iri = result['iri']['value']\n",
    "        print(iri)\n",
    "        nomenclature_id = extract_local_name(iri)\n",
    "        id_dict['nomenclature'] = nomenclature_id\n",
    "        id_dict['n_pref'] = result['prefLabel']['value']\n",
    "        if 'otherConcept' in result:\n",
    "            other = result['otherConcept']['value']\n",
    "            if 'http://vocab.getty.edu/aat/' in other:\n",
    "                print(other)\n",
    "                id_dict['getty'] = extract_local_name(other)\n",
    "            elif 'http://www.wikidata.org/entity/' in other:\n",
    "                print(other)\n",
    "                id_dict['wikidata'] = extract_local_name(other)\n",
    "                \n",
    "    if not('nomenclature' in id_dict):\n",
    "        id_dict['nomenclature'] = ''\n",
    "    if not('n_pref' in id_dict):\n",
    "        id_dict['n_pref'] = ''\n",
    "    if not('getty' in id_dict):\n",
    "        id_dict['getty'] = ''\n",
    "    if not('wikidata' in id_dict):\n",
    "        id_dict['wikidata'] = ''\n",
    "    print()\n",
    "    thesauri_ids.append(id_dict)\n",
    "    \n",
    "#print(json.dumps(thesauri_ids, indent = 2))\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for concept_index in range(len(thesauri_ids)):\n",
    "    # Do the AAT lookup only if it's not already known from the Nomenclature search\n",
    "    # If it is found, try to also get the equivalent Wikidata concept\n",
    "    if thesauri_ids[concept_index]['getty'] == '':\n",
    "        noun = thesauri_ids[concept_index]['noun']\n",
    "        print(noun)\n",
    "        query_string = '''prefix skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "        prefix skosxl: <http://www.w3.org/2008/05/skos-xl#>\n",
    "        select distinct ?iri ?otherConcept \n",
    "        from <http://AATOut_2Terms>\n",
    "        from <http://AATOut_WikidataCoref>\n",
    "        where {\n",
    "        {?iri skosxl:prefLabel ?labelObject.}\n",
    "        union\n",
    "        {?iri skosxl:altLabel ?labelObject.}\n",
    "        ?labelObject skosxl:literalForm \"''' + noun +'''\"@en.\n",
    "        optional {?iri skos:exactMatch ?otherConcept.}\n",
    "        }'''\n",
    "\n",
    "        #print(query_string)\n",
    "\n",
    "        results = send_sparql_query(query_string, endpoint)\n",
    "        #print(json.dumps(results, indent = 2))\n",
    "\n",
    "        found = False\n",
    "        for result in results:\n",
    "            iri = result['iri']['value']\n",
    "            getty_id = extract_local_name(iri)\n",
    "            found = True\n",
    "            print(getty_id)\n",
    "            thesauri_ids[concept_index]['getty'] = getty_id\n",
    "            \n",
    "            if 'otherConcept' in result:\n",
    "                other = result['otherConcept']['value']\n",
    "                if 'http://www.wikidata.org/entity/' in other:\n",
    "                    print(other)\n",
    "                    thesauri_ids[concept_index]['wikidata'] = extract_local_name(other)\n",
    "            \n",
    "        if not found:\n",
    "            print('no match')\n",
    "        print()\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for concept_index in range(len(thesauri_ids)):\n",
    "    print(thesauri_ids[concept_index]['noun'])\n",
    "    # Do the AAT label lookup only if we know the Getty ID\n",
    "    if thesauri_ids[concept_index]['getty'] != '':\n",
    "        query_string = '''prefix skos: <http://www.w3.org/2004/02/skos/core#>\n",
    "        prefix skosxl: <http://www.w3.org/2008/05/skos-xl#>\n",
    "        select distinct ?prefLabel \n",
    "        from <http://AATOut_2Terms>\n",
    "        where {\n",
    "\n",
    "        optional {\n",
    "        <http://vocab.getty.edu/aat/''' + thesauri_ids[concept_index]['getty'] +'''> skosxl:prefLabel ?labelObject.\n",
    "        ?labelObject skosxl:literalForm ?prefLabel.\n",
    "        filter(lang(?prefLabel)=\"en\")\n",
    "        }\n",
    "\n",
    "        }'''\n",
    "\n",
    "        #print(query_string)\n",
    "\n",
    "        results = send_sparql_query(query_string, endpoint)\n",
    "        #print(json.dumps(results, indent = 2))\n",
    "\n",
    "        try:\n",
    "            for result in results:\n",
    "                thesauri_ids[concept_index]['g_pref'] = result['prefLabel']['value']\n",
    "        except:\n",
    "            thesauri_ids[concept_index]['g_pref'] = ''\n",
    "    else:\n",
    "        thesauri_ids[concept_index]['g_pref'] = ''\n",
    "        \n",
    "    # Do the Wikidata label lookup only if we know the Q ID\n",
    "    if thesauri_ids[concept_index]['wikidata'] != '':\n",
    "        query_string = '''prefix rdfs: <http://www.w3.org/2000/01/rdf-schema#>\n",
    "        select distinct ?prefLabel \n",
    "        where {\n",
    "\n",
    "        optional {\n",
    "        <http://www.wikidata.org/entity/''' + thesauri_ids[concept_index]['wikidata'] +'''> rdfs:label ?prefLabel.\n",
    "        filter(lang(?prefLabel)=\"en\")\n",
    "        }\n",
    "\n",
    "        }'''\n",
    "\n",
    "        #print(query_string)\n",
    "\n",
    "        results = send_sparql_query(query_string, wdqs_endpoint)\n",
    "        #print(json.dumps(results, indent = 2))\n",
    "\n",
    "        try:\n",
    "            for result in results:\n",
    "                thesauri_ids[concept_index]['w_pref'] = result['prefLabel']['value']\n",
    "        except:\n",
    "            thesauri_ids[concept_index]['w_pref'] = ''\n",
    "    else:\n",
    "        thesauri_ids[concept_index]['w_pref'] = ''\n",
    "    sleep(0.2)\n",
    "        \n",
    "#print(json.dumps(thesauri_ids, indent = 2))\n",
    "\n",
    "write_dicts_to_csv(thesauri_ids, 'thesauri_ids.csv', ['noun', 'nomenclature', 'n_pref', 'wikidata', 'w_pref', 'getty', 'g_pref'])\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
