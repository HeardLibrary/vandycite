{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ryiludIFd_I"
   },
   "source": [
    "# Code to set up NLTK\n",
    "\n",
    "On a local Jupyter notebook, these are one-time actions that need to be done to install packages and data. On Colab, you probably need to do them every time you use the notebook in a new session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VrkjOSthFd_K",
    "outputId": "8153c441-9a21-48a0-eea3-9aa75acf7ccc"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "!pip install langdetect\n",
    "\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UXNJtsC9Fd_M"
   },
   "source": [
    "# Fine arts gallery chunking tests\n",
    "\n",
    "Based on Chapter 7 of Natural Language Processing with Python\n",
    "https://www.nltk.org/book/ch07.html\n",
    "\n",
    "NOTE: the NLTK setup must be done before running this notebook!\n",
    "\n",
    "## Function section\n",
    "\n",
    "This needs to be run before any of the later cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eVRcbQg9Fd_O",
    "outputId": "c9d5744e-92e8-4843-f783-d689efe8a86b"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk import RegexpParser\n",
    "\n",
    "import csv\n",
    "import json\n",
    "import requests\n",
    "\n",
    "from langdetect import detect_langs\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# Write list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# read a CSV from a URL into a list of dictionaries\n",
    "def url_csv_to_list_of_dicts(url):\n",
    "    response = requests.get(url)\n",
    "    file_text = response.text.splitlines()\n",
    "    file_rows = csv.DictReader(file_text)\n",
    "    list_of_dicts = []\n",
    "    for row in file_rows:\n",
    "        list_of_dicts.append(row)\n",
    "    return list_of_dicts\n",
    "\n",
    "def words_in_phrase(piece):\n",
    "    \"\"\"Tokenizes a phrase, then removes non-word punctuation, etc. and counts words. Input a string, output an integer.\"\"\"\n",
    "    tokens = nltk.word_tokenize(piece)\n",
    "    if '.' in tokens:\n",
    "        tokens.remove('.')\n",
    "    if 's' in tokens:\n",
    "        tokens.remove('s')\n",
    "    if '’' in tokens:\n",
    "        tokens.remove('’')\n",
    "    return len(tokens)\n",
    "\n",
    "def detect_language(string):\n",
    "    \"\"\"Runs Google language detection. Input is a string, output is a tuple of (language code, confidence).\"\"\"\n",
    "    try:\n",
    "        lang_list = detect_langs(string)\n",
    "        lang_string = str(lang_list[0])\n",
    "        confidence = float(lang_string[3:])\n",
    "        lang = lang_string[:2]\n",
    "    except: #exceptions occur when no info to decide, e.g. numbers\n",
    "        lang = 'zxx'\n",
    "        confidence = float(0)\n",
    "    return lang, confidence\n",
    "\n",
    "def treat_as_single_string(pieces):\n",
    "    \"\"\"Determine whether the pair of phrases should be considered the same language. Input a list of tokens, output a boolean.\"\"\"\n",
    "    abort = False\n",
    "    lang_list = []\n",
    "    for piece in pieces:\n",
    "        lang, prec = detect_language(piece.strip())\n",
    "        lang_list.append(lang)\n",
    "\n",
    "        # Can't be considered different languages if any confidence assessment fails to meet the minimum score\n",
    "        if prec < precision_cutoff:\n",
    "            abort = True\n",
    "            print('fail precision cutoff')\n",
    "        # Can't be considered different languages if the phrase doesn't include the minimum number of words.\n",
    "        if words_in_phrase(piece) < phrase_length_cutoff:\n",
    "            abort = True\n",
    "            print('fail word length cutoff')\n",
    "\n",
    "    # If the previous screens on the two phrases pass, then not considered different languages if the\n",
    "    # two phrases are assigned to the same language.\n",
    "    if lang_list[0] == lang_list[1]:\n",
    "        abort = True\n",
    "        print('fail same language test')\n",
    "    print(lang_list)\n",
    "    return abort\n",
    "\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NZ3aISyMFd_P"
   },
   "source": [
    "## Load data\n",
    "\n",
    "Loads gallery metadata (Wikidata Q ID, label, description)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DizHIWCDFd_P",
    "outputId": "69a71999-7b0e-489e-f553-852a205c4779"
   },
   "outputs": [],
   "source": [
    "# Use this code to load a CSV table from a URL\n",
    "#url = 'https://gist.githubusercontent.com/baskaufs/f76c243a4a4ad94d0dd00cdcaca6d8df/raw/3410f020df72cdbdf65d81fed8d0c344c66e7e5b/gallery_works.csv'\n",
    "#works = url_csv_to_list_of_dicts(url)\n",
    "\n",
    "# Use this code to load a CSV table from local file\n",
    "#filename = 'works_multiprop.csv'\n",
    "#works = read_dict(filename)\n",
    "\n",
    "# Read preprocessed JSON data from secret Gist\n",
    "url = 'https://gist.githubusercontent.com/baskaufs/054662389fcd08f107c01b06cd024338/raw/bfe474d67708da64033ae61e95d113cc3cbc2ba6/3d_parts.json'\n",
    "response_object = requests.get(url)\n",
    "file_text = response_object.text\n",
    "works = json.loads(file_text)\n",
    "\n",
    "print(works[0])\n",
    "\n",
    "print('Done!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNdO9smoFd_Q"
   },
   "source": [
    "Select a work and examine the data structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6V1uYjvFd_R"
   },
   "outputs": [],
   "source": [
    "work_number = 1\n",
    "field_name = 'object_description'\n",
    "print(json.dumps(works[work_number], indent=2))\n",
    "print()\n",
    "print(works[work_number][field_name])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCfZ46GFFd_R"
   },
   "source": [
    "## Perform tokenization and tagging\n",
    "\n",
    "See https://www.nltk.org/book/ch03.html about tokenization.\n",
    "\n",
    "See https://www.nltk.org/book/ch05.html about tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HpmvDolxFd_S"
   },
   "outputs": [],
   "source": [
    "work = works[work_number]\n",
    "tokens = nltk.word_tokenize(work[field_name])\n",
    "print('tokens:', tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4f27y8tHFd_T"
   },
   "outputs": [],
   "source": [
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print('tagged:', tagged_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D3z8_BbSWsN5"
   },
   "source": [
    "## Extract nouns from object_description\n",
    "\n",
    "This script puts together bits from the code above and encloses it in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NkbCEk8WejbO",
    "outputId": "08b18ebd-75a1-42d1-a446-f911d15f34c6"
   },
   "outputs": [],
   "source": [
    "# We discovered text segments beginning with these words that did not belong in the main subject description.\n",
    "# So we will remove any text following these words.\n",
    "splitter_words = [' of ', ' with ', ' in ', ' (2 parts)']\n",
    "\n",
    "summary_noun_list = []\n",
    "\n",
    "field_name = 'object_description'\n",
    "for work_number in range(len(works)):\n",
    "    #print(json.dumps(works[work_number], indent=2))\n",
    "    #print()\n",
    "    # This is the object_description phrase previously extracted based on some rules\n",
    "    #print(works[work_number][field_name])\n",
    "    # Convert to lower case, since NLTK seems overly dependent on capitalization to categorize parts of speech\n",
    "    phrase = works[work_number][field_name].lower()\n",
    "\n",
    "    # Check for each of the possible words that begin unwanted text segments\n",
    "    # If the subject description does not contain the split_word, nothing happens.\n",
    "    for split_word in splitter_words:\n",
    "        pieces = phrase.split(split_word) # split the subject description into two pieces if it contains the split_word\n",
    "        #print(pieces)\n",
    "        phrase = pieces[0].strip() # replace the original phrase with the first piece. Strip extra whitespace if any from ends.\n",
    "        #print(phrase)\n",
    "\n",
    "    # Often a parenthetical expression translates a non-English term to an English one. So this code extracts the\n",
    "    # text inside the first set of parentheses and discards other text.\n",
    "    # This is sometimes wrong when the non-English translation is in the parentheses or if it's just an English\n",
    "    # parenthetical expression following an English phrase. These will have to be detected and corrected manually.\n",
    "\n",
    "    # Split the phrase to pull out anything in a first set of parentheses\n",
    "    pieces = phrase.split('(') # Split label into parts before and after left parenthesis\n",
    "    if len(pieces) > 2:\n",
    "        pieces = list(pieces[:2]) # Throw away anything after a second set of parentheses (limit to first parentheses only)    \n",
    "\n",
    "    if len(pieces) == 2: # Don't do anything if no parentheses.\n",
    "        phrase = pieces[1].split(')')[0] # Remove anything after the right parenthesis for the parenthetical phrase\n",
    "\n",
    "    tokens = nltk.word_tokenize(phrase)\n",
    "    #print('tokens:', tokens)\n",
    "    tagged_tokens = nltk.pos_tag(tokens)\n",
    "    #print('tagged:', tagged_tokens)\n",
    "\n",
    "    # Danni observed that if there were consecutive nouns, the last one is nearly always what we want.\n",
    "    # NLTK frequently mis-identifies adjectives as nouns, but since adjectives usually preceed nouns in English,\n",
    "    # using the rule of extracting the last \"noun\" (according to NLTK) nearly always results in the real noun.\n",
    "    # So the strategy here is to step through the tagged_tokens and look for the part of speech (item 1 in the tuple), \n",
    "    # and see if the first two letters contain \"NN\", which is used for all noun tags. If it does, save the actual word \n",
    "    # (the token) in a list of nouns. Finally, select the last noun in the list as the primary noun of the description.\n",
    "    noun_list = []\n",
    "    for tagged_token in tagged_tokens:\n",
    "        #print(tagged_token)\n",
    "        #print(tagged_token[1])\n",
    "        if tagged_token[1][:2] == 'NN': # 1 is for tuple item 1 (the second item) and :2 is the last 2 characters in the string\n",
    "            noun_list.append(tagged_token[0]) # add tuple item 0 (the first item) to the list of nouns.\n",
    "    if len(noun_list) == 0:\n",
    "        result = '* Noun not detected!'\n",
    "    else:\n",
    "        result = noun_list[-1] # list item -1 is the last item.\n",
    "    #print(result)\n",
    "    #print()\n",
    "    works[work_number]['noun'] = result\n",
    "       \n",
    "    if not(result in summary_noun_list) and result != '* Noun not detected!':\n",
    "        summary_noun_list.append(result)\n",
    "\n",
    "summary_noun_list.sort()\n",
    "with open('noun_list.txt', 'wt', encoding='utf-8') as file_object:\n",
    "    file_object.write('\\n'.join(summary_noun_list))\n",
    "\n",
    "fieldnames = ['qid', 'type', 'noun', 'label', 'object_description', 'form_description', 'design_description', 'includes', 'noun']\n",
    "write_dicts_to_csv(works, '3d_parts.csv', fieldnames)\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S6BU1fVpbhJj"
   },
   "source": [
    "# REMAINDER NOT USED\n",
    "\n",
    "The remaining cells weren't used in the project but were kept here for legacy reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jmm5cOkdbrka"
   },
   "source": [
    "## Language detection test\n",
    "\n",
    "This was a good idea, but the phrases are too short for accurate detection. Usually neither of the detected languages was English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nGFi2GUJbwbm"
   },
   "outputs": [],
   "source": [
    "test_string = 'Chashaku (tea scoop)'\n",
    "    \n",
    "precision_cutoff = 0.5\n",
    "phrase_length_cutoff = 1\n",
    "\n",
    "translations_list = [] # Create a list to hold the translations\n",
    "\n",
    "# Split the phrase to pull out anything in a first set of parentheses\n",
    "pieces = test_string.split('(') # Split label into parts before and after left parenthesis\n",
    "if len(pieces) > 2:\n",
    "    pieces = list(pieces[:2]) # Throw away anything after a second set of parentheses (limit to first parentheses only)    \n",
    "\n",
    "# Code to decide whether parenthetical text is a translation\n",
    "if len(pieces) == 2: # Don't analyze if no parentheses\n",
    "    pieces[1] = pieces[1].split(')')[0] # Remove anything after the right parenthesis for the parenthetical phrase\n",
    "    if treat_as_single_string(pieces):\n",
    "        pieces = [test_string] # If it fails the translations screening test, treat it as a single string\n",
    "\n",
    "print(pieces)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2bedGwNFd_U"
   },
   "source": [
    "## Chunking text\n",
    "\n",
    "The commented out `grammar` assignments offer alternative sets of rules for doing the chunking.\n",
    "\n",
    "See https://www.h2kinfosys.com/blog/part-of-speech-tagging-chunking-with-nltk/ for codes used to tag the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKQ5KJ3tFd_U"
   },
   "outputs": [],
   "source": [
    "# grammar= \"\"\"chunk:{<NN.?>*<VBD.?>*<JJ.?>*<CC>?}\"\"\" # test pattern\n",
    "\n",
    "# grammar = \"NP: {<DT>?<JJ>*<NN>}\" # noun phrase detection\n",
    "\n",
    "grammar = \"NP: {<DT>?<JJ.*>*<NN.*>+}\" # modified NP detection\n",
    "\n",
    "#grammar = r\"\"\"\n",
    "#  NP: {<DT|PP\\$>?<JJ>*<NN>.*}   # chunk determiner/possessive, adjectives and noun\n",
    "#      {<NNP>+}                # chunk sequences of proper nouns\n",
    "#\"\"\"\n",
    "  \t\n",
    "# Chinking example:\n",
    "#grammar = r\"\"\"\n",
    "#  NP:\n",
    "#    {<.*>+}          # Chunk everything\n",
    "#    }<VBD|IN>+{      # Chink sequences of VBD and IN\n",
    "#  \"\"\"\n",
    "\n",
    "chunker = RegexpParser(grammar)\n",
    "print(\"Chunker summary:\", chunker)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qOPTytyxFd_W"
   },
   "outputs": [],
   "source": [
    "chunks = chunker.parse(tagged_tokens)\n",
    "print(\"chunks:\",chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6WVwaePFd_W"
   },
   "source": [
    "**Does not work on Colab!**\n",
    "\n",
    "When you run the following cell on a local Jupyter notebook, the diagram pops up in a separate window.  That window must be closed to stop the cell from running in order to be able re-run the chunking cell again.\n",
    "\n",
    "Sometimes the popup is below other windows and you may need to click on the \"python\" icon in the task bar to bring it to the front."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g90GUYNHFd_X"
   },
   "outputs": [],
   "source": [
    "chunks.draw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xj4BddX7Fd_X"
   },
   "source": [
    "# Named entity recognition\n",
    "\n",
    "Seems to be heavily dependent on capitalization, so not that great for titles\n",
    "\n",
    "Code hacked from https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n8ZObM0WFd_Y"
   },
   "outputs": [],
   "source": [
    "# Requires the \"Perform tokenization and tagging\" cell to be run first.\n",
    "# Try with row 6293\n",
    "named_entity_chunks = nltk.ne_chunk(tagged_tokens)\n",
    "print('NE chunks:', named_entity_chunks)\n",
    "print()\n",
    "\n",
    "ne_list = []\n",
    "for chunk in named_entity_chunks:\n",
    "    if hasattr(chunk, 'label'):\n",
    "        ne_dict = {'ne_label': chunk.label()}\n",
    "        # A chunk is some kind of iterable of tuples\n",
    "        # Each tuple contains (word, noun_descriptor)\n",
    "        ne_string = chunk[0][0] # 0th tuple, word\n",
    "        # Iterate through the rest of the tuples in the chunk\n",
    "        for additional_tuple in chunk[1:len(chunk)]:\n",
    "            ne_string += ' ' + additional_tuple[0]\n",
    "        ne_dict['ne_string'] = ne_string\n",
    "        ne_list.append(ne_dict)\n",
    "\n",
    "        # Print results for humans to see\n",
    "        print(chunk.label(), ' '.join(c[0] for c in chunk))\n",
    "\n",
    "# List of dictionaries format for subsequent use or output as a CSV\n",
    "print()\n",
    "print('NE list:', ne_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "02aIwyQvH1Vl"
   },
   "source": [
    "**Does not work on Colab!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7T2EnWpFd_Y"
   },
   "outputs": [],
   "source": [
    "# Run this cell if running locally and you want a diagram of the NER chunks.\n",
    "# It will open in a separate window that must be closed before any cell can be run again.\n",
    "# Sometimes it opens under other windows and you must click on its icon in the dock to make\n",
    "# it come to the frong.\n",
    "named_entity_chunks.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tKCna6oqFd_Y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "nltk_on_3d.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
