{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2021-08-17 I used this notebook to work out how to convert the Commons raw filenames required by the API and the URL-encoded URLs that are returned from the Query Service. The two functions several cells below resulted and were used in the VanderBot 1.8 update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "from time import sleep\n",
    "import json\n",
    "import csv\n",
    "import os\n",
    "import sys # Read CLI arguments\n",
    "import urllib.parse\n",
    "\n",
    "# ----------------\n",
    "# Configuration settings\n",
    "# ----------------\n",
    "\n",
    "if len(sys.argv) == 2: # if exactly one argument passed (i.e. the configuration file path)\n",
    "    file_path = sys.argv[1] # sys.argv[0] is the script name\n",
    "else:\n",
    "    file_path = 'act.csv'\n",
    "\n",
    "sparql_sleep = 0.1 # number of seconds to wait between queries to SPARQL endpoint\n",
    "home = str(Path.home()) # gets path to home directory; supposed to work for both Win and Mac\n",
    "endpoint = 'https://query.wikidata.org/sparql'\n",
    "accept_media_type = 'application/json'\n",
    "\n",
    "# ----------------\n",
    "# Utility functions\n",
    "# ----------------\n",
    "\n",
    "# Best to send a user-agent header because some Wikimedia servers don't like unidentified clients\n",
    "def generate_header_dictionary(accept_media_type):\n",
    "    user_agent_header = 'VanderBot/1.7.1 (https://github.com/HeardLibrary/linked-data/tree/master/vanderbot; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : accept_media_type,\n",
    "        'Content-Type': 'application/sparql-query',\n",
    "        'User-Agent': user_agent_header\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "requestheader = generate_header_dictionary(accept_media_type)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def read_dict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# extracts the UUID and qId from a statement IRI\n",
    "def extract_statement_uuid(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/statement/Q7552806-8B88E0CA-BCC8-49D5-9AC2-F1755464F1A2\n",
    "    pieces = iri.split('/')\n",
    "    statement_id = pieces[5]\n",
    "    pieces = statement_id.split('-')\n",
    "    return pieces[1] + '-' + pieces[2] + '-' + pieces[3] + '-' + pieces[4] + '-' + pieces[5], pieces[0]\n",
    "\n",
    "# function to use in sort\n",
    "def sort_funct(row):\n",
    "    return row['filename']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "select distinct ?qid ?iri\n",
      "where {\n",
      "#?qid wdt:P9092 ?id.\n",
      "#?qid wdt:P18 ?iri.\n",
      "wd:Q13406268 wdt:P18 ?iri.\n",
      "}\n",
      "limit 10\n"
     ]
    }
   ],
   "source": [
    "query = '''\n",
    "select distinct ?qid ?iri\n",
    "where {\n",
    "#?qid wdt:P9092 ?id.\n",
    "#?qid wdt:P18 ?iri.\n",
    "wd:Q13406268 wdt:P18 ?iri.\n",
    "}\n",
    "limit 10'''\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tested uploading a file with spaces and it worked. The value returned from SPARQL had escaped spaces.\n",
    "\n",
    "Tested uploading a file with underscores replacing the spaces (as in displayed commons URL) and it worked. The value returned from SPARQL had underscores. In the web interface there was a warning saying that the commons link should be well-formed. When clicked on, the text had underscores. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "querying SPARQL endpoint to acquire item metadata\n",
      "done retrieving data\n",
      "[\n",
      "  {\n",
      "    \"iri\": {\n",
      "      \"type\": \"uri\",\n",
      "      \"value\": \"http://commons.wikimedia.org/wiki/Special:FilePath/Castle%20De%20Haar%20%281892-1913%29%20-%20360%C2%B0%20Panorama%20of%20Castle%20%26%20Castle%20Grounds.jpg\"\n",
      "    }\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# ----------------\n",
    "# send request to Wikidata Query Service\n",
    "# ----------------\n",
    "\n",
    "print('querying SPARQL endpoint to acquire item metadata')\n",
    "response = requests.post(endpoint, data=query.encode('utf-8'), headers=requestheader)\n",
    "#print(response.text)\n",
    "data = response.json()\n",
    "\n",
    "# extract the values from the response JSON\n",
    "results = data['results']['bindings']\n",
    "\n",
    "print('done retrieving data')\n",
    "print(json.dumps(results, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Castle De Haar (1892-1913) - 360° Panorama of Castle & Castle Grounds.jpg'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value = results[0]['iri']['value']\n",
    "string = value.split('FilePath/')[1]\n",
    "urllib.parse.unquote(string)\n",
    "#urllib.parse.quote(string) # URL encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "commons_prefix = 'http://commons.wikimedia.org/wiki/Special:FilePath/'\n",
    "def commons_url_to_filename(url):\n",
    "    # form of URL is: http://commons.wikimedia.org/wiki/Special:FilePath/Castle%20De%20Haar%20%281892-1913%29%20-%20360%C2%B0%20Panorama%20of%20Castle%20%26%20Castle%20Grounds.jpg\n",
    "    string = url.split(commons_prefix)[1] # get local name file part of URL\n",
    "    filename = urllib.parse.unquote(string) # reverse URL-encode the string\n",
    "    return filename\n",
    "\n",
    "def filename_to_commons_url(filename):\n",
    "    encoded_filename = urllib.parse.quote(filename)\n",
    "    url = commons_prefix + encoded_filename\n",
    "    return url\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Castle De Haar (1892-1913) - 360° Panorama of Castle & Castle Grounds.jpg\n",
      "http://commons.wikimedia.org/wiki/Special:FilePath/Castle%20De%20Haar%20%281892-1913%29%20-%20360%C2%B0%20Panorama%20of%20Castle%20%26%20Castle%20Grounds.jpg\n"
     ]
    }
   ],
   "source": [
    "url = \"http://commons.wikimedia.org/wiki/Special:FilePath/Castle%20De%20Haar%20%281892-1913%29%20-%20360%C2%B0%20Panorama%20of%20Castle%20%26%20Castle%20Grounds.jpg\"\n",
    "filename = commons_url_to_filename(url)\n",
    "print(filename)\n",
    "\n",
    "new_url = filename_to_commons_url(filename)\n",
    "print(new_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_dict(file_path)\n",
    "input_list = []\n",
    "iri_values = ''  # VALUES list for query\n",
    "for record in data:\n",
    "    record_dict = {'act_id': record['RecordNumber']}\n",
    "    # some records have spaces with other junk after them\n",
    "    strings = record['filename'].split(' ')\n",
    "    # use only the first string in the list (item 0)\n",
    "    record_dict['filename'] = strings[0]\n",
    "    # to generate the IRIs, the underscores need to be replaced with escaped spaces (%20)\n",
    "    filename = strings[0].replace('_','%20')\n",
    "    url = 'http://commons.wikimedia.org/wiki/Special:FilePath/' + filename\n",
    "    record_dict['url'] = url\n",
    "    input_list.append(record_dict)\n",
    "    iri_values += '<' + url + '>\\n'\n",
    "\n",
    "# remove trailing newline\n",
    "iri_values = iri_values[:len(iri_values)-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(output_list, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iri_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------\n",
    "# extract Q IDs from the results and match them with the ACT IDs\n",
    "# ----------------\n",
    "\n",
    "output_list = []\n",
    "for record in input_list:\n",
    "    found = False\n",
    "    for result in results:\n",
    "        if record['url'] == result['iri']['value']:\n",
    "            found = True\n",
    "            qid = extract_qnumber(result['qid']['value'])\n",
    "            record['qid'] = qid\n",
    "            break\n",
    "    if not found:\n",
    "        record['qid'] = ''\n",
    "    output_list.append(record)\n",
    "print(json.dumps(output_list, indent = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_list.sort(key = sort_funct) # sort by the filename field\n",
    "fieldnames = ['act_id', 'qid', 'filename', 'url']\n",
    "write_dicts_to_csv(output_list, 'output.csv', fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
